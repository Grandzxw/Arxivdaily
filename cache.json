{"2023-03-21T00:00:00Z":{"Computation and Language":[{"id":"http://arxiv.org/abs/2303.12060v1","updated":"2023-03-21T17:51:23Z","published":"2023-03-21T17:51:23Z","title":"VideoXum: Cross-modal Visual and Textural Summarization of Videos","summary":"  Video summarization aims to distill the most important information from a\nsource video to produce either an abridged clip or a textual narrative.\nTraditionally, different methods have been proposed depending on whether the\noutput is a video or text, thus ignoring the correlation between the two\nsemantically related tasks of visual summarization and textual summarization.\nWe propose a new joint video and text summarization task. The goal is to\ngenerate both a shortened video clip along with the corresponding textual\nsummary from a long video, collectively referred to as a cross-modal summary.\nThe generated shortened video clip and text narratives should be semantically\nwell aligned. To this end, we first build a large-scale human-annotated dataset\n-- VideoXum (X refers to different modalities). The dataset is reannotated\nbased on ActivityNet. After we filter out the videos that do not meet the\nlength requirements, 14,001 long videos remain in our new dataset. Each video\nin our reannotated dataset has human-annotated video summaries and the\ncorresponding narrative summaries. We then design a novel end-to-end model --\nVTSUM-BILP to address the challenges of our proposed task. Moreover, we propose\na new metric called VT-CLIPScore to help evaluate the semantic consistency of\ncross-modality summary. The proposed model achieves promising performance on\nthis new task and establishes a benchmark for future research.\n","authors":["Jingyang Lin","Hang Hua","Ming Chen","Yikang Li","Jenhao Hsiao","Chiuman Ho","Jiebo Luo"],"pdf_url":"https://arxiv.org/pdf/2303.12060v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.12057v1","updated":"2023-03-21T17:48:00Z","published":"2023-03-21T17:48:00Z","title":"Large Language Models Can Be Used to Estimate the Ideologies of\n  Politicians in a Zero-Shot Learning Setting","summary":"  The mass aggregation of knowledge embedded in large language models (LLMs)\nholds the promise of new solutions to problems of observability and measurement\nin the social sciences. We examine the utility of one such model for a\nparticularly difficult measurement task: measuring the latent ideology of\nlawmakers, which allows us to better understand functions that are core to\ndemocracy, such as how politics shape policy and how political actors represent\ntheir constituents. We scale the senators of the 116th United States Congress\nalong the liberal-conservative spectrum by prompting ChatGPT to select the more\nliberal (or conservative) senator in pairwise comparisons. We show that the LLM\nproduced stable answers across repeated iterations, did not hallucinate, and\nwas not simply regurgitating information from a single source. This new scale\nstrongly correlates with pre-existing liberal-conservative scales such as\nNOMINATE, but also differs in several important ways, such as correctly placing\nsenators who vote against their party for far-left or far-right ideological\nreasons on the extreme ends. The scale also highly correlates with ideological\nmeasures based on campaign giving and political activists' perceptions of these\nsenators. In addition to the potential for better-automated data collection and\ninformation retrieval, our results suggest LLMs are likely to open new avenues\nfor measuring latent constructs like ideology that rely on aggregating large\nquantities of data from public sources.\n","authors":["Patrick Y. Wu","Joshua A. Tucker","Jonathan Nagler","Solomon Messing"],"pdf_url":"https://arxiv.org/pdf/2303.12057v1.pdf","comment":"18 pages, 4 figures"},{"id":"http://arxiv.org/abs/2303.12029v1","updated":"2023-03-21T17:14:04Z","published":"2023-03-21T17:14:04Z","title":"Wearing Masks Implies Refuting Trump?: Towards Target-specific User\n  Stance Prediction across Events in COVID-19 and US Election 2020","summary":"  People who share similar opinions towards controversial topics could form an\necho chamber and may share similar political views toward other topics as well.\nThe existence of such connections, which we call connected behavior, gives\nresearchers a unique opportunity to predict how one would behave for a future\nevent given their past behaviors. In this work, we propose a framework to\nconduct connected behavior analysis. Neural stance detection models are trained\non Twitter data collected on three seemingly independent topics, i.e., wearing\na mask, racial equality, and Trump, to detect people's stance, which we\nconsider as their online behavior in each topic-related event. Our results\nreveal a strong connection between the stances toward the three topical events\nand demonstrate the power of past behaviors in predicting one's future\nbehavior.\n","authors":["Hong Zhang","Haewoon Kwak","Wei Gao","Jisun An"],"pdf_url":"https://arxiv.org/pdf/2303.12029v1.pdf","comment":"10 pages, 2 pages, WebSci 2023, April 30-May 1, 2023, Evanston, TX,\n  USA"},{"id":"http://arxiv.org/abs/2303.12024v1","updated":"2023-03-21T17:04:44Z","published":"2023-03-21T17:04:44Z","title":"cTBL: Augmenting Large Language Models for Conversational Tables","summary":"  An open challenge in multimodal conversational AI requires augmenting large\nlanguage models with information from textual and non-textual sources for\nmulti-turn dialogue. To address this problem, this paper introduces\nConversational Tables (cTBL), a three-step encoder-decoder approach to retrieve\ntabular information and generate dialogue responses grounded on the retrieved\ninformation. cTBL uses Transformer encoder embeddings for Dense Table Retrieval\nand obtains up to 5% relative improvement in Top-1 and Top-3 accuracy over\nsparse retrieval on the HyrbiDialogue dataset. Additionally, cTBL performs\ntabular knowledge retrieval using both encoder and decoder models, resulting in\nup to 46% relative improvement in ROUGE scores and better human evaluation for\nresponse generation on HyrbiDialogue.\n","authors":["Anirudh S Sundar","Larry Heck"],"pdf_url":"https://arxiv.org/pdf/2303.12024v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.12023v1","updated":"2023-03-21T16:56:05Z","published":"2023-03-21T16:56:05Z","title":"Logical Reasoning over Natural Language as Knowledge Representation: A\n  Survey","summary":"  Logical reasoning is central to human cognition and intelligence. Past\nresearch of logical reasoning within AI uses formal language as knowledge\nrepresentation~(and symbolic reasoners). However, reasoning with formal\nlanguage has proved challenging~(e.g., brittleness and knowledge-acquisition\nbottleneck). This paper provides a comprehensive overview on a new paradigm of\nlogical reasoning, which uses natural language as knowledge representation~(and\npretrained language models as reasoners), including philosophical definition\nand categorization of logical reasoning, advantages of the new paradigm,\nbenchmarks and methods, challenges of the new paradigm, desirable tasks &\nmethods in the future, and relation to related NLP fields. This new paradigm is\npromising since it not only alleviates many challenges of formal representation\nbut also has advantages over end-to-end neural methods.\n","authors":["Zonglin Yang","Xinya Du","Rui Mao","Jinjie Ni","Erik Cambria"],"pdf_url":"https://arxiv.org/pdf/2303.12023v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.01593v2","updated":"2023-03-21T14:22:00Z","published":"2023-03-02T21:35:15Z","title":"QAID: Question Answering Inspired Few-shot Intent Detection","summary":"  Intent detection with semantically similar fine-grained intents is a\nchallenging task. To address it, we reformulate intent detection as a\nquestion-answering retrieval task by treating utterances and intent names as\nquestions and answers. To that end, we utilize a question-answering retrieval\narchitecture and adopt a two stages training schema with batch contrastive\nloss. In the pre-training stage, we improve query representations through\nself-supervised training. Then, in the fine-tuning stage, we increase\ncontextualized token-level similarity scores between queries and answers from\nthe same intent. Our results on three few-shot intent detection benchmarks\nachieve state-of-the-art performance.\n","authors":["Asaf Yehudai","Matan Vetzler","Yosi Mass","Koren Lazar","Doron Cohen","Boaz Carmeli"],"pdf_url":"https://arxiv.org/pdf/2303.01593v2.pdf","comment":"ICLR paper"},{"id":"http://arxiv.org/abs/2204.04916v3","updated":"2023-03-21T12:58:01Z","published":"2022-04-11T07:33:26Z","title":"A Token-level Contrastive Framework for Sign Language Translation","summary":"  Sign Language Translation (SLT) is a promising technology to bridge the\ncommunication gap between the deaf and the hearing people. Recently,\nresearchers have adopted Neural Machine Translation (NMT) methods, which\nusually require large-scale corpus for training, to achieve SLT. However, the\npublicly available SLT corpus is very limited, which causes the collapse of the\ntoken representations and the inaccuracy of the generated tokens. To alleviate\nthis issue, we propose ConSLT, a novel token-level \\textbf{Con}trastive\nlearning framework for \\textbf{S}ign \\textbf{L}anguage \\textbf{T}ranslation ,\nwhich learns effective token representations by incorporating token-level\ncontrastive learning into the SLT decoding process. Concretely, ConSLT treats\neach token and its counterpart generated by different dropout masks as positive\npairs during decoding, and then randomly samples $K$ tokens in the vocabulary\nthat are not in the current sentence to construct negative examples. We conduct\ncomprehensive experiments on two benchmarks (PHOENIX14T and CSL-Daily) for both\nend-to-end and cascaded settings. The experimental results demonstrate that\nConSLT can achieve better translation quality than the strong baselines.\n","authors":["Biao Fu","Peigen Ye","Liang Zhang","Pei Yu","Cong Hu","Yidong Chen","Xiaodong Shi"],"pdf_url":"https://arxiv.org/pdf/2204.04916v3.pdf","comment":"Accepted to ICASSP 2023"},{"id":"http://arxiv.org/abs/2303.11812v1","updated":"2023-03-21T12:55:54Z","published":"2023-03-21T12:55:54Z","title":"Chinese Intermediate English Learners outdid ChatGPT in deep cohesion:\n  Evidence from English narrative writing","summary":"  ChatGPT is a publicly available chatbot that can quickly generate texts on\ngiven topics, but it is unknown whether the chatbot is really superior to human\nwriters in all aspects of writing and whether its writing quality can be\nprominently improved on the basis of updating commands. Consequently, this\nstudy compared the writing performance on a narrative topic by ChatGPT and\nChinese intermediate English (CIE) learners so as to reveal the chatbot's\nadvantage and disadvantage in writing. The data were analyzed in terms of five\ndiscourse components using Coh-Metrix (a special instrument for analyzing\nlanguage discourses), and the results revealed that ChatGPT performed better\nthan human writers in narrativity, word concreteness, and referential cohesion,\nbut worse in syntactic simplicity and deep cohesion in its initial version.\nAfter more revision commands were updated, while the resulting version was\nfacilitated in syntactic simplicity, yet it is still lagged far behind CIE\nlearners' writing in deep cohesion. In addition, the correlation analysis of\nthe discourse components suggests that narrativity was correlated with\nreferential cohesion in both ChatGPT and human writers, but the correlations\nvaried within each group.\n","authors":["Tongquan Zhou","Siyi Cao","Siruo Zhou","Yao Zhang","Aijing He"],"pdf_url":"https://arxiv.org/pdf/2303.11812v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2211.09699v2","updated":"2023-03-21T12:10:18Z","published":"2022-11-15T19:07:53Z","title":"PromptCap: Prompt-Guided Image Captioning for VQA with GPT-3","summary":"  Knowledge-based visual question answering (VQA) involves questions that\nrequire world knowledge beyond the image to yield the correct answer. Large\nlanguage models (LMs) like GPT-3 are particularly helpful for this task because\nof their strong knowledge retrieval and reasoning capabilities. To enable LM to\nunderstand images, prior work uses a captioning model to convert images into\ntext. However, when summarizing an image in a single caption sentence, which\nvisual entities to describe are often underspecified. Generic image captions\noften miss visual details essential for the LM to answer visual questions\ncorrectly. To address this challenge, we propose PromptCap (Prompt-guided image\nCaptioning), a captioning model designed to serve as a better connector between\nimages and black-box LMs. Different from generic captions, PromptCap takes a\nnatural-language prompt to control the visual entities to describe in the\ngenerated caption. The prompt contains a question that the caption should aid\nin answering. To avoid extra annotation, PromptCap is trained by examples\nsynthesized with GPT-3 and existing datasets. We demonstrate PromptCap's\neffectiveness on an existing pipeline in which GPT-3 is prompted with image\ncaptions to carry out VQA. PromptCap outperforms generic captions by a large\nmargin and achieves state-of-the-art accuracy on knowledge-based VQA tasks\n(60.4% on OK-VQA and 59.6% on A-OKVQA). Zero-shot results on WebQA show that\nPromptCap generalizes well to unseen domains.\n","authors":["Yushi Hu","Hang Hua","Zhengyuan Yang","Weijia Shi","Noah A. Smith","Jiebo Luo"],"pdf_url":"https://arxiv.org/pdf/2211.09699v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.11750v1","updated":"2023-03-21T11:17:37Z","published":"2023-03-21T11:17:37Z","title":"LEAPT: Learning Adaptive Prefix-to-prefix Translation For Simultaneous\n  Machine Translation","summary":"  Simultaneous machine translation, which aims at a real-time translation, is\nuseful in many live scenarios but very challenging due to the trade-off between\naccuracy and latency. To achieve the balance for both, the model needs to wait\nfor appropriate streaming text (READ policy) and then generates its translation\n(WRITE policy). However, WRITE policies of previous work either are specific to\nthe method itself due to the end-to-end training or suffer from the input\nmismatch between training and decoding for the non-end-to-end training.\nTherefore, it is essential to learn a generic and better WRITE policy for\nsimultaneous machine translation. Inspired by strategies utilized by human\ninterpreters and \"wait\" policies, we propose a novel adaptive prefix-to-prefix\ntraining policy called LEAPT, which allows our machine translation model to\nlearn how to translate source sentence prefixes and make use of the future\ncontext. Experiments show that our proposed methods greatly outperform\ncompetitive baselines and achieve promising results.\n","authors":["Lei Lin","Shuangtao Li","Xiaodong Shi"],"pdf_url":"https://arxiv.org/pdf/2303.11750v1.pdf","comment":"Accepted by ICASSP 2023"},{"id":"http://arxiv.org/abs/2302.14115v2","updated":"2023-03-21T11:01:09Z","published":"2023-02-27T19:53:49Z","title":"Vid2Seq: Large-Scale Pretraining of a Visual Language Model for Dense\n  Video Captioning","summary":"  In this work, we introduce Vid2Seq, a multi-modal single-stage dense event\ncaptioning model pretrained on narrated videos which are readily-available at\nscale. The Vid2Seq architecture augments a language model with special time\ntokens, allowing it to seamlessly predict event boundaries and textual\ndescriptions in the same output sequence. Such a unified model requires\nlarge-scale training data, which is not available in current annotated\ndatasets. We show that it is possible to leverage unlabeled narrated videos for\ndense video captioning, by reformulating sentence boundaries of transcribed\nspeech as pseudo event boundaries, and using the transcribed speech sentences\nas pseudo event captions. The resulting Vid2Seq model pretrained on the\nYT-Temporal-1B dataset improves the state of the art on a variety of dense\nvideo captioning benchmarks including YouCook2, ViTT and ActivityNet Captions.\nVid2Seq also generalizes well to the tasks of video paragraph captioning and\nvideo clip captioning, and to few-shot settings. Our code is publicly available\nat https://antoyang.github.io/vid2seq.html.\n","authors":["Antoine Yang","Arsha Nagrani","Paul Hongsuck Seo","Antoine Miech","Jordi Pont-Tuset","Ivan Laptev","Josef Sivic","Cordelia Schmid"],"pdf_url":"https://arxiv.org/pdf/2302.14115v2.pdf","comment":"CVPR 2023 Camera-Ready; Project Webpage:\n  https://antoyang.github.io/vid2seq.html ; 18 pages; 6 figures"},{"id":"http://arxiv.org/abs/2303.11708v1","updated":"2023-03-21T10:01:49Z","published":"2023-03-21T10:01:49Z","title":"The Open-domain Paradox for Chatbots: Common Ground as the Basis for\n  Human-like Dialogue","summary":"  There is a surge in interest in the development of open-domain chatbots,\ndriven by the recent advancements of large language models. The \"openness\" of\nthe dialogue is expected to be maximized by providing minimal information to\nthe users about the common ground they can expect, including the presumed joint\nactivity. However, evidence suggests that the effect is the opposite. Asking\nusers to \"just chat about anything\" results in a very narrow form of dialogue,\nwhich we refer to as the \"open-domain paradox\". In this paper, we explain this\nparadox through the theory of common ground as the basis for human-like\ncommunication. Furthermore, we question the assumptions behind open-domain\nchatbots and identify paths forward for enabling common ground in\nhuman-computer dialogue.\n","authors":["Gabriel Skantze","A. Seza Doğruöz"],"pdf_url":"https://arxiv.org/pdf/2303.11708v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.11141v2","updated":"2023-03-21T09:03:14Z","published":"2023-03-20T14:19:58Z","title":"DocRED-FE: A Document-Level Fine-Grained Entity And Relation Extraction\n  Dataset","summary":"  Joint entity and relation extraction (JERE) is one of the most important\ntasks in information extraction. However, most existing works focus on\nsentence-level coarse-grained JERE, which have limitations in real-world\nscenarios. In this paper, we construct a large-scale document-level\nfine-grained JERE dataset DocRED-FE, which improves DocRED with Fine-Grained\nEntity Type. Specifically, we redesign a hierarchical entity type schema\nincluding 11 coarse-grained types and 119 fine-grained types, and then\nre-annotate DocRED manually according to this schema. Through comprehensive\nexperiments we find that: (1) DocRED-FE is challenging to existing JERE models;\n(2) Our fine-grained entity types promote relation classification. We make\nDocRED-FE with instruction and the code for our baselines publicly available at\nhttps://github.com/PKU-TANGENT/DOCRED-FE.\n","authors":["Hongbo Wang","Weimin Xiong","Yifan Song","Dawei Zhu","Yu Xia","Sujian Li"],"pdf_url":"https://arxiv.org/pdf/2303.11141v2.pdf","comment":"Accepted by IEEE ICASSP 2023. The first two authors contribute\n  equally"},{"id":"http://arxiv.org/abs/2206.06924v5","updated":"2023-03-21T08:47:13Z","published":"2022-06-14T15:43:44Z","title":"The Maximum Linear Arrangement Problem for trees under projectivity and\n  planarity","summary":"  A linear arrangement is a mapping $\\pi$ from the $n$ vertices of a graph $G$\nto $n$ distinct consecutive integers. Linear arrangements can be represented by\ndrawing the vertices along a horizontal line and drawing the edges as\nsemicircles above said line. In this setting, the length of an edge is defined\nas the absolute value of the difference between the positions of its two\nvertices in the arrangement, and the cost of an arrangement as the sum of all\nedge lengths. Here we study two variants of the Maximum Linear Arrangement\nproblem (MaxLA), which consists of finding an arrangement that maximizes the\ncost. In the planar variant for free trees, vertices have to be arranged in\nsuch a way that there are no edge crossings. In the projective variant for\nrooted trees, arrangements have to be planar and the root of the tree cannot be\ncovered by any edge. In this paper we present algorithms that are linear in\ntime and space to solve planar and projective MaxLA for trees. We also prove\nseveral properties of maximum projective and planar arrangements, and show that\ncaterpillar trees maximize planar MaxLA over all trees of a fixed size thereby\ngeneralizing a previous extremal result on trees.\n","authors":["Lluís Alemany-Puig","Juan Luis Esteban","Ramon Ferrer-i-Cancho"],"pdf_url":"https://arxiv.org/pdf/2206.06924v5.pdf","comment":"The fourth version is incorrect. We are sure the right files were\n  uploaded but for whatever reason, it looks like we uploaded the wrong files.\n  The abstract in the fourth version is correct though"},{"id":"http://arxiv.org/abs/2303.11660v1","updated":"2023-03-21T08:08:04Z","published":"2023-03-21T08:08:04Z","title":"Simple Yet Effective Synthetic Dataset Construction for Unsupervised\n  Opinion Summarization","summary":"  Opinion summarization provides an important solution for summarizing opinions\nexpressed among a large number of reviews. However, generating aspect-specific\nand general summaries is challenging due to the lack of annotated data. In this\nwork, we propose two simple yet effective unsupervised approaches to generate\nboth aspect-specific and general opinion summaries by training on synthetic\ndatasets constructed with aspect-related review contents. Our first approach,\nSeed Words Based Leave-One-Out (SW-LOO), identifies aspect-related portions of\nreviews simply by exact-matching aspect seed words and outperforms existing\nmethods by 3.4 ROUGE-L points on SPACE and 0.5 ROUGE-1 point on OPOSUM+ for\naspect-specific opinion summarization. Our second approach, Natural Language\nInference Based Leave-One-Out (NLI-LOO) identifies aspect-related sentences\nutilizing an NLI model in a more general setting without using seed words and\noutperforms existing approaches by 1.2 ROUGE-L points on SPACE for\naspect-specific opinion summarization and remains competitive on other metrics.\n","authors":["Ming Shen","Jie Ma","Shuai Wang","Yogarshi Vyas","Kalpit Dixit","Miguel Ballesteros","Yassine Benajiba"],"pdf_url":"https://arxiv.org/pdf/2303.11660v1.pdf","comment":"EACL 2023 Findings"},{"id":"http://arxiv.org/abs/2303.11648v1","updated":"2023-03-21T07:46:57Z","published":"2023-03-21T07:46:57Z","title":"Improving Content Retrievability in Search with Controllable Query\n  Generation","summary":"  An important goal of online platforms is to enable content discovery, i.e.\nallow users to find a catalog entity they were not familiar with. A\npre-requisite to discover an entity, e.g. a book, with a search engine is that\nthe entity is retrievable, i.e. there are queries for which the system will\nsurface such entity in the top results. However, machine-learned search engines\nhave a high retrievability bias, where the majority of the queries return the\nsame entities. This happens partly due to the predominance of narrow intent\nqueries, where users create queries using the title of an already known entity,\ne.g. in book search 'harry potter'. The amount of broad queries where users\nwant to discover new entities, e.g. in music search 'chill lyrical electronica\nwith an atmospheric feeling to it', and have a higher tolerance to what they\nmight find, is small in comparison. We focus here on two factors that have a\nnegative impact on the retrievability of the entities (I) the training data\nused for dense retrieval models and (II) the distribution of narrow and broad\nintent queries issued in the system. We propose CtrlQGen, a method that\ngenerates queries for a chosen underlying intent-narrow or broad. We can use\nCtrlQGen to improve factor (I) by generating training data for dense retrieval\nmodels comprised of diverse synthetic queries. CtrlQGen can also be used to\ndeal with factor (II) by suggesting queries with broader intents to users. Our\nresults on datasets from the domains of music, podcasts, and books reveal that\nwe can significantly decrease the retrievability bias of a dense retrieval\nmodel when using CtrlQGen. First, by using the generated queries as training\ndata for dense models we make 9% of the entities retrievable (go from zero to\nnon-zero retrievability). Second, by suggesting broader queries to users, we\ncan make 12% of the entities retrievable in the best case.\n","authors":["Gustavo Penha","Enrico Palumbo","Maryam Aziz","Alice Wang","Hugues Bouchard"],"pdf_url":"https://arxiv.org/pdf/2303.11648v1.pdf","comment":"Accepted for publication in the International World Wide Web\n  Conference 2023"},{"id":"http://arxiv.org/abs/2203.03235v2","updated":"2023-03-21T07:43:30Z","published":"2022-03-07T09:47:53Z","title":"Pre-trained Token-replaced Detection Model as Few-shot Learner","summary":"  Pre-trained masked language models have demonstrated remarkable ability as\nfew-shot learners. In this paper, as an alternative, we propose a novel\napproach to few-shot learning with pre-trained token-replaced detection models\nlike ELECTRA. In this approach, we reformulate a classification or a regression\ntask as a token-replaced detection problem. Specifically, we first define a\ntemplate and label description words for each task and put them into the input\nto form a natural language prompt. Then, we employ the pre-trained\ntoken-replaced detection model to predict which label description word is the\nmost original (i.e., least replaced) among all label description words in the\nprompt. A systematic evaluation on 16 datasets demonstrates that our approach\noutperforms few-shot learners with pre-trained masked language models in both\none-sentence and two-sentence learning tasks.\n","authors":["Zicheng Li","Shoushan Li","Guodong Zhou"],"pdf_url":"https://arxiv.org/pdf/2203.03235v2.pdf","comment":"Accepted to COLING 2022. The code is publicly available at\n  https://github.com/cjfarmer/TRD_FSL"},{"id":"http://arxiv.org/abs/2005.13316v2","updated":"2023-03-21T07:39:27Z","published":"2020-05-27T12:21:36Z","title":"Tracking, exploring and analyzing recent developments in German-language\n  online press in the face of the coronavirus crisis: cOWIDplus Analysis and\n  cOWIDplus Viewer","summary":"  The coronavirus pandemic may be the largest crisis the world has had to face\nsince World War II. It does not come as a surprise that it is also having an\nimpact on language as our primary communication tool. We present three\ninter-connected resources that are designed to capture and illustrate these\neffects on a subset of the German language: An RSS corpus of German-language\nnewsfeeds (with freely available untruncated unigram frequency lists), a static\nbut continuously updated HTML page tracking the diversity of the used\nvocabulary and a web application that enables other researchers and the broader\npublic to explore these effects without any or with little knowledge of corpus\nrepresentation/exploration or statistical analyses.\n","authors":["Sascha Wolfer","Alexander Koplenig","Frank Michaelis","Carolin Müller-Spitzer"],"pdf_url":"https://arxiv.org/pdf/2005.13316v2.pdf","comment":"13 pages, 6 figures, 1 table, 3852 words"},{"id":"http://arxiv.org/abs/2303.11621v1","updated":"2023-03-21T06:41:50Z","published":"2023-03-21T06:41:50Z","title":"Heterogeneous-Branch Collaborative Learning for Dialogue Generation","summary":"  With the development of deep learning, advanced dialogue generation methods\nusually require a greater amount of computational resources. One promising\napproach to obtaining a high-performance and lightweight model is knowledge\ndistillation, which relies heavily on the pre-trained powerful teacher.\nCollaborative learning, also known as online knowledge distillation, is an\neffective way to conduct one-stage group distillation in the absence of a\nwell-trained large teacher model. However, previous work has a severe branch\nhomogeneity problem due to the same training objective and the independent\nidentical training sets. To alleviate this problem, we consider the dialogue\nattributes in the training of network branches. Each branch learns the\nattribute-related features based on the selected subset. Furthermore, we\npropose a dual group-based knowledge distillation method, consisting of\npositive distillation and negative distillation, to further diversify the\nfeatures of different branches in a steadily and interpretable way. The\nproposed approach significantly improves branch heterogeneity and outperforms\nstate-of-the-art collaborative learning methods on two widely used open-domain\ndialogue datasets.\n","authors":["Yiwei Li","Shaoxiong Feng","Bin Sun","Kan Li"],"pdf_url":"https://arxiv.org/pdf/2303.11621v1.pdf","comment":"Accepted by AAAI 2023"},{"id":"http://arxiv.org/abs/2211.06552v3","updated":"2023-03-21T06:38:48Z","published":"2022-11-12T02:36:32Z","title":"Collecting Interactive Multi-modal Datasets for Grounded Language\n  Understanding","summary":"  Human intelligence can remarkably adapt quickly to new tasks and\nenvironments. Starting from a very young age, humans acquire new skills and\nlearn how to solve new tasks either by imitating the behavior of others or by\nfollowing provided natural language instructions. To facilitate research which\ncan enable similar capabilities in machines, we made the following\ncontributions (1) formalized the collaborative embodied agent using natural\nlanguage task; (2) developed a tool for extensive and scalable data collection;\nand (3) collected the first dataset for interactive grounded language\nunderstanding.\n","authors":["Shrestha Mohanty","Negar Arabzadeh","Milagro Teruel","Yuxuan Sun","Artem Zholus","Alexey Skrynnik","Mikhail Burtsev","Kavya Srinet","Aleksandr Panov","Arthur Szlam","Marc-Alexandre Côté","Julia Kiseleva"],"pdf_url":"https://arxiv.org/pdf/2211.06552v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.11607v1","updated":"2023-03-21T06:00:39Z","published":"2023-03-21T06:00:39Z","title":"Transformers in Speech Processing: A Survey","summary":"  The remarkable success of transformers in the field of natural language\nprocessing has sparked the interest of the speech-processing community, leading\nto an exploration of their potential for modeling long-range dependencies\nwithin speech sequences. Recently, transformers have gained prominence across\nvarious speech-related domains, including automatic speech recognition, speech\nsynthesis, speech translation, speech para-linguistics, speech enhancement,\nspoken dialogue systems, and numerous multimodal applications. In this paper,\nwe present a comprehensive survey that aims to bridge research studies from\ndiverse subfields within speech technology. By consolidating findings from\nacross the speech technology landscape, we provide a valuable resource for\nresearchers interested in harnessing the power of transformers to advance the\nfield. We identify the challenges encountered by transformers in speech\nprocessing while also offering insights into potential solutions to address\nthese issues.\n","authors":["Siddique Latif","Aun Zaidi","Heriberto Cuayahuitl","Fahad Shamshad","Moazzam Shoukat","Junaid Qadir"],"pdf_url":"https://arxiv.org/pdf/2303.11607v1.pdf","comment":"under-review"},{"id":"http://arxiv.org/abs/2303.05382v2","updated":"2023-03-21T05:47:11Z","published":"2023-03-06T16:36:17Z","title":"ChatGPT Is on the Horizon: Could a Large Language Model Be All We Need\n  for Intelligent Transportation?","summary":"  ChatGPT, developed by OpenAI, is one of the milestone large language models\n(LLMs) with 6 billion parameters. ChatGPT has demonstrated the impressive\nlanguage understanding capability of LLM, particularly in generating\nconversational response. As LLMs start to gain more attention in various\nresearch or engineering domains, it is time to envision how LLM may\nrevolutionize the way we approach intelligent transportation systems. This\npaper explores the future applications of LLM in addressing key transportation\nproblems. By leveraging LLM with cross-modal encoder, an intelligent system can\nalso process traffic data from different modalities and execute transportation\noperations through an LLM. We present and validate these potential\ntransportation applications equipped by LLM. To further demonstrate this\npotential, we also provide a concrete smartphone-based crash report\nauto-generation and analysis framework as a use case. Despite the potential\nbenefits, challenges related to data privacy, data quality, and model bias must\nbe considered. Overall, the use of LLM in intelligent transport systems holds\npromise for more efficient, intelligent, and sustainable transportation systems\nthat further improve daily life around the world.\n","authors":["Ou Zheng","Mohamed Abdel-Aty","Dongdong Wang","Zijin Wang","Shengxuan Ding"],"pdf_url":"https://arxiv.org/pdf/2303.05382v2.pdf","comment":"Submitted to Nature - Machine Intelligence (13 Pages, 8 Figures)"},{"id":"http://arxiv.org/abs/2111.09543v3","updated":"2023-03-21T05:17:08Z","published":"2021-11-18T06:48:00Z","title":"DeBERTaV3: Improving DeBERTa using ELECTRA-Style Pre-Training with\n  Gradient-Disentangled Embedding Sharing","summary":"  This paper presents a new pre-trained language model, DeBERTaV3, which\nimproves the original DeBERTa model by replacing mask language modeling (MLM)\nwith replaced token detection (RTD), a more sample-efficient pre-training task.\nOur analysis shows that vanilla embedding sharing in ELECTRA hurts training\nefficiency and model performance. This is because the training losses of the\ndiscriminator and the generator pull token embeddings in different directions,\ncreating the \"tug-of-war\" dynamics. We thus propose a new gradient-disentangled\nembedding sharing method that avoids the tug-of-war dynamics, improving both\ntraining efficiency and the quality of the pre-trained model. We have\npre-trained DeBERTaV3 using the same settings as DeBERTa to demonstrate its\nexceptional performance on a wide range of downstream natural language\nunderstanding (NLU) tasks. Taking the GLUE benchmark with eight tasks as an\nexample, the DeBERTaV3 Large model achieves a 91.37% average score, which is\n1.37% over DeBERTa and 1.91% over ELECTRA, setting a new state-of-the-art\n(SOTA) among the models with a similar structure. Furthermore, we have\npre-trained a multi-lingual model mDeBERTa and observed a larger improvement\nover strong baselines compared to English models. For example, the mDeBERTa\nBase achieves a 79.8% zero-shot cross-lingual accuracy on XNLI and a 3.6%\nimprovement over XLM-R Base, creating a new SOTA on this benchmark. We have\nmade our pre-trained models and inference code publicly available at\nhttps://github.com/microsoft/DeBERTa.\n","authors":["Pengcheng He","Jianfeng Gao","Weizhu Chen"],"pdf_url":"https://arxiv.org/pdf/2111.09543v3.pdf","comment":"16 pages, 10 tables, 2 Figures. The DeBERTaV3 model significantly\n  improves performance of the downstream NLU tasks over models with a similar\n  structure, e.g. DeBERTaV3 large achieves 91.37% average GLUE score which is\n  1.37% over DeBERTa large. XSmall has only 22M backbone parameters, but\n  significantly outperforms RoBERTa/XLNet-base. Paper is published as a\n  conference paper at ICLR 2023"},{"id":"http://arxiv.org/abs/2211.09778v3","updated":"2023-03-21T04:54:55Z","published":"2022-11-17T18:52:19Z","title":"I Can't Believe There's No Images! Learning Visual Tasks Using only\n  Language Data","summary":"  Many high-level skills that are required for computer vision tasks, such as\nparsing questions, comparing and contrasting semantics, and writing\ndescriptions, are also required in other domains such as natural language\nprocessing. In this paper, we ask whether it is possible to learn those skills\nfrom textual data and then transfer them to vision tasks without ever training\non visual training data. Key to our approach is exploiting the joint embedding\nspace of contrastively trained vision and language encoders. In practice, there\ncan be systematic differences between embedding spaces for different modalities\nin contrastive models, and we analyze how these differences affect our approach\nand study strategies to mitigate this concern. We produce models using only\ntext training data on four representative tasks: image captioning, visual\nentailment, visual question answering and visual news, and evaluate them on\nstandard benchmarks using images. We find these models generally perform close\nto models trained on images, while surpassing prior work for captioning and\nvisual entailment in this text only setting by over 9 points, and outperforming\nall prior work on visual news by over 30 points. We also showcase a variety of\nstylistic image captioning models that are trained using no image data and no\nhuman-curated language data, but instead using readily-available text data from\nbooks, the web, or language models.\n","authors":["Sophia Gu","Christopher Clark","Aniruddha Kembhavi"],"pdf_url":"https://arxiv.org/pdf/2211.09778v3.pdf","comment":"website (https://prior.allenai.org/projects/close), code\n  (https://github.com/allenai/close)"},{"id":"http://arxiv.org/abs/2303.11593v1","updated":"2023-03-21T04:47:45Z","published":"2023-03-21T04:47:45Z","title":"Difficulty in learning chirality for Transformer fed with SMILES","summary":"  Recent years have seen development of descriptor generation based on\nrepresentation learning of extremely diverse molecules, especially those that\napply natural language processing (NLP) models to SMILES, a literal\nrepresentation of molecular structure. However, little research has been done\non how these models understand chemical structure. To address this, we\ninvestigated the relationship between the learning progress of SMILES and\nchemical structure using a representative NLP model, the Transformer. The\nresults suggest that while the Transformer learns partial structures of\nmolecules quickly, it requires extended training to understand overall\nstructures. Consistently, the accuracy of molecular property predictions using\ndescriptors generated from models at different learning steps was similar from\nthe beginning to the end of training. Furthermore, we found that the\nTransformer requires particularly long training to learn chirality and\nsometimes stagnates with low translation accuracy due to misunderstanding of\nenantiomers. These findings are expected to deepen understanding of NLP models\nin chemistry.\n","authors":["Yasuhiro Yoshikai","Tadahaya Mizuno","Shumpei Nemoto","Hiroyuki Kusuhara"],"pdf_url":"https://arxiv.org/pdf/2303.11593v1.pdf","comment":"20 pages, 6 figures"},{"id":"http://arxiv.org/abs/2303.08006v2","updated":"2023-03-21T03:35:10Z","published":"2023-03-09T00:09:58Z","title":"Data-Efficient Learning of Natural Language to Linear Temporal Logic\n  Translators for Robot Task Specification","summary":"  To make robots accessible to a broad audience, it is critical to endow them\nwith the ability to take universal modes of communication, like commands given\nin natural language, and extract a concrete desired task specification, defined\nusing a formal language like linear temporal logic (LTL). In this paper, we\npresent a learning-based approach for translating from natural language\ncommands to LTL specifications with very limited human-labeled training data.\nThis is in stark contrast to existing natural-language to LTL translators,\nwhich require large human-labeled datasets, often in the form of labeled pairs\nof LTL formulas and natural language commands, to train the translator. To\nreduce reliance on human data, our approach generates a large synthetic\ntraining dataset through algorithmic generation of LTL formulas, conversion to\nstructured English, and then exploiting the paraphrasing capabilities of modern\nlarge language models (LLMs) to synthesize a diverse corpus of natural language\ncommands corresponding to the LTL formulas. We use this generated data to\nfinetune an LLM and apply a constrained decoding procedure at inference time to\nensure the returned LTL formula is syntactically correct. We evaluate our\napproach on three existing LTL/natural language datasets and show that we can\ntranslate natural language commands at 75\\% accuracy with far less human data\n($\\le$12 annotations). Moreover, when training on large human-annotated\ndatasets, our method achieves higher test accuracy (95\\% on average) than prior\nwork. Finally, we show the translated formulas can be used to plan\nlong-horizon, multi-stage tasks on a 12D quadrotor.\n","authors":["Jiayi Pan","Glen Chou","Dmitry Berenson"],"pdf_url":"https://arxiv.org/pdf/2303.08006v2.pdf","comment":"Accepted at ICRA 2023"},{"id":"http://arxiv.org/abs/2205.12676v2","updated":"2023-03-21T03:34:36Z","published":"2022-05-25T11:38:04Z","title":"Evaluating Inclusivity, Equity, and Accessibility of NLP Technology: A\n  Case Study for Indian Languages","summary":"  In order for NLP technology to be widely applicable, fair, and useful, it\nneeds to serve a diverse set of speakers across the world's languages, be\nequitable, i.e., not unduly biased towards any particular language, and be\ninclusive of all users, particularly in low-resource settings where compute\nconstraints are common. In this paper, we propose an evaluation paradigm that\nassesses NLP technologies across all three dimensions. While diversity and\ninclusion have received attention in recent literature, equity is currently\nunexplored. We propose to address this gap using the Gini coefficient, a\nwell-established metric used for estimating societal wealth inequality. Using\nour paradigm, we highlight the distressed state of current technologies for\nIndian (IN) languages (a linguistically large and diverse set, with a varied\nspeaker population), across all three dimensions. To improve upon these\nmetrics, we demonstrate the importance of region-specific choices in model\nbuilding and dataset creation, and more importantly, propose a novel,\ngeneralisable approach to optimal resource allocation during fine-tuning.\nFinally, we discuss steps to mitigate these biases and encourage the community\nto employ multi-faceted evaluation when building linguistically diverse and\nequitable technologies.\n","authors":["Simran Khanuja","Sebastian Ruder","Partha Talukdar"],"pdf_url":"https://arxiv.org/pdf/2205.12676v2.pdf","comment":"Accepted to EACL Findings, 2023"},{"id":"http://arxiv.org/abs/2301.04347v2","updated":"2023-03-21T03:04:09Z","published":"2023-01-11T07:52:59Z","title":"Counteracts: Testing Stereotypical Representation in Pre-trained\n  Language Models","summary":"  Language models have demonstrated strong performance on various natural\nlanguage understanding tasks. Similar to humans, language models could also\nhave their own bias that is learned from the training data. As more and more\ndownstream tasks integrate language models as part of the pipeline, it is\nnecessary to understand the internal stereotypical representation and the\nmethods to mitigate the negative effects. In this paper, we proposed a simple\nmethod to test the internal stereotypical representation in pre-trained\nlanguage models using counterexamples. We mainly focused on gender bias, but\nthe method can be extended to other types of bias. We evaluated models on 9\ndifferent cloze-style prompts consisting of knowledge and base prompts. Our\nresults indicate that pre-trained language models show a certain amount of\nrobustness when using unrelated knowledge, and prefer shallow linguistic cues,\nsuch as word position and syntactic structure, to alter the internal\nstereotypical representation. Such findings shed light on how to manipulate\nlanguage models in a neutral approach for both finetuning and evaluation.\n","authors":["Damin Zhang"],"pdf_url":"https://arxiv.org/pdf/2301.04347v2.pdf","comment":"FACCT; to be submitted to"},{"id":"http://arxiv.org/abs/2303.10475v2","updated":"2023-03-21T01:27:16Z","published":"2023-03-18T19:17:47Z","title":"Is Prompt All You Need? No. A Comprehensive and Broader View of\n  Instruction Learning","summary":"  Task semantics can be expressed by a set of input-to-output examples or a\npiece of textual instruction. Conventional machine learning approaches for\nnatural language processing (NLP) mainly rely on the availability of\nlarge-scale sets of task-specific examples. Two issues arise: first, collecting\ntask-specific labeled examples does not apply to scenarios where tasks may be\ntoo complicated or costly to annotate, or the system is required to handle a\nnew task immediately; second, this is not user-friendly since end-users are\nprobably more willing to provide task description rather than a set of examples\nbefore using the system. Therefore, the community is paying increasing interest\nin a new supervision-seeking paradigm for NLP: learning from task instructions.\nDespite its impressive progress, there are some common issues that the\ncommunity struggles with. This survey paper tries to summarize the current\nresearch on instruction learning, particularly, by answering the following\nquestions: (i) what is task instruction, and what instruction types exist? (ii)\nhow to model instructions? (iii) what factors influence and explain the\ninstructions' performance? (iv) what challenges remain in instruction learning?\nTo our knowledge, this is the first comprehensive survey about textual\ninstructions.\n","authors":["Renze Lou","Kai Zhang","Wenpeng Yin"],"pdf_url":"https://arxiv.org/pdf/2303.10475v2.pdf","comment":"Work is still in progress. The paper list is available at\n  https://github.com/RenzeLou/awesome-instruction-learning"},{"id":"http://arxiv.org/abs/2303.11525v1","updated":"2023-03-21T01:06:37Z","published":"2023-03-21T01:06:37Z","title":"SIFT: Sparse Iso-FLOP Transformations for Maximizing Training Efficiency","summary":"  Recent works have explored the use of weight sparsity to improve the training\nefficiency (test accuracy w.r.t training FLOPs) of deep neural networks (DNNs).\nThese works aim to reduce training FLOPs but training with sparse weights often\nleads to accuracy loss or requires longer train schedules, making the resulting\ntraining efficiency less clear. In contrast, we focus on using sparsity to\nincrease accuracy while using the same FLOPS as the dense model and show\ntraining efficiency gains through higher accuracy. In this work, we introduce\nSIFT, a family of Sparse Iso-FLOP Transformations which are used as drop-in\nreplacements for dense layers to improve their representational capacity and\nFLOP efficiency. Each transformation is parameterized by a single parameter\n(sparsity level) and provides a larger search space to find optimal sparse\nmasks. Without changing any training hyperparameters, replacing dense layers\nwith SIFT leads to significant improvements across computer vision (CV) and\nnatural language processing (NLP) tasks, including ResNet-18 on ImageNet\n(+3.5%) and GPT-3 Small on WikiText-103 (-0.4 PPL), both matching larger dense\nmodel variants with 2x or more FLOPs. To the best of our knowledge, this is the\nfirst work to demonstrate the use of sparsity for improving accuracy of dense\nmodels via a simple-to-use set of sparse transformations. Code is available at:\nhttps://github.com/CerebrasResearch/SIFT.\n","authors":["Shreyas Saxena","Vithursan Thangarasa","Abhay Gupta","Sean Lie"],"pdf_url":"https://arxiv.org/pdf/2303.11525v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.12220v1","updated":"2023-03-21T22:43:41Z","published":"2023-03-21T22:43:41Z","title":"A Unified Taxonomy of Deep Syntactic Relations","summary":"  This paper analyzes multiple deep-syntactic frameworks with the goal of\ncreating a proposal for a set of universal semantic role labels. The proposal\nexamines various theoretic linguistic perspectives and focuses on Meaning-Text\nTheory and Functional Generative Description frameworks.\n  For the purpose of this research, data from four languages is used -- Spanish\nand Catalan (Taule et al., 2011), Czech (Hajic et al., 2017), and English\n(Hajic et al., 2012). This proposal is oriented towards Universal Dependencies\n(de Marneffe et al., 2021) with a further intention of applying the universal\nsemantic role labels to the UD data.\n","authors":["Kira Droganova","Daniel Zeman"],"pdf_url":"https://arxiv.org/pdf/2303.12220v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.12208v1","updated":"2023-03-21T21:49:39Z","published":"2023-03-21T21:49:39Z","title":"MAGVLT: Masked Generative Vision-and-Language Transformer","summary":"  While generative modeling on multimodal image-text data has been actively\ndeveloped with large-scale paired datasets, there have been limited attempts to\ngenerate both image and text data by a single model rather than a generation of\none fixed modality conditioned on the other modality. In this paper, we explore\na unified generative vision-and-language (VL) model that can produce both\nimages and text sequences. Especially, we propose a generative VL transformer\nbased on the non-autoregressive mask prediction, named MAGVLT, and compare it\nwith an autoregressive generative VL transformer (ARGVLT). In comparison to\nARGVLT, the proposed MAGVLT enables bidirectional context encoding, fast\ndecoding by parallel token predictions in an iterative refinement, and extended\nediting capabilities such as image and text infilling. For rigorous training of\nour MAGVLT with image-text pairs from scratch, we combine the image-to-text,\ntext-to-image, and joint image-and-text mask prediction tasks. Moreover, we\ndevise two additional tasks based on the step-unrolled mask prediction and the\nselective prediction on the mixture of two image-text pairs. Experimental\nresults on various downstream generation tasks of VL benchmarks show that our\nMAGVLT outperforms ARGVLT by a large margin even with significant inference\nspeedup. Particularly, MAGVLT achieves competitive results on both zero-shot\nimage-to-text and text-to-image generation tasks from MS-COCO by one\nmoderate-sized model (fewer than 500M parameters) even without the use of\nmonomodal data and networks.\n","authors":["Sungwoong Kim","Daejin Jo","Donghoon Lee","Jongmin Kim"],"pdf_url":"https://arxiv.org/pdf/2303.12208v1.pdf","comment":"CVPR 2023"},{"id":"http://arxiv.org/abs/2303.12135v1","updated":"2023-03-21T18:48:11Z","published":"2023-03-21T18:48:11Z","title":"Understand Legal Documents with Contextualized Large Language Models","summary":"  The growth of pending legal cases in populous countries, such as India, has\nbecome a major issue. Developing effective techniques to process and understand\nlegal documents is extremely useful in resolving this problem. In this paper,\nwe present our systems for SemEval-2023 Task 6: understanding legal texts (Modi\net al., 2023). Specifically, we first develop the Legal-BERT-HSLN model that\nconsiders the comprehensive context information in both intra- and\ninter-sentence levels to predict rhetorical roles (subtask A) and then train a\nLegal-LUKE model, which is legal-contextualized and entity-aware, to recognize\nlegal entities (subtask B). Our evaluations demonstrate that our designed\nmodels are more accurate than baselines, e.g., with an up to 15.0% better F1\nscore in subtask B. We achieved notable performance in the task leaderboard,\ne.g., 0.834 micro F1 score, and ranked No.5 out of 27 teams in subtask A.\n","authors":["Xin Jin","Yuchen Wang"],"pdf_url":"https://arxiv.org/pdf/2303.12135v1.pdf","comment":"SemEval 2023"},{"id":"http://arxiv.org/abs/2303.12132v1","updated":"2023-03-21T18:45:09Z","published":"2023-03-21T18:45:09Z","title":"Fundamentals of Generative Large Language Models and Perspectives in\n  Cyber-Defense","summary":"  Generative Language Models gained significant attention in late 2022 / early\n2023, notably with the introduction of models refined to act consistently with\nusers' expectations of interactions with AI (conversational models). Arguably\nthe focal point of public attention has been such a refinement of the GPT3\nmodel -- the ChatGPT and its subsequent integration with auxiliary\ncapabilities, including search as part of Microsoft Bing. Despite extensive\nprior research invested in their development, their performance and\napplicability to a range of daily tasks remained unclear and niche. However,\ntheir wider utilization without a requirement for technical expertise, made in\nlarge part possible through conversational fine-tuning, revealed the extent of\ntheir true capabilities in a real-world environment. This has garnered both\npublic excitement for their potential applications and concerns about their\ncapabilities and potential malicious uses. This review aims to provide a brief\noverview of the history, state of the art, and implications of Generative\nLanguage Models in terms of their principles, abilities, limitations, and\nfuture prospects -- especially in the context of cyber-defense, with a focus on\nthe Swiss operational environment.\n","authors":["Andrei Kucharavy","Zachary Schillaci","Loïc Maréchal","Maxime Würsch","Ljiljana Dolamic","Remi Sabonnadiere","Dimitri Percia David","Alain Mermoud","Vincent Lenders"],"pdf_url":"https://arxiv.org/pdf/2303.12132v1.pdf","comment":"41 pages (without references), 13 figures; public report of\n  Cyber-Defence Campus"},{"id":"http://arxiv.org/abs/2303.12112v1","updated":"2023-03-21T18:03:14Z","published":"2023-03-21T18:03:14Z","title":"Positive-Augmented Constrastive Learning for Image and Video Captioning\n  Evaluation","summary":"  The CLIP model has been recently proven to be very effective for a variety of\ncross-modal tasks, including the evaluation of captions generated from\nvision-and-language architectures. In this paper, we propose a new recipe for a\ncontrastive-based evaluation metric for image captioning, namely\nPositive-Augmented Contrastive learning Score (PAC-S), that in a novel way\nunifies the learning of a contrastive visual-semantic space with the addition\nof generated images and text on curated data. Experiments spanning several\ndatasets demonstrate that our new metric achieves the highest correlation with\nhuman judgments on both images and videos, outperforming existing\nreference-based metrics like CIDEr and SPICE and reference-free metrics like\nCLIP-Score. Finally, we test the system-level correlation of the proposed\nmetric when considering popular image captioning approaches, and assess the\nimpact of employing different cross-modal features. Our source code and trained\nmodels are publicly available at: https://github.com/aimagelab/pacscore.\n","authors":["Sara Sarto","Manuele Barraco","Marcella Cornia","Lorenzo Baraldi","Rita Cucchiara"],"pdf_url":"https://arxiv.org/pdf/2303.12112v1.pdf","comment":"CVPR 2023 (highlight paper)"},{"id":"http://arxiv.org/abs/2303.12513v1","updated":"2023-03-21T17:30:40Z","published":"2023-03-21T17:30:40Z","title":"Is BERT Blind? Exploring the Effect of Vision-and-Language Pretraining\n  on Visual Language Understanding","summary":"  Most humans use visual imagination to understand and reason about language,\nbut models such as BERT reason about language using knowledge acquired during\ntext-only pretraining. In this work, we investigate whether vision-and-language\npretraining can improve performance on text-only tasks that involve implicit\nvisual reasoning, focusing primarily on zero-shot probing methods. We propose a\nsuite of visual language understanding (VLU) tasks for probing the visual\nreasoning abilities of text encoder models, as well as various non-visual\nnatural language understanding (NLU) tasks for comparison. We also contribute a\nnovel zero-shot knowledge probing method, Stroop probing, for applying models\nsuch as CLIP to text-only tasks without needing a prediction head such as the\nmasked language modelling head of models like BERT. We show that SOTA\nmultimodally trained text encoders outperform unimodally trained text encoders\non the VLU tasks while being underperformed by them on the NLU tasks, lending\nnew context to previously mixed results regarding the NLU capabilities of\nmultimodal models. We conclude that exposure to images during pretraining\naffords inherent visual reasoning knowledge that is reflected in language-only\ntasks that require implicit visual reasoning. Our findings bear importance in\nthe broader context of multimodal learning, providing principled guidelines for\nthe choice of text encoders used in such contexts.\n","authors":["Morris Alper","Michael Fiman","Hadar Averbuch-Elor"],"pdf_url":"https://arxiv.org/pdf/2303.12513v1.pdf","comment":"To be presented in CVPR 2023. Project webpage:\n  https://isbertblind.github.io/"},{"id":"http://arxiv.org/abs/2207.03038v2","updated":"2023-03-21T11:32:18Z","published":"2022-07-07T01:47:19Z","title":"Dual-Stream Transformer for Generic Event Boundary Captioning","summary":"  This paper describes our champion solution for the CVPR2022 Generic Event\nBoundary Captioning (GEBC) competition. GEBC requires the captioning model to\nhave a comprehension of instantaneous status changes around the given video\nboundary, which makes it much more challenging than conventional video\ncaptioning task. In this paper, a Dual-Stream Transformer with improvements on\nboth video content encoding and captions generation is proposed: (1) We utilize\nthree pre-trained models to extract the video features from different\ngranularities. Moreover, we exploit the types of boundary as hints to help the\nmodel generate captions. (2) We particularly design an model, termed as\nDual-Stream Transformer, to learn discriminative representations for boundary\ncaptioning. (3) Towards generating content-relevant and human-like captions, we\nimprove the description quality by designing a word-level ensemble strategy.\nThe promising results on the GEBC test split demonstrate the efficacy of our\nproposed model.\n","authors":["Xin Gu","Hanhua Ye","Guang Chen","Yufei Wang","Libo Zhang","Longyin Wen"],"pdf_url":"https://arxiv.org/pdf/2207.03038v2.pdf","comment":"Accepted to CVPR 2023 LOVEU Workshop"},{"id":"http://arxiv.org/abs/2303.13367v1","updated":"2023-03-21T14:35:07Z","published":"2023-03-21T14:35:07Z","title":"ChatGPT and a New Academic Reality: AI-Written Research Papers and the\n  Ethics of the Large Language Models in Scholarly Publishing","summary":"  This paper discusses OpenAIs ChatGPT, a generative pre-trained transformer,\nwhich uses natural language processing to fulfill text-based user requests\n(i.e., a chatbot). The history and principles behind ChatGPT and similar models\nare discussed. This technology is then discussed in relation to its potential\nimpact on academia and scholarly research and publishing. ChatGPT is seen as a\npotential model for the automated preparation of essays and other types of\nscholarly manuscripts. Potential ethical issues that could arise with the\nemergence of large language models like GPT-3, the underlying technology behind\nChatGPT, and its usage by academics and researchers, are discussed and situated\nwithin the context of broader advancements in artificial intelligence, machine\nlearning, and natural language processing for research and scholarly\npublishing.\n","authors":["Brady Lund","Ting Wang","Nishith Reddy Mannuru","Bing Nie","Somipam Shimray","Ziang Wang"],"pdf_url":"https://arxiv.org/pdf/2303.13367v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.13373v1","updated":"2023-03-21T07:25:36Z","published":"2023-03-21T07:25:36Z","title":"Fine-tuning ClimateBert transformer with ClimaText for the disclosure\n  analysis of climate-related financial risks","summary":"  In recent years there has been a growing demand from financial agents,\nespecially from particular and institutional investors, for companies to report\non climate-related financial risks. A vast amount of information, in text\nformat, can be expected to be disclosed in the short term by firms in order to\nidentify these types of risks in their financial and non financial reports,\nparticularly in response to the growing regulation that is being passed on the\nmatter. To this end, this paper applies state-of-the-art NLP techniques to\nachieve the detection of climate change in text corpora. We use transfer\nlearning to fine-tune two transformer models, BERT and ClimateBert -a recently\npublished DistillRoBERTa-based model that has been specifically tailored for\nclimate text classification-. These two algorithms are based on the transformer\narchitecture which enables learning the contextual relationships between words\nin a text. We carry out the fine-tuning process of both models on the novel\nClima-Text database, consisting of data collected from Wikipedia, 10K Files\nReports and web-based claims. Our text classification model obtained from the\nClimateBert fine-tuning process on ClimaText, outperforms the models created\nwith BERT and the current state-of-the-art transformer in this particular\nproblem. Our study is the first one to implement on the ClimaText database the\nrecently published ClimateBert algorithm. Based on our results, it can be said\nthat ClimateBert fine-tuned on ClimaText is an outstanding tool within the NLP\npre-trained transformer models that may and should be used by investors,\ninstitutional agents and companies themselves to monitor the disclosure of\nclimate risk in financial reports. In addition, our transfer learning\nmethodology is cheap in computational terms, thus allowing any organization to\nperform it.\n","authors":["Eduardo C. Garrido-Merchán","Cristina González-Barthe","María Coronado Vaca"],"pdf_url":"https://arxiv.org/pdf/2303.13373v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.13549v1","updated":"2023-03-21T21:38:44Z","published":"2023-03-21T21:38:44Z","title":"Optical Character Recognition and Transcription of Berber Signs from\n  Images in a Low-Resource Language Amazigh","summary":"  The Berber, or Amazigh language family is a low-resource North African\nvernacular language spoken by the indigenous Berber ethnic group. It has its\nown unique alphabet called Tifinagh used across Berber communities in Morocco,\nAlgeria, and others. The Afroasiatic language Berber is spoken by 14 million\npeople, yet lacks adequate representation in education, research, web\napplications etc. For instance, there is no option of translation to or from\nAmazigh / Berber on Google Translate, which hosts over 100 languages today.\nConsequently, we do not find specialized educational apps, L2 (2nd language\nlearner) acquisition, automated language translation, and remote-access\nfacilities enabled in Berber. Motivated by this background, we propose a\nsupervised approach called DaToBS for Detection and Transcription of Berber\nSigns. The DaToBS approach entails the automatic recognition and transcription\nof Tifinagh characters from signs in photographs of natural environments. This\nis achieved by self-creating a corpus of 1862 pre-processed character images;\ncurating the corpus with human-guided annotation; and feeding it into an OCR\nmodel via the deployment of CNN for deep learning based on computer vision\nmodels. We deploy computer vision modeling (rather than language models)\nbecause there are pictorial symbols in this alphabet, this deployment being a\nnovel aspect of our work. The DaToBS experimentation and analyses yield over 92\npercent accuracy in our research. To the best of our knowledge, ours is among\nthe first few works in the automated transcription of Berber signs from\nroadside images with deep learning, yielding high accuracy. This can pave the\nway for developing pedagogical applications in the Berber language, thereby\naddressing an important goal of outreach to underrepresented communities via AI\nin education.\n","authors":["Levi Corallo","Aparna S. Varde"],"pdf_url":"https://arxiv.org/pdf/2303.13549v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.13631v1","updated":"2023-03-21T08:39:56Z","published":"2023-03-21T08:39:56Z","title":"In-depth analysis of music structure as a self-organized network","summary":"  Words in a natural language not only transmit information but also evolve\nwith the development of civilization and human migration. The same is true for\nmusic. To understand the complex structure behind the music, we introduced an\nalgorithm called the Essential Element Network (EEN) to encode the audio into\ntext. The network is obtained by calculating the correlations between scales,\ntime, and volume. Optimizing EEN to generate Zipfs law for the frequency and\nrank of the clustering coefficient enables us to generate and regard the\nsemantic relationships as words. We map these encoded words into the\nscale-temporal space, which helps us organize systematically the syntax in the\ndeep structure of music. Our algorithm provides precise descriptions of the\ncomplex network behind the music, as opposed to the black-box nature of other\ndeep learning approaches. As a result, the experience and properties\naccumulated through these processes can offer not only a new approach to the\napplications of Natural Language Processing (NLP) but also an easier and more\nobjective way to analyze the evolution and development of music.\n","authors":["Ping-Rui Tsai","Yen-Ting Chou","Nathan-Christopher Wang","Hui-Ling Chen","Hong-Yue Huang","Zih-Jia Luo","Tzay-Ming Hong"],"pdf_url":"https://arxiv.org/pdf/2303.13631v1.pdf","comment":"5 pages"}],"Computer Vision and Pattern Recognition":[{"id":"http://arxiv.org/abs/2303.12079v1","updated":"2023-03-21T17:59:57Z","published":"2023-03-21T17:59:57Z","title":"OmniTracker: Unifying Object Tracking by Tracking-with-Detection","summary":"  Object tracking (OT) aims to estimate the positions of target objects in a\nvideo sequence. Depending on whether the initial states of target objects are\nspecified by provided annotations in the first frame or the categories, OT\ncould be classified as instance tracking (e.g., SOT and VOS) and category\ntracking (e.g., MOT, MOTS, and VIS) tasks. Combing the advantages of the best\npractices developed in both communities, we propose a novel\ntracking-with-detection paradigm, where tracking supplements appearance priors\nfor detection and detection provides tracking with candidate bounding boxes for\nassociation. Equipped with such a design, a unified tracking model,\nOmniTracker, is further presented to resolve all the tracking tasks with a\nfully shared network architecture, model weights, and inference pipeline.\nExtensive experiments on 7 tracking datasets, including LaSOT, TrackingNet,\nDAVIS16-17, MOT17, MOTS20, and YTVIS19, demonstrate that OmniTracker achieves\non-par or even better results than both task-specific and unified tracking\nmodels.\n","authors":["Junke Wang","Dongdong Chen","Zuxuan Wu","Chong Luo","Xiyang Dai","Lu Yuan","Yu-Gang Jiang"],"pdf_url":"https://arxiv.org/pdf/2303.12079v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.12080v1","updated":"2023-03-21T17:59:57Z","published":"2023-03-21T17:59:57Z","title":"Natural Language-Assisted Sign Language Recognition","summary":"  Sign languages are visual languages which convey information by signers'\nhandshape, facial expression, body movement, and so forth. Due to the inherent\nrestriction of combinations of these visual ingredients, there exist a\nsignificant number of visually indistinguishable signs (VISigns) in sign\nlanguages, which limits the recognition capacity of vision neural networks. To\nmitigate the problem, we propose the Natural Language-Assisted Sign Language\nRecognition (NLA-SLR) framework, which exploits semantic information contained\nin glosses (sign labels). First, for VISigns with similar semantic meanings, we\npropose language-aware label smoothing by generating soft labels for each\ntraining sign whose smoothing weights are computed from the normalized semantic\nsimilarities among the glosses to ease training. Second, for VISigns with\ndistinct semantic meanings, we present an inter-modality mixup technique which\nblends vision and gloss features to further maximize the separability of\ndifferent signs under the supervision of blended labels. Besides, we also\nintroduce a novel backbone, video-keypoint network, which not only models both\nRGB videos and human body keypoints but also derives knowledge from sign videos\nof different temporal receptive fields. Empirically, our method achieves\nstate-of-the-art performance on three widely-adopted benchmarks: MSASL, WLASL,\nand NMFs-CSL. Codes are available at https://github.com/FangyunWei/SLRT.\n","authors":["Ronglai Zuo","Fangyun Wei","Brian Mak"],"pdf_url":"https://arxiv.org/pdf/2303.12080v1.pdf","comment":"Accepted by CVPR 2023. Codes are available at\n  https://github.com/FangyunWei/SLRT"},{"id":"http://arxiv.org/abs/2303.12078v1","updated":"2023-03-21T17:59:56Z","published":"2023-03-21T17:59:56Z","title":"Two-shot Video Object Segmentation","summary":"  Previous works on video object segmentation (VOS) are trained on densely\nannotated videos. Nevertheless, acquiring annotations in pixel level is\nexpensive and time-consuming. In this work, we demonstrate the feasibility of\ntraining a satisfactory VOS model on sparsely annotated videos-we merely\nrequire two labeled frames per training video while the performance is\nsustained. We term this novel training paradigm as two-shot video object\nsegmentation, or two-shot VOS for short. The underlying idea is to generate\npseudo labels for unlabeled frames during training and to optimize the model on\nthe combination of labeled and pseudo-labeled data. Our approach is extremely\nsimple and can be applied to a majority of existing frameworks. We first\npre-train a VOS model on sparsely annotated videos in a semi-supervised manner,\nwith the first frame always being a labeled one. Then, we adopt the pre-trained\nVOS model to generate pseudo labels for all unlabeled frames, which are\nsubsequently stored in a pseudo-label bank. Finally, we retrain a VOS model on\nboth labeled and pseudo-labeled data without any restrictions on the first\nframe. For the first time, we present a general way to train VOS models on\ntwo-shot VOS datasets. By using 7.3% and 2.9% labeled data of YouTube-VOS and\nDAVIS benchmarks, our approach achieves comparable results in contrast to the\ncounterparts trained on fully labeled set. Code and models are available at\nhttps://github.com/yk-pku/Two-shot-Video-Object-Segmentation.\n","authors":["Kun Yan","Xiao Li","Fangyun Wei","Jinglu Wang","Chenbin Zhang","Ping Wang","Yan Lu"],"pdf_url":"https://arxiv.org/pdf/2303.12078v1.pdf","comment":"Accepted by CVPR 2023. Code and models are available at\n  https://github.com/yk-pku/Two-shot-Video-Object-Segmentation"},{"id":"http://arxiv.org/abs/2303.12077v1","updated":"2023-03-21T17:59:22Z","published":"2023-03-21T17:59:22Z","title":"VAD: Vectorized Scene Representation for Efficient Autonomous Driving","summary":"  Autonomous driving requires a comprehensive understanding of the surrounding\nenvironment for reliable trajectory planning. Previous works rely on dense\nrasterized scene representation (e.g., agent occupancy and semantic map) to\nperform planning, which is computationally intensive and misses the\ninstance-level structure information. In this paper, we propose VAD, an\nend-to-end vectorized paradigm for autonomous driving, which models the driving\nscene as fully vectorized representation. The proposed vectorized paradigm has\ntwo significant advantages. On one hand, VAD exploits the vectorized agent\nmotion and map elements as explicit instance-level planning constraints which\neffectively improves planning safety. On the other hand, VAD runs much faster\nthan previous end-to-end planning methods by getting rid of\ncomputation-intensive rasterized representation and hand-designed\npost-processing steps. VAD achieves state-of-the-art end-to-end planning\nperformance on the nuScenes dataset, outperforming the previous best method by\na large margin (reducing the average collision rate by 48.4%). Besides, VAD\ngreatly improves the inference speed (up to 9.3x), which is critical for the\nreal-world deployment of an autonomous driving system. Code and models will be\nreleased for facilitating future research.\n","authors":["Bo Jiang","Shaoyu Chen","Qing Xu","Bencheng Liao","Jiajie Chen","Helong Zhou","Qian Zhang","Wenyu Liu","Chang Huang","Xinggang Wang"],"pdf_url":"https://arxiv.org/pdf/2303.12077v1.pdf","comment":"Code&Demos: https://github.com/hustvl/VAD"},{"id":"http://arxiv.org/abs/2303.12076v1","updated":"2023-03-21T17:59:20Z","published":"2023-03-21T17:59:20Z","title":"Dexterity from Touch: Self-Supervised Pre-Training of Tactile\n  Representations with Robotic Play","summary":"  Teaching dexterity to multi-fingered robots has been a longstanding challenge\nin robotics. Most prominent work in this area focuses on learning controllers\nor policies that either operate on visual observations or state estimates\nderived from vision. However, such methods perform poorly on fine-grained\nmanipulation tasks that require reasoning about contact forces or about objects\noccluded by the hand itself. In this work, we present T-Dex, a new approach for\ntactile-based dexterity, that operates in two phases. In the first phase, we\ncollect 2.5 hours of play data, which is used to train self-supervised tactile\nencoders. This is necessary to bring high-dimensional tactile readings to a\nlower-dimensional embedding. In the second phase, given a handful of\ndemonstrations for a dexterous task, we learn non-parametric policies that\ncombine the tactile observations with visual ones. Across five challenging\ndexterous tasks, we show that our tactile-based dexterity models outperform\npurely vision and torque-based models by an average of 1.7X. Finally, we\nprovide a detailed analysis on factors critical to T-Dex including the\nimportance of play data, architectures, and representation learning.\n","authors":["Irmak Guzey","Ben Evans","Soumith Chintala","Lerrel Pinto"],"pdf_url":"https://arxiv.org/pdf/2303.12076v1.pdf","comment":"Video and code can be accessed here:\n  https://tactile-dexterity.github.io/"},{"id":"http://arxiv.org/abs/2303.12074v1","updated":"2023-03-21T17:59:02Z","published":"2023-03-21T17:59:02Z","title":"CC3D: Layout-Conditioned Generation of Compositional 3D Scenes","summary":"  In this work, we introduce CC3D, a conditional generative model that\nsynthesizes complex 3D scenes conditioned on 2D semantic scene layouts, trained\nusing single-view images. Different from most existing 3D GANs that limit their\napplicability to aligned single objects, we focus on generating complex scenes\nwith multiple objects, by modeling the compositional nature of 3D scenes. By\ndevising a 2D layout-based approach for 3D synthesis and implementing a new 3D\nfield representation with a stronger geometric inductive bias, we have created\na 3D GAN that is both efficient and of high quality, while allowing for a more\ncontrollable generation process. Our evaluations on synthetic 3D-FRONT and\nreal-world KITTI-360 datasets demonstrate that our model generates scenes of\nimproved visual and geometric quality in comparison to previous works.\n","authors":["Sherwin Bahmani","Jeong Joon Park","Despoina Paschalidou","Xingguang Yan","Gordon Wetzstein","Leonidas Guibas","Andrea Tagliasacchi"],"pdf_url":"https://arxiv.org/pdf/2303.12074v1.pdf","comment":"Webpage: https://sherwinbahmani.github.io/cc3d/"},{"id":"http://arxiv.org/abs/2303.12073v1","updated":"2023-03-21T17:58:49Z","published":"2023-03-21T17:58:49Z","title":"3D Mitochondria Instance Segmentation with Spatio-Temporal Transformers","summary":"  Accurate 3D mitochondria instance segmentation in electron microscopy (EM) is\na challenging problem and serves as a prerequisite to empirically analyze their\ndistributions and morphology. Most existing approaches employ 3D convolutions\nto obtain representative features. However, these convolution-based approaches\nstruggle to effectively capture long-range dependencies in the volume\nmitochondria data, due to their limited local receptive field. To address this,\nwe propose a hybrid encoder-decoder framework based on a split spatio-temporal\nattention module that efficiently computes spatial and temporal self-attentions\nin parallel, which are later fused through a deformable convolution. Further,\nwe introduce a semantic foreground-background adversarial loss during training\nthat aids in delineating the region of mitochondria instances from the\nbackground clutter. Our extensive experiments on three benchmarks, Lucchi,\nMitoEM-R and MitoEM-H, reveal the benefits of the proposed contributions\nachieving state-of-the-art results on all three datasets. Our code and models\nare available at https://github.com/OmkarThawakar/STT-UNET.\n","authors":["Omkar Thawakar","Rao Muhammad Anwer","Jorma Laaksonen","Orly Reiner","Mubarak Shah","Fahad Shahbaz Khan"],"pdf_url":"https://arxiv.org/pdf/2303.12073v1.pdf","comment":"8 pages, 3 figures, 5 Tables, 2 page references"},{"id":"http://arxiv.org/abs/2303.12071v1","updated":"2023-03-21T17:58:28Z","published":"2023-03-21T17:58:28Z","title":"ProphNet: Efficient Agent-Centric Motion Forecasting with\n  Anchor-Informed Proposals","summary":"  Motion forecasting is a key module in an autonomous driving system. Due to\nthe heterogeneous nature of multi-sourced input, multimodality in agent\nbehavior, and low latency required by onboard deployment, this task is\nnotoriously challenging. To cope with these difficulties, this paper proposes a\nnovel agent-centric model with anchor-informed proposals for efficient\nmultimodal motion prediction. We design a modality-agnostic strategy to\nconcisely encode the complex input in a unified manner. We generate diverse\nproposals, fused with anchors bearing goal-oriented scene context, to induce\nmultimodal prediction that covers a wide range of future trajectories. Our\nnetwork architecture is highly uniform and succinct, leading to an efficient\nmodel amenable for real-world driving deployment. Experiments reveal that our\nagent-centric network compares favorably with the state-of-the-art methods in\nprediction accuracy, while achieving scene-centric level inference latency.\n","authors":["Xishun Wang","Tong Su","Fang Da","Xiaodong Yang"],"pdf_url":"https://arxiv.org/pdf/2303.12071v1.pdf","comment":"CVPR 2023"},{"id":"http://arxiv.org/abs/2303.12068v1","updated":"2023-03-21T17:57:33Z","published":"2023-03-21T17:57:33Z","title":"Machine Learning for Brain Disorders: Transformers and Visual\n  Transformers","summary":"  Transformers were initially introduced for natural language processing (NLP)\ntasks, but fast they were adopted by most deep learning fields, including\ncomputer vision. They measure the relationships between pairs of input tokens\n(words in the case of text strings, parts of images for visual Transformers),\ntermed attention. The cost is exponential with the number of tokens. For image\nclassification, the most common Transformer Architecture uses only the\nTransformer Encoder in order to transform the various input tokens. However,\nthere are also numerous other applications in which the decoder part of the\ntraditional Transformer Architecture is also used. Here, we first introduce the\nAttention mechanism (Section 1), and then the Basic Transformer Block including\nthe Vision Transformer (Section 2). Next, we discuss some improvements of\nvisual Transformers to account for small datasets or less computation(Section\n3). Finally, we introduce Visual Transformers applied to tasks other than image\nclassification, such as detection, segmentation, generation and training\nwithout labels (Section 4) and other domains, such as video or multimodality\nusing text or audio data (Section 5).\n","authors":["Robin Courant","Maika Edberg","Nicolas Dufour","Vicky Kalogeiton"],"pdf_url":"https://arxiv.org/pdf/2303.12068v1.pdf","comment":"To appear in O. Colliot (Ed.), Machine Learning for Brain Disorders,\n  Springer"},{"id":"http://arxiv.org/abs/2303.12059v1","updated":"2023-03-21T17:51:23Z","published":"2023-03-21T17:51:23Z","title":"Motion Matters: Neural Motion Transfer for Better Camera Physiological\n  Sensing","summary":"  Machine learning models for camera-based physiological measurement can have\nweak generalization due to a lack of representative training data. Body motion\nis one of the most significant sources of noise when attempting to recover the\nsubtle cardiac pulse from a video. We explore motion transfer as a form of data\naugmentation to introduce motion variation while preserving physiological\nchanges. We adapt a neural video synthesis approach to augment videos for the\ntask of remote photoplethysmography (PPG) and study the effects of motion\naugmentation with respect to 1) the magnitude and 2) the type of motion. After\ntraining on motion-augmented versions of publicly available datasets, the\npresented inter-dataset results on five benchmark datasets show improvements of\nup to 75% over existing state-of-the-art results. Our findings illustrate the\nutility of motion transfer as a data augmentation technique for improving the\ngeneralization of models for camera-based physiological sensing. We release our\ncode and pre-trained models for using motion transfer as a data augmentation\ntechnique on our project page: https://motion-matters.github.io/\n","authors":["Akshay Paruchuri","Xin Liu","Yulu Pan","Shwetak Patel","Daniel McDuff","Soumyadip Sengupta"],"pdf_url":"https://arxiv.org/pdf/2303.12059v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.12060v1","updated":"2023-03-21T17:51:23Z","published":"2023-03-21T17:51:23Z","title":"VideoXum: Cross-modal Visual and Textural Summarization of Videos","summary":"  Video summarization aims to distill the most important information from a\nsource video to produce either an abridged clip or a textual narrative.\nTraditionally, different methods have been proposed depending on whether the\noutput is a video or text, thus ignoring the correlation between the two\nsemantically related tasks of visual summarization and textual summarization.\nWe propose a new joint video and text summarization task. The goal is to\ngenerate both a shortened video clip along with the corresponding textual\nsummary from a long video, collectively referred to as a cross-modal summary.\nThe generated shortened video clip and text narratives should be semantically\nwell aligned. To this end, we first build a large-scale human-annotated dataset\n-- VideoXum (X refers to different modalities). The dataset is reannotated\nbased on ActivityNet. After we filter out the videos that do not meet the\nlength requirements, 14,001 long videos remain in our new dataset. Each video\nin our reannotated dataset has human-annotated video summaries and the\ncorresponding narrative summaries. We then design a novel end-to-end model --\nVTSUM-BILP to address the challenges of our proposed task. Moreover, we propose\na new metric called VT-CLIPScore to help evaluate the semantic consistency of\ncross-modality summary. The proposed model achieves promising performance on\nthis new task and establishes a benchmark for future research.\n","authors":["Jingyang Lin","Hang Hua","Ming Chen","Yikang Li","Jenhao Hsiao","Chiuman Ho","Jiebo Luo"],"pdf_url":"https://arxiv.org/pdf/2303.12060v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.12054v1","updated":"2023-03-21T17:45:38Z","published":"2023-03-21T17:45:38Z","title":"Influencer Backdoor Attack on Semantic Segmentation","summary":"  When a small number of poisoned samples are injected into the training\ndataset of a deep neural network, the network can be induced to exhibit\nmalicious behavior during inferences, which poses potential threats to\nreal-world applications. While they have been intensively studied in\nclassification, backdoor attacks on semantic segmentation have been largely\noverlooked. Unlike classification, semantic segmentation aims to classify every\npixel within a given image. In this work, we explore backdoor attacks on\nsegmentation models to misclassify all pixels of a victim class by injecting a\nspecific trigger on non-victim pixels during inferences, which is dubbed\nInfluencer Backdoor Attack (IBA). IBA is expected to maintain the\nclassification accuracy of non-victim pixels and misleads classifications of\nall victim pixels in every single inference. Specifically, we consider two\ntypes of IBA scenarios, i.e., 1) Free-position IBA: the trigger can be\npositioned freely except for pixels of the victim class, and 2) Long-distance\nIBA: the trigger can only be positioned somewhere far from victim pixels, given\nthe possible practical constraint. Based on the context aggregation ability of\nsegmentation models, we propose techniques to improve IBA for the scenarios.\nConcretely, for free-position IBA, we propose a simple, yet effective Nearest\nNeighbor trigger injection strategy for poisoned sample creation. For\nlong-distance IBA, we propose a novel Pixel Random Labeling strategy. Our\nextensive experiments reveal that current segmentation models do suffer from\nbackdoor attacks, and verify that our proposed techniques can further increase\nattack performance.\n","authors":["Haoheng Lan","Jindong Gu","Philip Torr","Hengshuang Zhao"],"pdf_url":"https://arxiv.org/pdf/2303.12054v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.12051v1","updated":"2023-03-21T17:43:26Z","published":"2023-03-21T17:43:26Z","title":"A Novel and Optimal Spectral Method for Permutation Synchronization","summary":"  Permutation synchronization is an important problem in computer science that\nconstitutes the key step of many computer vision tasks. The goal is to recover\n$n$ latent permutations from their noisy and incomplete pairwise measurements.\nIn recent years, spectral methods have gained increasing popularity thanks to\ntheir simplicity and computational efficiency. Spectral methods utilize the\nleading eigenspace $U$ of the data matrix and its block submatrices\n$U_1,U_2,\\ldots, U_n$ to recover the permutations. In this paper, we propose a\nnovel and statistically optimal spectral algorithm. Unlike the existing methods\nwhich use $\\{U_jU_1^\\top\\}_{j\\geq 2}$, ours constructs an anchor matrix $M$ by\naggregating useful information from all the block submatrices and estimates the\nlatent permutations through $\\{U_jM^\\top\\}_{j\\geq 1}$. This modification\novercomes a crucial limitation of the existing methods caused by the repetitive\nuse of $U_1$ and leads to an improved numerical performance. To establish the\noptimality of the proposed method, we carry out a fine-grained spectral\nanalysis and obtain a sharp exponential error bound that matches the minimax\nrate.\n","authors":["Duc Nguyen","Anderson Ye Zhang"],"pdf_url":"https://arxiv.org/pdf/2303.12051v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.12050v1","updated":"2023-03-21T17:41:36Z","published":"2023-03-21T17:41:36Z","title":"CurveCloudNet: Processing Point Clouds with 1D Structure","summary":"  Modern depth sensors such as LiDAR operate by sweeping laser-beams across the\nscene, resulting in a point cloud with notable 1D curve-like structures. In\nthis work, we introduce a new point cloud processing scheme and backbone,\ncalled CurveCloudNet, which takes advantage of the curve-like structure\ninherent to these sensors. While existing backbones discard the rich 1D\ntraversal patterns and rely on Euclidean operations, CurveCloudNet\nparameterizes the point cloud as a collection of polylines (dubbed a \"curve\ncloud\"), establishing a local surface-aware ordering on the points. Our method\napplies curve-specific operations to process the curve cloud, including a\nsymmetric 1D convolution, a ball grouping for merging points along curves, and\nan efficient 1D farthest point sampling algorithm on curves. By combining these\ncurve operations with existing point-based operations, CurveCloudNet is an\nefficient, scalable, and accurate backbone with low GPU memory requirements.\nEvaluations on the ShapeNet, Kortx, Audi Driving, and nuScenes datasets\ndemonstrate that CurveCloudNet outperforms both point-based and sparse-voxel\nbackbones in various segmentation settings, notably scaling better to large\nscenes than point-based alternatives while exhibiting better single object\nperformance than sparse-voxel alternatives.\n","authors":["Colton Stearns","Jiateng Liu","Davis Rempe","Despoina Paschalidou","Jeong Joon Park","Sebastien Mascha","Leonidas J. Guibas"],"pdf_url":"https://arxiv.org/pdf/2303.12050v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.12048v1","updated":"2023-03-21T17:36:36Z","published":"2023-03-21T17:36:36Z","title":"Vox-E: Text-guided Voxel Editing of 3D Objects","summary":"  Large scale text-guided diffusion models have garnered significant attention\ndue to their ability to synthesize diverse images that convey complex visual\nconcepts. This generative power has more recently been leveraged to perform\ntext-to-3D synthesis. In this work, we present a technique that harnesses the\npower of latent diffusion models for editing existing 3D objects. Our method\ntakes oriented 2D images of a 3D object as input and learns a grid-based\nvolumetric representation of it. To guide the volumetric representation to\nconform to a target text prompt, we follow unconditional text-to-3D methods and\noptimize a Score Distillation Sampling (SDS) loss. However, we observe that\ncombining this diffusion-guided loss with an image-based regularization loss\nthat encourages the representation not to deviate too strongly from the input\nobject is challenging, as it requires achieving two conflicting goals while\nviewing only structure-and-appearance coupled 2D projections. Thus, we\nintroduce a novel volumetric regularization loss that operates directly in 3D\nspace, utilizing the explicit nature of our 3D representation to enforce\ncorrelation between the global structure of the original and edited object.\nFurthermore, we present a technique that optimizes cross-attention volumetric\ngrids to refine the spatial extent of the edits. Extensive experiments and\ncomparisons demonstrate the effectiveness of our approach in creating a myriad\nof edits which cannot be achieved by prior works.\n","authors":["Etai Sella","Gal Fiebelman","Peter Hedman","Hadar Averbuch-Elor"],"pdf_url":"https://arxiv.org/pdf/2303.12048v1.pdf","comment":"Project webpage: https://tau-vailab.github.io/Vox-E/"},{"id":"http://arxiv.org/abs/2303.12031v1","updated":"2023-03-21T17:16:01Z","published":"2023-03-21T17:16:01Z","title":"Semantic Latent Space Regression of Diffusion Autoencoders for Vertebral\n  Fracture Grading","summary":"  Vertebral fractures are a consequence of osteoporosis, with significant\nhealth implications for affected patients. Unfortunately, grading their\nseverity using CT exams is hard and subjective, motivating automated grading\nmethods. However, current approaches are hindered by imbalance and scarcity of\ndata and a lack of interpretability. To address these challenges, this paper\nproposes a novel approach that leverages unlabelled data to train a generative\nDiffusion Autoencoder (DAE) model as an unsupervised feature extractor. We\nmodel fracture grading as a continuous regression, which is more reflective of\nthe smooth progression of fractures. Specifically, we use a binary, supervised\nfracture classifier to construct a hyperplane in the DAE's latent space. We\nthen regress the severity of the fracture as a function of the distance to this\nhyperplane, calibrating the results to the Genant scale. Importantly, the\ngenerative nature of our method allows us to visualize different grades of a\ngiven vertebra, providing interpretability and insight into the features that\ncontribute to automated grading.\n","authors":["Matthias Keicher","Matan Atad","David Schinz","Alexandra S. Gersing","Sarah C. Foreman","Sophia S. Goller","Juergen Weissinger","Jon Rischewski","Anna-Sophia Dietrich","Benedikt Wiestler","Jan S. Kirschke","Nassir Navab"],"pdf_url":"https://arxiv.org/pdf/2303.12031v1.pdf","comment":"Under review"},{"id":"http://arxiv.org/abs/2212.04825v2","updated":"2023-03-21T17:13:58Z","published":"2022-12-09T18:59:57Z","title":"A Whac-A-Mole Dilemma: Shortcuts Come in Multiples Where Mitigating One\n  Amplifies Others","summary":"  Machine learning models have been found to learn shortcuts -- unintended\ndecision rules that are unable to generalize -- undermining models'\nreliability. Previous works address this problem under the tenuous assumption\nthat only a single shortcut exists in the training data. Real-world images are\nrife with multiple visual cues from background to texture. Key to advancing the\nreliability of vision systems is understanding whether existing methods can\novercome multiple shortcuts or struggle in a Whac-A-Mole game, i.e., where\nmitigating one shortcut amplifies reliance on others. To address this\nshortcoming, we propose two benchmarks: 1) UrbanCars, a dataset with precisely\ncontrolled spurious cues, and 2) ImageNet-W, an evaluation set based on\nImageNet for watermark, a shortcut we discovered affects nearly every modern\nvision model. Along with texture and background, ImageNet-W allows us to study\nmultiple shortcuts emerging from training on natural images. We find computer\nvision models, including large foundation models -- regardless of training set,\narchitecture, and supervision -- struggle when multiple shortcuts are present.\nEven methods explicitly designed to combat shortcuts struggle in a Whac-A-Mole\ndilemma. To tackle this challenge, we propose Last Layer Ensemble, a\nsimple-yet-effective method to mitigate multiple shortcuts without Whac-A-Mole\nbehavior. Our results surface multi-shortcut mitigation as an overlooked\nchallenge critical to advancing the reliability of vision systems. The datasets\nand code are released: https://github.com/facebookresearch/Whac-A-Mole.\n","authors":["Zhiheng Li","Ivan Evtimov","Albert Gordo","Caner Hazirbas","Tal Hassner","Cristian Canton Ferrer","Chenliang Xu","Mark Ibrahim"],"pdf_url":"https://arxiv.org/pdf/2212.04825v2.pdf","comment":"CVPR 2023. Code is available at\n  https://github.com/facebookresearch/Whac-A-Mole"},{"id":"http://arxiv.org/abs/2303.12027v1","updated":"2023-03-21T17:09:03Z","published":"2023-03-21T17:09:03Z","title":"Joint Visual Grounding and Tracking with Natural Language Specification","summary":"  Tracking by natural language specification aims to locate the referred target\nin a sequence based on the natural language description. Existing algorithms\nsolve this issue in two steps, visual grounding and tracking, and accordingly\ndeploy the separated grounding model and tracking model to implement these two\nsteps, respectively. Such a separated framework overlooks the link between\nvisual grounding and tracking, which is that the natural language descriptions\nprovide global semantic cues for localizing the target for both two steps.\nBesides, the separated framework can hardly be trained end-to-end. To handle\nthese issues, we propose a joint visual grounding and tracking framework, which\nreformulates grounding and tracking as a unified task: localizing the referred\ntarget based on the given visual-language references. Specifically, we propose\na multi-source relation modeling module to effectively build the relation\nbetween the visual-language references and the test image. In addition, we\ndesign a temporal modeling module to provide a temporal clue with the guidance\nof the global semantic information for our model, which effectively improves\nthe adaptability to the appearance variations of the target. Extensive\nexperimental results on TNL2K, LaSOT, OTB99, and RefCOCOg demonstrate that our\nmethod performs favorably against state-of-the-art algorithms for both tracking\nand grounding. Code is available at https://github.com/lizhou-cs/JointNLT.\n","authors":["Li Zhou","Zikun Zhou","Kaige Mao","Zhenyu He"],"pdf_url":"https://arxiv.org/pdf/2303.12027v1.pdf","comment":"Accepted by CVPR 2023"},{"id":"http://arxiv.org/abs/2210.11456v3","updated":"2023-03-21T16:57:57Z","published":"2022-10-20T17:54:03Z","title":"MixMask: Revisiting Masking Strategy for Siamese ConvNets","summary":"  Recent advances in self-supervised learning have integrated Masked Image\nModeling (MIM) and Siamese Networks into a unified framework that leverages the\nbenefits of both techniques. However, several issues remain unaddressed when\napplying conventional erase-based masking with Siamese ConvNets. These include\n(I) the inability to drop uninformative masked regions in ConvNets as they\nprocess data continuously, resulting in low training efficiency compared to ViT\nmodels; and (II) the mismatch between erase-based masking and the\ncontrastive-based objective in Siamese ConvNets, which differs from the MIM\napproach. In this paper, we propose a filling-based masking strategy called\nMixMask to prevent information incompleteness caused by the randomly erased\nregions in an image in the vanilla masking method. Furthermore, we introduce a\nflexible loss function design that considers the semantic distance change\nbetween two different mixed views to adapt the integrated architecture and\nprevent mismatches between the transformed input and objective in Masked\nSiamese ConvNets (MSCN). We conducted extensive experiments on various\ndatasets, including CIFAR-100, Tiny-ImageNet, and ImageNet-1K. The results\ndemonstrate that our proposed framework achieves superior accuracy on linear\nprobing, semi-supervised, and supervised finetuning, outperforming the\nstate-of-the-art MSCN by a significant margin. Additionally, we demonstrate the\nsuperiority of our approach in object detection and segmentation tasks. Our\nsource code is available at https://github.com/LightnessOfBeing/MixMask.\n","authors":["Kirill Vishniakov","Eric Xing","Zhiqiang Shen"],"pdf_url":"https://arxiv.org/pdf/2210.11456v3.pdf","comment":"Technical report. Code is available at\n  https://github.com/LightnessOfBeing/MixMask"},{"id":"http://arxiv.org/abs/2303.12017v1","updated":"2023-03-21T16:54:01Z","published":"2023-03-21T16:54:01Z","title":"Learning Optical Flow and Scene Flow with Bidirectional Camera-LiDAR\n  Fusion","summary":"  In this paper, we study the problem of jointly estimating the optical flow\nand scene flow from synchronized 2D and 3D data. Previous methods either employ\na complex pipeline that splits the joint task into independent stages, or fuse\n2D and 3D information in an ``early-fusion'' or ``late-fusion'' manner. Such\none-size-fits-all approaches suffer from a dilemma of failing to fully utilize\nthe characteristic of each modality or to maximize the inter-modality\ncomplementarity. To address the problem, we propose a novel end-to-end\nframework, which consists of 2D and 3D branches with multiple bidirectional\nfusion connections between them in specific layers. Different from previous\nwork, we apply a point-based 3D branch to extract the LiDAR features, as it\npreserves the geometric structure of point clouds. To fuse dense image features\nand sparse point features, we propose a learnable operator named bidirectional\ncamera-LiDAR fusion module (Bi-CLFM). We instantiate two types of the\nbidirectional fusion pipeline, one based on the pyramidal coarse-to-fine\narchitecture (dubbed CamLiPWC), and the other one based on the recurrent\nall-pairs field transforms (dubbed CamLiRAFT). On FlyingThings3D, both CamLiPWC\nand CamLiRAFT surpass all existing methods and achieve up to a 47.9\\% reduction\nin 3D end-point-error from the best published result. Our best-performing\nmodel, CamLiRAFT, achieves an error of 4.26\\% on the KITTI Scene Flow\nbenchmark, ranking 1st among all submissions with much fewer parameters.\nBesides, our methods have strong generalization performance and the ability to\nhandle non-rigid motion. Code is available at\nhttps://github.com/MCG-NJU/CamLiFlow.\n","authors":["Haisong Liu","Tao Lu","Yihui Xu","Jia Liu","Limin Wang"],"pdf_url":"https://arxiv.org/pdf/2303.12017v1.pdf","comment":"arXiv admin note: text overlap with arXiv:2111.10502"},{"id":"http://arxiv.org/abs/2303.12016v1","updated":"2023-03-21T16:52:08Z","published":"2023-03-21T16:52:08Z","title":"Automatic evaluation of herding behavior in towed fishing gear using\n  end-to-end training of CNN and attention-based networks","summary":"  This paper considers the automatic classification of herding behavior in the\ncluttered low-visibility environment that typically surrounds towed fishing\ngear. The paper compares three convolutional and attention-based deep action\nrecognition network architectures trained end-to-end on a small set of video\nsequences captured by a remotely controlled camera and classified by an expert\nin fishing technology. The sequences depict a scene in front of a fishing trawl\nwhere the conventional herding mechanism has been replaced by directed laser\nlight. The goal is to detect the presence of a fish in the sequence and\nclassify whether or not the fish reacts to the lasers. A two-stream CNN model,\na CNN-transformer hybrid, and a pure transformer model were trained end-to-end\nto achieve 63%, 54%, and 60% 10-fold classification accuracy on the three-class\ntask when compared to the human expert. Inspection of the activation maps\nlearned by the three networks raises questions about the attributes of the\nsequences the models may be learning, specifically whether changes in viewpoint\nintroduced by human camera operators that affect the position of laser lines in\nthe video frames may interfere with the classification. This underlines the\nimportance of careful experimental design when capturing scientific data for\nautomatic end-to-end evaluation and the usefulness of inspecting the trained\nmodels.\n","authors":["Orri Steinn Guðfinnsson","Týr Vilhjálmsson","Martin Eineborg","Torfi Thorhallsson"],"pdf_url":"https://arxiv.org/pdf/2303.12016v1.pdf","comment":"15 pages, 10 figures. To appear in Proceedings of the 5th Workshop on\n  Computer Vision for Analysis of Underwater Imagery (CVAUI) 2022, published as\n  part of the CVPR 2022 Proceedings"},{"id":"http://arxiv.org/abs/2303.12012v1","updated":"2023-03-21T16:49:41Z","published":"2023-03-21T16:49:41Z","title":"NeAT: Learning Neural Implicit Surfaces with Arbitrary Topologies from\n  Multi-view Images","summary":"  Recent progress in neural implicit functions has set new state-of-the-art in\nreconstructing high-fidelity 3D shapes from a collection of images. However,\nthese approaches are limited to closed surfaces as they require the surface to\nbe represented by a signed distance field. In this paper, we propose NeAT, a\nnew neural rendering framework that can learn implicit surfaces with arbitrary\ntopologies from multi-view images. In particular, NeAT represents the 3D\nsurface as a level set of a signed distance function (SDF) with a validity\nbranch for estimating the surface existence probability at the query positions.\nWe also develop a novel neural volume rendering method, which uses SDF and\nvalidity to calculate the volume opacity and avoids rendering points with low\nvalidity. NeAT supports easy field-to-mesh conversion using the classic\nMarching Cubes algorithm. Extensive experiments on DTU, MGN, and Deep Fashion\n3D datasets indicate that our approach is able to faithfully reconstruct both\nwatertight and non-watertight surfaces. In particular, NeAT significantly\noutperforms the state-of-the-art methods in the task of open surface\nreconstruction both quantitatively and qualitatively.\n","authors":["Xiaoxu Meng","Weikai Chen","Bo Yang"],"pdf_url":"https://arxiv.org/pdf/2303.12012v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.10766v2","updated":"2023-03-21T16:39:10Z","published":"2023-03-19T20:52:44Z","title":"Multi-modal reward for visual relationships-based image captioning","summary":"  Deep neural networks have achieved promising results in automatic image\ncaptioning due to their effective representation learning and context-based\ncontent generation capabilities. As a prominent type of deep features used in\nmany of the recent image captioning methods, the well-known bottomup features\nprovide a detailed representation of different objects of the image in\ncomparison with the feature maps directly extracted from the raw image.\nHowever, the lack of high-level semantic information about the relationships\nbetween these objects is an important drawback of bottom-up features, despite\ntheir expensive and resource-demanding extraction procedure. To take advantage\nof visual relationships in caption generation, this paper proposes a deep\nneural network architecture for image captioning based on fusing the visual\nrelationships information extracted from an image's scene graph with the\nspatial feature maps of the image. A multi-modal reward function is then\nintroduced for deep reinforcement learning of the proposed network using a\ncombination of language and vision similarities in a common embedding space.\nThe results of extensive experimentation on the MSCOCO dataset show the\neffectiveness of using visual relationships in the proposed captioning method.\nMoreover, the results clearly indicate that the proposed multi-modal reward in\ndeep reinforcement learning leads to better model optimization, outperforming\nseveral state-of-the-art image captioning algorithms, while using light and\neasy to extract image features. A detailed experimental study of the components\nconstituting the proposed method is also presented.\n","authors":["Ali Abedi","Hossein Karshenas","Peyman Adibi"],"pdf_url":"https://arxiv.org/pdf/2303.10766v2.pdf","comment":"18 pages, 10 figures"},{"id":"http://arxiv.org/abs/2111.07632v2","updated":"2023-03-21T16:36:35Z","published":"2021-11-15T09:35:54Z","title":"CoReS: Compatible Representations via Stationarity","summary":"  In this paper, we propose a novel method to learn internal feature\nrepresentation models that are \\textit{compatible} with previously learned\nones. Compatible features enable for direct comparison of old and new learned\nfeatures, allowing them to be used interchangeably over time. This eliminates\nthe need for visual search systems to extract new features for all previously\nseen images in the gallery-set when sequentially upgrading the representation\nmodel. Extracting new features is typically quite expensive or infeasible in\nthe case of very large gallery-sets and/or real time systems (i.e.,\nface-recognition systems, social networks, life-long learning systems, robotics\nand surveillance systems). Our approach, called Compatible Representations via\nStationarity (CoReS), achieves compatibility by encouraging stationarity to the\nlearned representation model without relying on previously learned models.\nStationarity allows features' statistical properties not to change under time\nshift so that the current learned features are inter-operable with the old\nones. We evaluate single and sequential multi-model upgrading in growing\nlarge-scale training datasets and we show that our method improves the\nstate-of-the-art in achieving compatible features by a large margin. In\nparticular, upgrading ten times with training data taken from CASIA-WebFace and\nevaluating in Labeled Face in the Wild (LFW), we obtain a 49\\% increase in\nmeasuring the average number of times compatibility is achieved, which is a\n544\\% relative improvement over previous state-of-the-art.\n","authors":["Niccolo Biondi","Federico Pernici","Matteo Bruni","Alberto Del Bimbo"],"pdf_url":"https://arxiv.org/pdf/2111.07632v2.pdf","comment":"in IEEE Transactions on Pattern Analysis and Machine Intelligence"},{"id":"http://arxiv.org/abs/2302.06891v3","updated":"2023-03-21T16:33:56Z","published":"2023-02-14T08:27:42Z","title":"UKnow: A Unified Knowledge Protocol for Common-Sense Reasoning and\n  Vision-Language Pre-training","summary":"  This work presents a unified knowledge protocol, called UKnow, which\nfacilitates knowledge-based studies from the perspective of data. Particularly\nfocusing on visual and linguistic modalities, we categorize data knowledge into\nfive unit types, namely, in-image, in-text, cross-image, cross-text, and\nimage-text, and set up an efficient pipeline to help construct the multimodal\nknowledge graph from any data collection. Thanks to the logical information\nnaturally contained in knowledge graph, organizing datasets under UKnow format\nopens up more possibilities of data usage compared to the commonly used\nimage-text pairs. Following UKnow protocol, we collect, from public\ninternational news, a large-scale multimodal knowledge graph dataset that\nconsists of 1,388,568 nodes (with 571,791 vision-related ones) and 3,673,817\ntriplets. The dataset is also annotated with rich event tags, including 11\ncoarse labels and 9,185 fine labels. Experiments on four benchmarks demonstrate\nthe potential of UKnow in supporting common-sense reasoning and boosting\nvision-language pre-training with a single dataset, benefiting from its unified\nform of knowledge organization. Code, dataset, and models will be made publicly\navailable.\n","authors":["Biao Gong","Xiaoying Xie","Yutong Feng","Yiliang Lv","Yujun Shen","Deli Zhao"],"pdf_url":"https://arxiv.org/pdf/2302.06891v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.12001v1","updated":"2023-03-21T16:33:40Z","published":"2023-03-21T16:33:40Z","title":"Visual Representation Learning from Unlabeled Video using Contrastive\n  Masked Autoencoders","summary":"  Masked Autoencoders (MAEs) learn self-supervised representations by randomly\nmasking input image patches and a reconstruction loss. Alternatively,\ncontrastive learning self-supervised methods encourage two versions of the same\ninput to have a similar representation, while pulling apart the representations\nfor different inputs. We propose ViC-MAE, a general method that combines both\nMAE and contrastive learning by pooling the local feature representations\nlearned under the MAE reconstruction objective and leveraging this global\nrepresentation under a contrastive objective across video frames. We show that\nvisual representations learned under ViC-MAE generalize well to both video\nclassification and image classification tasks. Using a backbone ViT-B/16\nnetwork pre-trained on the Moments in Time (MiT) dataset, we obtain\nstate-of-the-art transfer learning from video to images on Imagenet-1k by\nimproving 1.58% in absolute top-1 accuracy from a recent previous work.\nMoreover, our method maintains a competitive transfer-learning performance of\n81.50% top-1 accuracy on the Kinetics-400 video classification benchmark. In\naddition, we show that despite its simplicity, ViC-MAE yields improved results\ncompared to combining MAE pre-training with previously proposed contrastive\nobjectives such as VicReg and SiamSiam.\n","authors":["Jefferson Hernandez","Ruben Villegas","Vicente Ordonez"],"pdf_url":"https://arxiv.org/pdf/2303.12001v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.11997v1","updated":"2023-03-21T16:31:53Z","published":"2023-03-21T16:31:53Z","title":"E-MLB: Multilevel Benchmark for Event-Based Camera Denoising","summary":"  Event cameras, such as dynamic vision sensors (DVS), are biologically\ninspired vision sensors that have advanced over conventional cameras in high\ndynamic range, low latency and low power consumption, showing great application\npotential in many fields. Event cameras are more sensitive to junction leakage\ncurrent and photocurrent as they output differential signals, losing the\nsmoothing function of the integral imaging process in the RGB camera. The\nlogarithmic conversion further amplifies noise, especially in low-contrast\nconditions. Recently, researchers proposed a series of datasets and evaluation\nmetrics but limitations remain: 1) the existing datasets are small in scale and\ninsufficient in noise diversity, which cannot reflect the authentic working\nenvironments of event cameras; and 2) the existing denoising evaluation metrics\nare mostly referenced evaluation metrics, relying on APS information or manual\nannotation. To address the above issues, we construct a large-scale event\ndenoising dataset (multilevel benchmark for event denoising, E-MLB) for the\nfirst time, which consists of 100 scenes, each with four noise levels, that is\n12 times larger than the largest existing denoising dataset. We also propose\nthe first nonreference event denoising metric, the event structural ratio\n(ESR), which measures the structural intensity of given events. ESR is inspired\nby the contrast metric, but is independent of the number of events and\nprojection direction. Based on the proposed benchmark and ESR, we evaluate the\nmost representative denoising algorithms, including classic and SOTA, and\nprovide denoising baselines under various scenes and noise levels. The\ncorresponding results and codes are available at\nhttps://github.com/KugaMaxx/cuke-emlb.\n","authors":["Saizhe Ding","Jinze Chen","Yang Wang","Yu Kang","Weiguo Song","Jie Cheng","Yang Cao"],"pdf_url":"https://arxiv.org/pdf/2303.11997v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.11989v1","updated":"2023-03-21T16:21:02Z","published":"2023-03-21T16:21:02Z","title":"Text2Room: Extracting Textured 3D Meshes from 2D Text-to-Image Models","summary":"  We present Text2Room, a method for generating room-scale textured 3D meshes\nfrom a given text prompt as input. To this end, we leverage pre-trained 2D\ntext-to-image models to synthesize a sequence of images from different poses.\nIn order to lift these outputs into a consistent 3D scene representation, we\ncombine monocular depth estimation with a text-conditioned inpainting model.\nThe core idea of our approach is a tailored viewpoint selection such that the\ncontent of each image can be fused into a seamless, textured 3D mesh. More\nspecifically, we propose a continuous alignment strategy that iteratively fuses\nscene frames with the existing geometry to create a seamless mesh. Unlike\nexisting works that focus on generating single objects or zoom-out trajectories\nfrom text, our method generates complete 3D scenes with multiple objects and\nexplicit 3D geometry. We evaluate our approach using qualitative and\nquantitative metrics, demonstrating it as the first method to generate\nroom-scale 3D geometry with compelling textures from only text as input.\n","authors":["Lukas Höllein","Ang Cao","Andrew Owens","Justin Johnson","Matthias Nießner"],"pdf_url":"https://arxiv.org/pdf/2303.11989v1.pdf","comment":"video: https://youtu.be/fjRnFL91EZc project page:\n  https://lukashoel.github.io/text-to-room/ code:\n  https://github.com/lukasHoel/text2room"},{"id":"http://arxiv.org/abs/2212.07398v4","updated":"2023-03-21T16:16:41Z","published":"2022-12-14T18:31:47Z","title":"Policy Adaptation from Foundation Model Feedback","summary":"  Recent progress on vision-language foundation models have brought significant\nadvancement to building general-purpose robots. By using the pre-trained models\nto encode the scene and instructions as inputs for decision making, the\ninstruction-conditioned policy can generalize across different objects and\ntasks. While this is encouraging, the policy still fails in most cases given an\nunseen task or environment. In this work, we propose Policy Adaptation from\nFoundation model Feedback (PAFF). When deploying the trained policy to a new\ntask or a new environment, we first let the policy play with randomly generated\ninstructions to record the demonstrations. While the execution could be wrong,\nwe can use the pre-trained foundation models to provide feedback to relabel the\ndemonstrations. This automatically provides new pairs of\ndemonstration-instruction data for policy fine-tuning. We evaluate our method\non a broad range of experiments with the focus on generalization on unseen\nobjects, unseen tasks, unseen environments, and sim-to-real transfer. We show\nPAFF improves baselines by a large margin in all cases. Our project page is\navailable at https://geyuying.github.io/PAFF/\n","authors":["Yuying Ge","Annabella Macaluso","Li Erran Li","Ping Luo","Xiaolong Wang"],"pdf_url":"https://arxiv.org/pdf/2212.07398v4.pdf","comment":"Accepted by CVPR 2023; Project page: https://geyuying.github.io/PAFF/"},{"id":"http://arxiv.org/abs/2303.09554v3","updated":"2023-03-21T16:09:25Z","published":"2023-03-16T17:59:22Z","title":"PartNeRF: Generating Part-Aware Editable 3D Shapes without 3D\n  Supervision","summary":"  Impressive progress in generative models and implicit representations gave\nrise to methods that can generate 3D shapes of high quality. However, being\nable to locally control and edit shapes is another essential property that can\nunlock several content creation applications. Local control can be achieved\nwith part-aware models, but existing methods require 3D supervision and cannot\nproduce textures. In this work, we devise PartNeRF, a novel part-aware\ngenerative model for editable 3D shape synthesis that does not require any\nexplicit 3D supervision. Our model generates objects as a set of locally\ndefined NeRFs, augmented with an affine transformation. This enables several\nediting operations such as applying transformations on parts, mixing parts from\ndifferent objects etc. To ensure distinct, manipulable parts we enforce a hard\nassignment of rays to parts that makes sure that the color of each ray is only\ndetermined by a single NeRF. As a result, altering one part does not affect the\nappearance of the others. Evaluations on various ShapeNet categories\ndemonstrate the ability of our model to generate editable 3D objects of\nimproved fidelity, compared to previous part-based generative approaches that\nrequire 3D supervision or models relying on NeRFs.\n","authors":["Konstantinos Tertikas","Despoina Paschalidou","Boxiao Pan","Jeong Joon Park","Mikaela Angelina Uy","Ioannis Emiris","Yannis Avrithis","Leonidas Guibas"],"pdf_url":"https://arxiv.org/pdf/2303.09554v3.pdf","comment":"To appear in CVPR 2023, Project Page:\n  https://ktertikas.github.io/part_nerf"},{"id":"http://arxiv.org/abs/2203.15793v4","updated":"2023-03-21T16:06:20Z","published":"2022-03-29T17:50:43Z","title":"Instance Relation Graph Guided Source-Free Domain Adaptive Object\n  Detection","summary":"  Unsupervised Domain Adaptation (UDA) is an effective approach to tackle the\nissue of domain shift. Specifically, UDA methods try to align the source and\ntarget representations to improve the generalization on the target domain.\nFurther, UDA methods work under the assumption that the source data is\naccessible during the adaptation process. However, in real-world scenarios, the\nlabelled source data is often restricted due to privacy regulations, data\ntransmission constraints, or proprietary data concerns. The Source-Free Domain\nAdaptation (SFDA) setting aims to alleviate these concerns by adapting a\nsource-trained model for the target domain without requiring access to the\nsource data. In this paper, we explore the SFDA setting for the task of\nadaptive object detection. To this end, we propose a novel training strategy\nfor adapting a source-trained object detector to the target domain without\nsource data. More precisely, we design a novel contrastive loss to enhance the\ntarget representations by exploiting the objects relations for a given target\ndomain input. These object instance relations are modelled using an Instance\nRelation Graph (IRG) network, which are then used to guide the contrastive\nrepresentation learning. In addition, we utilize a student-teacher based\nknowledge distillation strategy to avoid overfitting to the noisy pseudo-labels\ngenerated by the source-trained model. Extensive experiments on multiple object\ndetection benchmark datasets show that the proposed approach is able to\nefficiently adapt source-trained object detectors to the target domain,\noutperforming previous state-of-the-art domain adaptive detection methods. Code\nand models are provided in\n\\href{https://viudomain.github.io/irg-sfda-web/}{https://viudomain.github.io/irg-sfda-web/}.\n","authors":["Vibashan VS","Poojan Oza","Vishal M. Patel"],"pdf_url":"https://arxiv.org/pdf/2203.15793v4.pdf","comment":"Accepted to CVPR 2023. Project site:\n  \\href{https://viudomain.github.io/irg-sfda-web/}{https://viudomain.github.io/irg-sfda-web/}"},{"id":"http://arxiv.org/abs/2303.11971v1","updated":"2023-03-21T16:04:09Z","published":"2023-03-21T16:04:09Z","title":"Defect Detection Approaches Based on Simulated Reference Image","summary":"  This work is addressing the problem of defect anomaly detection based on a\nclean reference image. Specifically, we focus on SEM semiconductor defects in\naddition to several natural image anomalies. There are well-known methods to\ncreate a simulation of an artificial reference image by its defect specimen. In\nthis work, we introduce several applications for this capability, that the\nsimulated reference is beneficial for improving their results. Among these\ndefect detection methods are classic computer vision applied on\ndifference-image, supervised deep-learning (DL) based on human labels, and\nunsupervised DL which is trained on feature-level patterns of normal reference\nimages. We show in this study how to incorporate correctly the simulated\nreference image for these defect and anomaly detection applications. As our\nexperiment demonstrates, simulated reference achieves higher performance than\nthe real reference of an image of a defect and anomaly. This advantage of\nsimulated reference occurs mainly due to the less noise and geometric\nvariations together with better alignment and registration to the original\ndefect background.\n","authors":["Nati Ofir","Yotam Ben Shoshan","Ran Badanes","Boris Sherman"],"pdf_url":"https://arxiv.org/pdf/2303.11971v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2210.11035v3","updated":"2023-03-21T16:03:50Z","published":"2022-10-20T06:08:03Z","title":"PointTAD: Multi-Label Temporal Action Detection with Learnable Query\n  Points","summary":"  Traditional temporal action detection (TAD) usually handles untrimmed videos\nwith small number of action instances from a single label (e.g., ActivityNet,\nTHUMOS). However, this setting might be unrealistic as different classes of\nactions often co-occur in practice. In this paper, we focus on the task of\nmulti-label temporal action detection that aims to localize all action\ninstances from a multi-label untrimmed video. Multi-label TAD is more\nchallenging as it requires for fine-grained class discrimination within a\nsingle video and precise localization of the co-occurring instances. To\nmitigate this issue, we extend the sparse query-based detection paradigm from\nthe traditional TAD and propose the multi-label TAD framework of PointTAD.\nSpecifically, our PointTAD introduces a small set of learnable query points to\nrepresent the important frames of each action instance. This point-based\nrepresentation provides a flexible mechanism to localize the discriminative\nframes at boundaries and as well the important frames inside the action.\nMoreover, we perform the action decoding process with the Multi-level\nInteractive Module to capture both point-level and instance-level action\nsemantics. Finally, our PointTAD employs an end-to-end trainable framework\nsimply based on RGB input for easy deployment. We evaluate our proposed method\non two popular benchmarks and introduce the new metric of detection-mAP for\nmulti-label TAD. Our model outperforms all previous methods by a large margin\nunder the detection-mAP metric, and also achieves promising results under the\nsegmentation-mAP metric. Code is available at\nhttps://github.com/MCG-NJU/PointTAD.\n","authors":["Jing Tan","Xiaotong Zhao","Xintian Shi","Bin Kang","Limin Wang"],"pdf_url":"https://arxiv.org/pdf/2210.11035v3.pdf","comment":"NeurIPS 2022 camera ready version"},{"id":"http://arxiv.org/abs/2303.11969v1","updated":"2023-03-21T16:01:55Z","published":"2023-03-21T16:01:55Z","title":"Explain To Me: Salience-Based Explainability for Synthetic Face\n  Detection Models","summary":"  The performance of convolutional neural networks has continued to improve\nover the last decade. At the same time, as model complexity grows, it becomes\nincreasingly more difficult to explain model decisions. Such explanations may\nbe of critical importance for reliable operation of human-machine pairing\nsetups, or for model selection when the \"best\" model among many\nequally-accurate models must be established. Saliency maps represent one\npopular way of explaining model decisions by highlighting image regions models\ndeem important when making a prediction. However, examining salience maps at\nscale is not practical. In this paper, we propose five novel methods of\nleveraging model salience to explain a model behavior at scale. These methods\nask: (a) what is the average entropy for a model's salience maps, (b) how does\nmodel salience change when fed out-of-set samples, (c) how closely does model\nsalience follow geometrical transformations, (d) what is the stability of model\nsalience across independent training runs, and (e) how does model salience\nreact to salience-guided image degradations. To assess the proposed measures on\na concrete and topical problem, we conducted a series of experiments for the\ntask of synthetic face detection with two types of models: those trained\ntraditionally with cross-entropy loss, and those guided by human salience when\ntraining to increase model generalizability. These two types of models are\ncharacterized by different, interpretable properties of their salience maps,\nwhich allows for the evaluation of the correctness of the proposed measures. We\noffer source codes for each measure along with this paper.\n","authors":["Colton Crum","Patrick Tinsley","Aidan Boyd","Jacob Piland","Christopher Sweet","Timothy Kelley","Kevin Bowyer","Adam Czajka"],"pdf_url":"https://arxiv.org/pdf/2303.11969v1.pdf","comment":"13 pages, 10 figures"},{"id":"http://arxiv.org/abs/2303.11963v1","updated":"2023-03-21T15:50:08Z","published":"2023-03-21T15:50:08Z","title":"NEMTO: Neural Environment Matting for Novel View and Relighting\n  Synthesis of Transparent Objects","summary":"  We propose NEMTO, the first end-to-end neural rendering pipeline to model 3D\ntransparent objects with complex geometry and unknown indices of refraction.\nCommonly used appearance modeling such as the Disney BSDF model cannot\naccurately address this challenging problem due to the complex light paths\nbending through refractions and the strong dependency of surface appearance on\nillumination. With 2D images of the transparent object as input, our method is\ncapable of high-quality novel view and relighting synthesis. We leverage\nimplicit Signed Distance Functions (SDF) to model the object geometry and\npropose a refraction-aware ray bending network to model the effects of light\nrefraction within the object. Our ray bending network is more tolerant to\ngeometric inaccuracies than traditional physically-based methods for rendering\ntransparent objects. We provide extensive evaluations on both synthetic and\nreal-world datasets to demonstrate our high-quality synthesis and the\napplicability of our method.\n","authors":["Dongqing Wang","Tong Zhang","Sabine Süsstrunk"],"pdf_url":"https://arxiv.org/pdf/2303.11963v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.11950v1","updated":"2023-03-21T15:41:57Z","published":"2023-03-21T15:41:57Z","title":"Learning A Sparse Transformer Network for Effective Image Deraining","summary":"  Transformers-based methods have achieved significant performance in image\nderaining as they can model the non-local information which is vital for\nhigh-quality image reconstruction. In this paper, we find that most existing\nTransformers usually use all similarities of the tokens from the query-key\npairs for the feature aggregation. However, if the tokens from the query are\ndifferent from those of the key, the self-attention values estimated from these\ntokens also involve in feature aggregation, which accordingly interferes with\nthe clear image restoration. To overcome this problem, we propose an effective\nDeRaining network, Sparse Transformer (DRSformer) that can adaptively keep the\nmost useful self-attention values for feature aggregation so that the\naggregated features better facilitate high-quality image reconstruction.\nSpecifically, we develop a learnable top-k selection operator to adaptively\nretain the most crucial attention scores from the keys for each query for\nbetter feature aggregation. Simultaneously, as the naive feed-forward network\nin Transformers does not model the multi-scale information that is important\nfor latent clear image restoration, we develop an effective mixed-scale\nfeed-forward network to generate better features for image deraining. To learn\nan enriched set of hybrid features, which combines local context from CNN\noperators, we equip our model with mixture of experts feature compensator to\npresent a cooperation refinement deraining scheme. Extensive experimental\nresults on the commonly used benchmarks demonstrate that the proposed method\nachieves favorable performance against state-of-the-art approaches. The source\ncode and trained models are available at\nhttps://github.com/cschenxiang/DRSformer.\n","authors":["Xiang Chen","Hao Li","Mingqiang Li","Jinshan Pan"],"pdf_url":"https://arxiv.org/pdf/2303.11950v1.pdf","comment":"Accepted as a highlight paper in CVPR 2023"},{"id":"http://arxiv.org/abs/2303.11938v1","updated":"2023-03-21T15:38:26Z","published":"2023-03-21T15:38:26Z","title":"3D-CLFusion: Fast Text-to-3D Rendering with Contrastive Latent Diffusion","summary":"  We tackle the task of text-to-3D creation with pre-trained latent-based NeRFs\n(NeRFs that generate 3D objects given input latent code). Recent works such as\nDreamFusion and Magic3D have shown great success in generating 3D content using\nNeRFs and text prompts, but the current approach of optimizing a NeRF for every\ntext prompt is 1) extremely time-consuming and 2) often leads to low-resolution\noutputs. To address these challenges, we propose a novel method named\n3D-CLFusion which leverages the pre-trained latent-based NeRFs and performs\nfast 3D content creation in less than a minute. In particular, we introduce a\nlatent diffusion prior network for learning the w latent from the input CLIP\ntext/image embeddings. This pipeline allows us to produce the w latent without\nfurther optimization during inference and the pre-trained NeRF is able to\nperform multi-view high-resolution 3D synthesis based on the latent. We note\nthat the novelty of our model lies in that we introduce contrastive learning\nduring training the diffusion prior which enables the generation of the valid\nview-invariant latent code. We demonstrate through experiments the\neffectiveness of our proposed view-invariant diffusion process for fast\ntext-to-3D creation, e.g., 100 times faster than DreamFusion. We note that our\nmodel is able to serve as the role of a plug-and-play tool for text-to-3D with\npre-trained NeRFs.\n","authors":["Yu-Jhe Li","Kris Kitani"],"pdf_url":"https://arxiv.org/pdf/2303.11938v1.pdf","comment":"15 pages. Non-CMU authors are currently hidden due to an internal\n  legal review in progress of their company"},{"id":"http://arxiv.org/abs/2303.11932v1","updated":"2023-03-21T15:34:50Z","published":"2023-03-21T15:34:50Z","title":"Using Explanations to Guide Models","summary":"  Deep neural networks are highly performant, but might base their decision on\nspurious or background features that co-occur with certain classes, which can\nhurt generalization. To mitigate this issue, the usage of 'model guidance' has\ngained popularity recently: for this, models are guided to be \"right for the\nright reasons\" by regularizing the models' explanations to highlight the right\nfeatures. Experimental validation of these approaches has thus far however been\nlimited to relatively simple and / or synthetic datasets. To gain a better\nunderstanding of which model-guiding approaches actually transfer to more\nchallenging real-world datasets, in this work we conduct an in-depth evaluation\nacross various loss functions, attribution methods, models, and 'guidance\ndepths' on the PASCAL VOC 2007 and MS COCO 2014 datasets, and show that model\nguidance can sometimes even improve model performance. In this context, we\nfurther propose a novel energy loss, show its effectiveness in directing the\nmodel to focus on object features. We also show that these gains can be\nachieved even with a small fraction (e.g. 1%) of bounding box annotations,\nhighlighting the cost effectiveness of this approach. Lastly, we show that this\napproach can also improve generalization under distribution shifts. Code will\nbe made available.\n","authors":["Sukrut Rao","Moritz Böhle","Amin Parchami-Araghi","Bernt Schiele"],"pdf_url":"https://arxiv.org/pdf/2303.11932v1.pdf","comment":"38 pages, 35 figures, 4 tables"},{"id":"http://arxiv.org/abs/2106.09614v3","updated":"2023-03-21T15:25:02Z","published":"2021-06-17T15:52:19Z","title":"Robust Model-based Face Reconstruction through Weakly-Supervised Outlier\n  Segmentation","summary":"  In this work, we aim to enhance model-based face reconstruction by avoiding\nfitting the model to outliers, i.e. regions that cannot be well-expressed by\nthe model such as occluders or make-up. The core challenge for localizing\noutliers is that they are highly variable and difficult to annotate. To\novercome this challenging problem, we introduce a joint Face-autoencoder and\noutlier segmentation approach (FOCUS).In particular, we exploit the fact that\nthe outliers cannot be fitted well by the face model and hence can be localized\nwell given a high-quality model fitting. The main challenge is that the model\nfitting and the outlier segmentation are mutually dependent on each other, and\nneed to be inferred jointly. We resolve this chicken-and-egg problem with an\nEM-type training strategy, where a face autoencoder is trained jointly with an\noutlier segmentation network. This leads to a synergistic effect, in which the\nsegmentation network prevents the face encoder from fitting to the outliers,\nenhancing the reconstruction quality. The improved 3D face reconstruction, in\nturn, enables the segmentation network to better predict the outliers. To\nresolve the ambiguity between outliers and regions that are difficult to fit,\nsuch as eyebrows, we build a statistical prior from synthetic data that\nmeasures the systematic bias in model fitting. Experiments on the NoW testset\ndemonstrate that FOCUS achieves SOTA 3D face reconstruction performance among\nall baselines that are trained without 3D annotation. Moreover, our results on\nCelebA-HQ and the AR database show that the segmentation network can localize\noccluders accurately despite being trained without any segmentation annotation.\n","authors":["Chunlu Li","Andreas Morel-Forster","Thomas Vetter","Bernhard Egger","Adam Kortylewski"],"pdf_url":"https://arxiv.org/pdf/2106.09614v3.pdf","comment":"20 pages, CVPR2023"},{"id":"http://arxiv.org/abs/2211.14308v2","updated":"2023-03-21T15:22:30Z","published":"2022-11-25T18:59:46Z","title":"WALDO: Future Video Synthesis using Object Layer Decomposition and\n  Parametric Flow Prediction","summary":"  This paper presents WALDO (WArping Layer-Decomposed Objects), a novel\napproach to the prediction of future video frames from past ones. Individual\nimages are decomposed into multiple layers combining object masks and a small\nset of control points. The layer structure is shared across all frames in each\nvideo to build dense inter-frame connections. Complex scene motions are modeled\nby combining parametric geometric transformations associated with individual\nlayers, and video synthesis is broken down into discovering the layers\nassociated with past frames, predicting the corresponding transformations for\nupcoming ones and warping the associated object regions accordingly, and\nfilling in the remaining image parts. Extensive experiments on multiple\nbenchmarks including urban videos (Cityscapes and KITTI) and videos featuring\nnonrigid motions (UCF-Sports and H3.6M), show that our method consistently\noutperforms the state of the art by a significant margin in every case. Code,\npretrained models, and video samples synthesized by our approach can be found\nin the project webpage https://16lemoing.github.io/waldo.\n","authors":["Guillaume Le Moing","Jean Ponce","Cordelia Schmid"],"pdf_url":"https://arxiv.org/pdf/2211.14308v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.11926v1","updated":"2023-03-21T15:19:20Z","published":"2023-03-21T15:19:20Z","title":"Exploring Object-Centric Temporal Modeling for Efficient Multi-View 3D\n  Object Detection","summary":"  In this paper, we propose a long-sequence modeling framework, named\nStreamPETR, for multi-view 3D object detection. Built upon the sparse query\ndesign in the PETR series, we systematically develop an object-centric temporal\nmechanism. The model is performed in an online manner and the long-term\nhistorical information is propagated through object queries frame by frame.\nBesides, we introduce a motion-aware layer normalization to model the movement\nof the objects. StreamPETR achieves significant performance improvements only\nwith negligible computation cost, compared to the single-frame baseline. On the\nstandard nuScenes benchmark, it reaches a new state-of-the-art performance\n(63.6% NDS). The lightweight version realizes 45.0% mAP and 31.7 FPS,\noutperforming the state-of-the-art method (SOLOFusion) by 2.3% mAP and 1.8x\nfaster FPS. Code will be available at\nhttps://github.com/exiawsh/StreamPETR.git.\n","authors":["Shihao Wang","Yingfei Liu","Tiancai Wang","Ying Li","Xiangyu Zhang"],"pdf_url":"https://arxiv.org/pdf/2303.11926v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.11923v1","updated":"2023-03-21T15:15:21Z","published":"2023-03-21T15:15:21Z","title":"Performance-aware Approximation of Global Channel Pruning for Multitask\n  CNNs","summary":"  Global channel pruning (GCP) aims to remove a subset of channels (filters)\nacross different layers from a deep model without hurting the performance.\nPrevious works focus on either single task model pruning or simply adapting it\nto multitask scenario, and still face the following problems when handling\nmultitask pruning: 1) Due to the task mismatch, a well-pruned backbone for\nclassification task focuses on preserving filters that can extract\ncategory-sensitive information, causing filters that may be useful for other\ntasks to be pruned during the backbone pruning stage; 2) For multitask\npredictions, different filters within or between layers are more closely\nrelated and interacted than that for single task prediction, making multitask\npruning more difficult. Therefore, aiming at multitask model compression, we\npropose a Performance-Aware Global Channel Pruning (PAGCP) framework. We first\ntheoretically present the objective for achieving superior GCP, by considering\nthe joint saliency of filters from intra- and inter-layers. Then a sequentially\ngreedy pruning strategy is proposed to optimize the objective, where a\nperformance-aware oracle criterion is developed to evaluate sensitivity of\nfilters to each task and preserve the globally most task-related filters.\nExperiments on several multitask datasets show that the proposed PAGCP can\nreduce the FLOPs and parameters by over 60% with minor performance drop, and\nachieves 1.2x$\\sim$3.3x acceleration on both cloud and mobile platforms.\n","authors":["Hancheng Ye","Bo Zhang","Tao Chen","Jiayuan Fan","Bin Wang"],"pdf_url":"https://arxiv.org/pdf/2303.11923v1.pdf","comment":"Accepted for publication in T-PAMI, our code is available at\n  http://www.github.com/HankYe/PAGCP.git"},{"id":"http://arxiv.org/abs/2303.11921v1","updated":"2023-03-21T15:12:20Z","published":"2023-03-21T15:12:20Z","title":"Context De-confounded Emotion Recognition","summary":"  Context-Aware Emotion Recognition (CAER) is a crucial and challenging task\nthat aims to perceive the emotional states of the target person with contextual\ninformation. Recent approaches invariably focus on designing sophisticated\narchitectures or mechanisms to extract seemingly meaningful representations\nfrom subjects and contexts. However, a long-overlooked issue is that a context\nbias in existing datasets leads to a significantly unbalanced distribution of\nemotional states among different context scenarios. Concretely, the harmful\nbias is a confounder that misleads existing models to learn spurious\ncorrelations based on conventional likelihood estimation, significantly\nlimiting the models' performance. To tackle the issue, this paper provides a\ncausality-based perspective to disentangle the models from the impact of such\nbias, and formulate the causalities among variables in the CAER task via a\ntailored causal graph. Then, we propose a Contextual Causal Intervention Module\n(CCIM) based on the backdoor adjustment to de-confound the confounder and\nexploit the true causal effect for model training. CCIM is plug-in and\nmodel-agnostic, which improves diverse state-of-the-art approaches by\nconsiderable margins. Extensive experiments on three benchmark datasets\ndemonstrate the effectiveness of our CCIM and the significance of causal\ninsight.\n","authors":["Dingkang Yang","Zhaoyu Chen","Yuzheng Wang","Shunli Wang","Mingcheng Li","Siao Liu","Xiao Zhao","Shuai Huang","Zhiyan Dong","Peng Zhai","Lihua Zhang"],"pdf_url":"https://arxiv.org/pdf/2303.11921v1.pdf","comment":"Accepted by CVPR 2023"},{"id":"http://arxiv.org/abs/2107.02600v2","updated":"2023-03-21T15:11:44Z","published":"2021-07-06T13:20:14Z","title":"Stateless actor-critic for instance segmentation with high-level priors","summary":"  Instance segmentation is an important computer vision problem which remains\nchallenging despite impressive recent advances due to deep learning-based\nmethods. Given sufficient training data, fully supervised methods can yield\nexcellent performance, but annotation of ground-truth data remains a major\nbottleneck, especially for biomedical applications where it has to be performed\nby domain experts. The amount of labels required can be drastically reduced by\nusing rules derived from prior knowledge to guide the segmentation. However,\nthese rules are in general not differentiable and thus cannot be used with\nexisting methods. Here, we relax this requirement by using stateless actor\ncritic reinforcement learning, which enables non-differentiable rewards. We\nformulate the instance segmentation problem as graph partitioning and the actor\ncritic predicts the edge weights driven by the rewards, which are based on the\nconformity of segmented instances to high-level priors on object shape,\nposition or size. The experiments on toy and real datasets demonstrate that we\ncan achieve excellent performance without any direct supervision based only on\na rich set of priors.\n","authors":["Paul Hilt","Maedeh Zarvandi","Edgar Kaziakhmedov","Sourabh Bhide","Maria Leptin","Constantin Pape","Anna Kreshuk"],"pdf_url":"https://arxiv.org/pdf/2107.02600v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.11917v1","updated":"2023-03-21T15:08:35Z","published":"2023-03-21T15:08:35Z","title":"Efficient Decision-based Black-box Patch Attacks on Video Recognition","summary":"  Although Deep Neural Networks (DNNs) have demonstrated excellent performance,\nthey are vulnerable to adversarial patches that introduce perceptible and\nlocalized perturbations to the input. Generating adversarial patches on images\nhas received much attention, while adversarial patches on videos have not been\nwell investigated. Further, decision-based attacks, where attackers only access\nthe predicted hard labels by querying threat models, have not been well\nexplored on video models either, even if they are practical in real-world video\nrecognition scenes. The absence of such studies leads to a huge gap in the\nrobustness assessment for video models. To bridge this gap, this work first\nexplores decision-based patch attacks on video models. We analyze that the huge\nparameter space brought by videos and the minimal information returned by\ndecision-based models both greatly increase the attack difficulty and query\nburden. To achieve a query-efficient attack, we propose a spatial-temporal\ndifferential evolution (STDE) framework. First, STDE introduces target videos\nas patch textures and only adds patches on keyframes that are adaptively\nselected by temporal difference. Second, STDE takes minimizing the patch area\nas the optimization objective and adopts spatialtemporal mutation and crossover\nto search for the global optimum without falling into the local optimum.\nExperiments show STDE has demonstrated state-of-the-art performance in terms of\nthreat, efficiency and imperceptibility. Hence, STDE has the potential to be a\npowerful tool for evaluating the robustness of video recognition models.\n","authors":["Kaixun Jiang","Zhaoyu Chen","Tony Huang","Jiafeng Wang","Dingkang Yang","Bo Li","Yan Wang","Wenqiang Zhang"],"pdf_url":"https://arxiv.org/pdf/2303.11917v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.11916v1","updated":"2023-03-21T15:06:35Z","published":"2023-03-21T15:06:35Z","title":"CompoDiff: Versatile Composed Image Retrieval With Latent Diffusion","summary":"  This paper proposes a novel diffusion-based model, CompoDiff, for solving\nComposed Image Retrieval (CIR) with latent diffusion and presents a newly\ncreated dataset of 18 million reference images, conditions, and corresponding\ntarget image triplets to train the model. CompoDiff not only achieves a new\nzero-shot state-of-the-art on a CIR benchmark such as FashionIQ but also\nenables a more versatile CIR by accepting various conditions, such as negative\ntext and image mask conditions, which are unavailable with existing CIR\nmethods. In addition, the CompoDiff features are on the intact CLIP embedding\nspace so that they can be directly used for all existing models exploiting the\nCLIP space. The code and dataset used for the training, and the pre-trained\nweights are available at https://github.com/navervision/CompoDiff\n","authors":["Geonmo Gu","Sanghyuk Chun","Wonjae Kim","HeeJae Jun","Yoohoon Kang","Sangdoo Yun"],"pdf_url":"https://arxiv.org/pdf/2303.11916v1.pdf","comment":"First two authors contributed equally; 23 pages, 4.8MB"},{"id":"http://arxiv.org/abs/2303.11910v1","updated":"2023-03-21T15:01:02Z","published":"2023-03-21T15:01:02Z","title":"360BEV: Panoramic Semantic Mapping for Indoor Bird's-Eye View","summary":"  Seeing only a tiny part of the whole is not knowing the full circumstance.\nBird's-eye-view (BEV) perception, a process of obtaining allocentric maps from\negocentric views, is restricted when using a narrow Field of View (FoV) alone.\nIn this work, mapping from 360{\\deg} panoramas to BEV semantics, the 360BEV\ntask, is established for the first time to achieve holistic representations of\nindoor scenes in a top-down view. Instead of relying on narrow-FoV image\nsequences, a panoramic image with depth information is sufficient to generate a\nholistic BEV semantic map. To benchmark 360BEV, we present two indoor datasets,\n360BEV-Matterport and 360BEV-Stanford, both of which include egocentric\npanoramic images and semantic segmentation labels, as well as allocentric\nsemantic maps. Besides delving deep into different mapping paradigms, we\npropose a dedicated solution for panoramic semantic mapping, namely 360Mapper.\nThrough extensive experiments, our methods achieve 44.32% and 45.78% in mIoU on\nboth datasets respectively, surpassing previous counterparts with gains of\n+7.60% and +9.70% in mIoU. Code and datasets will be available at:\n\\url{https://jamycheung.github.io/360BEV.html}.\n","authors":["Zhifeng Teng","Jiaming Zhang","Kailun Yang","Kunyu Peng","Hao Shi","Simon Reiß","Ke Cao","Rainer Stiefelhagen"],"pdf_url":"https://arxiv.org/pdf/2303.11910v1.pdf","comment":"Code and datasets will be available at:\n  \\url{https://jamycheung.github.io/360BEV.html}"},{"id":"http://arxiv.org/abs/2303.11909v1","updated":"2023-03-21T15:00:17Z","published":"2023-03-21T15:00:17Z","title":"The Multiscale Surface Vision Transformer","summary":"  Surface meshes are a favoured domain for representing structural and\nfunctional information on the human cortex, but their complex topology and\ngeometry pose significant challenges for deep learning analysis. While\nTransformers have excelled as domain-agnostic architectures for\nsequence-to-sequence learning, notably for structures where the translation of\nthe convolution operation is non-trivial, the quadratic cost of the\nself-attention operation remains an obstacle for many dense prediction tasks.\nInspired by some of the latest advances in hierarchical modelling with vision\ntransformers, we introduce the Multiscale Surface Vision Transformer (MS-SiT)\nas a backbone architecture for surface deep learning. The self-attention\nmechanism is applied within local-mesh-windows to allow for high-resolution\nsampling of the underlying data, while a shifted-window strategy improves the\nsharing of information between windows. Neighbouring patches are successively\nmerged, allowing the MS-SiT to learn hierarchical representations suitable for\nany prediction task. Results demonstrate that the MS-SiT outperforms existing\nsurface deep learning methods for neonatal phenotyping prediction tasks using\nthe Developing Human Connectome Project (dHCP) dataset. Furthermore, building\nthe MS-SiT backbone into a U-shaped architecture for surface segmentation\ndemonstrates competitive results on cortical parcellation using the UK Biobank\n(UKB) and manually-annotated MindBoggle datasets. Code and trained models are\npublicly available at\nhttps://github.com/metrics-lab/surface-vision-transformers .\n","authors":["Simon Dahan","Abdulah Fawaz","Mohamed A. Suliman","Mariana da Silva","Logan Z. J. Williams","Daniel Rueckert","Emma C. Robinson"],"pdf_url":"https://arxiv.org/pdf/2303.11909v1.pdf","comment":"14 pages, 4 figures"},{"id":"http://arxiv.org/abs/2303.11906v1","updated":"2023-03-21T14:52:52Z","published":"2023-03-21T14:52:52Z","title":"Solving Oscillation Problem in Post-Training Quantization Through a\n  Theoretical Perspective","summary":"  Post-training quantization (PTQ) is widely regarded as one of the most\nefficient compression methods practically, benefitting from its data privacy\nand low computation costs. We argue that an overlooked problem of oscillation\nis in the PTQ methods. In this paper, we take the initiative to explore and\npresent a theoretical proof to explain why such a problem is essential in PTQ.\nAnd then, we try to solve this problem by introducing a principled and\ngeneralized framework theoretically. In particular, we first formulate the\noscillation in PTQ and prove the problem is caused by the difference in module\ncapacity. To this end, we define the module capacity (ModCap) under\ndata-dependent and data-free scenarios, where the differentials between\nadjacent modules are used to measure the degree of oscillation. The problem is\nthen solved by selecting top-k differentials, in which the corresponding\nmodules are jointly optimized and quantized. Extensive experiments demonstrate\nthat our method successfully reduces the performance drop and is generalized to\ndifferent neural networks and PTQ methods. For example, with 2/4 bit ResNet-50\nquantization, our method surpasses the previous state-of-the-art method by\n1.9%. It becomes more significant on small model quantization, e.g. surpasses\nBRECQ method by 6.61% on MobileNetV2*0.5.\n","authors":["Yuexiao Ma","Huixia Li","Xiawu Zheng","Xuefeng Xiao","Rui Wang","Shilei Wen","Xin Pan","Fei Chao","Rongrong Ji"],"pdf_url":"https://arxiv.org/pdf/2303.11906v1.pdf","comment":"Accepted by CVPR 2023"},{"id":"http://arxiv.org/abs/2303.11898v1","updated":"2023-03-21T14:41:25Z","published":"2023-03-21T14:41:25Z","title":"Real-time volumetric rendering of dynamic humans","summary":"  We present a method for fast 3D reconstruction and real-time rendering of\ndynamic humans from monocular videos with accompanying parametric body fits.\nOur method can reconstruct a dynamic human in less than 3h using a single GPU,\ncompared to recent state-of-the-art alternatives that take up to 72h. These\nspeedups are obtained by using a lightweight deformation model solely based on\nlinear blend skinning, and an efficient factorized volumetric representation\nfor modeling the shape and color of the person in canonical pose. Moreover, we\npropose a novel local ray marching rendering which, by exploiting standard GPU\nhardware and without any baking or conversion of the radiance field, allows\nvisualizing the neural human on a mobile VR device at 40 frames per second with\nminimal loss of visual quality. Our experimental evaluation shows superior or\ncompetitive results with state-of-the art methods while obtaining large\ntraining speedup, using a simple model, and achieving real-time rendering.\n","authors":["Ignacio Rocco","Iurii Makarov","Filippos Kokkinos","David Novotny","Benjamin Graham","Natalia Neverova","Andrea Vedaldi"],"pdf_url":"https://arxiv.org/pdf/2303.11898v1.pdf","comment":"Project page: https://real-time-humans.github.io/"},{"id":"http://arxiv.org/abs/2303.11897v1","updated":"2023-03-21T14:41:02Z","published":"2023-03-21T14:41:02Z","title":"TIFA: Accurate and Interpretable Text-to-Image Faithfulness Evaluation\n  with Question Answering","summary":"  Despite thousands of researchers, engineers, and artists actively working on\nimproving text-to-image generation models, systems often fail to produce images\nthat accurately align with the text inputs. We introduce TIFA (Text-to-Image\nFaithfulness evaluation with question Answering), an automatic evaluation\nmetric that measures the faithfulness of a generated image to its text input\nvia visual question answering (VQA). Specifically, given a text input, we\nautomatically generate several question-answer pairs using a language model. We\ncalculate image faithfulness by checking whether existing VQA models can answer\nthese questions using the generated image. TIFA is a reference-free metric that\nallows for fine-grained and interpretable evaluations of generated images. TIFA\nalso has better correlations with human judgments than existing metrics. Based\non this approach, we introduce TIFA v1.0, a benchmark consisting of 4K diverse\ntext inputs and 25K questions across 12 categories (object, counting, etc.). We\npresent a comprehensive evaluation of existing text-to-image models using TIFA\nv1.0 and highlight the limitations and challenges of current models. For\ninstance, we find that current text-to-image models, despite doing well on\ncolor and material, still struggle in counting, spatial relations, and\ncomposing multiple objects. We hope our benchmark will help carefully measure\nthe research progress in text-to-image synthesis and provide valuable insights\nfor further research.\n","authors":["Yushi Hu","Benlin Liu","Jungo Kasai","Yizhong Wang","Mari Ostendorf","Ranjay Krishna","Noah A. Smith"],"pdf_url":"https://arxiv.org/pdf/2303.11897v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2205.00701v3","updated":"2023-03-21T14:39:49Z","published":"2022-05-02T07:45:51Z","title":"DeepGraviLens: a Multi-Modal Architecture for Classifying Gravitational\n  Lensing Data","summary":"  Gravitational lensing is the relativistic effect generated by massive bodies,\nwhich bend the space-time surrounding them. It is a deeply investigated topic\nin astrophysics and allows validating theoretical relativistic results and\nstudying faint astrophysical objects that would not be visible otherwise. In\nrecent years Machine Learning methods have been applied to support the analysis\nof the gravitational lensing phenomena by detecting lensing effects in data\nsets consisting of images associated with brightness variation time series.\nHowever, the state-of-art approaches either consider only images and neglect\ntime-series data or achieve relatively low accuracy on the most difficult data\nsets. This paper introduces DeepGraviLens, a novel multi-modal network that\nclassifies spatio-temporal data belonging to one non-lensed system type and\nthree lensed system types. It surpasses the current state of the art accuracy\nresults by $\\approx$ 19% to $\\approx$ 43%, depending on the considered data\nset. Such an improvement will enable the acceleration of the analysis of lensed\nobjects in upcoming astrophysical surveys, which will exploit the petabytes of\ndata collected, e.g., from the Vera C. Rubin Observatory.\n","authors":["Nicolò Oreste Pinciroli Vago","Piero Fraternali"],"pdf_url":"https://arxiv.org/pdf/2205.00701v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.11884v1","updated":"2023-03-21T14:24:58Z","published":"2023-03-21T14:24:58Z","title":"Better Understanding Differences in Attribution Methods via Systematic\n  Evaluations","summary":"  Deep neural networks are very successful on many vision tasks, but hard to\ninterpret due to their black box nature. To overcome this, various post-hoc\nattribution methods have been proposed to identify image regions most\ninfluential to the models' decisions. Evaluating such methods is challenging\nsince no ground truth attributions exist. We thus propose three novel\nevaluation schemes to more reliably measure the faithfulness of those methods,\nto make comparisons between them more fair, and to make visual inspection more\nsystematic. To address faithfulness, we propose a novel evaluation setting\n(DiFull) in which we carefully control which parts of the input can influence\nthe output in order to distinguish possible from impossible attributions. To\naddress fairness, we note that different methods are applied at different\nlayers, which skews any comparison, and so evaluate all methods on the same\nlayers (ML-Att) and discuss how this impacts their performance on quantitative\nmetrics. For more systematic visualizations, we propose a scheme (AggAtt) to\nqualitatively evaluate the methods on complete datasets. We use these\nevaluation schemes to study strengths and shortcomings of some widely used\nattribution methods over a wide range of models. Finally, we propose a\npost-processing smoothing step that significantly improves the performance of\nsome attribution methods, and discuss its applicability.\n","authors":["Sukrut Rao","Moritz Böhle","Bernt Schiele"],"pdf_url":"https://arxiv.org/pdf/2303.11884v1.pdf","comment":"35 pages, 37 figures, 2 tables, extended version of arXiv:2205.10435"},{"id":"http://arxiv.org/abs/2303.11881v1","updated":"2023-03-21T14:24:26Z","published":"2023-03-21T14:24:26Z","title":"Protective Self-Adaptive Pruning to Better Compress DNNs","summary":"  Adaptive network pruning approach has recently drawn significant attention\ndue to its excellent capability to identify the importance and redundancy of\nlayers and filters and customize a suitable pruning solution. However, it\nremains unsatisfactory since current adaptive pruning methods rely mostly on an\nadditional monitor to score layer and filter importance, and thus faces high\ncomplexity and weak interpretability. To tackle these issues, we have deeply\nresearched the weight reconstruction process in iterative prune-train process\nand propose a Protective Self-Adaptive Pruning (PSAP) method. First of all,\nPSAP can utilize its own information, weight sparsity ratio, to adaptively\nadjust pruning ratio of layers before each pruning step. Moreover, we propose a\nprotective reconstruction mechanism to prevent important filters from being\npruned through supervising gradients and to avoid unrecoverable information\nloss as well. Our PSAP is handy and explicit because it merely depends on\nweights and gradients of model itself, instead of requiring an additional\nmonitor as in early works. Experiments on ImageNet and CIFAR-10 also\ndemonstrate its superiority to current works in both accuracy and compression\nratio, especially for compressing with a high ratio or pruning from scratch.\n","authors":["Liang Li","Pengfei Zhao"],"pdf_url":"https://arxiv.org/pdf/2303.11881v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.11880v1","updated":"2023-03-21T14:24:06Z","published":"2023-03-21T14:24:06Z","title":"Focused and Collaborative Feedback Integration for Interactive Image\n  Segmentation","summary":"  Interactive image segmentation aims at obtaining a segmentation mask for an\nimage using simple user annotations. During each round of interaction, the\nsegmentation result from the previous round serves as feedback to guide the\nuser's annotation and provides dense prior information for the segmentation\nmodel, effectively acting as a bridge between interactions. Existing methods\noverlook the importance of feedback or simply concatenate it with the original\ninput, leading to underutilization of feedback and an increase in the number of\nrequired annotations. To address this, we propose an approach called Focused\nand Collaborative Feedback Integration (FCFI) to fully exploit the feedback for\nclick-based interactive image segmentation. FCFI first focuses on a local area\naround the new click and corrects the feedback based on the similarities of\nhigh-level features. It then alternately and collaboratively updates the\nfeedback and deep features to integrate the feedback into the features. The\nefficacy and efficiency of FCFI were validated on four benchmarks, namely\nGrabCut, Berkeley, SBD, and DAVIS. Experimental results show that FCFI achieved\nnew state-of-the-art performance with less computational overhead than previous\nmethods. The source code is available at\nhttps://github.com/veizgyauzgyauz/FCFI.\n","authors":["Qiaoqiao Wei","Hui Zhang","Jun-Hai Yong"],"pdf_url":"https://arxiv.org/pdf/2303.11880v1.pdf","comment":"Accepted for publication at CVPR 2023"},{"id":"http://arxiv.org/abs/2303.11866v1","updated":"2023-03-21T14:12:08Z","published":"2023-03-21T14:12:08Z","title":"Contrastive Alignment of Vision to Language Through Parameter-Efficient\n  Transfer Learning","summary":"  Contrastive vision-language models (e.g. CLIP) are typically created by\nupdating all the parameters of a vision model and language model through\ncontrastive training. Can such models be created by a small number of parameter\nupdates to an already-trained language model and vision model? The literature\ndescribes techniques that can create vision-language models by updating a small\nnumber of parameters in a language model, but these require already aligned\nvisual representations and are non-contrastive, hence unusable for\nlatency-sensitive applications such as neural search. We explore the\nfeasibility and benefits of parameter-efficient contrastive vision-language\nalignment through transfer learning: creating a model such as CLIP by minimally\nupdating an already-trained vision and language model. We find that a minimal\nset of parameter updates ($<$7%) can achieve the same performance as full-model\ntraining, and updating specific components ($<$1% of parameters) can match 75%\nof full-model training. We describe a series of experiments: we show that\nexisting knowledge is conserved more strongly in parameter-efficient training\nand that parameter-efficient scaling scales with model and dataset size. Where\npaired-image text data is scarce but strong multilingual language models exist\n(e.g. low resource languages), parameter-efficient training is even preferable\nto full-model training. Given a fixed compute budget, parameter-efficient\ntraining allows training larger models on the same hardware, achieving\nequivalent performance in less time. Parameter-efficient training hence\nconstitutes an energy-efficient and effective training strategy for contrastive\nvision-language models that may be preferable to the full-model training\nparadigm for common use cases. Code and weights at\nhttps://github.com/codezakh/LilT.\n","authors":["Zaid Khan","Yun Fu"],"pdf_url":"https://arxiv.org/pdf/2303.11866v1.pdf","comment":"Accepted to ICLR 2023"},{"id":"http://arxiv.org/abs/2302.10574v2","updated":"2023-03-21T14:10:33Z","published":"2023-02-21T10:00:58Z","title":"MulGT: Multi-task Graph-Transformer with Task-aware Knowledge Injection\n  and Domain Knowledge-driven Pooling for Whole Slide Image Analysis","summary":"  Whole slide image (WSI) has been widely used to assist automated diagnosis\nunder the deep learning fields. However, most previous works only discuss the\nSINGLE task setting which is not aligned with real clinical setting, where\npathologists often conduct multiple diagnosis tasks simultaneously. Also, it is\ncommonly recognized that the multi-task learning paradigm can improve learning\nefficiency by exploiting commonalities and differences across multiple tasks.\nTo this end, we present a novel multi-task framework (i.e., MulGT) for WSI\nanalysis by the specially designed Graph-Transformer equipped with Task-aware\nKnowledge Injection and Domain Knowledge-driven Graph Pooling modules.\nBasically, with the Graph Neural Network and Transformer as the building\ncommons, our framework is able to learn task-agnostic low-level local\ninformation as well as task-specific high-level global representation.\nConsidering that different tasks in WSI analysis depend on different features\nand properties, we also design a novel Task-aware Knowledge Injection module to\ntransfer the task-shared graph embedding into task-specific feature spaces to\nlearn more accurate representation for different tasks. Further, we elaborately\ndesign a novel Domain Knowledge-driven Graph Pooling module for each task to\nimprove both the accuracy and robustness of different tasks by leveraging\ndifferent diagnosis patterns of multiple tasks. We evaluated our method on two\npublic WSI datasets from TCGA projects, i.e., esophageal carcinoma and kidney\ncarcinoma. Experimental results show that our method outperforms single-task\ncounterparts and the state-of-theart methods on both tumor typing and staging\ntasks.\n","authors":["Weiqin Zhao","Shujun Wang","Maximus Yeung","Tianye Niu","Lequan Yu"],"pdf_url":"https://arxiv.org/pdf/2302.10574v2.pdf","comment":"AAAI 2023"},{"id":"http://arxiv.org/abs/2303.11859v1","updated":"2023-03-21T13:59:32Z","published":"2023-03-21T13:59:32Z","title":"LEAPS: End-to-End One-Step Person Search With Learnable Proposals","summary":"  We propose an end-to-end one-step person search approach with learnable\nproposals, named LEAPS. Given a set of sparse and learnable proposals, LEAPS\nemploys a dynamic person search head to directly perform person detection and\ncorresponding re-id feature generation without non-maximum suppression\npost-processing. The dynamic person search head comprises a detection head and\na novel flexible re-id head. Our flexible re-id head first employs a dynamic\nregion-of-interest (RoI) operation to extract discriminative RoI features of\nthe proposals. Then, it generates re-id features using a plain and a\nhierarchical interaction re-id module. To better guide discriminative re-id\nfeature learning, we introduce a diverse re-id sample matching strategy,\ninstead of bipartite matching in detection head. Comprehensive experiments\nreveal the benefit of the proposed LEAPS, achieving a favorable performance on\ntwo public person search benchmarks: CUHK-SYSU and PRW. When using the same\nResNet50 backbone, our LEAPS obtains a mAP score of 55.0%, outperforming the\nbest reported results in literature by 1.7%, while achieving around a two-fold\nspeedup on the challenging PRW dataset. Our source code and models will be\nreleased.\n","authors":["Zhiqiang Dong","Jiale Cao","Rao Muhammad Anwer","Jin Xie","Fahad Khan","Yanwei Pang"],"pdf_url":"https://arxiv.org/pdf/2303.11859v1.pdf","comment":"11 pages, 9 figures"},{"id":"http://arxiv.org/abs/2303.11855v1","updated":"2023-03-21T13:55:27Z","published":"2023-03-21T13:55:27Z","title":"CLIP-ReIdent: Contrastive Training for Player Re-Identification","summary":"  Sports analytics benefits from recent advances in machine learning providing\na competitive advantage for teams or individuals. One important task in this\ncontext is the performance measurement of individual players to provide reports\nand log files for subsequent analysis. During sport events like basketball,\nthis involves the re-identification of players during a match either from\nmultiple camera viewpoints or from a single camera viewpoint at different\ntimes. In this work, we investigate whether it is possible to transfer the\nout-standing zero-shot performance of pre-trained CLIP models to the domain of\nplayer re-identification. For this purpose we reformulate the contrastive\nlanguage-to-image pre-training approach from CLIP to a contrastive\nimage-to-image training approach using the InfoNCE loss as training objective.\nUnlike previous work, our approach is entirely class-agnostic and benefits from\nlarge-scale pre-training. With a fine-tuned CLIP ViT-L/14 model we achieve\n98.44 % mAP on the MMSports 2022 Player Re-Identification challenge.\nFurthermore we show that the CLIP Vision Transformers have already strong OCR\ncapabilities to identify useful player features like shirt numbers in a\nzero-shot manner without any fine-tuning on the dataset. By applying the\nScore-CAM algorithm we visualise the most important image regions that our\nfine-tuned model identifies when calculating the similarity score between two\nimages of a player.\n","authors":["Konrad Habel","Fabian Deuser","Norbert Oswald"],"pdf_url":"https://arxiv.org/pdf/2303.11855v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.11851v1","updated":"2023-03-21T13:49:49Z","published":"2023-03-21T13:49:49Z","title":"Sample4Geo: Hard Negative Sampling For Cross-View Geo-Localisation","summary":"  Cross-View Geo-Localisation is still a challenging task where additional\nmodules, specific pre-processing or zooming strategies are necessary to\ndetermine accurate positions of images. Since different views have different\ngeometries, pre-processing like polar transformation helps to merge them.\nHowever, this results in distorted images which then have to be rectified.\nAdding hard negatives to the training batch could improve the overall\nperformance but with the default loss functions in geo-localisation it is\ndifficult to include them. In this article, we present a simplified but\neffective architecture based on contrastive learning with symmetric InfoNCE\nloss that outperforms current state-of-the-art results. Our framework consists\nof a narrow training pipeline that eliminates the need of using aggregation\nmodules, avoids further pre-processing steps and even increases the\ngeneralisation capability of the model to unknown regions. We introduce two\ntypes of sampling strategies for hard negatives. The first explicitly exploits\ngeographically neighboring locations to provide a good starting point. The\nsecond leverages the visual similarity between the image embeddings in order to\nmine hard negative samples. Our work shows excellent performance on common\ncross-view datasets like CVUSA, CVACT, University-1652 and VIGOR. A comparison\nbetween cross-area and same-area settings demonstrate the good generalisation\ncapability of our model.\n","authors":["Fabian Deuser","Konrad Habel","Norbert Oswald"],"pdf_url":"https://arxiv.org/pdf/2303.11851v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.11848v1","updated":"2023-03-21T13:48:53Z","published":"2023-03-21T13:48:53Z","title":"Dens-PU: PU Learning with Density-Based Positive Labeled Augmentation","summary":"  This study proposes a novel approach for solving the PU learning problem\nbased on an anomaly-detection strategy. Latent encodings extracted from\npositive-labeled data are linearly combined to acquire new samples. These new\nsamples are used as embeddings to increase the density of positive-labeled data\nand, thus, define a boundary that approximates the positive class. The further\na sample is from the boundary the more it is considered as a negative sample.\nOnce a set of negative samples is obtained, the PU learning problem reduces to\nbinary classification. The approach, named Dens-PU due to its reliance on the\ndensity of positive-labeled data, was evaluated using benchmark image datasets,\nand state-of-the-art results were attained.\n","authors":["Vasileios Sevetlidis","George Pavlidis","Spyridon Mouroutsos","Antonios Gasteratos"],"pdf_url":"https://arxiv.org/pdf/2303.11848v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.10449v2","updated":"2023-03-21T13:41:03Z","published":"2023-03-18T16:22:59Z","title":"Uncertainty-Aware Optimal Transport for Semantically Coherent\n  Out-of-Distribution Detection","summary":"  Semantically coherent out-of-distribution (SCOOD) detection aims to discern\noutliers from the intended data distribution with access to unlabeled extra\nset. The coexistence of in-distribution and out-of-distribution samples will\nexacerbate the model overfitting when no distinction is made. To address this\nproblem, we propose a novel uncertainty-aware optimal transport scheme. Our\nscheme consists of an energy-based transport (ET) mechanism that estimates the\nfluctuating cost of uncertainty to promote the assignment of semantic-agnostic\nrepresentation, and an inter-cluster extension strategy that enhances the\ndiscrimination of semantic property among different clusters by widening the\ncorresponding margin distance. Furthermore, a T-energy score is presented to\nmitigate the magnitude gap between the parallel transport and classifier\nbranches. Extensive experiments on two standard SCOOD benchmarks demonstrate\nthe above-par OOD detection performance, outperforming the state-of-the-art\nmethods by a margin of 27.69% and 34.4% on FPR@95, respectively.\n","authors":["Fan Lu","Kai Zhu","Wei Zhai","Kecheng Zheng","Yang Cao"],"pdf_url":"https://arxiv.org/pdf/2303.10449v2.pdf","comment":"Accepted by CVPR2023"},{"id":"http://arxiv.org/abs/2211.08326v2","updated":"2023-03-21T13:37:04Z","published":"2022-11-14T10:07:30Z","title":"Contrastive learning for regression in multi-site brain age prediction","summary":"  Building accurate Deep Learning (DL) models for brain age prediction is a\nvery relevant topic in neuroimaging, as it could help better understand\nneurodegenerative disorders and find new biomarkers. To estimate accurate and\ngeneralizable models, large datasets have been collected, which are often\nmulti-site and multi-scanner. This large heterogeneity negatively affects the\ngeneralization performance of DL models since they are prone to overfit\nsite-related noise. Recently, contrastive learning approaches have been shown\nto be more robust against noise in data or labels. For this reason, we propose\na novel contrastive learning regression loss for robust brain age prediction\nusing MRI scans. Our method achieves state-of-the-art performance on the\nOpenBHB challenge, yielding the best generalization capability and robustness\nto site-related noise.\n","authors":["Carlo Alberto Barbano","Benoit Dufumier","Edouard Duchesnay","Marco Grangetto","Pietro Gori"],"pdf_url":"https://arxiv.org/pdf/2211.08326v2.pdf","comment":"5 pages"},{"id":"http://arxiv.org/abs/2303.11840v1","updated":"2023-03-21T13:34:12Z","published":"2023-03-21T13:34:12Z","title":"Self-Paced Neutral Expression-Disentangled Learning for Facial\n  Expression Recognition","summary":"  The accuracy of facial expression recognition is typically affected by the\nfollowing factors: high similarities across different expressions, disturbing\nfactors, and micro-facial movement of rapid and subtle changes. One potentially\nviable solution for addressing these barriers is to exploit the neutral\ninformation concealed in neutral expression images. To this end, in this paper\nwe propose a self-Paced Neutral Expression-Disentangled Learning (SPNDL) model.\nSPNDL disentangles neutral information from facial expressions, making it\neasier to extract key and deviation features. Specifically, it allows to\ncapture discriminative information among similar expressions and perceive\nmicro-facial movements. In order to better learn these neutral\nexpression-disentangled features (NDFs) and to alleviate the non-convex\noptimization problem, a self-paced learning (SPL) strategy based on NDFs is\nproposed in the training stage. SPL learns samples from easy to complex by\nincreasing the number of samples selected into the training process, which\nenables to effectively suppress the negative impacts introduced by low-quality\nsamples and inconsistently distributed NDFs. Experiments on three popular\ndatabases (i.e., CK+, Oulu-CASIA, and RAF-DB) show the effectiveness of our\nproposed method.\n","authors":["Zhenqian Wu","Xiaoyuan Li","Yazhou Ren","Xiaorong Pu","Xiaofeng Zhu","Lifang He"],"pdf_url":"https://arxiv.org/pdf/2303.11840v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.11420v1","updated":"2023-03-21T13:31:15Z","published":"2023-03-21T13:31:15Z","title":"ADCNet: End-to-end perception with raw radar ADC data","summary":"  There is a renewed interest in radar sensors in the autonomous driving\nindustry. As a relatively mature technology, radars have seen steady\nimprovement over the last few years, making them an appealing alternative or\ncomplement to the commonly used LiDARs. An emerging trend is to leverage rich,\nlow-level radar data for perception. In this work we push this trend to the\nextreme -- we propose a method to perform end-to-end learning on the raw radar\nanalog-to-digital (ADC) data. Specifically, we design a learnable signal\nprocessing module inside the neural network, and a pre-training method guided\nby traditional signal processing algorithms. Experiment results corroborate the\noverall efficacy of the end-to-end learning method, while an ablation study\nvalidates the effectiveness of our individual innovations.\n","authors":["Bo Yang","Ishan Khatri","Michael Happold","Chulong Chen"],"pdf_url":"https://arxiv.org/pdf/2303.11420v1.pdf","comment":"10 pages, 8 figures"},{"id":"http://arxiv.org/abs/2203.04838v3","updated":"2023-03-21T13:30:43Z","published":"2022-03-09T16:12:08Z","title":"CMX: Cross-Modal Fusion for RGB-X Semantic Segmentation with\n  Transformers","summary":"  Scene understanding based on image segmentation is a crucial component for\nautonomous vehicles. Pixel-wise semantic segmentation of RGB images can be\nadvanced by exploiting informative features from the supplementary modality\n(X-modality). In this work, we propose CMX, a transformer-based cross-modal\nfusion framework for RGB-X semantic segmentation. To generalize to different\nsensing modalities encompassing various supplements and uncertainties, we\nconsider that comprehensive cross-modal interactions should be provided. CMX is\nbuilt with two streams to extract features from RGB images and the X-modality.\nIn each feature extraction stage, we design a Cross-Modal Feature Rectification\nModule (CM-FRM) to calibrate the feature of the current modality by combining\nthe feature from the other modality, in spatial- and channel-wise dimensions.\nWith rectified feature pairs, we deploy a Feature Fusion Module (FFM) to mix\nthem for the final semantic prediction. FFM is constructed with a\ncross-attention mechanism, which enables exchange of long-range contexts,\nenhancing bi-modal features globally. Extensive experiments show that CMX\ngeneralizes to diverse multi-modal combinations, achieving state-of-the-art\nperformances on five RGB-Depth benchmarks, as well as RGB-Thermal,\nRGB-Polarization, and RGB-LiDAR datasets. Besides, to investigate the\ngeneralizability to dense-sparse data fusion, we establish an RGB-Event\nsemantic segmentation benchmark based on the EventScape dataset, on which CMX\nsets the new state-of-the-art. The source code of CMX is publicly available at\nhttps://github.com/huaaaliu/RGBX_Semantic_Segmentation.\n","authors":["Jiaming Zhang","Huayao Liu","Kailun Yang","Xinxin Hu","Ruiping Liu","Rainer Stiefelhagen"],"pdf_url":"https://arxiv.org/pdf/2203.04838v3.pdf","comment":"Code is available at\n  https://github.com/huaaaliu/RGBX_Semantic_Segmentation"},{"id":"http://arxiv.org/abs/2303.11837v1","updated":"2023-03-21T13:29:17Z","published":"2023-03-21T13:29:17Z","title":"Self-supervised learning of a tailored Convolutional Auto Encoder for\n  histopathological prostate grading","summary":"  According to GLOBOCAN 2020, prostate cancer is the second most common cancer\nin men worldwide and the fourth most prevalent cancer overall. For\npathologists, grading prostate cancer is challenging, especially when\ndiscriminating between Grade 3 (G3) and Grade 4 (G4). This paper proposes a\nSelf-Supervised Learning (SSL) framework to classify prostate histopathological\nimages when labeled images are scarce. In particular, a tailored Convolutional\nAuto Encoder (CAE) is trained to reconstruct 128x128x3 patches of prostate\ncancer Whole Slide Images (WSIs) as a pretext task. The downstream task of the\nproposed SSL paradigm is the automatic grading of histopathological patches of\nprostate cancer. The presented framework reports promising results on the\nvalidation set, obtaining an overall accuracy of 83% and on the test set,\nachieving an overall accuracy value of 76% with F1-score of 77% in G4.\n","authors":["Zahra Tabatabaei","Adrian colomer","Kjersti Engan","Javier Oliver","Valery Naranjo"],"pdf_url":"https://arxiv.org/pdf/2303.11837v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.05725v2","updated":"2023-03-21T13:28:49Z","published":"2023-03-10T06:12:36Z","title":"CVT-SLR: Contrastive Visual-Textual Transformation for Sign Language\n  Recognition with Variational Alignment","summary":"  Sign language recognition (SLR) is a weakly supervised task that annotates\nsign videos as textual glosses. Recent studies show that insufficient training\ncaused by the lack of large-scale available sign language datasets becomes the\nmain bottleneck for SLR. The majority of SLR works thereby adopt pretrained\nvisual modules and develop two mainstream solutions. The multi-stream\narchitectures extend multi-cue visual features, yielding the current SOTA\nperformances but requiring complex designs and might introduce potential noise.\nAlternatively, the advanced single-cue SLR frameworks using explicit\ncross-modal alignment between visual and textual modalities are simple and\neffective, potentially competitive with the multi-cue framework. In this work,\nwe propose a novel contrastive visual-textual transformation for SLR, CVT-SLR,\nto fully explore the pretrained knowledge of both the visual and language\nmodalities. Based on the single-cue cross-modal alignment framework, we propose\na variational autoencoder (VAE) for pretrained contextual knowledge while\nintroducing the complete pretrained language module. The VAE implicitly aligns\nvisual and textual modalities while benefiting from pretrained contextual\nknowledge as the traditional contextual module. Meanwhile, a contrastive\ncross-modal alignment algorithm is proposed to further enhance the explicit\nconsistency constraints. Extensive experiments conducted on the two most\npopular public datasets, PHOENIX-2014 and PHOENIX-2014T, demonstrate that our\nproposed SLR framework not only consistently outperforms existing single-cue\nmethods but even outperforms SOTA multi-cue methods.\n","authors":["Jiangbin Zheng","Yile Wang","Cheng Tan","Siyuan Li","Ge Wang","Jun Xia","Yidong Chen","Stan Z. Li"],"pdf_url":"https://arxiv.org/pdf/2303.05725v2.pdf","comment":"Accepted to CVPR 2023 (highlight)"},{"id":"http://arxiv.org/abs/2303.11831v1","updated":"2023-03-21T13:19:51Z","published":"2023-03-21T13:19:51Z","title":"GLADE: Gradient Loss Augmented Degradation Enhancement for Unpaired\n  Super-Resolution of Anisotropic MRI","summary":"  We present a novel approach to synthesise high-resolution isotropic 3D\nabdominal MR images, from anisotropic 3D images in an unpaired fashion. Using a\nmodified CycleGAN architecture with a gradient mapping loss, we leverage\ndisjoint patches from the high-resolution (in-plane) data of an anisotropic\nvolume to enforce the network generator to increase the resolution of the\nlow-resolution (through-plane) slices. This will enable accelerated\nwhole-abdomen scanning with high-resolution isotropic images within short\nbreath-hold times.\n","authors":["Michele Pascale","Vivek Muthurangu","Javier Montalt Tordera","Heather E Fitzke","Gauraang Bhatnagar","Stuart Taylor","Jennifer Steeden"],"pdf_url":"https://arxiv.org/pdf/2303.11831v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.11828v1","updated":"2023-03-21T13:14:36Z","published":"2023-03-21T13:14:36Z","title":"The Treasure Beneath Multiple Annotations: An Uncertainty-aware Edge\n  Detector","summary":"  Deep learning-based edge detectors heavily rely on pixel-wise labels which\nare often provided by multiple annotators. Existing methods fuse multiple\nannotations using a simple voting process, ignoring the inherent ambiguity of\nedges and labeling bias of annotators. In this paper, we propose a novel\nuncertainty-aware edge detector (UAED), which employs uncertainty to\ninvestigate the subjectivity and ambiguity of diverse annotations.\nSpecifically, we first convert the deterministic label space into a learnable\nGaussian distribution, whose variance measures the degree of ambiguity among\ndifferent annotations. Then we regard the learned variance as the estimated\nuncertainty of the predicted edge maps, and pixels with higher uncertainty are\nlikely to be hard samples for edge detection. Therefore we design an adaptive\nweighting loss to emphasize the learning from those pixels with high\nuncertainty, which helps the network to gradually concentrate on the important\npixels. UAED can be combined with various encoder-decoder backbones, and the\nextensive experiments demonstrate that UAED achieves superior performance\nconsistently across multiple edge detection benchmarks. The source code is\navailable at \\url{https://github.com/ZhouCX117/UAED}\n","authors":["Caixia Zhou","Yaping Huang","Mengyang Pu","Qingji Guan","Li Huang","Haibin Ling"],"pdf_url":"https://arxiv.org/pdf/2303.11828v1.pdf","comment":"CVPR2023"},{"id":"http://arxiv.org/abs/2302.08722v3","updated":"2023-03-21T12:59:20Z","published":"2023-02-17T06:33:06Z","title":"GPT4MIA: Utilizing Generative Pre-trained Transformer (GPT-3) as A\n  Plug-and-Play Transductive Model for Medical Image Analysis","summary":"  In this paper, we propose a novel approach (called GPT4MIA) that utilizes\nGenerative Pre-trained Transformer (GPT) as a plug-and-play transductive\ninference tool for medical image analysis (MIA). We provide theoretical\nanalysis on why a large pre-trained language model such as GPT-3 can be used as\na plug-and-play transductive inference model for MIA. At the methodological\nlevel, we develop several technical treatments to improve the efficiency and\neffectiveness of GPT4MIA, including better prompt structure design, sample\nselection, and prompt ordering of representative samples/features. We present\ntwo concrete use cases (with workflow) of GPT4MIA: (1) detecting prediction\nerrors and (2) improving prediction accuracy, working in conjecture with\nwell-established vision-based models for image classification (e.g., ResNet).\nExperiments validate that our proposed method is effective for these two tasks.\nWe further discuss the opportunities and challenges in utilizing\nTransformer-based large language models for broader MIA applications.\n","authors":["Yizhe Zhang","Danny Z. Chen"],"pdf_url":"https://arxiv.org/pdf/2302.08722v3.pdf","comment":"Version 3: Added appendix with more results and visualizations.\n  Questions and suggestions are welcome"},{"id":"http://arxiv.org/abs/2208.06643v3","updated":"2023-03-21T12:54:36Z","published":"2022-08-13T13:13:41Z","title":"Recent Progress in Transformer-based Medical Image Analysis","summary":"  The transformer is primarily used in the field of natural language\nprocessing. Recently, it has been adopted and shows promise in the computer\nvision (CV) field. Medical image analysis (MIA), as a critical branch of CV,\nalso greatly benefits from this state-of-the-art technique. In this review, we\nfirst recap the core component of the transformer, the attention mechanism, and\nthe detailed structures of the transformer. After that, we depict the recent\nprogress of the transformer in the field of MIA. We organize the applications\nin a sequence of different tasks, including classification, segmentation,\ncaptioning, registration, detection, reconstruction, denoising, localization,\nand synthesis. The mainstream classification and segmentation tasks are further\ndivided into eleven medical image modalities. Finally, We discuss the open\nchallenges and future opportunities in this field. This review with the latest\ncontents, detailed information, and task-modality organization mode may greatly\nbenefit the broad MIA community.\n","authors":["Zhaoshan Liu","Qiujie Lv","Ziduo Yang","Yifan Li","Chau Hung Lee","Lei Shen"],"pdf_url":"https://arxiv.org/pdf/2208.06643v3.pdf","comment":"83 pages, 12 figures"},{"id":"http://arxiv.org/abs/2303.11239v2","updated":"2023-03-21T12:43:11Z","published":"2023-03-20T16:24:06Z","title":"Training Invertible Neural Networks as Autoencoders","summary":"  Autoencoders are able to learn useful data representations in an unsupervised\nmatter and have been widely used in various machine learning and computer\nvision tasks. In this work, we present methods to train Invertible Neural\nNetworks (INNs) as (variational) autoencoders which we call INN (variational)\nautoencoders. Our experiments on MNIST, CIFAR and CelebA show that for low\nbottleneck sizes our INN autoencoder achieves results similar to the classical\nautoencoder. However, for large bottleneck sizes our INN autoencoder\noutperforms its classical counterpart. Based on the empirical results, we\nhypothesize that INN autoencoders might not have any intrinsic information loss\nand thereby are not bounded to a maximal number of layers (depth) after which\nonly suboptimal results can be achieved.\n","authors":["The-Gia Leo Nguyen","Lynton Ardizzone","Ullrich Köthe"],"pdf_url":"https://arxiv.org/pdf/2303.11239v2.pdf","comment":"Conference Paper at GCPR2019"},{"id":"http://arxiv.org/abs/2303.11803v1","updated":"2023-03-21T12:36:58Z","published":"2023-03-21T12:36:58Z","title":"Fighting over-fitting with quantization for learning deep neural\n  networks on noisy labels","summary":"  The rising performance of deep neural networks is often empirically\nattributed to an increase in the available computational power, which allows\ncomplex models to be trained upon large amounts of annotated data. However,\nincreased model complexity leads to costly deployment of modern neural\nnetworks, while gathering such amounts of data requires huge costs to avoid\nlabel noise. In this work, we study the ability of compression methods to\ntackle both of these problems at once. We hypothesize that quantization-aware\ntraining, by restricting the expressivity of neural networks, behaves as a\nregularization. Thus, it may help fighting overfitting on noisy data while also\nallowing for the compression of the model at inference. We first validate this\nclaim on a controlled test with manually introduced label noise. Furthermore,\nwe also test the proposed method on Facial Action Unit detection, where labels\nare typically noisy due to the subtlety of the task. In all cases, our results\nsuggests that quantization significantly improve the results compared with\nexisting baselines, regularization as well as other compression methods.\n","authors":["Gauthier Tallec","Edouard Yvinec","Arnaud Dapogny","Kevin Bailly"],"pdf_url":"https://arxiv.org/pdf/2303.11803v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.11797v1","updated":"2023-03-21T12:28:21Z","published":"2023-03-21T12:28:21Z","title":"CAT-Seg: Cost Aggregation for Open-Vocabulary Semantic Segmentation","summary":"  Existing works on open-vocabulary semantic segmentation have utilized\nlarge-scale vision-language models, such as CLIP, to leverage their exceptional\nopen-vocabulary recognition capabilities. However, the problem of transferring\nthese capabilities learned from image-level supervision to the pixel-level task\nof segmentation and addressing arbitrary unseen categories at inference makes\nthis task challenging. To address these issues, we aim to attentively relate\nobjects within an image to given categories by leveraging relational\ninformation among class categories and visual semantics through aggregation,\nwhile also adapting the CLIP representations to the pixel-level task. However,\nwe observe that direct optimization of the CLIP embeddings can harm its\nopen-vocabulary capabilities. In this regard, we propose an alternative\napproach to optimize the image-text similarity map, i.e. the cost map, using a\nnovel cost aggregation-based method. Our framework, namely CAT-Seg, achieves\nstate-of-the-art performance across all benchmarks. We provide extensive\nablation studies to validate our choices. Project page:\nhttps://ku-cvlab.github.io/CAT-Seg/.\n","authors":["Seokju Cho","Heeseong Shin","Sunghwan Hong","Seungjun An","Seungjun Lee","Anurag Arnab","Paul Hongsuck Seo","Seungryong Kim"],"pdf_url":"https://arxiv.org/pdf/2303.11797v1.pdf","comment":"Project page: https://ku-cvlab.github.io/CAT-Seg/"},{"id":"http://arxiv.org/abs/2303.11793v1","updated":"2023-03-21T12:22:59Z","published":"2023-03-21T12:22:59Z","title":"OTJR: Optimal Transport Meets Optimal Jacobian Regularization for\n  Adversarial Robustness","summary":"  Deep neural networks are widely recognized as being vulnerable to adversarial\nperturbation. To overcome this challenge, developing a robust classifier is\ncrucial. So far, two well-known defenses have been adopted to improve the\nlearning of robust classifiers, namely adversarial training (AT) and Jacobian\nregularization. However, each approach behaves differently against adversarial\nperturbations. First, our work carefully analyzes and characterizes these two\nschools of approaches, both theoretically and empirically, to demonstrate how\neach approach impacts the robust learning of a classifier. Next, we propose our\nnovel Optimal Transport with Jacobian regularization method, dubbed OTJR,\njointly incorporating the input-output Jacobian regularization into the AT by\nleveraging the optimal transport theory. In particular, we employ the Sliced\nWasserstein (SW) distance that can efficiently push the adversarial samples'\nrepresentations closer to those of clean samples, regardless of the number of\nclasses within the dataset. The SW distance provides the adversarial samples'\nmovement directions, which are much more informative and powerful for the\nJacobian regularization. Our extensive experiments demonstrate the\neffectiveness of our proposed method, which jointly incorporates Jacobian\nregularization into AT. Furthermore, we demonstrate that our proposed method\nconsistently enhances the model's robustness with CIFAR-100 dataset under\nvarious adversarial attack settings, achieving up to 28.49% under AutoAttack.\n","authors":["Binh M. Le","Shahroz Tariq","Simon S. Woo"],"pdf_url":"https://arxiv.org/pdf/2303.11793v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.11791v1","updated":"2023-03-21T12:18:57Z","published":"2023-03-21T12:18:57Z","title":"Propagate And Calibrate: Real-time Passive Non-line-of-sight Tracking","summary":"  Non-line-of-sight (NLOS) tracking has drawn increasing attention in recent\nyears, due to its ability to detect object motion out of sight. Most previous\nworks on NLOS tracking rely on active illumination, e.g., laser, and suffer\nfrom high cost and elaborate experimental conditions. Besides, these techniques\nare still far from practical application due to oversimplified settings. In\ncontrast, we propose a purely passive method to track a person walking in an\ninvisible room by only observing a relay wall, which is more in line with real\napplication scenarios, e.g., security. To excavate imperceptible changes in\nvideos of the relay wall, we introduce difference frames as an essential\ncarrier of temporal-local motion messages. In addition, we propose PAC-Net,\nwhich consists of alternating propagation and calibration, making it capable of\nleveraging both dynamic and static messages on a frame-level granularity. To\nevaluate the proposed method, we build and publish the first dynamic passive\nNLOS tracking dataset, NLOS-Track, which fills the vacuum of realistic NLOS\ndatasets. NLOS-Track contains thousands of NLOS video clips and corresponding\ntrajectories. Both real-shot and synthetic data are included.\n","authors":["Yihao Wang","Zhigang Wang","Bin Zhao","Dong Wang","Mulin Chen","Xuelong Li"],"pdf_url":"https://arxiv.org/pdf/2303.11791v1.pdf","comment":"CVPR 2023 camera-ready version. Codes and dataset are available at\n  https://againstentropy.github.io/NLOS-Track/"},{"id":"http://arxiv.org/abs/2303.11790v1","updated":"2023-03-21T12:17:21Z","published":"2023-03-21T12:17:21Z","title":"Probabilistic Domain Adaptation for Biomedical Image Segmentation","summary":"  Segmentation is a key analysis tasks in biomedical imaging. Given the many\ndifferent experimental settings in this field, the lack of generalization\nlimits the use of deep learning in practice. Domain adaptation is a promising\nremedy: it trains a model for a given task on a source dataset with labels and\nadapts it to a target dataset without additional labels. We introduce a\nprobabilistic domain adaptation method, building on self-training approaches\nand the Probabilistic UNet. We use the latter to sample multiple segmentation\nhypothesis to implement better pseudo-label filtering. We further study joint\nand separate source-target training strategies and evaluate our method on three\nchallenging domain adaptation tasks for biomedical segmentation.\n","authors":["Anwai Archit","Constantin Pape"],"pdf_url":"https://arxiv.org/pdf/2303.11790v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2211.09699v2","updated":"2023-03-21T12:10:18Z","published":"2022-11-15T19:07:53Z","title":"PromptCap: Prompt-Guided Image Captioning for VQA with GPT-3","summary":"  Knowledge-based visual question answering (VQA) involves questions that\nrequire world knowledge beyond the image to yield the correct answer. Large\nlanguage models (LMs) like GPT-3 are particularly helpful for this task because\nof their strong knowledge retrieval and reasoning capabilities. To enable LM to\nunderstand images, prior work uses a captioning model to convert images into\ntext. However, when summarizing an image in a single caption sentence, which\nvisual entities to describe are often underspecified. Generic image captions\noften miss visual details essential for the LM to answer visual questions\ncorrectly. To address this challenge, we propose PromptCap (Prompt-guided image\nCaptioning), a captioning model designed to serve as a better connector between\nimages and black-box LMs. Different from generic captions, PromptCap takes a\nnatural-language prompt to control the visual entities to describe in the\ngenerated caption. The prompt contains a question that the caption should aid\nin answering. To avoid extra annotation, PromptCap is trained by examples\nsynthesized with GPT-3 and existing datasets. We demonstrate PromptCap's\neffectiveness on an existing pipeline in which GPT-3 is prompted with image\ncaptions to carry out VQA. PromptCap outperforms generic captions by a large\nmargin and achieves state-of-the-art accuracy on knowledge-based VQA tasks\n(60.4% on OK-VQA and 59.6% on A-OKVQA). Zero-shot results on WebQA show that\nPromptCap generalizes well to unseen domains.\n","authors":["Yushi Hu","Hang Hua","Zhengyuan Yang","Weijia Shi","Noah A. Smith","Jiebo Luo"],"pdf_url":"https://arxiv.org/pdf/2211.09699v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2207.02625v6","updated":"2023-03-21T11:58:36Z","published":"2022-07-06T12:34:33Z","title":"$L_2$BN: Enhancing Batch Normalization by Equalizing the $L_2$ Norms of\n  Features","summary":"  In this paper, we analyze batch normalization from the perspective of\ndiscriminability and find the disadvantages ignored by previous studies: the\ndifference in $l_2$ norms of sample features can hinder batch normalization\nfrom obtaining more distinguished inter-class features and more compact\nintra-class features. To address this issue, we propose a simple yet effective\nmethod to equalize the $l_2$ norms of sample features. Concretely, we\n$l_2$-normalize each sample feature before feeding them into batch\nnormalization, and therefore the features are of the same magnitude. Since the\nproposed method combines the $l_2$ normalization and batch normalization, we\nname our method $L_2$BN. The $L_2$BN can strengthen the compactness of\nintra-class features and enlarge the discrepancy of inter-class features. The\n$L_2$BN is easy to implement and can exert its effect without any additional\nparameters or hyper-parameters. We evaluate the effectiveness of $L_2$BN\nthrough extensive experiments with various models on image classification and\nacoustic scene classification tasks. The results demonstrate that the $L_2$BN\ncan boost the generalization ability of various neural network models and\nachieve considerable performance improvements.\n","authors":["Zhennan Wang","Kehan Li","Runyi Yu","Yian Zhao","Pengchong Qiao","Chang Liu","Fan Xu","Xiangyang Ji","Guoli Song","Jie Chen"],"pdf_url":"https://arxiv.org/pdf/2207.02625v6.pdf","comment":"12 pages, 8 figures"},{"id":"http://arxiv.org/abs/2303.05933v3","updated":"2023-03-21T11:52:47Z","published":"2023-03-10T14:11:09Z","title":"Self-Paced Learning for Open-Set Domain Adaptation","summary":"  Domain adaptation tackles the challenge of generalizing knowledge acquired\nfrom a source domain to a target domain with different data distributions.\nTraditional domain adaptation methods presume that the classes in the source\nand target domains are identical, which is not always the case in real-world\nscenarios. Open-set domain adaptation (OSDA) addresses this limitation by\nallowing previously unseen classes in the target domain. Open-set domain\nadaptation aims to not only recognize target samples belonging to common\nclasses shared by source and target domains but also perceive unknown class\nsamples. We propose a novel framework based on self-paced learning to\ndistinguish common and unknown class samples precisely, referred to as SPLOS\n(self-paced learning for open-set). To utilize unlabeled target samples for\nself-paced learning, we generate pseudo labels and design a cross-domain mixup\nmethod tailored for OSDA scenarios. This strategy minimizes the noise from\npseudo labels and ensures our model progressively learns common class features\nof the target domain, beginning with simpler examples and advancing to more\ncomplex ones. Furthermore, unlike existing OSDA methods that require manual\nhyperparameter $threshold$ tuning to separate common and unknown classes, our\napproach self-tunes a suitable threshold, eliminating the need for empirical\ntuning during testing. Comprehensive experiments illustrate that our method\nconsistently achieves superior performance on different benchmarks compared\nwith various state-of-the-art methods.\n","authors":["Xinghong Liu","Yi Zhou","Tao Zhou","Jie Qin","Shengcai Liao"],"pdf_url":"https://arxiv.org/pdf/2303.05933v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.11771v1","updated":"2023-03-21T11:42:57Z","published":"2023-03-21T11:42:57Z","title":"Self-Sufficient Framework for Continuous Sign Language Recognition","summary":"  The goal of this work is to develop self-sufficient framework for Continuous\nSign Language Recognition (CSLR) that addresses key issues of sign language\nrecognition. These include the need for complex multi-scale features such as\nhands, face, and mouth for understanding, and absence of frame-level\nannotations. To this end, we propose (1) Divide and Focus Convolution (DFConv)\nwhich extracts both manual and non-manual features without the need for\nadditional networks or annotations, and (2) Dense Pseudo-Label Refinement\n(DPLR) which propagates non-spiky frame-level pseudo-labels by combining the\nground truth gloss sequence labels with the predicted sequence. We demonstrate\nthat our model achieves state-of-the-art performance among RGB-based methods on\nlarge-scale CSLR benchmarks, PHOENIX-2014 and PHOENIX-2014-T, while showing\ncomparable results with better efficiency when compared to other approaches\nthat use multi-modality or extra annotations.\n","authors":["Youngjoon Jang","Youngtaek Oh","Jae Won Cho","Myungchul Kim","Dong-Jin Kim","In So Kweon","Joon Son Chung"],"pdf_url":"https://arxiv.org/pdf/2303.11771v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2301.07389v2","updated":"2023-03-21T11:40:47Z","published":"2023-01-18T09:36:41Z","title":"Towards Models that Can See and Read","summary":"  Visual Question Answering (VQA) and Image Captioning (CAP), which are among\nthe most popular vision-language tasks, have analogous scene-text versions that\nrequire reasoning from the text in the image. Despite their obvious\nresemblance, the two are treated independently and, as we show, yield\ntask-specific methods that can either see or read, but not both. In this work,\nwe conduct an in-depth analysis of this phenomenon and propose UniTNT, a\nUnified Text-Non-Text approach, which grants existing multimodal architectures\nscene-text understanding capabilities. Specifically, we treat scene-text\ninformation as an additional modality, fusing it with any pretrained\nencoder-decoder-based architecture via designated modules. Thorough experiments\nreveal that UniTNT leads to the first single model that successfully handles\nboth task types. Moreover, we show that scene-text understanding capabilities\ncan boost vision-language models' performance on general VQA and CAP by up to\n2.69% and 0.6 CIDEr, respectively.\n","authors":["Roy Ganz","Oren Nuriel","Aviad Aberdam","Yair Kittenplon","Shai Mazor","Ron Litman"],"pdf_url":"https://arxiv.org/pdf/2301.07389v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.11759v1","updated":"2023-03-21T11:23:59Z","published":"2023-03-21T11:23:59Z","title":"Simulating Malaria Detection in Laboratories using Deep Learning","summary":"  Malaria is usually diagnosed by a microbiologist by examining a small sample\nof blood smear. Reducing mortality from malaria infection is possible if it is\ndiagnosed early and followed with appropriate treatment. While the WHO has set\naudacious goals of reducing malaria incidence and mortality rates by 90% in\n2030 and eliminating malaria in 35 countries by that time, it still remains a\ndifficult challenge. Computer-assisted diagnostics are on the rise these days\nas they can be used effectively as a primary test in the absence of or\nproviding assistance to a physician or pathologist. The purpose of this paper\nis to describe an approach to detecting, localizing and counting parasitic\ncells in blood sample images towards easing the burden on healthcare workers.\n","authors":["Onyekachukwu R. Okonji"],"pdf_url":"https://arxiv.org/pdf/2303.11759v1.pdf","comment":"16 pages, 8 figures"},{"id":"http://arxiv.org/abs/2303.11755v1","updated":"2023-03-21T11:20:34Z","published":"2023-03-21T11:20:34Z","title":"LIMITR: Leveraging Local Information for Medical Image-Text\n  Representation","summary":"  Medical imaging analysis plays a critical role in the diagnosis and treatment\nof various medical conditions. This paper focuses on chest X-ray images and\ntheir corresponding radiological reports. It presents a new model that learns a\njoint X-ray image & report representation. The model is based on a novel\nalignment scheme between the visual data and the text, which takes into account\nboth local and global information. Furthermore, the model integrates\ndomain-specific information of two types -- lateral images and the consistent\nvisual structure of chest images. Our representation is shown to benefit three\ntypes of retrieval tasks: text-image retrieval, class-based retrieval, and\nphrase-grounding.\n","authors":["Gefen Dawidowicz","Elad Hirsch","Ayellet Tal"],"pdf_url":"https://arxiv.org/pdf/2303.11755v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2211.15377v3","updated":"2023-03-21T11:19:03Z","published":"2022-11-23T09:57:17Z","title":"Whose Emotion Matters? Speaking Activity Localisation without Prior\n  Knowledge","summary":"  The task of emotion recognition in conversations (ERC) benefits from the\navailability of multiple modalities, as provided, for example, in the\nvideo-based Multimodal EmotionLines Dataset (MELD). However, only a few\nresearch approaches use both acoustic and visual information from the MELD\nvideos. There are two reasons for this: First, label-to-video alignments in\nMELD are noisy, making those videos an unreliable source of emotional speech\ndata. Second, conversations can involve several people in the same scene, which\nrequires the localisation of the utterance source. In this paper, we introduce\nMELD with Fixed Audiovisual Information via Realignment (MELD-FAIR) by using\nrecent active speaker detection and automatic speech recognition models, we are\nable to realign the videos of MELD and capture the facial expressions from\nspeakers in 96.92% of the utterances provided in MELD. Experiments with a\nself-supervised voice recognition model indicate that the realigned MELD-FAIR\nvideos more closely match the transcribed utterances given in the MELD dataset.\nFinally, we devise a model for emotion recognition in conversations trained on\nthe realigned MELD-FAIR videos, which outperforms state-of-the-art models for\nERC based on vision alone. This indicates that localising the source of\nspeaking activities is indeed effective for extracting facial expressions from\nthe uttering speakers and that faces provide more informative visual cues than\nthe visual features state-of-the-art models have been using so far. The\nMELD-FAIR realignment data, and the code of the realignment procedure and of\nthe emotional recognition, are available at\nhttps://github.com/knowledgetechnologyuhh/MELD-FAIR.\n","authors":["Hugo Carneiro","Cornelius Weber","Stefan Wermter"],"pdf_url":"https://arxiv.org/pdf/2211.15377v3.pdf","comment":"17 pages, 8 figures, 7 tables"},{"id":"http://arxiv.org/abs/2303.11749v1","updated":"2023-03-21T11:15:03Z","published":"2023-03-21T11:15:03Z","title":"Detecting Everything in the Open World: Towards Universal Object\n  Detection","summary":"  In this paper, we formally address universal object detection, which aims to\ndetect every scene and predict every category. The dependence on human\nannotations, the limited visual information, and the novel categories in the\nopen world severely restrict the universality of traditional detectors. We\npropose \\textbf{UniDetector}, a universal object detector that has the ability\nto recognize enormous categories in the open world. The critical points for the\nuniversality of UniDetector are: 1) it leverages images of multiple sources and\nheterogeneous label spaces for training through the alignment of image and text\nspaces, which guarantees sufficient information for universal representations.\n2) it generalizes to the open world easily while keeping the balance between\nseen and unseen classes, thanks to abundant information from both vision and\nlanguage modalities. 3) it further promotes the generalization ability to novel\ncategories through our proposed decoupling training manner and probability\ncalibration. These contributions allow UniDetector to detect over 7k\ncategories, the largest measurable category size so far, with only about 500\nclasses participating in training. Our UniDetector behaves the strong zero-shot\ngeneralization ability on large-vocabulary datasets like LVIS, ImageNetBoxes,\nand VisualGenome - it surpasses the traditional supervised baselines by more\nthan 4\\% on average without seeing any corresponding images. On 13 public\ndetection datasets with various scenes, UniDetector also achieves\nstate-of-the-art performance with only a 3\\% amount of training data.\n","authors":["Zhenyu Wang","Yali Li","Xi Chen","Ser-Nam Lim","Antonio Torralba","Hengshuang Zhao","Shengjin Wang"],"pdf_url":"https://arxiv.org/pdf/2303.11749v1.pdf","comment":"Accepted by CVPR2023"},{"id":"http://arxiv.org/abs/2303.05312v2","updated":"2023-03-21T11:01:23Z","published":"2023-03-09T15:00:12Z","title":"3D Video Loops from Asynchronous Input","summary":"  Looping videos are short video clips that can be looped endlessly without\nvisible seams or artifacts. They provide a very attractive way to capture the\ndynamism of natural scenes. Existing methods have been mostly limited to 2D\nrepresentations. In this paper, we take a step forward and propose a practical\nsolution that enables an immersive experience on dynamic 3D looping scenes. The\nkey challenge is to consider the per-view looping conditions from asynchronous\ninput while maintaining view consistency for the 3D representation. We propose\na novel sparse 3D video representation, namely Multi-Tile Video (MTV), which\nnot only provides a view-consistent prior, but also greatly reduces memory\nusage, making the optimization of a 4D volume tractable. Then, we introduce a\ntwo-stage pipeline to construct the 3D looping MTV from completely asynchronous\nmulti-view videos with no time overlap. A novel looping loss based on video\ntemporal retargeting algorithms is adopted during the optimization to loop the\n3D scene. Experiments of our framework have shown promise in successfully\ngenerating and rendering photorealistic 3D looping videos in real time even on\nmobile devices. The code, dataset, and live demos are available in\nhttps://limacv.github.io/VideoLoop3D_web/.\n","authors":["Li Ma","Xiaoyu Li","Jing Liao","Pedro V. Sander"],"pdf_url":"https://arxiv.org/pdf/2303.05312v2.pdf","comment":"For more information, please visit the homepage at\n  https://limacv.github.io/VideoLoop3D_web/"},{"id":"http://arxiv.org/abs/2302.14115v2","updated":"2023-03-21T11:01:09Z","published":"2023-02-27T19:53:49Z","title":"Vid2Seq: Large-Scale Pretraining of a Visual Language Model for Dense\n  Video Captioning","summary":"  In this work, we introduce Vid2Seq, a multi-modal single-stage dense event\ncaptioning model pretrained on narrated videos which are readily-available at\nscale. The Vid2Seq architecture augments a language model with special time\ntokens, allowing it to seamlessly predict event boundaries and textual\ndescriptions in the same output sequence. Such a unified model requires\nlarge-scale training data, which is not available in current annotated\ndatasets. We show that it is possible to leverage unlabeled narrated videos for\ndense video captioning, by reformulating sentence boundaries of transcribed\nspeech as pseudo event boundaries, and using the transcribed speech sentences\nas pseudo event captions. The resulting Vid2Seq model pretrained on the\nYT-Temporal-1B dataset improves the state of the art on a variety of dense\nvideo captioning benchmarks including YouCook2, ViTT and ActivityNet Captions.\nVid2Seq also generalizes well to the tasks of video paragraph captioning and\nvideo clip captioning, and to few-shot settings. Our code is publicly available\nat https://antoyang.github.io/vid2seq.html.\n","authors":["Antoine Yang","Arsha Nagrani","Paul Hongsuck Seo","Antoine Miech","Jordi Pont-Tuset","Ivan Laptev","Josef Sivic","Cordelia Schmid"],"pdf_url":"https://arxiv.org/pdf/2302.14115v2.pdf","comment":"CVPR 2023 Camera-Ready; Project Webpage:\n  https://antoyang.github.io/vid2seq.html ; 18 pages; 6 figures"},{"id":"http://arxiv.org/abs/2303.11739v1","updated":"2023-03-21T10:56:57Z","published":"2023-03-21T10:56:57Z","title":"Data-efficient Large Scale Place Recognition with Graded Similarity\n  Supervision","summary":"  Visual place recognition (VPR) is a fundamental task of computer vision for\nvisual localization. Existing methods are trained using image pairs that either\ndepict the same place or not. Such a binary indication does not consider\ncontinuous relations of similarity between images of the same place taken from\ndifferent positions, determined by the continuous nature of camera pose. The\nbinary similarity induces a noisy supervision signal into the training of VPR\nmethods, which stall in local minima and require expensive hard mining\nalgorithms to guarantee convergence. Motivated by the fact that two images of\nthe same place only partially share visual cues due to camera pose differences,\nwe deploy an automatic re-annotation strategy to re-label VPR datasets. We\ncompute graded similarity labels for image pairs based on available\nlocalization metadata. Furthermore, we propose a new Generalized Contrastive\nLoss (GCL) that uses graded similarity labels for training contrastive\nnetworks. We demonstrate that the use of the new labels and GCL allow to\ndispense from hard-pair mining, and to train image descriptors that perform\nbetter in VPR by nearest neighbor search, obtaining superior or comparable\nresults than methods that require expensive hard-pair mining and re-ranking\ntechniques. Code and models available at:\nhttps://github.com/marialeyvallina/generalized_contrastive_loss\n","authors":["Maria Leyva-Vallina","Nicola Strisciuglio","Nicolai Petkov"],"pdf_url":"https://arxiv.org/pdf/2303.11739v1.pdf","comment":"Accepted at CVPR 2023"},{"id":"http://arxiv.org/abs/2303.11732v1","updated":"2023-03-21T10:40:13Z","published":"2023-03-21T10:40:13Z","title":"Multi-modal Prompting for Low-Shot Temporal Action Localization","summary":"  In this paper, we consider the problem of temporal action localization under\nlow-shot (zero-shot & few-shot) scenario, with the goal of detecting and\nclassifying the action instances from arbitrary categories within some\nuntrimmed videos, even not seen at training time. We adopt a Transformer-based\ntwo-stage action localization architecture with class-agnostic action proposal,\nfollowed by open-vocabulary classification. We make the following\ncontributions. First, to compensate image-text foundation models with temporal\nmotions, we improve category-agnostic action proposal by explicitly aligning\nembeddings of optical flows, RGB and texts, which has largely been ignored in\nexisting low-shot methods. Second, to improve open-vocabulary action\nclassification, we construct classifiers with strong discriminative power,\ni.e., avoid lexical ambiguities. To be specific, we propose to prompt the\npre-trained CLIP text encoder either with detailed action descriptions\n(acquired from large-scale language models), or visually-conditioned\ninstance-specific prompt vectors. Third, we conduct thorough experiments and\nablation studies on THUMOS14 and ActivityNet1.3, demonstrating the superior\nperformance of our proposed model, outperforming existing state-of-the-art\napproaches by one significant margin.\n","authors":["Chen Ju","Zeqian Li","Peisen Zhao","Ya Zhang","Xiaopeng Zhang","Qi Tian","Yanfeng Wang","Weidi Xie"],"pdf_url":"https://arxiv.org/pdf/2303.11732v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.11730v1","updated":"2023-03-21T10:34:39Z","published":"2023-03-21T10:34:39Z","title":"Abstract Visual Reasoning: An Algebraic Approach for Solving Raven's\n  Progressive Matrices","summary":"  We introduce algebraic machine reasoning, a new reasoning framework that is\nwell-suited for abstract reasoning. Effectively, algebraic machine reasoning\nreduces the difficult process of novel problem-solving to routine algebraic\ncomputation. The fundamental algebraic objects of interest are the ideals of\nsome suitably initialized polynomial ring. We shall explain how solving Raven's\nProgressive Matrices (RPMs) can be realized as computational problems in\nalgebra, which combine various well-known algebraic subroutines that include:\nComputing the Gr\\\"obner basis of an ideal, checking for ideal containment, etc.\nCrucially, the additional algebraic structure satisfied by ideals allows for\nmore operations on ideals beyond set-theoretic operations.\n  Our algebraic machine reasoning framework is not only able to select the\ncorrect answer from a given answer set, but also able to generate the correct\nanswer with only the question matrix given. Experiments on the I-RAVEN dataset\nyield an overall $93.2\\%$ accuracy, which significantly outperforms the current\nstate-of-the-art accuracy of $77.0\\%$ and exceeds human performance at $84.4\\%$\naccuracy.\n","authors":["Jingyi Xu","Tushar Vaidya","Yufei Wu","Saket Chandra","Zhangsheng Lai","Kai Fong Ernest Chong"],"pdf_url":"https://arxiv.org/pdf/2303.11730v1.pdf","comment":"Accepted at IEEE/CVF Conference on Computer Vision and Pattern\n  Recognition (CVPR) 2023. 30 pages, 7 figures (including supplementary\n  material). First three authors contributed equally. Code is available at:\n  https://github.com/Xu-Jingyi/AlgebraicMR"},{"id":"http://arxiv.org/abs/2303.11728v1","updated":"2023-03-21T10:32:27Z","published":"2023-03-21T10:32:27Z","title":"ExtremeNeRF: Few-shot Neural Radiance Fields Under Unconstrained\n  Illumination","summary":"  In this paper, we propose a new challenge that synthesizes a novel view in a\nmore practical environment, where the number of input multi-view images is\nlimited and illumination variations are significant. Despite recent success,\nneural radiance fields (NeRF) require a massive amount of input multi-view\nimages taken under constrained illuminations. To address the problem, we\nsuggest ExtremeNeRF, which utilizes occlusion-aware multiview albedo\nconsistency, supported by geometric alignment and depth consistency. We extract\nintrinsic image components that should be illumination-invariant across\ndifferent views, enabling direct appearance comparison between the input and\nnovel view under unconstrained illumination. We provide extensive experimental\nresults for an evaluation of the task, using the newly built NeRF Extreme\nbenchmark, which is the first in-the-wild novel view synthesis benchmark taken\nunder multiple viewing directions and varying illuminations. The project page\nis at https://seokyeong94.github.io/ExtremeNeRF/\n","authors":["SeokYeong Lee","JunYong Choi","Seungryong Kim","Ig-Jae Kim","Junghyun Cho"],"pdf_url":"https://arxiv.org/pdf/2303.11728v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2111.12273v5","updated":"2023-03-21T10:31:46Z","published":"2021-11-24T05:16:41Z","title":"Sharpness-aware Quantization for Deep Neural Networks","summary":"  Network quantization is a dominant paradigm of model compression. However,\nthe abrupt changes in quantized weights during training often lead to severe\nloss fluctuations and result in a sharp loss landscape, making the gradients\nunstable and thus degrading the performance. Recently, Sharpness-Aware\nMinimization (SAM) has been proposed to smooth the loss landscape and improve\nthe generalization performance of the models. Nevertheless, directly applying\nSAM to the quantized models can lead to perturbation mismatch or diminishment\nissues, resulting in suboptimal performance. In this paper, we propose a novel\nmethod, dubbed Sharpness-Aware Quantization (SAQ), to explore the effect of SAM\nin model compression, particularly quantization for the first time.\nSpecifically, we first provide a unified view of quantization and SAM by\ntreating them as introducing quantization noises and adversarial perturbations\nto the model weights, respectively. According to whether the noise and\nperturbation terms depend on each other, SAQ can be formulated into three\ncases, which are analyzed and compared comprehensively. Furthermore, by\nintroducing an efficient training strategy, SAQ only incurs a little additional\ntraining overhead compared with the default optimizer (e.g., SGD or AdamW).\nExtensive experiments on both convolutional neural networks and Transformers\nacross various datasets (i.e., ImageNet, CIFAR-10/100, Oxford Flowers-102,\nOxford-IIIT Pets) show that SAQ improves the generalization performance of the\nquantized models, yielding the SOTA results in uniform quantization. For\nexample, on ImageNet, SAQ outperforms AdamW by 1.2% on the Top-1 accuracy for\n4-bit ViT-B/16. Our 4-bit ResNet-50 surpasses the previous SOTA method by 0.9%\non the Top-1 accuracy.\n","authors":["Jing Liu","Jianfei Cai","Bohan Zhuang"],"pdf_url":"https://arxiv.org/pdf/2111.12273v5.pdf","comment":"Tech report"},{"id":"http://arxiv.org/abs/2303.11726v1","updated":"2023-03-21T10:30:43Z","published":"2023-03-21T10:30:43Z","title":"3D Human Mesh Estimation from Virtual Markers","summary":"  Inspired by the success of volumetric 3D pose estimation, some recent human\nmesh estimators propose to estimate 3D skeletons as intermediate\nrepresentations, from which, the dense 3D meshes are regressed by exploiting\nthe mesh topology. However, body shape information is lost in extracting\nskeletons, leading to mediocre performance. The advanced motion capture systems\nsolve the problem by placing dense physical markers on the body surface, which\nallows to extract realistic meshes from their non-rigid motions. However, they\ncannot be applied to wild images without markers. In this work, we present an\nintermediate representation, named virtual markers, which learns 64 landmark\nkeypoints on the body surface based on the large-scale mocap data in a\ngenerative style, mimicking the effects of physical markers. The virtual\nmarkers can be accurately detected from wild images and can reconstruct the\nintact meshes with realistic shapes by simple interpolation. Our approach\noutperforms the state-of-the-art methods on three datasets. In particular, it\nsurpasses the existing methods by a notable margin on the SURREAL dataset,\nwhich has diverse body shapes. Code is available at\nhttps://github.com/ShirleyMaxx/VirtualMarker.\n","authors":["Xiaoxuan Ma","Jiajun Su","Chunyu Wang","Wentao Zhu","Yizhou Wang"],"pdf_url":"https://arxiv.org/pdf/2303.11726v1.pdf","comment":"CVPR 2023"},{"id":"http://arxiv.org/abs/2303.11724v1","updated":"2023-03-21T10:29:30Z","published":"2023-03-21T10:29:30Z","title":"Task-based Generation of Optimized Projection Sets using Differentiable\n  Ranking","summary":"  We present a method for selecting valuable projections in computed tomography\n(CT) scans to enhance image reconstruction and diagnosis. The approach\nintegrates two important factors, projection-based detectability and data\ncompleteness, into a single feed-forward neural network. The network evaluates\nthe value of projections, processes them through a differentiable ranking\nfunction and makes the final selection using a straight-through estimator. Data\ncompleteness is ensured through the label provided during training. The\napproach eliminates the need for heuristically enforcing data completeness,\nwhich may exclude valuable projections. The method is evaluated on simulated\ndata in a non-destructive testing scenario, where the aim is to maximize the\nreconstruction quality within a specified region of interest. We achieve\ncomparable results to previous methods, laying the foundation for using\nreconstruction-based loss functions to learn the selection of projections.\n","authors":["Linda-Sophie Schneider","Mareike Thies","Christopher Syben","Richard Schielein","Mathias Unberath","Andreas Maier"],"pdf_url":"https://arxiv.org/pdf/2303.11724v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.11722v1","updated":"2023-03-21T10:24:29Z","published":"2023-03-21T10:24:29Z","title":"Implicit Neural Representation for Cooperative Low-light Image\n  Enhancement","summary":"  The following three factors restrict the application of existing low-light\nimage enhancement methods: unpredictable brightness degradation and noise,\ninherent gap between metric-favorable and visual-friendly versions, and the\nlimited paired training data. To address these limitations, we propose an\nimplicit Neural Representation method for Cooperative low-light image\nenhancement, dubbed NeRCo. It robustly recovers perceptual-friendly results in\nan unsupervised manner. Concretely, NeRCo unifies the diverse degradation\nfactors of real-world scenes with a controllable fitting function, leading to\nbetter robustness. In addition, for the output results, we introduce\nsemantic-orientated supervision with priors from the pre-trained\nvision-language model. Instead of merely following reference images, it\nencourages results to meet subjective expectations, finding more\nvisual-friendly solutions. Further, to ease the reliance on paired data and\nreduce solution space, we develop a dual-closed-loop constrained enhancement\nmodule. It is trained cooperatively with other affiliated modules in a\nself-supervised manner. Finally, extensive experiments demonstrate the\nrobustness and superior effectiveness of our proposed NeRCo. Our code is\navailable at https://github.com/Ysz2022/NeRCo.\n","authors":["Shuzhou Yang","Moxuan Ding","Yanmin Wu","Zihan Li","Jian Zhang"],"pdf_url":"https://arxiv.org/pdf/2303.11722v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2212.09069v2","updated":"2023-03-21T10:23:40Z","published":"2022-12-18T11:43:32Z","title":"Masked Wavelet Representation for Compact Neural Radiance Fields","summary":"  Neural radiance fields (NeRF) have demonstrated the potential of\ncoordinate-based neural representation (neural fields or implicit neural\nrepresentation) in neural rendering. However, using a multi-layer perceptron\n(MLP) to represent a 3D scene or object requires enormous computational\nresources and time. There have been recent studies on how to reduce these\ncomputational inefficiencies by using additional data structures, such as grids\nor trees. Despite the promising performance, the explicit data structure\nnecessitates a substantial amount of memory. In this work, we present a method\nto reduce the size without compromising the advantages of having additional\ndata structures. In detail, we propose using the wavelet transform on\ngrid-based neural fields. Grid-based neural fields are for fast convergence,\nand the wavelet transform, whose efficiency has been demonstrated in\nhigh-performance standard codecs, is to improve the parameter efficiency of\ngrids. Furthermore, in order to achieve a higher sparsity of grid coefficients\nwhile maintaining reconstruction quality, we present a novel trainable masking\napproach. Experimental results demonstrate that non-spatial grid coefficients,\nsuch as wavelet coefficients, are capable of attaining a higher level of\nsparsity than spatial grid coefficients, resulting in a more compact\nrepresentation. With our proposed mask and compression pipeline, we achieved\nstate-of-the-art performance within a memory budget of 2 MB. Our code is\navailable at https://github.com/daniel03c1/masked_wavelet_nerf.\n","authors":["Daniel Rho","Byeonghyeon Lee","Seungtae Nam","Joo Chan Lee","Jong Hwan Ko","Eunbyung Park"],"pdf_url":"https://arxiv.org/pdf/2212.09069v2.pdf","comment":"Accepted to CVPR 2023"},{"id":"http://arxiv.org/abs/2303.11720v1","updated":"2023-03-21T10:14:11Z","published":"2023-03-21T10:14:11Z","title":"Lidar Line Selection with Spatially-Aware Shapley Value for\n  Cost-Efficient Depth Completion","summary":"  Lidar is a vital sensor for estimating the depth of a scene. Typical spinning\nlidars emit pulses arranged in several horizontal lines and the monetary cost\nof the sensor increases with the number of these lines. In this work, we\npresent the new problem of optimizing the positioning of lidar lines to find\nthe most effective configuration for the depth completion task. We propose a\nsolution to reduce the number of lines while retaining the up-to-the-mark\nquality of depth completion. Our method consists of two components, (1) line\nselection based on the marginal contribution of a line computed via the Shapley\nvalue and (2) incorporating line position spread to take into account its need\nto arrive at image-wide depth completion. Spatially-aware Shapley values (SaS)\nsucceed in selecting line subsets that yield a depth accuracy comparable to the\nfull lidar input while using just half of the lines.\n","authors":["Kamil Adamczewski","Christos Sakaridis","Vaishakh Patil","Luc Van Gool"],"pdf_url":"https://arxiv.org/pdf/2303.11720v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.09730v2","updated":"2023-03-21T10:11:01Z","published":"2023-03-17T02:19:28Z","title":"ElasticViT: Conflict-aware Supernet Training for Deploying Fast Vision\n  Transformer on Diverse Mobile Devices","summary":"  Neural Architecture Search (NAS) has shown promising performance in the\nautomatic design of vision transformers (ViT) exceeding 1G FLOPs. However,\ndesigning lightweight and low-latency ViT models for diverse mobile devices\nremains a big challenge. In this work, we propose ElasticViT, a two-stage NAS\napproach that trains a high-quality ViT supernet over a very large search space\nthat supports a wide range of mobile devices, and then searches an optimal\nsub-network (subnet) for direct deployment. However, prior supernet training\nmethods that rely on uniform sampling suffer from the gradient conflict issue:\nthe sampled subnets can have vastly different model sizes (e.g., 50M vs. 2G\nFLOPs), leading to different optimization directions and inferior performance.\nTo address this challenge, we propose two novel sampling techniques:\ncomplexity-aware sampling and performance-aware sampling. Complexity-aware\nsampling limits the FLOPs difference among the subnets sampled across adjacent\ntraining steps, while covering different-sized subnets in the search space.\nPerformance-aware sampling further selects subnets that have good accuracy,\nwhich can reduce gradient conflicts and improve supernet quality. Our\ndiscovered models, ElasticViT models, achieve top-1 accuracy from 67.2% to\n80.0% on ImageNet from 60M to 800M FLOPs without extra retraining,\noutperforming all prior CNNs and ViTs in terms of accuracy and latency. Our\ntiny and small models are also the first ViT models that surpass\nstate-of-the-art CNNs with significantly lower latency on mobile devices. For\ninstance, ElasticViT-S1 runs 2.62x faster than EfficientNet-B0 with 0.1% higher\naccuracy.\n","authors":["Chen Tang","Li Lyna Zhang","Huiqiang Jiang","Jiahang Xu","Ting Cao","Quanlu Zhang","Yuqing Yang","Zhi Wang","Mao Yang"],"pdf_url":"https://arxiv.org/pdf/2303.09730v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.11717v1","updated":"2023-03-21T10:09:47Z","published":"2023-03-21T10:09:47Z","title":"A Complete Survey on Generative AI (AIGC): Is ChatGPT from GPT-4 to\n  GPT-5 All You Need?","summary":"  As ChatGPT goes viral, generative AI (AIGC, a.k.a AI-generated content) has\nmade headlines everywhere because of its ability to analyze and create text,\nimages, and beyond. With such overwhelming media coverage, it is almost\nimpossible for us to miss the opportunity to glimpse AIGC from a certain angle.\nIn the era of AI transitioning from pure analysis to creation, it is worth\nnoting that ChatGPT, with its most recent language model GPT-4, is just a tool\nout of numerous AIGC tasks. Impressed by the capability of the ChatGPT, many\npeople are wondering about its limits: can GPT-5 (or other future GPT variants)\nhelp ChatGPT unify all AIGC tasks for diversified content creation? Toward\nanswering this question, a comprehensive review of existing AIGC tasks is\nneeded. As such, our work comes to fill this gap promptly by offering a first\nlook at AIGC, ranging from its techniques to applications. Modern generative AI\nrelies on various technical foundations, ranging from model architecture and\nself-supervised pretraining to generative modeling methods (like GAN and\ndiffusion models). After introducing the fundamental techniques, this work\nfocuses on the technological development of various AIGC tasks based on their\noutput type, including text, images, videos, 3D content, etc., which depicts\nthe full potential of ChatGPT's future. Moreover, we summarize their\nsignificant applications in some mainstream industries, such as education and\ncreativity content. Finally, we discuss the challenges currently faced and\npresent an outlook on how generative AI might evolve in the near future.\n","authors":["Chaoning Zhang","Chenshuang Zhang","Sheng Zheng","Yu Qiao","Chenghao Li","Mengchun Zhang","Sumit Kumar Dam","Chu Myaet Thwal","Ye Lin Tun","Le Luang Huy","Donguk kim","Sung-Ho Bae","Lik-Hang Lee","Yang Yang","Heng Tao Shen","In So Kweon","Choong Seon Hong"],"pdf_url":"https://arxiv.org/pdf/2303.11717v1.pdf","comment":"56 pages, 548 citations"},{"id":"http://arxiv.org/abs/2303.11705v1","updated":"2023-03-21T09:51:19Z","published":"2023-03-21T09:51:19Z","title":"A Single-Step Multiclass SVM based on Quantum Annealing for Remote\n  Sensing Data Classification","summary":"  In recent years, the development of quantum annealers has enabled\nexperimental demonstrations and has increased research interest in applications\nof quantum annealing, such as in quantum machine learning and in particular for\nthe popular quantum SVM. Several versions of the quantum SVM have been\nproposed, and quantum annealing has been shown to be effective in them.\nExtensions to multiclass problems have also been made, which consist of an\nensemble of multiple binary classifiers. This work proposes a novel quantum SVM\nformulation for direct multiclass classification based on quantum annealing,\ncalled Quantum Multiclass SVM (QMSVM). The multiclass classification problem is\nformulated as a single Quadratic Unconstrained Binary Optimization (QUBO)\nproblem solved with quantum annealing. The main objective of this work is to\nevaluate the feasibility, accuracy, and time performance of this approach.\nExperiments have been performed on the D-Wave Advantage quantum annealer for a\nclassification problem on remote sensing data. The results indicate that,\ndespite the memory demands of the quantum annealer, QMSVM can achieve accuracy\nthat is comparable to standard SVM methods and, more importantly, it scales\nmuch more efficiently with the number of training examples, resulting in nearly\nconstant time. This work shows an approach for bringing together classical and\nquantum computation, solving practical problems in remote sensing with current\nhardware.\n","authors":["Amer Delilbasic","Bertrand Le Saux","Morris Riedel","Kristel Michielsen","Gabriele Cavallaro"],"pdf_url":"https://arxiv.org/pdf/2303.11705v1.pdf","comment":"12 pages, 10 figures, 3 tables. Submitted to IEEE JSTARS"},{"id":"http://arxiv.org/abs/2303.11702v1","updated":"2023-03-21T09:42:27Z","published":"2023-03-21T09:42:27Z","title":"Linking generative semi-supervised learning and generative open-set\n  recognition","summary":"  This study investigates the relationship between semi-supervised learning\n(SSL) and open-set recognition (OSR) in the context of generative adversarial\nnetworks (GANs). Although no previous study has formally linked SSL and OSR,\ntheir respective methods share striking similarities. Specifically, SSL-GANs\nand OSR-GANs require generator to produce samples in the complementary space.\nSubsequently, by regularising networks with generated samples, both SSL and OSR\nclassifiers generalize the open space. To demonstrate the connection between\nSSL and OSR, we theoretically and experimentally compare state-of-the-art\nSSL-GAN methods with state-of-the-art OSR-GAN methods. Our results indicate\nthat the SSL optimised margin-GANs, which have a stronger foundation in\nliterature, set the new standard for the combined SSL-OSR task and achieves new\nstate-of-other art results in certain general OSR experiments. However, the OSR\noptimised adversarial reciprocal point (ARP)-GANs still slightly out-performed\nmargin-GANs at other OSR experiments. This result indicates unique insights for\nthe combined optimisation task of SSL-OSR.\n","authors":["Emile Reyn Engelbrecht","Johan du Preez"],"pdf_url":"https://arxiv.org/pdf/2303.11702v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.11701v1","updated":"2023-03-21T09:41:13Z","published":"2023-03-21T09:41:13Z","title":"A High-Frequency Focused Network for Lightweight Single Image\n  Super-Resolution","summary":"  Lightweight neural networks for single-image super-resolution (SISR) tasks\nhave made substantial breakthroughs in recent years. Compared to low-frequency\ninformation, high-frequency detail is much more difficult to reconstruct. Most\nSISR models allocate equal computational resources for low-frequency and\nhigh-frequency information, which leads to redundant processing of simple\nlow-frequency information and inadequate recovery of more challenging\nhigh-frequency information. We propose a novel High-Frequency Focused Network\n(HFFN) through High-Frequency Focused Blocks (HFFBs) that selectively enhance\nhigh-frequency information while minimizing redundant feature computation of\nlow-frequency information. The HFFB effectively allocates more computational\nresources to the more challenging reconstruction of high-frequency information.\nMoreover, we propose a Local Feature Fusion Block (LFFB) effectively fuses\nfeatures from multiple HFFBs in a local region, utilizing complementary\ninformation across layers to enhance feature representativeness and reduce\nartifacts in reconstructed images. We assess the efficacy of our proposed HFFN\non five benchmark datasets and show that it significantly enhances the\nsuper-resolution performance of the network. Our experimental results\ndemonstrate state-of-the-art performance in reconstructing high-frequency\ninformation while using a low number of parameters.\n","authors":["Xiaotian Weng","Yi Chen","Zhichao Zheng","Yanhui Gu","Junsheng Zhou","Yudong Zhang"],"pdf_url":"https://arxiv.org/pdf/2303.11701v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.11694v1","updated":"2023-03-21T09:28:47Z","published":"2023-03-21T09:28:47Z","title":"Anchor Free remote sensing detector based on solving discrete polar\n  coordinate equation","summary":"  As the rapid development of depth learning, object detection in aviatic\nremote sensing images has become increasingly popular in recent years. Most of\nthe current Anchor Free detectors based on key point detection sampling\ndirectly regression and classification features, with the design of object loss\nfunction based on the horizontal bounding box. It is more challenging for\ncomplex and diverse aviatic remote sensing object. In this paper, we propose an\nAnchor Free aviatic remote sensing object detector (BWP-Det) to detect rotating\nand multi-scale object. Specifically, we design a interactive\ndouble-branch(IDB) up-sampling network, in which one branch gradually\nup-sampling is used for the prediction of Heatmap, and the other branch is used\nfor the regression of boundary box parameters. We improve a weighted\nmulti-scale convolution (WmConv) in order to highlight the difference between\nforeground and background. We extracted Pixel level attention features from the\nmiddle layer to guide the two branches to pay attention to effective object\ninformation in the sampling process. Finally, referring to the calculation idea\nof horizontal IoU, we design a rotating IoU based on the split polar coordinate\nplane, namely JIoU, which is expressed as the intersection ratio following\ndiscretization of the inner ellipse of the rotating bounding box, to solve the\ncorrelation between angle and side length in the regression process of the\nrotating bounding box. Ultimately, BWP-Det, our experiments on DOTA, UCAS-AOD\nand NWPU VHR-10 datasets show, achieves advanced performance with simpler\nmodels and fewer regression parameters.\n","authors":["Linfeng Shi","Yan Li","Xi Zhu"],"pdf_url":"https://arxiv.org/pdf/2303.11694v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2211.11317v2","updated":"2023-03-21T09:18:20Z","published":"2022-11-21T10:01:03Z","title":"DeSTSeg: Segmentation Guided Denoising Student-Teacher for Anomaly\n  Detection","summary":"  Visual anomaly detection, an important problem in computer vision, is usually\nformulated as a one-class classification and segmentation task. The\nstudent-teacher (S-T) framework has proved to be effective in solving this\nchallenge. However, previous works based on S-T only empirically applied\nconstraints on normal data and fused multi-level information. In this study, we\npropose an improved model called DeSTSeg, which integrates a pre-trained\nteacher network, a denoising student encoder-decoder, and a segmentation\nnetwork into one framework. First, to strengthen the constraints on anomalous\ndata, we introduce a denoising procedure that allows the student network to\nlearn more robust representations. From synthetically corrupted normal images,\nwe train the student network to match the teacher network feature of the same\nimages without corruption. Second, to fuse the multi-level S-T features\nadaptively, we train a segmentation network with rich supervision from\nsynthetic anomaly masks, achieving a substantial performance improvement.\nExperiments on the industrial inspection benchmark dataset demonstrate that our\nmethod achieves state-of-the-art performance, 98.6% on image-level AUC, 75.8%\non pixel-level average precision, and 76.4% on instance-level average\nprecision.\n","authors":["Xuan Zhang","Shiyu Li","Xi Li","Ping Huang","Jiulong Shan","Ting Chen"],"pdf_url":"https://arxiv.org/pdf/2211.11317v2.pdf","comment":"Accepted by CVPR 2023"},{"id":"http://arxiv.org/abs/2303.11686v1","updated":"2023-03-21T09:08:30Z","published":"2023-03-21T09:08:30Z","title":"Learning a 3D Morphable Face Reflectance Model from Low-cost Data","summary":"  Modeling non-Lambertian effects such as facial specularity leads to a more\nrealistic 3D Morphable Face Model. Existing works build parametric models for\ndiffuse and specular albedo using Light Stage data. However, only diffuse and\nspecular albedo cannot determine the full BRDF. In addition, the requirement of\nLight Stage data is hard to fulfill for the research communities. This paper\nproposes the first 3D morphable face reflectance model with spatially varying\nBRDF using only low-cost publicly-available data. We apply linear shiness\nweighting into parametric modeling to represent spatially varying specular\nintensity and shiness. Then an inverse rendering algorithm is developed to\nreconstruct the reflectance parameters from non-Light Stage data, which are\nused to train an initial morphable reflectance model. To enhance the model's\ngeneralization capability and expressive power, we further propose an\nupdate-by-reconstruction strategy to finetune it on an in-the-wild dataset.\nExperimental results show that our method obtains decent rendering results with\nplausible facial specularities. Our code is released\n\\href{https://yxuhan.github.io/ReflectanceMM/index.html}{\\textcolor{magenta}{here}}.\n","authors":["Yuxuan Han","Zhibo Wang","Feng Xu"],"pdf_url":"https://arxiv.org/pdf/2303.11686v1.pdf","comment":"CVPR 2023. Project page:\n  https://yxuhan.github.io/ReflectanceMM/index.html"},{"id":"http://arxiv.org/abs/2303.11684v1","updated":"2023-03-21T09:00:12Z","published":"2023-03-21T09:00:12Z","title":"SpikeCV: Open a Continuous Computer Vision Era","summary":"  SpikeCV is a new open-source computer vision platform for the spike camera,\nwhich is a neuromorphic visual sensor that has developed rapidly in recent\nyears. In the spike camera, each pixel position directly accumulates the light\nintensity and asynchronously fires spikes. The output binary spikes can reach a\nfrequency of 40,000 Hz. As a new type of visual expression, spike sequence has\nhigh spatiotemporal completeness and preserves the continuous visual\ninformation of the external world. Taking advantage of the low latency and high\ndynamic range of the spike camera, many spike-based algorithms have made\nsignificant progress, such as high-quality imaging and ultra-high-speed target\ndetection.\n  To build up a community ecology for the spike vision to facilitate more users\nto take advantage of the spike camera, SpikeCV provides a variety of\nultra-high-speed scene datasets, hardware interfaces, and an easy-to-use\nmodules library. SpikeCV focuses on encapsulation for spike data,\nstandardization for dataset interfaces, modularization for vision tasks, and\nreal-time applications for challenging scenes. With the advent of the\nopen-source Python ecosystem, modules of SpikeCV can be used as a Python\nlibrary to fulfilled most of the numerical analysis needs of researchers. We\ndemonstrate the efficiency of the SpikeCV on offline inference and real-time\napplications. The project repository address are\n\\url{https://openi.pcl.ac.cn/Cordium/SpikeCV} and\n\\url{https://github.com/Zyj061/SpikeCV\n","authors":["Yajing Zheng","Jiyuan Zhang","Rui Zhao","Jianhao Ding","Shiyan Chen","Ruiqin Xiong","Zhaofei Yu","Tiejun Huang"],"pdf_url":"https://arxiv.org/pdf/2303.11684v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2208.10930v3","updated":"2023-03-21T08:57:23Z","published":"2022-08-23T13:02:14Z","title":"FS-BAN: Born-Again Networks for Domain Generalization Few-Shot\n  Classification","summary":"  Conventional Few-shot classification (FSC) aims to recognize samples from\nnovel classes given limited labeled data. Recently, domain generalization FSC\n(DG-FSC) has been proposed with the goal to recognize novel class samples from\nunseen domains. DG-FSC poses considerable challenges to many models due to the\ndomain shift between base classes (used in training) and novel classes\n(encountered in evaluation). In this work, we make two novel contributions to\ntackle DG-FSC. Our first contribution is to propose Born-Again Network (BAN)\nepisodic training and comprehensively investigate its effectiveness for DG-FSC.\nAs a specific form of knowledge distillation, BAN has been shown to achieve\nimproved generalization in conventional supervised classification with a\nclosed-set setup. This improved generalization motivates us to study BAN for\nDG-FSC, and we show that BAN is promising to address the domain shift\nencountered in DG-FSC. Building on the encouraging findings, our second (major)\ncontribution is to propose Few-Shot BAN (FS-BAN), a novel BAN approach for\nDG-FSC. Our proposed FS-BAN includes novel multi-task learning objectives:\nMutual Regularization, Mismatched Teacher, and Meta-Control Temperature, each\nof these is specifically designed to overcome central and unique challenges in\nDG-FSC, namely overfitting and domain discrepancy. We analyze different design\nchoices of these techniques. We conduct comprehensive quantitative and\nqualitative analysis and evaluation over six datasets and three baseline\nmodels. The results suggest that our proposed FS-BAN consistently improves the\ngeneralization performance of baseline models and achieves state-of-the-art\naccuracy for DG-FSC. Project Page: https://yunqing-me.github.io/Born-Again-FS/.\n","authors":["Yunqing Zhao","Ngai-Man Cheung"],"pdf_url":"https://arxiv.org/pdf/2208.10930v3.pdf","comment":"15 pages, 9 figures, 15 tables. IEEE Transactions on Image Processing\n  (TIP), 2023"},{"id":"http://arxiv.org/abs/2303.11681v1","updated":"2023-03-21T08:43:15Z","published":"2023-03-21T08:43:15Z","title":"DiffuMask: Synthesizing Images with Pixel-level Annotations for Semantic\n  Segmentation Using Diffusion Models","summary":"  Collecting and annotating images with pixel-wise labels is time-consuming and\nlaborious. In contrast, synthetic data can be freely available using a\ngenerative model (e.g., DALL-E, Stable Diffusion). In this paper, we show that\nit is possible to automatically obtain accurate semantic masks of synthetic\nimages generated by the Off-the-shelf Stable Diffusion model, which uses only\ntext-image pairs during training. Our approach, called DiffuMask, exploits the\npotential of the cross-attention map between text and image, which is natural\nand seamless to extend the text-driven image synthesis to semantic mask\ngeneration. DiffuMask uses text-guided cross-attention information to localize\nclass/word-specific regions, which are combined with practical techniques to\ncreate a novel high-resolution and class-discriminative pixel-wise mask. The\nmethods help to reduce data collection and annotation costs obviously.\nExperiments demonstrate that the existing segmentation methods trained on\nsynthetic data of DiffuMask can achieve a competitive performance over the\ncounterpart of real data (VOC 2012, Cityscapes). For some classes (e.g., bird),\nDiffuMask presents promising performance, close to the stateof-the-art result\nof real data (within 3% mIoU gap). Moreover, in the open-vocabulary\nsegmentation (zero-shot) setting, DiffuMask achieves a new SOTA result on\nUnseen class of VOC 2012. The project website can be found at\nhttps://weijiawu.github.io/DiffusionMask/.\n","authors":["Weijia Wu","Yuzhong Zhao","Mike Zheng Shou","Hong Zhou","Chunhua Shen"],"pdf_url":"https://arxiv.org/pdf/2303.11681v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.11678v1","updated":"2023-03-21T08:41:54Z","published":"2023-03-21T08:41:54Z","title":"Full or Weak annotations? An adaptive strategy for budget-constrained\n  annotation campaigns","summary":"  Annotating new datasets for machine learning tasks is tedious,\ntime-consuming, and costly. For segmentation applications, the burden is\nparticularly high as manual delineations of relevant image content are often\nextremely expensive or can only be done by experts with domain-specific\nknowledge. Thanks to developments in transfer learning and training with weak\nsupervision, segmentation models can now also greatly benefit from annotations\nof different kinds. However, for any new domain application looking to use weak\nsupervision, the dataset builder still needs to define a strategy to distribute\nfull segmentation and other weak annotations. Doing so is challenging, however,\nas it is a priori unknown how to distribute an annotation budget for a given\nnew dataset. To this end, we propose a novel approach to determine annotation\nstrategies for segmentation datasets, whereby estimating what proportion of\nsegmentation and classification annotations should be collected given a fixed\nbudget. To do so, our method sequentially determines proportions of\nsegmentation and classification annotations to collect for budget-fractions by\nmodeling the expected improvement of the final segmentation model. We show in\nour experiments that our approach yields annotations that perform very close to\nthe optimal for a number of different annotation budgets and datasets.\n","authors":["Javier Gamazo Tejero","Martin S. Zinkernagel","Sebastian Wolf","Raphael Sznitman","Pablo Márquez Neila"],"pdf_url":"https://arxiv.org/pdf/2303.11678v1.pdf","comment":"CVPR23"},{"id":"http://arxiv.org/abs/2303.11676v1","updated":"2023-03-21T08:37:15Z","published":"2023-03-21T08:37:15Z","title":"Deep Learning Pipeline for Preprocessing and Segmenting Cardiac Magnetic\n  Resonance of Single Ventricle Patients from an Image Registry","summary":"  Purpose: To develop and evaluate an end-to-end deep learning pipeline for\nsegmentation and analysis of cardiac magnetic resonance images to provide\ncore-lab processing for a multi-centre registry of Fontan patients.\n  Materials and Methods: This retrospective study used training (n = 175),\nvalidation (n = 25) and testing (n = 50) cardiac magnetic resonance image exams\ncollected from 13 institutions in the UK, US and Canada. The data was used to\ntrain and evaluate a pipeline containing three deep-learning models. The\npipeline's performance was assessed on the Dice and IoU score between the\nautomated and reference standard manual segmentation. Cardiac function values\nwere calculated from both the automated and manual segmentation and evaluated\nusing Bland-Altman analysis and paired t-tests. The overall pipeline was\nfurther evaluated qualitatively on 475 unseen patient exams.\n  Results: For the 50 testing dataset, the pipeline achieved a median Dice\nscore of 0.91 (0.89-0.94) for end-diastolic volume, 0.86 (0.82-0.89) for\nend-systolic volume, and 0.74 (0.70-0.77) for myocardial mass. The deep\nlearning-derived end-diastolic volume, end-systolic volume, myocardial mass,\nstroke volume and ejection fraction had no statistical difference compared to\nthe same values derived from manual segmentation with p values all greater than\n0.05. For the 475 unseen patient exams, the pipeline achieved 68% adequate\nsegmentation in both systole and diastole, 26% needed minor adjustments in\neither systole or diastole, 5% needed major adjustments, and the cropping model\nonly failed in 0.4%.\n  Conclusion: Deep learning pipeline can provide standardised 'core-lab'\nsegmentation for Fontan patients. This pipeline can now be applied to the >4500\ncardiac magnetic resonance exams currently in the FORCE registry as well as any\nnew patients that are recruited.\n","authors":["Tina Yao","Nicole St. Clair","Gabriel F. Miller","Adam L. Dorfman","Mark A. Fogel","Sunil Ghelani","Rajesh Krishnamurthy","Christopher Z. Lam","Joshua D. Robinson","David Schidlow","Timothy C. Slesnick","Justin Weigand","Michael Quail","Rahul Rathod","Jennifer A. Steeden","Vivek Muthurangu"],"pdf_url":"https://arxiv.org/pdf/2303.11676v1.pdf","comment":"17 pages, 6 figures"},{"id":"http://arxiv.org/abs/2303.11675v1","updated":"2023-03-21T08:36:59Z","published":"2023-03-21T08:36:59Z","title":"BoPR: Body-aware Part Regressor for Human Shape and Pose Estimation","summary":"  This paper presents a novel approach for estimating human body shape and pose\nfrom monocular images that effectively addresses the challenges of occlusions\nand depth ambiguity. Our proposed method BoPR, the Body-aware Part Regressor,\nfirst extracts features of both the body and part regions using an\nattention-guided mechanism. We then utilize these features to encode extra\npart-body dependency for per-part regression, with part features as queries and\nbody feature as a reference. This allows our network to infer the spatial\nrelationship of occluded parts with the body by leveraging visible parts and\nbody reference information. Our method outperforms existing state-of-the-art\nmethods on two benchmark datasets, and our experiments show that it\nsignificantly surpasses existing methods in terms of depth ambiguity and\nocclusion handling. These results provide strong evidence of the effectiveness\nof our approach.\n","authors":["Yongkang Cheng","Shaoli Huang","Jifeng Ning","Ying Shan"],"pdf_url":"https://arxiv.org/pdf/2303.11675v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.11674v1","updated":"2023-03-21T08:36:34Z","published":"2023-03-21T08:36:34Z","title":"ALOFT: A Lightweight MLP-like Architecture with Dynamic Low-frequency\n  Transform for Domain Generalization","summary":"  Domain generalization (DG) aims to learn a model that generalizes well to\nunseen target domains utilizing multiple source domains without re-training.\nMost existing DG works are based on convolutional neural networks (CNNs).\nHowever, the local operation of the convolution kernel makes the model focus\ntoo much on local representations (e.g., texture), which inherently causes the\nmodel more prone to overfit to the source domains and hampers its\ngeneralization ability. Recently, several MLP-based methods have achieved\npromising results in supervised learning tasks by learning global interactions\namong different patches of the image. Inspired by this, in this paper, we first\nanalyze the difference between CNN and MLP methods in DG and find that MLP\nmethods exhibit a better generalization ability because they can better capture\nthe global representations (e.g., structure) than CNN methods. Then, based on a\nrecent lightweight MLP method, we obtain a strong baseline that outperforms\nmost state-of-the-art CNN-based methods. The baseline can learn global\nstructure representations with a filter to suppress structure irrelevant\ninformation in the frequency space. Moreover, we propose a dynAmic\nLOw-Frequency spectrum Transform (ALOFT) that can perturb local texture\nfeatures while preserving global structure features, thus enabling the filter\nto remove structure-irrelevant information sufficiently. Extensive experiments\non four benchmarks have demonstrated that our method can achieve great\nperformance improvement with a small number of parameters compared to SOTA\nCNN-based DG methods. Our code is available at\nhttps://github.com/lingeringlight/ALOFT/.\n","authors":["Jintao Guo","Na Wang","Lei Qi","Yinghuan Shi"],"pdf_url":"https://arxiv.org/pdf/2303.11674v1.pdf","comment":"Accepted by CVPR2023. The code is available at\n  https://github.com/lingeringlight/ALOFT/"},{"id":"http://arxiv.org/abs/2303.10904v2","updated":"2023-03-21T08:32:22Z","published":"2023-03-20T06:47:59Z","title":"Actionlet-Dependent Contrastive Learning for Unsupervised Skeleton-Based\n  Action Recognition","summary":"  The self-supervised pretraining paradigm has achieved great success in\nskeleton-based action recognition. However, these methods treat the motion and\nstatic parts equally, and lack an adaptive design for different parts, which\nhas a negative impact on the accuracy of action recognition. To realize the\nadaptive action modeling of both parts, we propose an Actionlet-Dependent\nContrastive Learning method (ActCLR). The actionlet, defined as the\ndiscriminative subset of the human skeleton, effectively decomposes motion\nregions for better action modeling. In detail, by contrasting with the static\nanchor without motion, we extract the motion region of the skeleton data, which\nserves as the actionlet, in an unsupervised manner. Then, centering on\nactionlet, a motion-adaptive data transformation method is built. Different\ndata transformations are applied to actionlet and non-actionlet regions to\nintroduce more diversity while maintaining their own characteristics.\nMeanwhile, we propose a semantic-aware feature pooling method to build feature\nrepresentations among motion and static regions in a distinguished manner.\nExtensive experiments on NTU RGB+D and PKUMMD show that the proposed method\nachieves remarkable action recognition performance. More visualization and\nquantitative experiments demonstrate the effectiveness of our method. Our\nproject website is available at https://langlandslin.github.io/projects/ActCLR/\n","authors":["Lilang Lin","Jiahang Zhang","Jiaying Liu"],"pdf_url":"https://arxiv.org/pdf/2303.10904v2.pdf","comment":"Accepted by CVPR2023 (Highlight). The project page is at\n  https://langlandslin.github.io/projects/ActCLR/"},{"id":"http://arxiv.org/abs/2303.11668v1","updated":"2023-03-21T08:23:05Z","published":"2023-03-21T08:23:05Z","title":"Focus or Not: A Baseline for Anomaly Event Detection On the Open Public\n  Places with Satellite Images","summary":"  In recent years, monitoring the world wide area with satellite images has\nbeen emerged as an important issue.\n  Site monitoring task can be divided into two independent tasks; 1) Change\nDetection and 2) Anomaly Event Detection.\n  Unlike to change detection research is actively conducted based on the\nnumerous datasets(\\eg LEVIR-CD, WHU-CD, S2Looking, xView2 and etc...) to meet\nup the expectations of industries or governments, research on AI models for\ndetecting anomaly events is passively and rarely conducted.\n  In this paper, we introduce a novel satellite imagery dataset(AED-RS) for\ndetecting anomaly events on the open public places.\n  AED-RS Dataset contains satellite images of normal and abnormal situations of\n8 open public places from all over the world.\n  Each places are labeled with different criteria based on the difference of\ncharacteristics of each places.\n  With this dataset, we introduce a baseline model for our dataset TB-FLOW,\nwhich can be trained in weakly-supervised manner and shows reasonable\nperformance on the AED-RS Dataset compared with the other NF(Normalizing-Flow)\nbased anomaly detection models. Our dataset and code will be publicly open in\n\\url{https://github.com/SIAnalytics/RS_AnomalyDetection.git}.\n","authors":["Yongjin Jeon","Youngtack Oh","Doyoung Jeong","Hyunguk Choi","Junsik Kim"],"pdf_url":"https://arxiv.org/pdf/2303.11668v1.pdf","comment":"14 pages, 14 figures"},{"id":"http://arxiv.org/abs/2303.11661v1","updated":"2023-03-21T08:08:13Z","published":"2023-03-21T08:08:13Z","title":"Advanced Multi-Microscopic Views Cell Semi-supervised Segmentation","summary":"  Although deep learning (DL) shows powerful potential in cell segmentation\ntasks, it suffers from poor generalization as DL-based methods originally\nsimplified cell segmentation in detecting cell membrane boundary, lacking\nprominent cellular structures to position overall differentiating. Moreover,\nthe scarcity of annotated cell images limits the performance of DL models.\nSegmentation limitations of a single category of cell make massive practice\ndifficult, much less, with varied modalities. In this paper, we introduce a\nnovel semi-supervised cell segmentation method called Multi-Microscopic-view\nCell semi-supervised Segmentation (MMCS), which can train cell segmentation\nmodels utilizing less labeled multi-posture cell images with different\nmicroscopy well. Technically, MMCS consists of Nucleus-assisted global\nrecognition, Self-adaptive diameter filter, and Temporal-ensembling models.\nNucleus-assisted global recognition adds additional cell nucleus channel to\nimprove the global distinguishing performance of fuzzy cell membrane boundaries\neven when cells aggregate. Besides, self-adapted cell diameter filter can help\nseparate multi-resolution cells with different morphology properly. It further\nleverages the temporal-ensembling models to improve the semi-supervised\ntraining process, achieving effective training with less labeled data.\nAdditionally, optimizing the weight of unlabeled loss contributed to total loss\nalso improve the model performance. Evaluated on the Tuning Set of NeurIPS 2022\nCell Segmentation Challenge (NeurIPS CellSeg), MMCS achieves an F1-score of\n0.8239 and the running time for all cases is within the time tolerance.\n","authors":["Fang Hu","Xuexue Sun","Ke Qing","Fenxi Xiao","Zhi Wang","Xiaolu Fan"],"pdf_url":"https://arxiv.org/pdf/2303.11661v1.pdf","comment":"23 pages"},{"id":"http://arxiv.org/abs/2301.02239v2","updated":"2023-03-21T07:57:46Z","published":"2023-01-05T18:59:51Z","title":"Robust Dynamic Radiance Fields","summary":"  Dynamic radiance field reconstruction methods aim to model the time-varying\nstructure and appearance of a dynamic scene. Existing methods, however, assume\nthat accurate camera poses can be reliably estimated by Structure from Motion\n(SfM) algorithms. These methods, thus, are unreliable as SfM algorithms often\nfail or produce erroneous poses on challenging videos with highly dynamic\nobjects, poorly textured surfaces, and rotating camera motion. We address this\nrobustness issue by jointly estimating the static and dynamic radiance fields\nalong with the camera parameters (poses and focal length). We demonstrate the\nrobustness of our approach via extensive quantitative and qualitative\nexperiments. Our results show favorable performance over the state-of-the-art\ndynamic view synthesis methods.\n","authors":["Yu-Lun Liu","Chen Gao","Andreas Meuleman","Hung-Yu Tseng","Ayush Saraf","Changil Kim","Yung-Yu Chuang","Johannes Kopf","Jia-Bin Huang"],"pdf_url":"https://arxiv.org/pdf/2301.02239v2.pdf","comment":"CVPR 2023. Project page: https://robust-dynrf.github.io/"},{"id":"http://arxiv.org/abs/2303.11654v1","updated":"2023-03-21T07:54:58Z","published":"2023-03-21T07:54:58Z","title":"Mitigating climate and health impact of small-scale kiln industry using\n  multi-spectral classifier and deep learning","summary":"  Industrial air pollution has a direct health impact and is a major\ncontributor to climate change. Small scale industries particularly bull-trench\nbrick kilns are one of the major causes of air pollution in South Asia often\ncreating hazardous levels of smog that is injurious to human health. To\nmitigate the climate and health impact of the kiln industry, fine-grained kiln\nlocalization at different geographic locations is needed. Kiln localization\nusing multi-spectral remote sensing data such as vegetation index results in a\nnoisy estimates whereas use of high-resolution imagery is infeasible due to\ncost and compute complexities. This paper proposes a fusion of spatio-temporal\nmulti-spectral data with high-resolution imagery for detection of brick kilns\nwithin the \"Brick-Kiln-Belt\" of South Asia. We first perform classification\nusing low-resolution spatio-temporal multi-spectral data from Sentinel-2\nimagery by combining vegetation, burn, build up and moisture indices. Then\norientation aware object detector: YOLOv3 (with theta value) is implemented for\nremoval of false detections and fine-grained localization. Our proposed\ntechnique, when compared with other benchmarks, results in a 21x improvement in\nspeed with comparable or higher accuracy when tested over multiple countries.\n","authors":["Usman Nazir","Murtaza Taj","Momin Uppal","Sara Khalid"],"pdf_url":"https://arxiv.org/pdf/2303.11654v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2211.10206v4","updated":"2023-03-21T07:50:56Z","published":"2022-11-18T12:53:10Z","title":"Multi-view Inverse Rendering for Large-scale Real-world Indoor Scenes","summary":"  We present a efficient multi-view inverse rendering method for large-scale\nreal-world indoor scenes that reconstructs global illumination and\nphysically-reasonable SVBRDFs. Unlike previous representations, where the\nglobal illumination of large scenes is simplified as multiple environment maps,\nwe propose a compact representation called Texture-based Lighting (TBL). It\nconsists of 3D mesh and HDR textures, and efficiently models direct and\ninfinite-bounce indirect lighting of the entire large scene. Based on TBL, we\nfurther propose a hybrid lighting representation with precomputed irradiance,\nwhich significantly improves the efficiency and alleviates the rendering noise\nin the material optimization. To physically disentangle the ambiguity between\nmaterials, we propose a three-stage material optimization strategy based on the\npriors of semantic segmentation and room segmentation. Extensive experiments\nshow that the proposed method outperforms the state-of-the-art quantitatively\nand qualitatively, and enables physically-reasonable mixed-reality applications\nsuch as material editing, editable novel view synthesis and relighting. The\nproject page is at https://lzleejean.github.io/TexIR.\n","authors":["Zhen Li","Lingli Wang","Mofang Cheng","Cihui Pan","Jiaqi Yang"],"pdf_url":"https://arxiv.org/pdf/2211.10206v4.pdf","comment":"Accepted to CVPR 2023. The project page is at:\n  https://lzleejean.github.io/TexIR"},{"id":"http://arxiv.org/abs/2303.11649v1","updated":"2023-03-21T07:49:32Z","published":"2023-03-21T07:49:32Z","title":"CoopInit: Initializing Generative Adversarial Networks via Cooperative\n  Learning","summary":"  Numerous research efforts have been made to stabilize the training of the\nGenerative Adversarial Networks (GANs), such as through regularization and\narchitecture design. However, we identify the instability can also arise from\nthe fragile balance at the early stage of adversarial learning. This paper\nproposes the CoopInit, a simple yet effective cooperative learning-based\ninitialization strategy that can quickly learn a good starting point for GANs,\nwith a very small computation overhead during training. The proposed algorithm\nconsists of two learning stages: (i) Cooperative initialization stage: The\ndiscriminator of GAN is treated as an energy-based model (EBM) and is optimized\nvia maximum likelihood estimation (MLE), with the help of the GAN's generator\nto provide synthetic data to approximate the learning gradients. The EBM also\nguides the MLE learning of the generator via MCMC teaching; (ii) Adversarial\nfinalization stage: After a few iterations of initialization, the algorithm\nseamlessly transits to the regular mini-max adversarial training until\nconvergence. The motivation is that the MLE-based initialization stage drives\nthe model towards mode coverage, which is helpful in alleviating the issue of\nmode dropping during the adversarial learning stage. We demonstrate the\neffectiveness of the proposed approach on image generation and one-sided\nunpaired image-to-image translation tasks through extensive experiments.\n","authors":["Yang Zhao","Jianwen Xie","Ping Li"],"pdf_url":"https://arxiv.org/pdf/2303.11649v1.pdf","comment":"9 pages of main text, 2 pages of references"},{"id":"http://arxiv.org/abs/2302.02871v3","updated":"2023-03-21T07:37:12Z","published":"2023-02-06T15:38:21Z","title":"Top-Down Beats Bottom-Up in 3D Instance Segmentation","summary":"  Most 3D instance segmentation methods exploit a bottom-up strategy, typically\nincluding resource-exhaustive post-processing. For point grouping, bottom-up\nmethods rely on prior assumptions about the objects in the form of\nhyperparameters, which are domain-specific and need to be carefully tuned. On\nthe contrary, we address 3D instance segmentation with a TD3D: top-down, fully\ndata-driven, simple approach trained in an end-to-end manner. With its\nstraightforward fully-convolutional pipeline, it performs surprisingly well on\nthe standard benchmarks: ScanNet v2, its extension ScanNet200, and S3DIS.\nBesides, our method is much faster on inference than the current\nstate-of-the-art grouping-based approaches. Code is available at\nhttps://github.com/SamsungLabs/td3d .\n","authors":["Maksim Kolodiazhnyi","Danila Rukhovich","Anna Vorontsova","Anton Konushin"],"pdf_url":"https://arxiv.org/pdf/2302.02871v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.11642v1","updated":"2023-03-21T07:27:37Z","published":"2023-03-21T07:27:37Z","title":"Visibility Constrained Wide-band Illumination Spectrum Design for\n  Seeing-in-the-Dark","summary":"  Seeing-in-the-dark is one of the most important and challenging computer\nvision tasks due to its wide applications and extreme complexities of\nin-the-wild scenarios. Existing arts can be mainly divided into two threads: 1)\nRGB-dependent methods restore information using degraded RGB inputs only (\\eg,\nlow-light enhancement), 2) RGB-independent methods translate images captured\nunder auxiliary near-infrared (NIR) illuminants into RGB domain (\\eg, NIR2RGB\ntranslation). The latter is very attractive since it works in complete darkness\nand the illuminants are visually friendly to naked eyes, but tends to be\nunstable due to its intrinsic ambiguities. In this paper, we try to robustify\nNIR2RGB translation by designing the optimal spectrum of auxiliary illumination\nin the wide-band VIS-NIR range, while keeping visual friendliness. Our core\nidea is to quantify the visibility constraint implied by the human vision\nsystem and incorporate it into the design pipeline. By modeling the formation\nprocess of images in the VIS-NIR range, the optimal multiplexing of a wide\nrange of LEDs is automatically designed in a fully differentiable manner,\nwithin the feasible region defined by the visibility constraint. We also\ncollect a substantially expanded VIS-NIR hyperspectral image dataset for\nexperiments by using a customized 50-band filter wheel. Experimental results\nshow that the task can be significantly improved by using the optimized\nwide-band illumination than using NIR only. Codes Available:\nhttps://github.com/MyNiuuu/VCSD.\n","authors":["Muyao Niu","Zhuoxiao Li","Zhihang Zhong","Yinqiang Zheng"],"pdf_url":"https://arxiv.org/pdf/2303.11642v1.pdf","comment":"Accepted to CVPR 2023"},{"id":"http://arxiv.org/abs/2303.10883v2","updated":"2023-03-21T07:25:09Z","published":"2023-03-20T06:01:53Z","title":"Explicit Visual Prompting for Low-Level Structure Segmentations","summary":"  We consider the generic problem of detecting low-level structures in images,\nwhich includes segmenting the manipulated parts, identifying out-of-focus\npixels, separating shadow regions, and detecting concealed objects. Whereas\neach such topic has been typically addressed with a domain-specific solution,\nwe show that a unified approach performs well across all of them. We take\ninspiration from the widely-used pre-training and then prompt tuning protocols\nin NLP and propose a new visual prompting model, named Explicit Visual\nPrompting (EVP). Different from the previous visual prompting which is\ntypically a dataset-level implicit embedding, our key insight is to enforce the\ntunable parameters focusing on the explicit visual content from each individual\nimage, i.e., the features from frozen patch embeddings and the input's\nhigh-frequency components. The proposed EVP significantly outperforms other\nparameter-efficient tuning protocols under the same amount of tunable\nparameters (5.7% extra trainable parameters of each task). EVP also achieves\nstate-of-the-art performances on diverse low-level structure segmentation tasks\ncompared to task-specific solutions. Our code is available at:\nhttps://github.com/NiFangBaAGe/Explicit-Visual-Prompt.\n","authors":["Weihuang Liu","Xi Shen","Chi-Man Pun","Xiaodong Cun"],"pdf_url":"https://arxiv.org/pdf/2303.10883v2.pdf","comment":"Accepted by CVPR 2023"},{"id":"http://arxiv.org/abs/2207.06726v2","updated":"2023-03-21T07:23:13Z","published":"2022-07-14T08:22:58Z","title":"Octuplet Loss: Make Face Recognition Robust to Image Resolution","summary":"  Image resolution, or in general, image quality, plays an essential role in\nthe performance of today's face recognition systems. To address this problem,\nwe propose a novel combination of the popular triplet loss to improve\nrobustness against image resolution via fine-tuning of existing face\nrecognition models. With octuplet loss, we leverage the relationship between\nhigh-resolution images and their synthetically down-sampled variants jointly\nwith their identity labels. Fine-tuning several state-of-the-art approaches\nwith our method proves that we can significantly boost performance for\ncross-resolution (high-to-low resolution) face verification on various datasets\nwithout meaningfully exacerbating the performance on high-to-high resolution\nimages. Our method applied on the FaceTransformer network achieves 95.12% face\nverification accuracy on the challenging XQLFW dataset while reaching 99.73% on\nthe LFW database. Moreover, the low-to-low face verification accuracy benefits\nfrom our method. We release our code to allow seamless integration of the\noctuplet loss into existing frameworks.\n","authors":["Martin Knoche","Mohamed Elkadeem","Stefan Hörmann","Gerhard Rigoll"],"pdf_url":"https://arxiv.org/pdf/2207.06726v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.11638v1","updated":"2023-03-21T07:14:18Z","published":"2023-03-21T07:14:18Z","title":"Human Pose as Compositional Tokens","summary":"  Human pose is typically represented by a coordinate vector of body joints or\ntheir heatmap embeddings. While easy for data processing, unrealistic pose\nestimates are admitted due to the lack of dependency modeling between the body\njoints. In this paper, we present a structured representation, named Pose as\nCompositional Tokens (PCT), to explore the joint dependency. It represents a\npose by M discrete tokens with each characterizing a sub-structure with several\ninterdependent joints. The compositional design enables it to achieve a small\nreconstruction error at a low cost. Then we cast pose estimation as a\nclassification task. In particular, we learn a classifier to predict the\ncategories of the M tokens from an image. A pre-learned decoder network is used\nto recover the pose from the tokens without further post-processing. We show\nthat it achieves better or comparable pose estimation results as the existing\nmethods in general scenarios, yet continues to work well when occlusion occurs,\nwhich is ubiquitous in practice. The code and models are publicly available at\nhttps://github.com/Gengzigang/PCT.\n","authors":["Zigang Geng","Chunyu Wang","Yixuan Wei","Ze Liu","Houqiang Li","Han Hu"],"pdf_url":"https://arxiv.org/pdf/2303.11638v1.pdf","comment":"Accepted by CVPR2023"},{"id":"http://arxiv.org/abs/2202.06884v3","updated":"2023-03-21T07:12:46Z","published":"2022-02-14T17:19:23Z","title":"COLA: COarse LAbel pre-training for 3D semantic segmentation of sparse\n  LiDAR datasets","summary":"  Transfer learning is a proven technique in 2D computer vision to leverage the\nlarge amount of data available and achieve high performance with datasets\nlimited in size due to the cost of acquisition or annotation. In 3D, annotation\nis known to be a costly task; nevertheless, pre-training methods have only\nrecently been investigated. Due to this cost, unsupervised pre-training has\nbeen heavily favored. In this work, we tackle the case of real-time 3D semantic\nsegmentation of sparse autonomous driving LiDAR scans. Such datasets have been\nincreasingly released, but each has a unique label set. We propose here an\nintermediate-level label set called coarse labels, which can easily be used on\nany existing and future autonomous driving datasets, thus allowing all the data\navailable to be leveraged at once without any additional manual labeling. This\nway, we have access to a larger dataset, alongside a simple task of semantic\nsegmentation. With it, we introduce a new pre-training task: coarse label\npre-training, also called COLA. We thoroughly analyze the impact of COLA on\nvarious datasets and architectures and show that it yields a noticeable\nperformance improvement, especially when only a small dataset is available for\nthe finetuning task.\n","authors":["Jules Sanchez","Jean-Emmanuel Deschaud","François Goulette"],"pdf_url":"https://arxiv.org/pdf/2202.06884v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.11637v1","updated":"2023-03-21T07:08:51Z","published":"2023-03-21T07:08:51Z","title":"Equiangular Basis Vectors","summary":"  We propose Equiangular Basis Vectors (EBVs) for classification tasks. In deep\nneural networks, models usually end with a k-way fully connected layer with\nsoftmax to handle different classification tasks. The learning objective of\nthese methods can be summarized as mapping the learned feature representations\nto the samples' label space. While in metric learning approaches, the main\nobjective is to learn a transformation function that maps training data points\nfrom the original space to a new space where similar points are closer while\ndissimilar points become farther apart. Different from previous methods, our\nEBVs generate normalized vector embeddings as \"predefined classifiers\" which\nare required to not only be with the equal status between each other, but also\nbe as orthogonal as possible. By minimizing the spherical distance of the\nembedding of an input between its categorical EBV in training, the predictions\ncan be obtained by identifying the categorical EBV with the smallest distance\nduring inference. Various experiments on the ImageNet-1K dataset and other\ndownstream tasks demonstrate that our method outperforms the general fully\nconnected classifier while it does not introduce huge additional computation\ncompared with classical metric learning methods. Our EBVs won the first place\nin the 2022 DIGIX Global AI Challenge, and our code is open-source and\navailable at https://github.com/NJUST-VIPGroup/Equiangular-Basis-Vectors.\n","authors":["Yang Shen","Xuhao Sun","Xiu-Shen Wei"],"pdf_url":"https://arxiv.org/pdf/2303.11637v1.pdf","comment":"CVPR 2023"},{"id":"http://arxiv.org/abs/2303.11633v1","updated":"2023-03-21T07:00:35Z","published":"2023-03-21T07:00:35Z","title":"Learning Context-aware Classifier for Semantic Segmentation","summary":"  Semantic segmentation is still a challenging task for parsing diverse\ncontexts in different scenes, thus the fixed classifier might not be able to\nwell address varying feature distributions during testing. Different from the\nmainstream literature where the efficacy of strong backbones and effective\ndecoder heads has been well studied, in this paper, additional contextual hints\nare instead exploited via learning a context-aware classifier whose content is\ndata-conditioned, decently adapting to different latent distributions. Since\nonly the classifier is dynamically altered, our method is model-agnostic and\ncan be easily applied to generic segmentation models. Notably, with only\nnegligible additional parameters and +2\\% inference time, decent performance\ngain has been achieved on both small and large models with challenging\nbenchmarks, manifesting substantial practical merits brought by our simple yet\neffective method. The implementation is available at\n\\url{https://github.com/tianzhuotao/CAC}.\n","authors":["Zhuotao Tian","Jiequan Cui","Li Jiang","Xiaojuan Qi","Xin Lai","Yixin Chen","Shu Liu","Jiaya Jia"],"pdf_url":"https://arxiv.org/pdf/2303.11633v1.pdf","comment":"AAAI 2023. Code and models are available at\n  https://github.com/tianzhuotao/CAC"},{"id":"http://arxiv.org/abs/2303.11632v1","updated":"2023-03-21T07:00:13Z","published":"2023-03-21T07:00:13Z","title":"An Embarrassingly Simple Approach for Wafer Feature Extraction and\n  Defect Pattern Recognition","summary":"  Identifying defect patterns in a wafer map during manufacturing is crucial to\nfind the root cause of the underlying issue and provides valuable insights on\nimproving yield in the foundry. Currently used methods use deep neural networks\nto identify the defects. These methods are generally very huge and have\nsignificant inference time. They also require GPU support to efficiently\noperate. All these issues make these models not fit for on-line prediction in\nthe manufacturing foundry. In this paper, we propose an extremely simple yet\neffective technique to extract features from wafer images. The proposed method\nis extremely fast, intuitive, and non-parametric while being explainable. The\nexperiment results show that the proposed pipeline outperforms conventional\ndeep learning models. Our feature extraction requires no training or\nfine-tuning while preserving the relative shape and location of data points as\nrevealed by our interpretability analysis.\n","authors":["Nitish Shukla"],"pdf_url":"https://arxiv.org/pdf/2303.11632v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.11630v1","updated":"2023-03-21T06:54:18Z","published":"2023-03-21T06:54:18Z","title":"BoxSnake: Polygonal Instance Segmentation with Box Supervision","summary":"  Box-supervised instance segmentation has gained much attention as it requires\nonly simple box annotations instead of costly mask or polygon annotations.\nHowever, existing box-supervised instance segmentation models mainly focus on\nmask-based frameworks. We propose a new end-to-end training technique, termed\nBoxSnake, to achieve effective polygonal instance segmentation using only box\nannotations for the first time. Our method consists of two loss functions: (1)\na point-based unary loss that constrains the bounding box of predicted polygons\nto achieve coarse-grained segmentation; and (2) a distance-aware pairwise loss\nthat encourages the predicted polygons to fit the object boundaries. Compared\nwith the mask-based weakly-supervised methods, BoxSnake further reduces the\nperformance gap between the predicted segmentation and the bounding box, and\nshows significant superiority on the Cityscapes dataset.\n","authors":["Rui Yang","Lin Song","Yixiao Ge","Xiu Li"],"pdf_url":"https://arxiv.org/pdf/2303.11630v1.pdf","comment":"11 pages, 8 figures"},{"id":"http://arxiv.org/abs/2303.11629v1","updated":"2023-03-21T06:51:31Z","published":"2023-03-21T06:51:31Z","title":"TMA: Temporal Motion Aggregation for Event-based Optical Flow","summary":"  Event cameras have the ability to record continuous and detailed trajectories\nof objects with high temporal resolution, thereby providing intuitive motion\ncues for optical flow estimation. Nevertheless, most existing learning-based\napproaches for event optical flow estimation directly remould the paradigm of\nconventional images by representing the consecutive event stream as static\nframes, ignoring the inherent temporal continuity of event data. In this paper,\nwe argue that temporal continuity is a vital element of event-based optical\nflow and propose a novel Temporal Motion Aggregation (TMA) approach to unlock\nits potential. Technically, TMA comprises three components: an event splitting\nstrategy to incorporate intermediate motion information underlying the temporal\ncontext, a linear lookup strategy to align temporally continuous motion\nfeatures and a novel motion pattern aggregation module to emphasize consistent\npatterns for motion feature enhancement. By incorporating temporally continuous\nmotion information, TMA can derive better flow estimates than existing methods\nat early stages, which not only enables TMA to obtain more accurate final\npredictions, but also greatly reduces the demand for a number of refinements.\nExtensive experiments on DESC-Flow and MVSEC datasets verify the effectiveness\nand superiority of our TMA. Remarkably, compared to E-RAFT, TMA achieves a 6%\nimprovement in accuracy and a 40% reduction in inference time on DSEC-Flow.\n","authors":["Haotian Liu","Guang Chen","Sanqing Qu","Yanping Zhang","Zhijun Li","Alois Knoll","Changjun Jiang"],"pdf_url":"https://arxiv.org/pdf/2303.11629v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.11625v1","updated":"2023-03-21T06:48:14Z","published":"2023-03-21T06:48:14Z","title":"Information-containing Adversarial Perturbation for Combating Facial\n  Manipulation Systems","summary":"  With the development of deep learning technology, the facial manipulation\nsystem has become powerful and easy to use. Such systems can modify the\nattributes of the given facial images, such as hair color, gender, and age.\nMalicious applications of such systems pose a serious threat to individuals'\nprivacy and reputation. Existing studies have proposed various approaches to\nprotect images against facial manipulations. Passive defense methods aim to\ndetect whether the face is real or fake, which works for posterior forensics\nbut can not prevent malicious manipulation. Initiative defense methods protect\nimages upfront by injecting adversarial perturbations into images to disrupt\nfacial manipulation systems but can not identify whether the image is fake. To\naddress the limitation of existing methods, we propose a novel two-tier\nprotection method named Information-containing Adversarial Perturbation (IAP),\nwhich provides more comprehensive protection for {facial images}. We use an\nencoder to map a facial image and its identity message to a cross-model\nadversarial example which can disrupt multiple facial manipulation systems to\nachieve initiative protection. Recovering the message in adversarial examples\nwith a decoder serves passive protection, contributing to provenance tracking\nand fake image detection. We introduce a feature-level correlation measurement\nthat is more suitable to measure the difference between the facial images than\nthe commonly used mean squared error. Moreover, we propose a spectral diffusion\nmethod to spread messages to different frequency channels, thereby improving\nthe robustness of the message against facial manipulation. Extensive\nexperimental results demonstrate that our proposed IAP can recover the messages\nfrom the adversarial examples with high average accuracy and effectively\ndisrupt the facial manipulation systems.\n","authors":["Yao Zhu","Yuefeng Chen","Xiaodan Li","Rong Zhang","Xiang Tian","Bolun Zheng","Yaowu Chen"],"pdf_url":"https://arxiv.org/pdf/2303.11625v1.pdf","comment":"\\copyright 20XX IEEE. Personal use of this material is permitted.\n  Permission from IEEE must be obtained for all other uses, in any current or\n  future media, including reprinting/republishing this material for advertising\n  or promotional purposes, creating new collective works, for resale or\n  redistribution to servers or lists, or reuse of any copyrighted component of\n  this work in other works"},{"id":"http://arxiv.org/abs/2303.11623v1","updated":"2023-03-21T06:44:02Z","published":"2023-03-21T06:44:02Z","title":"Detecting the open-world objects with the help of the Brain","summary":"  Open World Object Detection (OWOD) is a novel computer vision task with a\nconsiderable challenge, bridging the gap between classic object detection (OD)\nbenchmarks and real-world object detection. In addition to detecting and\nclassifying seen/known objects, OWOD algorithms are expected to detect\nunseen/unknown objects and incrementally learn them. The natural instinct of\nhumans to identify unknown objects in their environments mainly depends on\ntheir brains' knowledge base. It is difficult for a model to do this only by\nlearning from the annotation of several tiny datasets. The large pre-trained\ngrounded language-image models - VL (\\ie GLIP) have rich knowledge about the\nopen world but are limited to the text prompt. We propose leveraging the VL as\nthe ``Brain'' of the open-world detector by simply generating unknown labels.\nLeveraging it is non-trivial because the unknown labels impair the model's\nlearning of known objects. In this paper, we alleviate these problems by\nproposing the down-weight loss function and decoupled detection structure.\nMoreover, our detector leverages the ``Brain'' to learn novel objects beyond VL\nthrough our pseudo-labeling scheme.\n","authors":["Shuailei Ma","Yuefeng Wang","Ying Wei","Peihao Chen","Zhixiang Ye","Jiaqi Fan","Enming Zhang","Thomas H. Li"],"pdf_url":"https://arxiv.org/pdf/2303.11623v1.pdf","comment":"arXiv admin note: text overlap with arXiv:2301.01970"},{"id":"http://arxiv.org/abs/2112.02869v4","updated":"2023-03-21T06:34:09Z","published":"2021-12-06T08:50:47Z","title":"Physics Driven Deep Retinex Fusion for Adaptive Infrared and Visible\n  Image Fusion","summary":"  Convolutional neural networks have turned into an illustrious tool for image\nfusion and super-resolution. However, their excellent performance cannot work\nwithout large fixed-paired datasets; and additionally, these high-demanded\nground truth data always cannot be obtained easily in fusion tasks. In this\nstudy, we show that, the structures of generative networks capture a great deal\nof image feature priors, and then these priors are sufficient to reconstruct\nhigh-quality fused super-resolution result using only low-resolution inputs. By\nthis way, we propose a novel self-supervised dataset-free method for adaptive\ninfrared (IR) and visible (VIS) image super-resolution fusion named Deep\nRetinex Fusion (DRF). The key idea of DRF is first generating component priors\nwhich are disentangled from physical model using our designed generative\nnetworks ZipperNet, LightingNet and AdjustingNet, then combining these priors\nwhich captured by networks via adaptive fusion loss functions based on Retinex\ntheory, and finally reconstructing the super-resolution fusion results.\nFurthermore, in order to verify the effectiveness of our reported DRF, both\nqualitative and quantitative experiments via comparing with other\nstate-of-the-art methods are performed using different test sets. These results\nprove that, comparing with large datasets trained methods, DRF which works\nwithout any dataset achieves the best super-resolution fusion performance; and\nmore importantly, DRF can adaptively balance IR and VIS information and has\ngood noise immunity. DRF codes are open source available at\nhttps://github.com/GuYuanjie/Deep-Retinex-fusion.\n","authors":["Yuanjie Gu","Zhibo Xiao","Yinghan Guan","Haoran Dai","Cheng Liu","Liang Xue","Shouyu Wang"],"pdf_url":"https://arxiv.org/pdf/2112.02869v4.pdf","comment":"20 pages, 9 figures"},{"id":"http://arxiv.org/abs/2303.11616v1","updated":"2023-03-21T06:26:18Z","published":"2023-03-21T06:26:18Z","title":"HRDFuse: Monocular 360°Depth Estimation by Collaboratively Learning\n  Holistic-with-Regional Depth Distributions","summary":"  Depth estimation from a monocular 360{\\deg} image is a burgeoning problem\nowing to its holistic sensing of a scene. Recently, some methods, \\eg,\nOmniFusion, have applied the tangent projection (TP) to represent a\n360{\\deg}image and predicted depth values via patch-wise regressions, which are\nmerged to get a depth map with equirectangular projection (ERP) format.\nHowever, these methods suffer from 1) non-trivial process of merging plenty of\npatches; 2) capturing less holistic-with-regional contextual information by\ndirectly regressing the depth value of each pixel. In this paper, we propose a\nnovel framework, \\textbf{HRDFuse}, that subtly combines the potential of\nconvolutional neural networks (CNNs) and transformers by collaboratively\nlearning the \\textit{holistic} contextual information from the ERP and the\n\\textit{regional} structural information from the TP. Firstly, we propose a\nspatial feature alignment (\\textbf{SFA}) module that learns feature\nsimilarities between the TP and ERP to aggregate the TP features into a\ncomplete ERP feature map in a pixel-wise manner. Secondly, we propose a\ncollaborative depth distribution classification (\\textbf{CDDC}) module that\nlearns the \\textbf{holistic-with-regional} histograms capturing the ERP and TP\ndepth distributions. As such, the final depth values can be predicted as a\nlinear combination of histogram bin centers. Lastly, we adaptively combine the\ndepth predictions from ERP and TP to obtain the final depth map. Extensive\nexperiments show that our method predicts\\textbf{ more smooth and accurate\ndepth} results while achieving \\textbf{favorably better} results than the SOTA\nmethods.\n","authors":["Hao Ai","Zidong cao","Yan-pei Cao","Ying Shan","Lin Wang"],"pdf_url":"https://arxiv.org/pdf/2303.11616v1.pdf","comment":"To appear at CVPR2023, 20 pages"},{"id":"http://arxiv.org/abs/2211.06689v4","updated":"2023-03-21T06:24:38Z","published":"2022-11-12T15:39:07Z","title":"TINC: Tree-structured Implicit Neural Compression","summary":"  Implicit neural representation (INR) can describe the target scenes with high\nfidelity using a small number of parameters, and is emerging as a promising\ndata compression technique. However, limited spectrum coverage is intrinsic to\nINR, and it is non-trivial to remove redundancy in diverse complex data\neffectively. Preliminary studies can only exploit either global or local\ncorrelation in the target data and thus of limited performance. In this paper,\nwe propose a Tree-structured Implicit Neural Compression (TINC) to conduct\ncompact representation for local regions and extract the shared features of\nthese local representations in a hierarchical manner. Specifically, we use\nMulti-Layer Perceptrons (MLPs) to fit the partitioned local regions, and these\nMLPs are organized in tree structure to share parameters according to the\nspatial distance. The parameter sharing scheme not only ensures the continuity\nbetween adjacent regions, but also jointly removes the local and non-local\nredundancy. Extensive experiments show that TINC improves the compression\nfidelity of INR, and has shown impressive compression capabilities over\ncommercial tools and other deep learning based methods. Besides, the approach\nis of high flexibility and can be tailored for different data and parameter\nsettings. The source code can be found at https://github.com/RichealYoung/TINC .\n","authors":["Runzhao Yang","Tingxiong Xiao","Yuxiao Cheng","Jinli Suo","Qionghai Dai"],"pdf_url":"https://arxiv.org/pdf/2211.06689v4.pdf","comment":"Accepted to CVPR2023"},{"id":"http://arxiv.org/abs/2303.11615v1","updated":"2023-03-21T06:20:49Z","published":"2023-03-21T06:20:49Z","title":"Robust Table Structure Recognition with Dynamic Queries Enhanced\n  Detection Transformer","summary":"  We present a new table structure recognition (TSR) approach, called\nTSRFormer, to robustly recognizing the structures of complex tables with\ngeometrical distortions from various table images. Unlike previous methods, we\nformulate table separation line prediction as a line regression problem instead\nof an image segmentation problem and propose a new two-stage dynamic queries\nenhanced DETR based separation line regression approach, named DQ-DETR, to\npredict separation lines from table images directly. Compared to Vallina DETR,\nwe propose three improvements in DQ-DETR to make the two-stage DETR framework\nwork efficiently and effectively for the separation line prediction task: 1) A\nnew query design, named Dynamic Query, to decouple single line query into\nseparable point queries which could intuitively improve the localization\naccuracy for regression tasks; 2) A dynamic queries based progressive line\nregression approach to progressively regressing points on the line which\nfurther enhances localization accuracy for distorted tables; 3) A\nprior-enhanced matching strategy to solve the slow convergence issue of DETR.\nAfter separation line prediction, a simple relation network based cell merging\nmodule is used to recover spanning cells. With these new techniques, our\nTSRFormer achieves state-of-the-art performance on several benchmark datasets,\nincluding SciTSR, PubTabNet, WTW and FinTabNet. Furthermore, we have validated\nthe robustness and high localization accuracy of our approach to tables with\ncomplex structures, borderless cells, large blank spaces, empty or spanning\ncells as well as distorted or even curved shapes on a more challenging\nreal-world in-house dataset.\n","authors":["Jiawei Wang","Weihong Lin","Chixiang Ma","Mingze Li","Zheng Sun","Lei Sun","Qiang Huo"],"pdf_url":"https://arxiv.org/pdf/2303.11615v1.pdf","comment":"18 pages, 11 figures, Preprint. arXiv admin note: substantial text\n  overlap with arXiv:2208.04921"},{"id":"http://arxiv.org/abs/2303.11611v1","updated":"2023-03-21T06:10:47Z","published":"2023-03-21T06:10:47Z","title":"Model Robustness Meets Data Privacy: Adversarial Robustness Distillation\n  without Original Data","summary":"  Large-scale deep learning models have achieved great performance based on\nlarge-scale datasets. Moreover, the existing Adversarial Training (AT) can\nfurther improve the robustness of these large models. However, these large\nmodels are difficult to deploy to mobile devices, and the effect of AT on small\nmodels is very limited. In addition, the data privacy issue (e.g., face data\nand diagnosis report) may lead to the original data being unavailable, which\nrelies on data-free knowledge distillation technology for training. To tackle\nthese issues, we propose a challenging novel task called Data-Free Adversarial\nRobustness Distillation (DFARD), which tries to train small, easily deployable,\nrobust models without relying on the original data. We find the combination of\nexisting techniques resulted in degraded model performance due to fixed\ntraining objectives and scarce information content. First, an interactive\nstrategy is designed for more efficient knowledge transfer to find more\nsuitable training objectives at each epoch. Then, we explore an adaptive\nbalance method to suppress information loss and obtain more data information\nthan previous methods. Experiments show that our method improves baseline\nperformance on the novel task.\n","authors":["Yuzheng Wang","Zhaoyu Chen","Dingkang Yang","Pinxue Guo","Kaixun Jiang","Wenqiang Zhang","Lizhe Qi"],"pdf_url":"https://arxiv.org/pdf/2303.11611v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.11610v1","updated":"2023-03-21T06:10:39Z","published":"2023-03-21T06:10:39Z","title":"Novel Class Discovery for 3D Point Cloud Semantic Segmentation","summary":"  Novel class discovery (NCD) for semantic segmentation is the task of learning\na model that can segment unlabelled (novel) classes using only the supervision\nfrom labelled (base) classes. This problem has recently been pioneered for 2D\nimage data, but no work exists for 3D point cloud data. In fact, the\nassumptions made for 2D are loosely applicable to 3D in this case. This paper\nis presented to advance the state of the art on point cloud data analysis in\nfour directions. Firstly, we address the new problem of NCD for point cloud\nsemantic segmentation. Secondly, we show that the transposition of the only\nexisting NCD method for 2D semantic segmentation to 3D data is suboptimal.\nThirdly, we present a new method for NCD based on online clustering that\nexploits uncertainty quantification to produce prototypes for pseudo-labelling\nthe points of the novel classes. Lastly, we introduce a new evaluation protocol\nto assess the performance of NCD for point cloud semantic segmentation. We\nthoroughly evaluate our method on SemanticKITTI and SemanticPOSS datasets,\nshowing that it can significantly outperform the baseline. Project page at this\nlink: https://github.com/LuigiRiz/NOPS.\n","authors":["Luigi Riz","Cristiano Saltori","Elisa Ricci","Fabio Poiesi"],"pdf_url":"https://arxiv.org/pdf/2303.11610v1.pdf","comment":"Paper accepted at CVPR 2023"},{"id":"http://arxiv.org/abs/2111.08913v2","updated":"2023-03-21T06:07:43Z","published":"2021-11-17T05:44:39Z","title":"Hierarchical Knowledge Guided Learning for Real-world Retinal Diseases\n  Recognition","summary":"  In the real world, medical datasets often exhibit a long-tailed data\ndistribution (i.e., a few classes occupy the majority of the data, while most\nclasses have only a limited number of samples), which results in a challenging\nlong-tailed learning scenario. Some recently published datasets in\nophthalmology AI consist of more than 40 kinds of retinal diseases with complex\nabnormalities and variable morbidity. Nevertheless, more than 30 conditions are\nrarely seen in global patient cohorts. From a modeling perspective, most deep\nlearning models trained on these datasets may lack the ability to generalize to\nrare diseases where only a few available samples are presented for training. In\naddition, there may be more than one disease for the presence of the retina,\nresulting in a challenging label co-occurrence scenario, also known as\n\\textit{multi-label}, which can cause problems when some re-sampling strategies\nare applied during training. To address the above two major challenges, this\npaper presents a novel method that enables the deep neural network to learn\nfrom a long-tailed fundus database for various retinal disease recognition.\nFirstly, we exploit the prior knowledge in ophthalmology to improve the feature\nrepresentation using a hierarchy-aware pre-training. Secondly, we adopt an\ninstance-wise class-balanced sampling strategy to address the label\nco-occurrence issue under the long-tailed medical dataset scenario. Thirdly, we\nintroduce a novel hybrid knowledge distillation to train a less biased\nrepresentation and classifier. We conducted extensive experiments on four\ndatabases, including two public datasets and two in-house databases with more\nthan one million fundus images. The experimental results demonstrate the\nsuperiority of our proposed methods with recognition accuracy outperforming the\nstate-of-the-art competitors, especially for these rare diseases.\n","authors":["Lie Ju","Zhen Yu","Lin Wang","Xin Zhao","Xin Wang","Paul Bonnington","Zongyuan Ge"],"pdf_url":"https://arxiv.org/pdf/2111.08913v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.11606v1","updated":"2023-03-21T05:56:53Z","published":"2023-03-21T05:56:53Z","title":"CAFS: Class Adaptive Framework for Semi-Supervised Semantic Segmentation","summary":"  Semi-supervised semantic segmentation learns a model for classifying pixels\ninto specific classes using a few labeled samples and numerous unlabeled\nimages. The recent leading approach is consistency regularization by\nselftraining with pseudo-labeling pixels having high confidences for unlabeled\nimages. However, using only highconfidence pixels for self-training may result\nin losing much of the information in the unlabeled datasets due to poor\nconfidence calibration of modern deep learning networks. In this paper, we\npropose a class-adaptive semisupervision framework for semi-supervised semantic\nsegmentation (CAFS) to cope with the loss of most information that occurs in\nexisting high-confidence-based pseudolabeling methods. Unlike existing\nsemi-supervised semantic segmentation frameworks, CAFS constructs a validation\nset on a labeled dataset, to leverage the calibration performance for each\nclass. On this basis, we propose a calibration aware class-wise adaptive\nthresholding and classwise adaptive oversampling using the analysis results\nfrom the validation set. Our proposed CAFS achieves state-ofthe-art performance\non the full data partition of the base PASCAL VOC 2012 dataset and on the 1/4\ndata partition of the Cityscapes dataset with significant margins of 83.0% and\n80.4%, respectively. The code is available at https://github.com/cjf8899/CAFS.\n","authors":["Jingi Ju","Hyeoncheol Noh","Yooseung Wang","Minseok Seo","Dong-Geol Choi"],"pdf_url":"https://arxiv.org/pdf/2303.11606v1.pdf","comment":"13 pages, 9 figures"},{"id":"http://arxiv.org/abs/2303.09026v3","updated":"2023-03-21T05:54:28Z","published":"2023-03-16T01:39:11Z","title":"Commonsense Knowledge Assisted Deep Learning for Resource-constrained\n  and Fine-grained Object Detection","summary":"  In this paper, we consider fine-grained image object detection in\nresource-constrained cases such as edge computing. Deep learning (DL), namely\nlearning with deep neural networks (DNNs), has become the dominating approach\nto object detection. To achieve accurate fine-grained detection, one needs to\nemploy a large enough DNN model and a vast amount of data annotations, which\nbrings a challenge for using modern DL object detectors in resource-constrained\ncases. To this end, we propose an approach, which leverages commonsense\nknowledge to assist a coarse-grained object detector to get accurate\nfine-grained detection results. Specifically, we introduce a commonsense\nknowledge inference module (CKIM) to translate coarse-grained labels given by a\nbackbone lightweight coarse-grained DL detector to fine-grained labels. We\nconsider both crisp-rule and fuzzy-rule based inference in our CKIM; the latter\nis used to handle ambiguity in the target semantic labels. We implement our\nmethod based on several modern DL detectors, namely YOLOv4, Mobilenetv3-SSD and\nYOLOv7-tiny. Experiment results show that our approach outperforms benchmark\ndetectors remarkably in terms of accuracy, model size and processing latency.\n","authors":["Pu Zhang","Bin Liu"],"pdf_url":"https://arxiv.org/pdf/2303.09026v3.pdf","comment":"7 pages"},{"id":"http://arxiv.org/abs/2303.11332v1","updated":"2023-03-21T05:50:53Z","published":"2023-03-21T05:50:53Z","title":"Deep Learning for Video-based Person Re-Identification: A Survey","summary":"  Video-based person re-identification (video re-ID) has lately fascinated\ngrowing attention due to its broad practical applications in various areas,\nsuch as surveillance, smart city, and public safety. Nevertheless, video re-ID\nis quite difficult and is an ongoing stage due to numerous uncertain challenges\nsuch as viewpoint, occlusion, pose variation, and uncertain video sequence,\netc. In the last couple of years, deep learning on video re-ID has continuously\nachieved surprising results on public datasets, with various approaches being\ndeveloped to handle diverse problems in video re-ID. Compared to image-based\nre-ID, video re-ID is much more challenging and complex. To encourage future\nresearch and challenges, this first comprehensive paper introduces a review of\nup-to-date advancements in deep learning approaches for video re-ID. It broadly\ncovers three important aspects, including brief video re-ID methods with their\nlimitations, major milestones with technical challenges, and architectural\ndesign. It offers comparative performance analysis on various available\ndatasets, guidance to improve video re-ID with valuable thoughts, and exciting\nresearch directions.\n","authors":["Khawar Islam"],"pdf_url":"https://arxiv.org/pdf/2303.11332v1.pdf","comment":"Under Review"},{"id":"http://arxiv.org/abs/2303.11599v1","updated":"2023-03-21T05:34:04Z","published":"2023-03-21T05:34:04Z","title":"Low-complexity Deep Video Compression with A Distributed Coding\n  Architecture","summary":"  Prevalent predictive coding-based video compression methods rely on a heavy\nencoder to reduce the temporal redundancy, which makes it challenging to deploy\nthem on resource-constrained devices. Meanwhile, as early as the 1970s,\ndistributed source coding theory has indicated that independent encoding and\njoint decoding with side information (SI) can achieve high-efficient\ncompression of correlated sources. This has inspired a distributed coding\narchitecture aiming at reducing the encoding complexity. However, traditional\ndistributed coding methods suffer from a substantial performance gap to\npredictive coding ones. Inspired by the great success of learning-based\ncompression, we propose the first end-to-end distributed deep video compression\nframework to improve the rate-distortion performance. A key ingredient is an\neffective SI generation module at the decoder, which helps to effectively\nexploit inter-frame correlations without computation-intensive encoder-side\nmotion estimation and compensation. Experiments show that our method\nsignificantly outperforms conventional distributed video coding and H.264.\nMeanwhile, it enjoys 6-7x encoding speedup against DVC [1] with comparable\ncompression performance. Code is released at\nhttps://github.com/Xinjie-Q/Distributed-DVC.\n","authors":["Xinjie Zhang","Jiawei Shao","Jun Zhang"],"pdf_url":"https://arxiv.org/pdf/2303.11599v1.pdf","comment":"Accepted by ICME 2023"},{"id":"http://arxiv.org/abs/2211.09778v3","updated":"2023-03-21T04:54:55Z","published":"2022-11-17T18:52:19Z","title":"I Can't Believe There's No Images! Learning Visual Tasks Using only\n  Language Data","summary":"  Many high-level skills that are required for computer vision tasks, such as\nparsing questions, comparing and contrasting semantics, and writing\ndescriptions, are also required in other domains such as natural language\nprocessing. In this paper, we ask whether it is possible to learn those skills\nfrom textual data and then transfer them to vision tasks without ever training\non visual training data. Key to our approach is exploiting the joint embedding\nspace of contrastively trained vision and language encoders. In practice, there\ncan be systematic differences between embedding spaces for different modalities\nin contrastive models, and we analyze how these differences affect our approach\nand study strategies to mitigate this concern. We produce models using only\ntext training data on four representative tasks: image captioning, visual\nentailment, visual question answering and visual news, and evaluate them on\nstandard benchmarks using images. We find these models generally perform close\nto models trained on images, while surpassing prior work for captioning and\nvisual entailment in this text only setting by over 9 points, and outperforming\nall prior work on visual news by over 30 points. We also showcase a variety of\nstylistic image captioning models that are trained using no image data and no\nhuman-curated language data, but instead using readily-available text data from\nbooks, the web, or language models.\n","authors":["Sophia Gu","Christopher Clark","Aniruddha Kembhavi"],"pdf_url":"https://arxiv.org/pdf/2211.09778v3.pdf","comment":"website (https://prior.allenai.org/projects/close), code\n  (https://github.com/allenai/close)"},{"id":"http://arxiv.org/abs/2303.11592v1","updated":"2023-03-21T04:42:44Z","published":"2023-03-21T04:42:44Z","title":"Lightweight Hybrid Video Compression Framework Using Reference-Guided\n  Restoration Network","summary":"  Recent deep-learning-based video compression methods brought coding gains\nover conventional codecs such as AVC and HEVC. However, learning-based codecs\ngenerally require considerable computation time and model complexity. In this\npaper, we propose a new lightweight hybrid video codec consisting of a\nconventional video codec(HEVC / VVC), a lossless image codec, and our new\nrestoration network. Precisely, our encoder consists of the conventional video\nencoder and a lossless image encoder, transmitting a lossy-compressed video\nbitstream along with a losslessly-compressed reference frame. The decoder is\nconstructed with corresponding video/image decoders and a new restoration\nnetwork, which enhances the compressed video in two-step processes. In the\nfirst step, a network trained with a large video dataset restores the details\nlost by the conventional encoder. Then, we further boost the video quality with\nthe guidance of a reference image, which is a losslessly compressed video\nframe. The reference image provides video-specific information, which can be\nutilized to better restore the details of a compressed video. Experimental\nresults show that the proposed method achieves comparable performance to\ntop-tier methods, even when applied to HEVC. Nevertheless, our method has lower\ncomplexity, a faster run time, and can be easily integrated into existing\nconventional codecs.\n","authors":["Hochang Rhee","Seyun Kim","Nam Ik Cho"],"pdf_url":"https://arxiv.org/pdf/2303.11592v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.11591v1","updated":"2023-03-21T04:42:39Z","published":"2023-03-21T04:42:39Z","title":"SVCNet: Scribble-based Video Colorization Network with Temporal\n  Aggregation","summary":"  In this paper, we propose a scribble-based video colorization network with\ntemporal aggregation called SVCNet. It can colorize monochrome videos based on\ndifferent user-given color scribbles. It addresses three common issues in the\nscribble-based video colorization area: colorization vividness, temporal\nconsistency, and color bleeding. To improve the colorization quality and\nstrengthen the temporal consistency, we adopt two sequential sub-networks in\nSVCNet for precise colorization and temporal smoothing, respectively. The first\nstage includes a pyramid feature encoder to incorporate color scribbles with a\ngrayscale frame, and a semantic feature encoder to extract semantics. The\nsecond stage finetunes the output from the first stage by aggregating the\ninformation of neighboring colorized frames (as short-range connections) and\nthe first colorized frame (as a long-range connection). To alleviate the color\nbleeding artifacts, we learn video colorization and segmentation\nsimultaneously. Furthermore, we set the majority of operations on a fixed small\nimage resolution and use a Super-resolution Module at the tail of SVCNet to\nrecover original sizes. It allows the SVCNet to fit different image resolutions\nat the inference. Finally, we evaluate the proposed SVCNet on DAVIS and Videvo\nbenchmarks. The experimental results demonstrate that SVCNet produces both\nhigher-quality and more temporally consistent videos than other well-known\nvideo colorization approaches. The codes and models can be found at\nhttps://github.com/zhaoyuzhi/SVCNet.\n","authors":["Yuzhi Zhao","Lai-Man Po","Kangcheng Liu","Xuehui Wang","Wing-Yin Yu","Pengfei Xian","Yujia Zhang","Mengyang Liu"],"pdf_url":"https://arxiv.org/pdf/2303.11591v1.pdf","comment":"under revision of IEEE Transactions on Image Processing"},{"id":"http://arxiv.org/abs/2303.11589v1","updated":"2023-03-21T04:41:02Z","published":"2023-03-21T04:41:02Z","title":"LayoutDiffusion: Improving Graphic Layout Generation by Discrete\n  Diffusion Probabilistic Models","summary":"  Creating graphic layouts is a fundamental step in graphic designs. In this\nwork, we present a novel generative model named LayoutDiffusion for automatic\nlayout generation. As layout is typically represented as a sequence of discrete\ntokens, LayoutDiffusion models layout generation as a discrete denoising\ndiffusion process. It learns to reverse a mild forward process, in which\nlayouts become increasingly chaotic with the growth of forward steps and\nlayouts in the neighboring steps do not differ too much. Designing such a mild\nforward process is however very challenging as layout has both categorical\nattributes and ordinal attributes. To tackle the challenge, we summarize three\ncritical factors for achieving a mild forward process for the layout, i.e.,\nlegality, coordinate proximity and type disruption. Based on the factors, we\npropose a block-wise transition matrix coupled with a piece-wise linear noise\nschedule. Experiments on RICO and PubLayNet datasets show that LayoutDiffusion\noutperforms state-of-the-art approaches significantly. Moreover, it enables two\nconditional layout generation tasks in a plug-and-play manner without\nre-training and achieves better performance than existing methods.\n","authors":["Junyi Zhang","Jiaqi Guo","Shizhao Sun","Jian-Guang Lou","Dongmei Zhang"],"pdf_url":"https://arxiv.org/pdf/2303.11589v1.pdf","comment":"27 pages, 20 figures"},{"id":"http://arxiv.org/abs/2206.08920v5","updated":"2023-03-21T04:38:27Z","published":"2022-06-17T17:57:13Z","title":"VectorMapNet: End-to-end Vectorized HD Map Learning","summary":"  Autonomous driving systems require a good understanding of surrounding\nenvironments, including moving obstacles and static High-Definition (HD)\nsemantic map elements. Existing methods approach the semantic map problem by\noffline manual annotation, which suffers from serious scalability issues.\nRecent learning-based methods produce dense rasterized segmentation predictions\nto construct maps. However, these predictions do not include instance\ninformation of individual map elements and require heuristic post-processing to\nobtain vectorized maps. To tackle these challenges, we introduce an end-to-end\nvectorized HD map learning pipeline, termed VectorMapNet. VectorMapNet takes\nonboard sensor observations and predicts a sparse set of polylines in the\nbird's-eye view. This pipeline can explicitly model the spatial relation\nbetween map elements and generate vectorized maps that are friendly to\ndownstream autonomous driving tasks. Extensive experiments show that\nVectorMapNet achieve strong map learning performance on both nuScenes and\nArgoverse2 dataset, surpassing previous state-of-the-art methods by 14.2 mAP\nand 14.6mAP. Qualitatively, we also show that VectorMapNet is capable of\ngenerating comprehensive maps and capturing more fine-grained details of road\ngeometry. To the best of our knowledge, VectorMapNet is the first work designed\ntowards end-to-end vectorized map learning from onboard observations. Our\nproject website is available at\nhttps://tsinghua-mars-lab.github.io/vectormapnet/.\n","authors":["Yicheng Liu","Yuantian Yuan","Yue Wang","Yilun Wang","Hang Zhao"],"pdf_url":"https://arxiv.org/pdf/2206.08920v5.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.12237v1","updated":"2023-03-21T23:44:02Z","published":"2023-03-21T23:44:02Z","title":"Automated deep learning segmentation of high-resolution 7 T ex vivo MRI\n  for quantitative analysis of structure-pathology correlations in\n  neurodegenerative diseases","summary":"  Ex vivo MRI of the brain provides remarkable advantages over in vivo MRI for\nvisualizing and characterizing detailed neuroanatomy, and helps to link\nmicroscale histology studies with morphometric measurements. However, automated\nsegmentation methods for brain mapping in ex vivo MRI are not well developed,\nprimarily due to limited availability of labeled datasets, and heterogeneity in\nscanner hardware and acquisition protocols. In this work, we present a high\nresolution dataset of 37 ex vivo post-mortem human brain tissue specimens\nscanned on a 7T whole-body MRI scanner. We developed a deep learning pipeline\nto segment the cortical mantle by benchmarking the performance of nine deep\nneural architectures. We then segment the four subcortical structures: caudate,\nputamen, globus pallidus, and thalamus; white matter hyperintensities, and the\nnormal appearing white matter. We show excellent generalizing capabilities\nacross whole brain hemispheres in different specimens, and also on unseen\nimages acquired at different magnetic field strengths and different imaging\nsequence. We then compute volumetric and localized cortical thickness\nmeasurements across key regions, and link them with semi-quantitative\nneuropathological ratings. Our code, containerized executables, and the\nprocessed datasets are publicly available at:\nhttps://github.com/Pulkit-Khandelwal/upenn-picsl-brain-ex-vivo.\n","authors":["Pulkit Khandelwal","Michael Tran Duong","Shokufeh Sadaghiani","Sydney Lim","Amanda Denning","Eunice Chung","Sadhana Ravikumar","Sanaz Arezoumandan","Claire Peterson","Madigan Bedard","Noah Capp","Ranjit Ittyerah","Elyse Migdal","Grace Choi","Emily Kopp","Bridget Loja","Eusha Hasan","Jiacheng Li","Karthik Prabhakaran","Gabor Mizsei","Marianna Gabrielyan","Theresa Schuck","Winifred Trotman","John Robinson","Daniel Ohm","Edward B. Lee","John Q. Trojanowski","Corey McMillan","Murray Grossman","David J. Irwin","John Detre","M. Dylan Tisdall","Sandhitsu R. Das","Laura E. M. Wisse","David A. Wolk","Paul A. Yushkevich"],"pdf_url":"https://arxiv.org/pdf/2303.12237v1.pdf","comment":"Preprint submitted to NeuroImage Project website:\n  https://github.com/Pulkit-Khandelwal/upenn-picsl-brain-ex-vivo"},{"id":"http://arxiv.org/abs/2303.12236v1","updated":"2023-03-21T23:43:58Z","published":"2023-03-21T23:43:58Z","title":"SALAD: Part-Level Latent Diffusion for 3D Shape Generation and\n  Manipulation","summary":"  We present a cascaded diffusion model based on a part-level implicit 3D\nrepresentation. Our model achieves state-of-the-art generation quality and also\nenables part-level shape editing and manipulation without any additional\ntraining in conditional setup. Diffusion models have demonstrated impressive\ncapabilities in data generation as well as zero-shot completion and editing via\na guided reverse process. Recent research on 3D diffusion models has focused on\nimproving their generation capabilities with various data representations,\nwhile the absence of structural information has limited their capability in\ncompletion and editing tasks. We thus propose our novel diffusion model using a\npart-level implicit representation. To effectively learn diffusion with\nhigh-dimensional embedding vectors of parts, we propose a cascaded framework,\nlearning diffusion first on a low-dimensional subspace encoding extrinsic\nparameters of parts and then on the other high-dimensional subspace encoding\nintrinsic attributes. In the experiments, we demonstrate the outperformance of\nour method compared with the previous ones both in generation and part-level\ncompletion and manipulation tasks.\n","authors":["Juil Koo","Seungwoo Yoo","Minh Hieu Nguyen","Minhyuk Sung"],"pdf_url":"https://arxiv.org/pdf/2303.12236v1.pdf","comment":"Project page: https://salad3d.github.io"},{"id":"http://arxiv.org/abs/2303.12234v1","updated":"2023-03-21T23:29:38Z","published":"2023-03-21T23:29:38Z","title":"Pre-NeRF 360: Enriching Unbounded Appearances for Neural Radiance Fields","summary":"  Neural radiance fields (NeRF) appeared recently as a powerful tool to\ngenerate realistic views of objects and confined areas. Still, they face\nserious challenges with open scenes, where the camera has unrestricted movement\nand content can appear at any distance. In such scenarios, current\nNeRF-inspired models frequently yield hazy or pixelated outputs, suffer slow\ntraining times, and might display irregularities, because of the challenging\ntask of reconstructing an extensive scene from a limited number of images. We\npropose a new framework to boost the performance of NeRF-based architectures\nyielding significantly superior outcomes compared to the prior work. Our\nsolution overcomes several obstacles that plagued earlier versions of NeRF,\nincluding handling multiple video inputs, selecting keyframes, and extracting\nposes from real-world frames that are ambiguous and symmetrical. Furthermore,\nwe applied our framework, dubbed as \"Pre-NeRF 360\", to enable the use of the\nNutrition5k dataset in NeRF and introduce an updated version of this dataset,\nknown as the N5k360 dataset.\n","authors":["Ahmad AlMughrabi","Umair Haroon","Ricardo Marques","Petia Radeva"],"pdf_url":"https://arxiv.org/pdf/2303.12234v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.12218v1","updated":"2023-03-21T22:37:16Z","published":"2023-03-21T22:37:16Z","title":"Compositional 3D Scene Generation using Locally Conditioned Diffusion","summary":"  Designing complex 3D scenes has been a tedious, manual process requiring\ndomain expertise. Emerging text-to-3D generative models show great promise for\nmaking this task more intuitive, but existing approaches are limited to\nobject-level generation. We introduce \\textbf{locally conditioned diffusion} as\nan approach to compositional scene diffusion, providing control over semantic\nparts using text prompts and bounding boxes while ensuring seamless transitions\nbetween these parts. We demonstrate a score distillation sampling--based\ntext-to-3D synthesis pipeline that enables compositional 3D scene generation at\na higher fidelity than relevant baselines.\n","authors":["Ryan Po","Gordon Wetzstein"],"pdf_url":"https://arxiv.org/pdf/2303.12218v1.pdf","comment":"For project page, see https://ryanpo.com/comp3d/"},{"id":"http://arxiv.org/abs/2302.05361v2","updated":"2023-03-21T22:37:15Z","published":"2023-02-10T16:21:07Z","title":"Leveraging Inpainting for Single-Image Shadow Removal","summary":"  Fully-supervised shadow removal methods achieve the best restoration\nqualities on public datasets but still generate some shadow remnants. One of\nthe reasons is the lack of large-scale shadow & shadow-free image pairs.\nUnsupervised methods can alleviate the issue but their restoration qualities\nare much lower than those of fully-supervised methods. In this work, we find\nthat pretraining shadow removal networks on the image inpainting dataset can\nreduce the shadow remnants significantly: a naive encoder-decoder network gets\ncompetitive restoration quality w.r.t. the state-of-the-art methods via only\n10% shadow & shadow-free image pairs. After analyzing networks with/without\ninpainting pre-training via the information stored in the weight (IIW), we find\nthat inpainting pretraining improves restoration quality in non-shadow regions\nand enhances the generalization ability of networks significantly.\nAdditionally, shadow removal fine-tuning enables networks to fill in the\ndetails of shadow regions. Inspired by these observations we formulate shadow\nremoval as an adaptive fusion task that takes advantage of both shadow removal\nand image inpainting. Specifically, we develop an adaptive fusion network\nconsisting of two encoders, an adaptive fusion block, and a decoder. The two\nencoders are responsible for extracting the feature from the shadow image and\nthe shadow-masked image respectively. The adaptive fusion block is responsible\nfor combining these features in an adaptive manner. Finally, the decoder\nconverts the adaptive fused features to the desired shadow-free result. The\nextensive experiments show that our method empowered with inpainting\noutperforms all state-of-the-art methods.\n","authors":["Xiaoguang Li","Qing Guo","Rabab Abdelfattah","Di Lin","Wei Feng","Ivor Tsang","Song Wang"],"pdf_url":"https://arxiv.org/pdf/2302.05361v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.12217v1","updated":"2023-03-21T22:35:04Z","published":"2023-03-21T22:35:04Z","title":"Image Reconstruction without Explicit Priors","summary":"  We consider solving ill-posed imaging inverse problems without access to an\nexplicit image prior or ground-truth examples. An overarching challenge in\ninverse problems is that there are many undesired images that fit to the\nobserved measurements, thus requiring image priors to constrain the space of\npossible solutions to more plausible reconstructions. However, in many\napplications it is difficult or potentially impossible to obtain ground-truth\nimages to learn an image prior. Thus, inaccurate priors are often used, which\ninevitably result in biased solutions. Rather than solving an inverse problem\nusing priors that encode the explicit structure of any one image, we propose to\nsolve a set of inverse problems jointly by incorporating prior constraints on\nthe collective structure of the underlying images.The key assumption of our\nwork is that the ground-truth images we aim to reconstruct share common,\nlow-dimensional structure. We show that such a set of inverse problems can be\nsolved simultaneously by learning a shared image generator with a\nlow-dimensional latent space. The parameters of the generator and latent\nembedding are learned by maximizing a proxy for the Evidence Lower Bound\n(ELBO). Once learned, the generator and latent embeddings can be combined to\nprovide reconstructions for each inverse problem. The framework we propose can\nhandle general forward model corruptions, and we show that measurements derived\nfrom only a few ground-truth images (O(10)) are sufficient for image\nreconstruction without explicit priors.\n","authors":["Angela F. Gao","Oscar Leong","He Sun","Katherine L. Bouman"],"pdf_url":"https://arxiv.org/pdf/2303.12217v1.pdf","comment":"ICASSP 2023"},{"id":"http://arxiv.org/abs/2303.12214v1","updated":"2023-03-21T22:24:27Z","published":"2023-03-21T22:24:27Z","title":"Prompt-MIL: Boosting Multi-Instance Learning Schemes via Task-specific\n  Prompt Tuning","summary":"  Whole slide image (WSI) classification is a critical task in computational\npathology, requiring the processing of gigapixel-sized images, which is\nchallenging for current deep-learning methods. Current state of the art methods\nare based on multi-instance learning schemes (MIL), which usually rely on\npretrained features to represent the instances. Due to the lack of\ntask-specific annotated data, these features are either obtained from\nwell-established backbones on natural images, or, more recently from\nself-supervised models pretrained on histopathology. However, both approaches\nyield task-agnostic features, resulting in performance loss compared to the\nappropriate task-related supervision, if available. In this paper, we show that\nwhen task-specific annotations are limited, we can inject such supervision into\ndownstream task training, to reduce the gap between fully task-tuned and task\nagnostic features. We propose Prompt-MIL, an MIL framework that integrates\nprompts into WSI classification. Prompt-MIL adopts a prompt tuning mechanism,\nwhere only a small fraction of parameters calibrates the pretrained features to\nencode task-specific information, rather than the conventional full fine-tuning\napproaches. Extensive experiments on three WSI datasets, TCGA-BRCA, TCGA-CRC,\nand BRIGHT, demonstrate the superiority of Prompt-MIL over conventional MIL\nmethods, achieving a relative improvement of 1.49%-4.03% in accuracy and\n0.25%-8.97% in AUROC while using fewer than 0.3% additional parameters.\nCompared to conventional full fine-tuning approaches, we fine-tune less than\n1.3% of the parameters, yet achieve a relative improvement of 1.29%-13.61% in\naccuracy and 3.22%-27.18% in AUROC and reduce GPU memory consumption by 38%-45%\nwhile training 21%-27% faster.\n","authors":["Jingwei Zhang","Saarthak Kapse","Ke Ma","Prateek Prasanna","Joel Saltz","Maria Vakalopoulou","Dimitris Samaras"],"pdf_url":"https://arxiv.org/pdf/2303.12214v1.pdf","comment":"Submitted to MICCAI 2023"},{"id":"http://arxiv.org/abs/2303.12208v1","updated":"2023-03-21T21:49:39Z","published":"2023-03-21T21:49:39Z","title":"MAGVLT: Masked Generative Vision-and-Language Transformer","summary":"  While generative modeling on multimodal image-text data has been actively\ndeveloped with large-scale paired datasets, there have been limited attempts to\ngenerate both image and text data by a single model rather than a generation of\none fixed modality conditioned on the other modality. In this paper, we explore\na unified generative vision-and-language (VL) model that can produce both\nimages and text sequences. Especially, we propose a generative VL transformer\nbased on the non-autoregressive mask prediction, named MAGVLT, and compare it\nwith an autoregressive generative VL transformer (ARGVLT). In comparison to\nARGVLT, the proposed MAGVLT enables bidirectional context encoding, fast\ndecoding by parallel token predictions in an iterative refinement, and extended\nediting capabilities such as image and text infilling. For rigorous training of\nour MAGVLT with image-text pairs from scratch, we combine the image-to-text,\ntext-to-image, and joint image-and-text mask prediction tasks. Moreover, we\ndevise two additional tasks based on the step-unrolled mask prediction and the\nselective prediction on the mixture of two image-text pairs. Experimental\nresults on various downstream generation tasks of VL benchmarks show that our\nMAGVLT outperforms ARGVLT by a large margin even with significant inference\nspeedup. Particularly, MAGVLT achieves competitive results on both zero-shot\nimage-to-text and text-to-image generation tasks from MS-COCO by one\nmoderate-sized model (fewer than 500M parameters) even without the use of\nmonomodal data and networks.\n","authors":["Sungwoong Kim","Daejin Jo","Donghoon Lee","Jongmin Kim"],"pdf_url":"https://arxiv.org/pdf/2303.12208v1.pdf","comment":"CVPR 2023"},{"id":"http://arxiv.org/abs/2303.08241v2","updated":"2023-03-21T21:24:53Z","published":"2023-03-14T21:22:26Z","title":"Subspace Perturbation Analysis for Data-Driven Radar Target Localization","summary":"  Recent works exploring data-driven approaches to classical problems in\nadaptive radar have demonstrated promising results pertaining to the task of\nradar target localization. Via the use of space-time adaptive processing (STAP)\ntechniques and convolutional neural networks, these data-driven approaches to\ntarget localization have helped benchmark the performance of neural networks\nfor matched scenarios. However, the thorough bridging of these topics across\nmismatched scenarios still remains an open problem. As such, in this work, we\naugment our data-driven approach to radar target localization by performing a\nsubspace perturbation analysis, which allows us to benchmark the localization\naccuracy of our proposed deep learning framework across mismatched scenarios.\nTo evaluate this framework, we generate comprehensive datasets by randomly\nplacing targets of variable strengths in mismatched constrained areas via\nRFView, a high-fidelity, site-specific modeling and simulation tool. For the\nradar returns from these constrained areas, we generate heatmap tensors in\nrange, azimuth, and elevation using the normalized adaptive matched filter\n(NAMF) test statistic. We estimate target locations from these heatmap tensors\nusing a convolutional neural network, and demonstrate that the predictive\nperformance of our framework in the presence of mismatches can be\npredetermined.\n","authors":["Shyam Venkatasubramanian","Sandeep Gogineni","Bosung Kang","Ali Pezeshki","Muralidhar Rangaswamy","Vahid Tarokh"],"pdf_url":"https://arxiv.org/pdf/2303.08241v2.pdf","comment":"6 pages, 3 figures. Submitted to 2023 IEEE Radar Conference\n  (RadarConf). Extension of arXiv:2209.02890"},{"id":"http://arxiv.org/abs/2303.12198v1","updated":"2023-03-21T21:10:35Z","published":"2023-03-21T21:10:35Z","title":"Autofluorescence Bronchoscopy Video Analysis for Lesion Frame Detection","summary":"  Because of the significance of bronchial lesions as indicators of early lung\ncancer and squamous cell carcinoma, a critical need exists for early detection\nof bronchial lesions. Autofluorescence bronchoscopy (AFB) is a primary modality\nused for bronchial lesion detection, as it shows high sensitivity to suspicious\nlesions. The physician, however, must interactively browse a long video stream\nto locate lesions, making the search exceedingly tedious and error prone.\nUnfortunately, limited research has explored the use of automated AFB video\nanalysis for efficient lesion detection. We propose a robust automatic AFB\nanalysis approach that distinguishes informative and uninformative AFB video\nframes in a video. In addition, for the informative frames, we determine the\nframes containing potential lesions and delineate candidate lesion regions. Our\napproach draws upon a combination of computer-based image analysis, machine\nlearning, and deep learning. Thus, the analysis of an AFB video stream becomes\nmore tractable. Tests with patient AFB video indicate that $\\ge$97\\% of frames\nwere correctly labeled as informative or uninformative. In addition, $\\ge$97\\%\nof lesion frames were correctly identified, with false positive and false\nnegative rates $\\le$3\\%.\n","authors":["Qi Chang","Rebecca Bascom","Jennifer Toth","Danish Ahmad","William E. Higgins"],"pdf_url":"https://arxiv.org/pdf/2303.12198v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.12194v1","updated":"2023-03-21T20:52:02Z","published":"2023-03-21T20:52:02Z","title":"LiDARFormer: A Unified Transformer-based Multi-task Network for LiDAR\n  Perception","summary":"  There is a recent trend in the LiDAR perception field towards unifying\nmultiple tasks in a single strong network with improved performance, as opposed\nto using separate networks for each task. In this paper, we introduce a new\nLiDAR multi-task learning paradigm based on the transformer. The proposed\nLiDARFormer utilizes cross-space global contextual feature information and\nexploits cross-task synergy to boost the performance of LiDAR perception tasks\nacross multiple large-scale datasets and benchmarks. Our novel\ntransformer-based framework includes a cross-space transformer module that\nlearns attentive features between the 2D dense Bird's Eye View (BEV) and 3D\nsparse voxel feature maps. Additionally, we propose a transformer decoder for\nthe segmentation task to dynamically adjust the learned features by leveraging\nthe categorical feature representations. Furthermore, we combine the\nsegmentation and detection features in a shared transformer decoder with\ncross-task attention layers to enhance and integrate the object-level and\nclass-level features. LiDARFormer is evaluated on the large-scale nuScenes and\nthe Waymo Open datasets for both 3D detection and semantic segmentation tasks,\nand it outperforms all previously published methods on both tasks. Notably,\nLiDARFormer achieves the state-of-the-art performance of 76.4% L2 mAPH and\n74.3% NDS on the challenging Waymo and nuScenes detection benchmarks for a\nsingle model LiDAR-only method.\n","authors":["Zixiang Zhou","Dongqiangzi Ye","Weijia Chen","Yufei Xie","Yu Wang","Panqu Wang","Hassan Foroosh"],"pdf_url":"https://arxiv.org/pdf/2303.12194v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2209.09385v2","updated":"2023-03-21T20:30:25Z","published":"2022-09-19T23:39:15Z","title":"LidarMultiNet: Towards a Unified Multi-Task Network for LiDAR Perception","summary":"  LiDAR-based 3D object detection, semantic segmentation, and panoptic\nsegmentation are usually implemented in specialized networks with distinctive\narchitectures that are difficult to adapt to each other. This paper presents\nLidarMultiNet, a LiDAR-based multi-task network that unifies these three major\nLiDAR perception tasks. Among its many benefits, a multi-task network can\nreduce the overall cost by sharing weights and computation among multiple\ntasks. However, it typically underperforms compared to independently combined\nsingle-task models. The proposed LidarMultiNet aims to bridge the performance\ngap between the multi-task network and multiple single-task networks. At the\ncore of LidarMultiNet is a strong 3D voxel-based encoder-decoder architecture\nwith a Global Context Pooling (GCP) module extracting global contextual\nfeatures from a LiDAR frame. Task-specific heads are added on top of the\nnetwork to perform the three LiDAR perception tasks. More tasks can be\nimplemented simply by adding new task-specific heads while introducing little\nadditional cost. A second stage is also proposed to refine the first-stage\nsegmentation and generate accurate panoptic segmentation results. LidarMultiNet\nis extensively tested on both Waymo Open Dataset and nuScenes dataset,\ndemonstrating for the first time that major LiDAR perception tasks can be\nunified in a single strong network that is trained end-to-end and achieves\nstate-of-the-art performance. Notably, LidarMultiNet reaches the official 1st\nplace in the Waymo Open Dataset 3D semantic segmentation challenge 2022 with\nthe highest mIoU and the best accuracy for most of the 22 classes on the test\nset, using only LiDAR points as input. It also sets the new state-of-the-art\nfor a single model on the Waymo 3D object detection benchmark and three\nnuScenes benchmarks.\n","authors":["Dongqiangzi Ye","Zixiang Zhou","Weijia Chen","Yufei Xie","Yu Wang","Panqu Wang","Hassan Foroosh"],"pdf_url":"https://arxiv.org/pdf/2209.09385v2.pdf","comment":"Accepted to AAAI 2023 (Oral). Full-length paper extending our\n  previous technical report of the 1st place solution of the 2022 Waymo Open\n  Dataset 3D Semantic Segmentation challenge, including evaluations on 5 major\n  benchmarks. arXiv admin note: text overlap with arXiv:2206.11428"},{"id":"http://arxiv.org/abs/2303.12175v1","updated":"2023-03-21T20:21:44Z","published":"2023-03-21T20:21:44Z","title":"Black-box Backdoor Defense via Zero-shot Image Purification","summary":"  Backdoor attacks inject poisoned data into the training set, resulting in\nmisclassification of the poisoned samples during model inference. Defending\nagainst such attacks is challenging, especially in real-world black-box\nsettings where only model predictions are available. In this paper, we propose\na novel backdoor defense framework that can effectively defend against various\nattacks through zero-shot image purification (ZIP). Our proposed framework can\nbe applied to black-box models without requiring any internal information about\nthe poisoned model or any prior knowledge of the clean/poisoned samples. Our\ndefense framework involves a two-step process. First, we apply a linear\ntransformation on the poisoned image to destroy the trigger pattern. Then, we\nuse a pre-trained diffusion model to recover the missing semantic information\nremoved by the transformation. In particular, we design a new reverse process\nusing the transformed image to guide the generation of high-fidelity purified\nimages, which can be applied in zero-shot settings. We evaluate our ZIP\nbackdoor defense framework on multiple datasets with different kinds of\nattacks. Experimental results demonstrate the superiority of our ZIP framework\ncompared to state-of-the-art backdoor defense baselines. We believe that our\nresults will provide valuable insights for future defense methods for black-box\nmodels.\n","authors":["Yucheng Shi","Mengnan Du","Xuansheng Wu","Zihan Guan","Ninghao Liu"],"pdf_url":"https://arxiv.org/pdf/2303.12175v1.pdf","comment":"11 pages, 2 figures"}],"Information Retrieval":[{"id":"http://arxiv.org/abs/2303.11991v1","updated":"2023-03-21T16:21:41Z","published":"2023-03-21T16:21:41Z","title":"Application of an ontology for model cards to generate computable\n  artifacts for linking machine learning information from biomedical research","summary":"  Model card reports provide a transparent description of machine learning\nmodels which includes information about their evaluation, limitations, intended\nuse, etc. Federal health agencies have expressed an interest in model cards\nreport for research studies using machine-learning based AI. Previously, we\nhave developed an ontology model for model card reports to structure and\nformalize these reports. In this paper, we demonstrate a Java-based library\n(OWL API, FaCT++) that leverages our ontology to publish computable model card\nreports. We discuss future directions and other use cases that highlight\napplicability and feasibility of ontology-driven systems to support FAIR\nchallenges.\n","authors":["Muhammad Amith","Licong Cui","Kirk Roberts","Cui Tao"],"pdf_url":"https://arxiv.org/pdf/2303.11991v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.11916v1","updated":"2023-03-21T15:06:35Z","published":"2023-03-21T15:06:35Z","title":"CompoDiff: Versatile Composed Image Retrieval With Latent Diffusion","summary":"  This paper proposes a novel diffusion-based model, CompoDiff, for solving\nComposed Image Retrieval (CIR) with latent diffusion and presents a newly\ncreated dataset of 18 million reference images, conditions, and corresponding\ntarget image triplets to train the model. CompoDiff not only achieves a new\nzero-shot state-of-the-art on a CIR benchmark such as FashionIQ but also\nenables a more versatile CIR by accepting various conditions, such as negative\ntext and image mask conditions, which are unavailable with existing CIR\nmethods. In addition, the CompoDiff features are on the intact CLIP embedding\nspace so that they can be directly used for all existing models exploiting the\nCLIP space. The code and dataset used for the training, and the pre-trained\nweights are available at https://github.com/navervision/CompoDiff\n","authors":["Geonmo Gu","Sanghyuk Chun","Wonjae Kim","HeeJae Jun","Yoohoon Kang","Sangdoo Yun"],"pdf_url":"https://arxiv.org/pdf/2303.11916v1.pdf","comment":"First two authors contributed equally; 23 pages, 4.8MB"},{"id":"http://arxiv.org/abs/2303.11879v1","updated":"2023-03-21T14:23:46Z","published":"2023-03-21T14:23:46Z","title":"Multimodal Pre-training Framework for Sequential Recommendation via\n  Contrastive Learning","summary":"  Sequential recommendation systems utilize the sequential interactions of\nusers with items as their main supervision signals in learning users'\npreferences. However, existing methods usually generate unsatisfactory results\ndue to the sparsity of user behavior data. To address this issue, we propose a\nnovel pre-training framework, named Multimodal Sequence Mixup for Sequential\nRecommendation (MSM4SR), which leverages both users' sequential behaviors and\nitems' multimodal content (\\ie text and images) for effectively recommendation.\nSpecifically, MSM4SR tokenizes each item image into multiple textual keywords\nand uses the pre-trained BERT model to obtain initial textual and visual\nfeatures of items, for eliminating the discrepancy between the text and image\nmodalities. A novel backbone network, \\ie Multimodal Mixup Sequence Encoder\n(M$^2$SE), is proposed to bridge the gap between the item multimodal content\nand the user behavior, using a complementary sequence mixup strategy. In\naddition, two contrastive learning tasks are developed to assist M$^2$SE in\nlearning generalized multimodal representations of the user behavior sequence.\nExtensive experiments on real-world datasets demonstrate that MSM4SR\noutperforms state-of-the-art recommendation methods. Moreover, we further\nverify the effectiveness of MSM4SR on other challenging tasks including\ncold-start and cross-domain recommendation.\n","authors":["Lingzi Zhang","Xin Zhou","Zhiqi Shen"],"pdf_url":"https://arxiv.org/pdf/2303.11879v1.pdf","comment":"10 pages, 4 figures"},{"id":"http://arxiv.org/abs/2303.01593v2","updated":"2023-03-21T14:22:00Z","published":"2023-03-02T21:35:15Z","title":"QAID: Question Answering Inspired Few-shot Intent Detection","summary":"  Intent detection with semantically similar fine-grained intents is a\nchallenging task. To address it, we reformulate intent detection as a\nquestion-answering retrieval task by treating utterances and intent names as\nquestions and answers. To that end, we utilize a question-answering retrieval\narchitecture and adopt a two stages training schema with batch contrastive\nloss. In the pre-training stage, we improve query representations through\nself-supervised training. Then, in the fine-tuning stage, we increase\ncontextualized token-level similarity scores between queries and answers from\nthe same intent. Our results on three few-shot intent detection benchmarks\nachieve state-of-the-art performance.\n","authors":["Asaf Yehudai","Matan Vetzler","Yosi Mass","Koren Lazar","Doron Cohen","Boaz Carmeli"],"pdf_url":"https://arxiv.org/pdf/2303.01593v2.pdf","comment":"ICLR paper"},{"id":"http://arxiv.org/abs/2303.11780v1","updated":"2023-03-21T11:56:35Z","published":"2023-03-21T11:56:35Z","title":"Debiased Contrastive Learning for Sequential Recommendation","summary":"  Current sequential recommender systems are proposed to tackle the dynamic\nuser preference learning with various neural techniques, such as Transformer\nand Graph Neural Networks (GNNs). However, inference from the highly sparse\nuser behavior data may hinder the representation ability of sequential pattern\nencoding. To address the label shortage issue, contrastive learning (CL)\nmethods are proposed recently to perform data augmentation in two fashions: (i)\nrandomly corrupting the sequence data (e.g. stochastic masking, reordering);\n(ii) aligning representations across pre-defined contrastive views. Although\neffective, we argue that current CL-based methods have limitations in\naddressing popularity bias and disentangling of user conformity and real\ninterest. In this paper, we propose a new Debiased Contrastive learning\nparadigm for Recommendation (DCRec) that unifies sequential pattern encoding\nwith global collaborative relation modeling through adaptive conformity-aware\naugmentation. This solution is designed to tackle the popularity bias issue in\nrecommendation systems. Our debiased contrastive learning framework effectively\ncaptures both the patterns of item transitions within sequences and the\ndependencies between users across sequences. Our experiments on various\nreal-world datasets have demonstrated that DCRec significantly outperforms\nstate-of-the-art baselines, indicating its efficacy for recommendation. To\nfacilitate reproducibility of our results, we make our implementation of DCRec\npublicly available at: https://github.com/HKUDS/DCRec.\n","authors":["Yuhao Yang","Chao Huang","Lianghao Xia","Chunzhen Huang","Da Luo","Kangyi Lin"],"pdf_url":"https://arxiv.org/pdf/2303.11780v1.pdf","comment":"This paper is accepted by WWW'2023"},{"id":"http://arxiv.org/abs/2303.11746v1","updated":"2023-03-21T11:13:01Z","published":"2023-03-21T11:13:01Z","title":"Recommendation Systems in Libraries: an Application with Heterogeneous\n  Data Sources","summary":"  The Reading&Machine project exploits the support of digitalization to\nincrease the attractiveness of libraries and improve the users' experience. The\nproject implements an application that helps the users in their decision-making\nprocess, providing recommendation system (RecSys)-generated lists of books the\nusers might be interested in, and showing them through an interactive Virtual\nReality (VR)-based Graphical User Interface (GUI). In this paper, we focus on\nthe design and testing of the recommendation system, employing data about all\nusers' loans over the past 9 years from the network of libraries located in\nTurin, Italy. In addition, we use data collected by the Anobii online social\ncommunity of readers, who share their feedback and additional information about\nbooks they read. Armed with this heterogeneous data, we build and evaluate\nContent Based (CB) and Collaborative Filtering (CF) approaches. Our results\nshow that the CF outperforms the CB approach, improving by up to 47\\% the\nrelevant recommendations provided to a reader. However, the performance of the\nCB approach is heavily dependent on the number of books the reader has already\nread, and it can work even better than CF for users with a large history.\nFinally, our evaluations highlight that the performances of both approaches are\nsignificantly improved if the system integrates and leverages the information\nfrom the Anobii dataset, which allows us to include more user readings (for CF)\nand richer book metadata (for CB).\n","authors":["Alessandro Speciale","Greta Vallero","Luca Vassio","Marco Mellia"],"pdf_url":"https://arxiv.org/pdf/2303.11746v1.pdf","comment":"Accepted at 7th International workshop on Data Analytics solutions\n  for Real-LIfe APplications - 28th March-31st March, 2023, Ioannina, Greece.\n  The paper will be published in the Proceedings of EDBT/ICDT 2023 Joint\n  Conference"},{"id":"http://arxiv.org/abs/2112.06400v2","updated":"2023-03-21T10:32:23Z","published":"2021-12-13T03:18:04Z","title":"Improving Query Representations for Dense Retrieval with Pseudo\n  Relevance Feedback: A Reproducibility Study","summary":"  Pseudo-Relevance Feedback (PRF) utilises the relevance signals from the top-k\npassages from the first round of retrieval to perform a second round of\nretrieval aiming to improve search effectiveness. A recent research direction\nhas been the study and development of PRF methods for deep language models\nbased rankers, and in particular in the context of dense retrievers. Dense\nretrievers, compared to more complex neural rankers, provide a trade-off\nbetween effectiveness, which is often reduced compared to more complex neural\nrankers, and query latency, which also is reduced making the retrieval pipeline\nmore efficient. The introduction of PRF methods for dense retrievers has been\nmotivated as an attempt to further improve their effectiveness.\n  In this paper, we reproduce and study a recent method for PRF with dense\nretrievers, called ANCE-PRF. This method concatenates the query text and that\nof the top-k feedback passages to form a new query input, which is then encoded\ninto a dense representation using a newly trained query encoder based on the\noriginal dense retriever used for the first round of retrieval. While the\nmethod can potentially be applied to any of the existing dense retrievers,\nprior work has studied it only in the context of the ANCE dense retriever.\n  We study the reproducibility of ANCE-PRF in terms of both its training\n(encoding of the PRF signal) and inference (ranking) steps. We further extend\nthe empirical analysis provided in the original work to investigate the effect\nof the hyper-parameters that govern the training process and the robustness of\nthe method across these different settings. Finally, we contribute a study of\nthe generalisability of the ANCE-PRF method when dense retrievers other than\nANCE are used for the first round of retrieval and for encoding the PRF signal.\n","authors":["Hang Li","Shengyao Zhuang","Ahmed Mourad","Xueguang Ma","Jimmy Lin","Guido Zuccon"],"pdf_url":"https://arxiv.org/pdf/2112.06400v2.pdf","comment":"Accepted at ECIR 2022"},{"id":"http://arxiv.org/abs/2303.11700v1","updated":"2023-03-21T09:37:37Z","published":"2023-03-21T09:37:37Z","title":"Dynamically Expandable Graph Convolution for Streaming Recommendation","summary":"  Personalized recommender systems have been widely studied and deployed to\nreduce information overload and satisfy users' diverse needs. However,\nconventional recommendation models solely conduct a one-time training-test\nfashion and can hardly adapt to evolving demands, considering user preference\nshifts and ever-increasing users and items in the real world. To tackle such\nchallenges, the streaming recommendation is proposed and has attracted great\nattention recently. Among these, continual graph learning is widely regarded as\na promising approach for the streaming recommendation by academia and industry.\nHowever, existing methods either rely on the historical data replay which is\noften not practical under increasingly strict data regulations, or can seldom\nsolve the \\textit{over-stability} issue. To overcome these difficulties, we\npropose a novel \\textbf{D}ynamically \\textbf{E}xpandable \\textbf{G}raph\n\\textbf{C}onvolution (DEGC) algorithm from a \\textit{model isolation}\nperspective for the streaming recommendation which is orthogonal to previous\nmethods. Based on the motivation of disentangling outdated short-term\npreferences from useful long-term preferences, we design a sequence of\noperations including graph convolution pruning, refining, and expanding to only\npreserve beneficial long-term preference-related parameters and extract fresh\nshort-term preferences. Moreover, we model the temporal user preference, which\nis utilized as user embedding initialization, for better capturing the\nindividual-level preference shifts. Extensive experiments on the three most\nrepresentative GCN-based recommendation models and four industrial datasets\ndemonstrate the effectiveness and robustness of our method.\n","authors":["Bowei He","Xu He","Yingxue Zhang","Ruiming Tang","Chen Ma"],"pdf_url":"https://arxiv.org/pdf/2303.11700v1.pdf","comment":"11 pages, 6 figures, published on The Web Conference 2023 (WWW 2023)"},{"id":"http://arxiv.org/abs/2303.11692v1","updated":"2023-03-21T09:27:40Z","published":"2023-03-21T09:27:40Z","title":"ByteCover3: Accurate Cover Song Identification on Short Queries","summary":"  Deep learning based methods have become a paradigm for cover song\nidentification (CSI) in recent years, where the ByteCover systems have achieved\nstate-of-the-art results on all the mainstream datasets of CSI. However, with\nthe burgeon of short videos, many real-world applications require matching\nshort music excerpts to full-length music tracks in the database, which is\nstill under-explored and waiting for an industrial-level solution. In this\npaper, we upgrade the previous ByteCover systems to ByteCover3 that utilizes\nlocal features to further improve the identification performance of short music\nqueries. ByteCover3 is designed with a local alignment loss (LAL) module and a\ntwo-stage feature retrieval pipeline, allowing the system to perform CSI in a\nmore precise and efficient way. We evaluated ByteCover3 on multiple datasets\nwith different benchmark settings, where ByteCover3 beat all the compared\nmethods including its previous versions.\n","authors":["Xingjian Du","Zijie Wang","Xia Liang","Huidong Liang","Bilei Zhu","Zejun Ma"],"pdf_url":"https://arxiv.org/pdf/2303.11692v1.pdf","comment":"Accepeted by ICASSP 2023"},{"id":"http://arxiv.org/abs/2302.04775v2","updated":"2023-03-21T08:48:20Z","published":"2023-02-09T17:09:53Z","title":"Adap-$τ$: Adaptively Modulating Embedding Magnitude for\n  Recommendation","summary":"  Recent years have witnessed the great successes of embedding-based methods in\nrecommender systems. Despite their decent performance, we argue one potential\nlimitation of these methods -- the embedding magnitude has not been explicitly\nmodulated, which may aggravate popularity bias and training instability,\nhindering the model from making a good recommendation. It motivates us to\nleverage the embedding normalization in recommendation. By normalizing\nuser/item embeddings to a specific value, we empirically observe impressive\nperformance gains (9\\% on average) on four real-world datasets. Although\nencouraging, we also reveal a serious limitation when applying normalization in\nrecommendation -- the performance is highly sensitive to the choice of the\ntemperature $\\tau$ which controls the scale of the normalized embeddings.\n  To fully foster the merits of the normalization while circumvent its\nlimitation, this work studied on how to adaptively set the proper $\\tau$.\nTowards this end, we first make a comprehensive analyses of $\\tau$ to fully\nunderstand its role on recommendation. We then accordingly develop an adaptive\nfine-grained strategy Adap-$\\tau$ for the temperature with satisfying four\ndesirable properties including adaptivity, personalized, efficiency and\nmodel-agnostic. Extensive experiments have been conducted to validate the\neffectiveness of the proposal. The code is available at\n\\url{https://github.com/junkangwu/Adap_tau}.\n","authors":["Jiawei Chen","Junkang Wu","Jiancan Wu","Sheng Zhou","Xuezhi Cao","Xiangnan He"],"pdf_url":"https://arxiv.org/pdf/2302.04775v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.11666v1","updated":"2023-03-21T08:20:43Z","published":"2023-03-21T08:20:43Z","title":"A Survey on Causal Inference for Recommendation","summary":"  Recently, causal inference has attracted increasing attention from\nresearchers of recommender systems (RS), which analyzes the relationship\nbetween a cause and its effect and has a wide range of real-world applications\nin multiple fields. Causal inference can model the causality in recommender\nsystems like confounding effects and deal with counterfactual problems such as\noffline policy evaluation and data augmentation. Although there are already\nsome valuable surveys on causal recommendations, these surveys introduce\napproaches in a relatively isolated way and lack theoretical analysis of\nexisting methods. Due to the unfamiliarity with causality to RS researchers, it\nis both necessary and challenging to comprehensively review the relevant\nstudies from the perspective of causal theory, which might be instructive for\nthe readers to propose new approaches in practice. This survey attempts to\nprovide a systematic review of up-to-date papers in this area from a\ntheoretical standpoint. Firstly, we introduce the fundamental concepts of\ncausal inference as the basis of the following review. Then we propose a new\ntaxonomy from the perspective of causal techniques and further discuss\ntechnical details about how existing methods apply causal inference to address\nspecific recommender issues. Finally, we highlight some promising directions\nfor future research in this field.\n","authors":["Huishi Luo","Fuzhen Zhuang","Ruobing Xie","Hengshu Zhu","Deqing Wang"],"pdf_url":"https://arxiv.org/pdf/2303.11666v1.pdf","comment":"Under peer review"},{"id":"http://arxiv.org/abs/2303.11648v1","updated":"2023-03-21T07:46:57Z","published":"2023-03-21T07:46:57Z","title":"Improving Content Retrievability in Search with Controllable Query\n  Generation","summary":"  An important goal of online platforms is to enable content discovery, i.e.\nallow users to find a catalog entity they were not familiar with. A\npre-requisite to discover an entity, e.g. a book, with a search engine is that\nthe entity is retrievable, i.e. there are queries for which the system will\nsurface such entity in the top results. However, machine-learned search engines\nhave a high retrievability bias, where the majority of the queries return the\nsame entities. This happens partly due to the predominance of narrow intent\nqueries, where users create queries using the title of an already known entity,\ne.g. in book search 'harry potter'. The amount of broad queries where users\nwant to discover new entities, e.g. in music search 'chill lyrical electronica\nwith an atmospheric feeling to it', and have a higher tolerance to what they\nmight find, is small in comparison. We focus here on two factors that have a\nnegative impact on the retrievability of the entities (I) the training data\nused for dense retrieval models and (II) the distribution of narrow and broad\nintent queries issued in the system. We propose CtrlQGen, a method that\ngenerates queries for a chosen underlying intent-narrow or broad. We can use\nCtrlQGen to improve factor (I) by generating training data for dense retrieval\nmodels comprised of diverse synthetic queries. CtrlQGen can also be used to\ndeal with factor (II) by suggesting queries with broader intents to users. Our\nresults on datasets from the domains of music, podcasts, and books reveal that\nwe can significantly decrease the retrievability bias of a dense retrieval\nmodel when using CtrlQGen. First, by using the generated queries as training\ndata for dense models we make 9% of the entities retrievable (go from zero to\nnon-zero retrievability). Second, by suggesting broader queries to users, we\ncan make 12% of the entities retrievable in the best case.\n","authors":["Gustavo Penha","Enrico Palumbo","Maryam Aziz","Alice Wang","Hugues Bouchard"],"pdf_url":"https://arxiv.org/pdf/2303.11648v1.pdf","comment":"Accepted for publication in the International World Wide Web\n  Conference 2023"},{"id":"http://arxiv.org/abs/2303.07797v2","updated":"2023-03-21T03:54:41Z","published":"2023-03-14T11:12:22Z","title":"Automated Self-Supervised Learning for Recommendation","summary":"  Graph neural networks (GNNs) have emerged as the state-of-the-art paradigm\nfor collaborative filtering (CF). To improve the representation quality over\nlimited labeled data, contrastive learning has attracted attention in\nrecommendation and benefited graph-based CF model recently. However, the\nsuccess of most contrastive methods heavily relies on manually generating\neffective contrastive views for heuristic-based data augmentation. This does\nnot generalize across different datasets and downstream recommendation tasks,\nwhich is difficult to be adaptive for data augmentation and robust to noise\nperturbation. To fill this crucial gap, this work proposes a unified Automated\nCollaborative Filtering (AutoCF) to automatically perform data augmentation for\nrecommendation. Specifically, we focus on the generative self-supervised\nlearning framework with a learnable augmentation paradigm that benefits the\nautomated distillation of important self-supervised signals. To enhance the\nrepresentation discrimination ability, our masked graph autoencoder is designed\nto aggregate global information during the augmentation via reconstructing the\nmasked subgraph structures. Experiments and ablation studies are performed on\nseveral public datasets for recommending products, venues, and locations.\nResults demonstrate the superiority of AutoCF against various baseline methods.\nWe release the model implementation at https://github.com/HKUDS/AutoCF.\n","authors":["Lianghao Xia","Chao Huang","Chunzhen Huang","Kangyi Lin","Tao Yu","Ben Kao"],"pdf_url":"https://arxiv.org/pdf/2303.07797v2.pdf","comment":"Accepted by ACM The Web Conference, 2023"},{"id":"http://arxiv.org/abs/2303.08537v3","updated":"2023-03-21T03:44:25Z","published":"2023-03-15T11:30:16Z","title":"Graph-less Collaborative Filtering","summary":"  Graph neural networks (GNNs) have shown the power in representation learning\nover graph-structured user-item interaction data for collaborative filtering\n(CF) task. However, with their inherently recursive message propagation among\nneighboring nodes, existing GNN-based CF models may generate indistinguishable\nand inaccurate user (item) representations due to the over-smoothing and noise\neffect with low-pass Laplacian smoothing operators. In addition, the recursive\ninformation propagation with the stacked aggregators in the entire graph\nstructures may result in poor scalability in practical applications. Motivated\nby these limitations, we propose a simple and effective collaborative filtering\nmodel (SimRec) that marries the power of knowledge distillation and contrastive\nlearning. In SimRec, adaptive transferring knowledge is enabled between the\nteacher GNN model and a lightweight student network, to not only preserve the\nglobal collaborative signals, but also address the over-smoothing issue with\nrepresentation recalibration. Empirical results on public datasets show that\nSimRec archives better efficiency while maintaining superior recommendation\nperformance compared with various strong baselines. Our implementations are\npublicly available at: https://github.com/HKUDS/SimRec.\n","authors":["Lianghao Xia","Chao Huang","Jiao Shi","Yong Xu"],"pdf_url":"https://arxiv.org/pdf/2303.08537v3.pdf","comment":"Accepted by ACM WWW 2023"},{"id":"http://arxiv.org/abs/2303.11574v1","updated":"2023-03-21T03:43:19Z","published":"2023-03-21T03:43:19Z","title":"Bounding System-Induced Biases in Recommender Systems with A Randomized\n  Dataset","summary":"  Debiased recommendation with a randomized dataset has shown very promising\nresults in mitigating the system-induced biases. However, it still lacks more\ntheoretical insights or an ideal optimization objective function compared with\nthe other more well studied route without a randomized dataset. To bridge this\ngap, we study the debiasing problem from a new perspective and propose to\ndirectly minimize the upper bound of an ideal objective function, which\nfacilitates a better potential solution to the system-induced biases. Firstly,\nwe formulate a new ideal optimization objective function with a randomized\ndataset. Secondly, according to the prior constraints that an adopted loss\nfunction may satisfy, we derive two different upper bounds of the objective\nfunction, i.e., a generalization error bound with the triangle inequality and a\ngeneralization error bound with the separability. Thirdly, we show that most\nexisting related methods can be regarded as the insufficient optimization of\nthese two upper bounds. Fourthly, we propose a novel method called debiasing\napproximate upper bound with a randomized dataset (DUB), which achieves a more\nsufficient optimization of these upper bounds. Finally, we conduct extensive\nexperiments on a public dataset and a real product dataset to verify the\neffectiveness of our DUB.\n","authors":["Dugang Liu","Pengxiang Cheng","Zinan Lin","Xiaolian Zhang","Zhenhua Dong","Rui Zhang","Xiuqiang He","Weike Pan","Zhong Ming"],"pdf_url":"https://arxiv.org/pdf/2303.11574v1.pdf","comment":"Accepted by ACM TOIS"},{"id":"http://arxiv.org/abs/2303.05039v2","updated":"2023-03-21T02:24:25Z","published":"2023-03-09T05:20:17Z","title":"Improving Recommendation Systems with User Personality Inferred from\n  Product Reviews","summary":"  Personality is a psychological factor that reflects people's preferences,\nwhich in turn influences their decision-making. We hypothesize that accurate\nmodeling of users' personalities improves recommendation systems' performance.\nHowever, acquiring such personality profiles is both sensitive and expensive.\nWe address this problem by introducing a novel method to automatically extract\npersonality profiles from public product review text. We then design and assess\nthree context-aware recommendation architectures that leverage the profiles to\ntest our hypothesis.\n  Experiments on our two newly contributed personality datasets --\nAmazon-beauty and Amazon-music -- validate our hypothesis, showing performance\nboosts of 3--28%.Our analysis uncovers that varying personality types\ncontribute differently to recommendation performance: open and extroverted\npersonalities are most helpful in music recommendation, while a conscientious\npersonality is most helpful in beauty product recommendation.\n","authors":["Xinyuan Lu","Min-Yen Kan"],"pdf_url":"https://arxiv.org/pdf/2303.05039v2.pdf","comment":"Accepted by IRS@WSDM'23"},{"id":"http://arxiv.org/abs/2302.11953v2","updated":"2023-03-21T18:38:10Z","published":"2023-02-23T12:02:49Z","title":"MFBE: Leveraging Multi-Field Information of FAQs for Efficient Dense\n  Retrieval","summary":"  In the domain of question-answering in NLP, the retrieval of Frequently Asked\nQuestions (FAQ) is an important sub-area which is well researched and has been\nworked upon for many languages. Here, in response to a user query, a retrieval\nsystem typically returns the relevant FAQs from a knowledge-base. The efficacy\nof such a system depends on its ability to establish semantic match between the\nquery and the FAQs in real-time. The task becomes challenging due to the\ninherent lexical gap between queries and FAQs, lack of sufficient context in\nFAQ titles, scarcity of labeled data and high retrieval latency. In this work,\nwe propose a bi-encoder-based query-FAQ matching model that leverages multiple\ncombinations of FAQ fields (like, question, answer, and category) both during\nmodel training and inference. Our proposed Multi-Field Bi-Encoder (MFBE) model\nbenefits from the additional context resulting from multiple FAQ fields and\nperforms well even with minimal labeled data. We empirically support this claim\nthrough experiments on proprietary as well as open-source public datasets in\nboth unsupervised and supervised settings. Our model achieves around 27% and\n20% better top-1 accuracy for the FAQ retrieval task on internal and open\ndatasets, respectively over the best performing baseline.\n","authors":["Debopriyo Banerjee","Mausam Jain","Ashish Kulkarni"],"pdf_url":"https://arxiv.org/pdf/2302.11953v2.pdf","comment":"The first two authors contributed equally to this work. 12 pages, 3\n  figures, 5 tables. Accepted at the 2023 Pacific-Asia Conference On Knowledge\n  Discovery And Data Mining (PAKDD)"}],"Machine Learning":[{"id":"http://arxiv.org/abs/2303.12076v1","updated":"2023-03-21T17:59:20Z","published":"2023-03-21T17:59:20Z","title":"Dexterity from Touch: Self-Supervised Pre-Training of Tactile\n  Representations with Robotic Play","summary":"  Teaching dexterity to multi-fingered robots has been a longstanding challenge\nin robotics. Most prominent work in this area focuses on learning controllers\nor policies that either operate on visual observations or state estimates\nderived from vision. However, such methods perform poorly on fine-grained\nmanipulation tasks that require reasoning about contact forces or about objects\noccluded by the hand itself. In this work, we present T-Dex, a new approach for\ntactile-based dexterity, that operates in two phases. In the first phase, we\ncollect 2.5 hours of play data, which is used to train self-supervised tactile\nencoders. This is necessary to bring high-dimensional tactile readings to a\nlower-dimensional embedding. In the second phase, given a handful of\ndemonstrations for a dexterous task, we learn non-parametric policies that\ncombine the tactile observations with visual ones. Across five challenging\ndexterous tasks, we show that our tactile-based dexterity models outperform\npurely vision and torque-based models by an average of 1.7X. Finally, we\nprovide a detailed analysis on factors critical to T-Dex including the\nimportance of play data, architectures, and representation learning.\n","authors":["Irmak Guzey","Ben Evans","Soumith Chintala","Lerrel Pinto"],"pdf_url":"https://arxiv.org/pdf/2303.12076v1.pdf","comment":"Video and code can be accessed here:\n  https://tactile-dexterity.github.io/"},{"id":"http://arxiv.org/abs/2110.13311v5","updated":"2023-03-21T17:37:26Z","published":"2021-10-25T22:57:40Z","title":"Physics informed machine learning with Smoothed particle hydrodynamics:\n  Hierarchy of reduced Lagrangian models of turbulence","summary":"  Building efficient, accurate and generalizable reduced order models of\ndeveloped turbulence remains a major challenge. This manuscript approaches this\nproblem by developing a hierarchy of parameterized reduced Lagrangian models\nfor turbulent flows, and investigates the effects of enforcing physical\nstructure through Smoothed Particle Hydrodynamics (SPH) versus relying on\nneural networks (NN)s as universal function approximators. Starting from Neural\nNetwork (NN) parameterizations of a Lagrangian acceleration operator, this\nhierarchy of models gradually incorporates a weakly compressible and\nparameterized SPH framework, which enforces physical symmetries, such as\nGalilean, rotational and translational invariances. Within this hierarchy, two\nnew parameterized smoothing kernels are developed in order to increase the\nflexibility of the learn-able SPH simulators. For each model we experiment with\ndifferent loss functions which are minimized using gradient based optimization,\nwhere efficient computations of gradients are obtained by using Automatic\nDifferentiation (AD) and Sensitivity Analysis (SA). Each model within the\nhierarchy is trained on two data sets associated with weekly compressible\nHomogeneous Isotropic Turbulence (HIT): (1) a validation set using weakly\ncompressible SPH; and (2) a high fidelity set from Direct Numerical Simulations\n(DNS). Numerical evidence shows that encoding more SPH structure improves\ngeneralizability to different turbulent Mach numbers and time shifts, and that\nincluding the novel parameterized smoothing kernels improves the accuracy of\nSPH at the resolved scales.\n","authors":["Michael Woodward","Yifeng Tian","Criston Hyett","Chris Fryer","Daniel Livescu","Mikhail Stepanov","Michael Chertkov"],"pdf_url":"https://arxiv.org/pdf/2110.13311v5.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.12032v1","updated":"2023-03-21T17:19:35Z","published":"2023-03-21T17:19:35Z","title":"The Representational Status of Deep Learning Models","summary":"  This paper aims to clarify the representational status of Deep Learning\nModels (DLMs). While commonly referred to as 'representations', what this\nentails is ambiguous due to a conflation of functional and relational\nconceptions of representation. This paper argues that while DLMs represent\ntheir targets in a relational sense, they are best understood as highly\nidealized models. This result has immediate implications for explainable AI\n(XAI) and directs philosophical attention toward examining the idealized nature\nof DLM representations and their role in future scientific investigation.\n","authors":["Eamon Duede"],"pdf_url":"https://arxiv.org/pdf/2303.12032v1.pdf","comment":"19 pages"},{"id":"http://arxiv.org/abs/2303.12031v1","updated":"2023-03-21T17:16:01Z","published":"2023-03-21T17:16:01Z","title":"Semantic Latent Space Regression of Diffusion Autoencoders for Vertebral\n  Fracture Grading","summary":"  Vertebral fractures are a consequence of osteoporosis, with significant\nhealth implications for affected patients. Unfortunately, grading their\nseverity using CT exams is hard and subjective, motivating automated grading\nmethods. However, current approaches are hindered by imbalance and scarcity of\ndata and a lack of interpretability. To address these challenges, this paper\nproposes a novel approach that leverages unlabelled data to train a generative\nDiffusion Autoencoder (DAE) model as an unsupervised feature extractor. We\nmodel fracture grading as a continuous regression, which is more reflective of\nthe smooth progression of fractures. Specifically, we use a binary, supervised\nfracture classifier to construct a hyperplane in the DAE's latent space. We\nthen regress the severity of the fracture as a function of the distance to this\nhyperplane, calibrating the results to the Genant scale. Importantly, the\ngenerative nature of our method allows us to visualize different grades of a\ngiven vertebra, providing interpretability and insight into the features that\ncontribute to automated grading.\n","authors":["Matthias Keicher","Matan Atad","David Schinz","Alexandra S. Gersing","Sarah C. Foreman","Sophia S. Goller","Juergen Weissinger","Jon Rischewski","Anna-Sophia Dietrich","Benedikt Wiestler","Jan S. Kirschke","Nassir Navab"],"pdf_url":"https://arxiv.org/pdf/2303.12031v1.pdf","comment":"Under review"},{"id":"http://arxiv.org/abs/2303.10135v2","updated":"2023-03-21T16:58:19Z","published":"2023-03-17T17:23:14Z","title":"Efficient and Feasible Robotic Assembly Sequence Planning via Graph\n  Representation Learning","summary":"  Automatic Robotic Assembly Sequence Planning (RASP) can significantly improve\nproductivity and resilience in modern manufacturing along with the growing need\nfor greater product customization. One of the main challenges in realizing such\nautomation resides in efficiently finding solutions from a growing number of\npotential sequences for increasingly complex assemblies. Besides, costly\nfeasibility checks are always required for the robotic system. To address this,\nwe propose a holistic graphical approach including a graph representation\ncalled Assembly Graph for product assemblies and a policy architecture, Graph\nAssembly Processing Network, dubbed GRACE for assembly sequence generation.\nSecondly, we use GRACE to extract meaningful information from the graph input\nand predict assembly sequences in a step-by-step manner. In experiments, we\nshow that our approach can predict feasible assembly sequences across product\nvariants of aluminum profiles based on data collected in simulation of a\ndual-armed robotic system. We further demonstrate that our method is capable of\ndetecting infeasible assemblies, substantially alleviating the undesirable\nimpacts from false predictions, and hence facilitating real-world deployment\nsoon. Code and training data will be open-sourced.\n","authors":["Matan Atad","Jianxiang Feng","Ismael Rodríguez","Maximilian Durner","Rudolph Triebel"],"pdf_url":"https://arxiv.org/pdf/2303.10135v2.pdf","comment":"Under review"},{"id":"http://arxiv.org/abs/2210.11456v3","updated":"2023-03-21T16:57:57Z","published":"2022-10-20T17:54:03Z","title":"MixMask: Revisiting Masking Strategy for Siamese ConvNets","summary":"  Recent advances in self-supervised learning have integrated Masked Image\nModeling (MIM) and Siamese Networks into a unified framework that leverages the\nbenefits of both techniques. However, several issues remain unaddressed when\napplying conventional erase-based masking with Siamese ConvNets. These include\n(I) the inability to drop uninformative masked regions in ConvNets as they\nprocess data continuously, resulting in low training efficiency compared to ViT\nmodels; and (II) the mismatch between erase-based masking and the\ncontrastive-based objective in Siamese ConvNets, which differs from the MIM\napproach. In this paper, we propose a filling-based masking strategy called\nMixMask to prevent information incompleteness caused by the randomly erased\nregions in an image in the vanilla masking method. Furthermore, we introduce a\nflexible loss function design that considers the semantic distance change\nbetween two different mixed views to adapt the integrated architecture and\nprevent mismatches between the transformed input and objective in Masked\nSiamese ConvNets (MSCN). We conducted extensive experiments on various\ndatasets, including CIFAR-100, Tiny-ImageNet, and ImageNet-1K. The results\ndemonstrate that our proposed framework achieves superior accuracy on linear\nprobing, semi-supervised, and supervised finetuning, outperforming the\nstate-of-the-art MSCN by a significant margin. Additionally, we demonstrate the\nsuperiority of our approach in object detection and segmentation tasks. Our\nsource code is available at https://github.com/LightnessOfBeing/MixMask.\n","authors":["Kirill Vishniakov","Eric Xing","Zhiqiang Shen"],"pdf_url":"https://arxiv.org/pdf/2210.11456v3.pdf","comment":"Technical report. Code is available at\n  https://github.com/LightnessOfBeing/MixMask"},{"id":"http://arxiv.org/abs/2303.12021v1","updated":"2023-03-21T16:55:18Z","published":"2023-03-21T16:55:18Z","title":"Graph Kalman Filters","summary":"  The well-known Kalman filters model dynamical systems by relying on\nstate-space representations with the next state updated, and its uncertainty\ncontrolled, by fresh information associated with newly observed system outputs.\nThis paper generalizes, for the first time in the literature, Kalman and\nextended Kalman filters to discrete-time settings where inputs, states, and\noutputs are represented as attributed graphs whose topology and attributes can\nchange with time. The setup allows us to adapt the framework to cases where the\noutput is a vector or a scalar too (node/graph level tasks). Within the\nproposed theoretical framework, the unknown state-transition and the readout\nfunctions are learned end-to-end along with the downstream prediction task.\n","authors":["Cesare Alippi","Daniele Zambon"],"pdf_url":"https://arxiv.org/pdf/2303.12021v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.10780v2","updated":"2023-03-21T16:48:53Z","published":"2023-03-19T22:07:27Z","title":"A Comprehensive Review of Spiking Neural Networks: Interpretation,\n  Optimization, Efficiency, and Best Practices","summary":"  Biological neural networks continue to inspire breakthroughs in neural\nnetwork performance. And yet, one key area of neural computation that has been\nunder-appreciated and under-investigated is biologically plausible,\nenergy-efficient spiking neural networks, whose potential is especially\nattractive for low-power, mobile, or otherwise hardware-constrained settings.\nWe present a literature review of recent developments in the interpretation,\noptimization, efficiency, and accuracy of spiking neural networks. Key\ncontributions include identification, discussion, and comparison of\ncutting-edge methods in spiking neural network optimization, energy-efficiency,\nand evaluation, starting from first principles so as to be accessible to new\npractitioners.\n","authors":["Kai Malcolm","Josue Casco-Rodriguez"],"pdf_url":"https://arxiv.org/pdf/2303.10780v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2111.07632v2","updated":"2023-03-21T16:36:35Z","published":"2021-11-15T09:35:54Z","title":"CoReS: Compatible Representations via Stationarity","summary":"  In this paper, we propose a novel method to learn internal feature\nrepresentation models that are \\textit{compatible} with previously learned\nones. Compatible features enable for direct comparison of old and new learned\nfeatures, allowing them to be used interchangeably over time. This eliminates\nthe need for visual search systems to extract new features for all previously\nseen images in the gallery-set when sequentially upgrading the representation\nmodel. Extracting new features is typically quite expensive or infeasible in\nthe case of very large gallery-sets and/or real time systems (i.e.,\nface-recognition systems, social networks, life-long learning systems, robotics\nand surveillance systems). Our approach, called Compatible Representations via\nStationarity (CoReS), achieves compatibility by encouraging stationarity to the\nlearned representation model without relying on previously learned models.\nStationarity allows features' statistical properties not to change under time\nshift so that the current learned features are inter-operable with the old\nones. We evaluate single and sequential multi-model upgrading in growing\nlarge-scale training datasets and we show that our method improves the\nstate-of-the-art in achieving compatible features by a large margin. In\nparticular, upgrading ten times with training data taken from CASIA-WebFace and\nevaluating in Labeled Face in the Wild (LFW), we obtain a 49\\% increase in\nmeasuring the average number of times compatibility is achieved, which is a\n544\\% relative improvement over previous state-of-the-art.\n","authors":["Niccolo Biondi","Federico Pernici","Matteo Bruni","Alberto Del Bimbo"],"pdf_url":"https://arxiv.org/pdf/2111.07632v2.pdf","comment":"in IEEE Transactions on Pattern Analysis and Machine Intelligence"},{"id":"http://arxiv.org/abs/2303.12002v1","updated":"2023-03-21T16:33:56Z","published":"2023-03-21T16:33:56Z","title":"End-to-End Integration of Speech Separation and Voice Activity Detection\n  for Low-Latency Diarization of Telephone Conversations","summary":"  Recent works show that speech separation guided diarization (SSGD) is an\nincreasingly promising direction, mainly thanks to the recent progress in\nspeech separation. It performs diarization by first separating the speakers and\nthen applying voice activity detection (VAD) on each separated stream. In this\nwork we conduct an in-depth study of SSGD in the conversational telephone\nspeech (CTS) domain, focusing mainly on low-latency streaming diarization\napplications. We consider three state-of-the-art speech separation (SSep)\nalgorithms and study their performance both in online and offline scenarios,\nconsidering non-causal and causal implementations as well as continuous SSep\n(CSS) windowed inference. We compare different SSGD algorithms on two widely\nused CTS datasets: CALLHOME and Fisher Corpus (Part 1 and 2) and evaluate both\nseparation and diarization performance. To improve performance, a novel, causal\nand computationally efficient leakage removal algorithm is proposed, which\nsignificantly decreases false alarms. We also explore, for the first time,\nfully end-to-end SSGD integration between SSep and VAD modules. Crucially, this\nenables fine-tuning on real-world data for which oracle speakers sources are\nnot available. In particular, our best model achieves 8.8% DER on CALLHOME,\nwhich outperforms the current state-of-the-art end-to-end neural diarization\nmodel, despite being trained on an order of magnitude less data and having\nsignificantly lower latency, i.e., 0.1 vs. 1 seconds. Finally, we also show\nthat the separated signals can be readily used also for automatic speech\nrecognition, reaching performance close to using oracle sources in some\nconfigurations.\n","authors":["Giovanni Morrone","Samuele Cornell","Luca Serafini","Enrico Zovato","Alessio Brutti","Stefano Squartini"],"pdf_url":"https://arxiv.org/pdf/2303.12002v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2302.02601v3","updated":"2023-03-21T16:30:06Z","published":"2023-02-06T07:45:57Z","title":"Learning Representations of Bi-level Knowledge Graphs for Reasoning\n  beyond Link Prediction","summary":"  Knowledge graphs represent known facts using triplets. While existing\nknowledge graph embedding methods only consider the connections between\nentities, we propose considering the relationships between triplets. For\nexample, let us consider two triplets $T_1$ and $T_2$ where $T_1$ is\n(Academy_Awards, Nominates, Avatar) and $T_2$ is (Avatar, Wins,\nAcademy_Awards). Given these two base-level triplets, we see that $T_1$ is a\nprerequisite for $T_2$. In this paper, we define a higher-level triplet to\nrepresent a relationship between triplets, e.g., $\\langle T_1$,\nPrerequisiteFor, $T_2\\rangle$ where PrerequisiteFor is a higher-level relation.\nWe define a bi-level knowledge graph that consists of the base-level and the\nhigher-level triplets. We also propose a data augmentation strategy based on\nthe random walks on the bi-level knowledge graph to augment plausible triplets.\nOur model called BiVE learns embeddings by taking into account the structures\nof the base-level and the higher-level triplets, with additional consideration\nof the augmented triplets. We propose two new tasks: triplet prediction and\nconditional link prediction. Given a triplet $T_1$ and a higher-level relation,\nthe triplet prediction predicts a triplet that is likely to be connected to\n$T_1$ by the higher-level relation, e.g., $\\langle T_1$, PrerequisiteFor,\n?$\\rangle$. The conditional link prediction predicts a missing entity in a\ntriplet conditioned on another triplet, e.g., $\\langle T_1$, PrerequisiteFor,\n(Avatar, Wins, ?)$\\rangle$. Experimental results show that BiVE significantly\noutperforms all other methods in the two new tasks and the typical base-level\nlink prediction in real-world bi-level knowledge graphs.\n","authors":["Chanyoung Chung","Joyce Jiyoung Whang"],"pdf_url":"https://arxiv.org/pdf/2302.02601v3.pdf","comment":"14 pages, 3 figures, 15 tables. 37th AAAI Conference on Artificial\n  Intelligence (AAAI 2023)"},{"id":"http://arxiv.org/abs/2212.07398v4","updated":"2023-03-21T16:16:41Z","published":"2022-12-14T18:31:47Z","title":"Policy Adaptation from Foundation Model Feedback","summary":"  Recent progress on vision-language foundation models have brought significant\nadvancement to building general-purpose robots. By using the pre-trained models\nto encode the scene and instructions as inputs for decision making, the\ninstruction-conditioned policy can generalize across different objects and\ntasks. While this is encouraging, the policy still fails in most cases given an\nunseen task or environment. In this work, we propose Policy Adaptation from\nFoundation model Feedback (PAFF). When deploying the trained policy to a new\ntask or a new environment, we first let the policy play with randomly generated\ninstructions to record the demonstrations. While the execution could be wrong,\nwe can use the pre-trained foundation models to provide feedback to relabel the\ndemonstrations. This automatically provides new pairs of\ndemonstration-instruction data for policy fine-tuning. We evaluate our method\non a broad range of experiments with the focus on generalization on unseen\nobjects, unseen tasks, unseen environments, and sim-to-real transfer. We show\nPAFF improves baselines by a large margin in all cases. Our project page is\navailable at https://geyuying.github.io/PAFF/\n","authors":["Yuying Ge","Annabella Macaluso","Li Erran Li","Ping Luo","Xiaolong Wang"],"pdf_url":"https://arxiv.org/pdf/2212.07398v4.pdf","comment":"Accepted by CVPR 2023; Project page: https://geyuying.github.io/PAFF/"},{"id":"http://arxiv.org/abs/2205.09809v5","updated":"2023-03-21T16:00:59Z","published":"2022-05-19T19:05:15Z","title":"Calibration Matters: Tackling Maximization Bias in Large-scale\n  Advertising Recommendation Systems","summary":"  Calibration is defined as the ratio of the average predicted click rate to\nthe true click rate. The optimization of calibration is essential to many\nonline advertising recommendation systems because it directly affects the\ndownstream bids in ads auctions and the amount of money charged to advertisers.\nDespite its importance, calibration optimization often suffers from a problem\ncalled \"maximization bias\". Maximization bias refers to the phenomenon that the\nmaximum of predicted values overestimates the true maximum. The problem is\nintroduced because the calibration is computed on the set selected by the\nprediction model itself. It persists even if unbiased predictions can be\nachieved on every datapoint and worsens when covariate shifts exist between the\ntraining and test sets. To mitigate this problem, we theorize the\nquantification of maximization bias and propose a variance-adjusting debiasing\n(VAD) meta-algorithm in this paper. The algorithm is efficient, robust, and\npractical as it is able to mitigate maximization bias problems under covariate\nshifts, neither incurring additional online serving costs nor compromising the\nranking performance. We demonstrate the effectiveness of the proposed algorithm\nusing a state-of-the-art recommendation neural network model on a large-scale\nreal-world dataset.\n","authors":["Yewen Fan","Nian Si","Kun Zhang"],"pdf_url":"https://arxiv.org/pdf/2205.09809v5.pdf","comment":"Accepted in ICLR 2023"},{"id":"http://arxiv.org/abs/2303.11954v1","updated":"2023-03-21T15:45:06Z","published":"2023-03-21T15:45:06Z","title":"Bayesian Optimization for Function Compositions with Applications to\n  Dynamic Pricing","summary":"  Bayesian Optimization (BO) is used to find the global optima of black box\nfunctions. In this work, we propose a practical BO method of function\ncompositions where the form of the composition is known but the constituent\nfunctions are expensive to evaluate. By assuming an independent Gaussian\nprocess (GP) model for each of the constituent black-box function, we propose\nEI and UCB based BO algorithms and demonstrate their ability to outperform\nvanilla BO and the current state-of-art algorithms. We demonstrate a novel\napplication of the proposed methods to dynamic pricing in revenue management\nwhen the underlying demand function is expensive to evaluate.\n","authors":["Kunal Jain","Prabuchandran K. J.","Tejas Bodas"],"pdf_url":"https://arxiv.org/pdf/2303.11954v1.pdf","comment":"16 pages, 6 figures, 1 table, To be published in: 17th Learning And\n  Intelligent Optimization Conference (LION17)"},{"id":"http://arxiv.org/abs/2303.10167v2","updated":"2023-03-21T15:42:16Z","published":"2023-03-17T17:54:25Z","title":"Generalized partitioned local depth","summary":"  In this paper we provide a generalization of the concept of cohesion as\nintroduced recently by Berenhaut, Moore and Melvin [Proceedings of the National\nAcademy of Sciences, 119 (4) (2022)]. The formulation presented builds on the\ntechnique of partitioned local depth by distilling two key probabilistic\nconcepts: local relevance and support division. Earlier results are extended\nwithin the new context, and examples of applications to revealing communities\nin data with uncertainty are included.\n","authors":["Kenneth S. Berenhaut","John D. Foley","Liangdongsheng Lyu"],"pdf_url":"https://arxiv.org/pdf/2303.10167v2.pdf","comment":"Improved exposition, examples in 5.1 & 5.3 expanded, 17 pages, 7\n  figures"},{"id":"http://arxiv.org/abs/2206.04928v5","updated":"2023-03-21T15:35:50Z","published":"2022-06-10T07:52:06Z","title":"GAMR: A Guided Attention Model for (visual) Reasoning","summary":"  Humans continue to outperform modern AI systems in their ability to flexibly\nparse and understand complex visual scenes. Here, we present a novel module for\nvisual reasoning, the Guided Attention Model for (visual) Reasoning (GAMR),\nwhich instantiates an active vision theory -- positing that the brain solves\ncomplex visual reasoning problems dynamically -- via sequences of attention\nshifts to select and route task-relevant visual information into memory.\nExperiments on an array of visual reasoning tasks and datasets demonstrate\nGAMR's ability to learn visual routines in a robust and sample-efficient\nmanner. In addition, GAMR is shown to be capable of zero-shot generalization on\ncompletely novel reasoning tasks. Overall, our work provides computational\nsupport for cognitive theories that postulate the need for a critical interplay\nbetween attention and memory to dynamically maintain and manipulate\ntask-relevant visual information to solve complex visual reasoning tasks.\n","authors":["Mohit Vaishnav","Thomas Serre"],"pdf_url":"https://arxiv.org/pdf/2206.04928v5.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.11932v1","updated":"2023-03-21T15:34:50Z","published":"2023-03-21T15:34:50Z","title":"Using Explanations to Guide Models","summary":"  Deep neural networks are highly performant, but might base their decision on\nspurious or background features that co-occur with certain classes, which can\nhurt generalization. To mitigate this issue, the usage of 'model guidance' has\ngained popularity recently: for this, models are guided to be \"right for the\nright reasons\" by regularizing the models' explanations to highlight the right\nfeatures. Experimental validation of these approaches has thus far however been\nlimited to relatively simple and / or synthetic datasets. To gain a better\nunderstanding of which model-guiding approaches actually transfer to more\nchallenging real-world datasets, in this work we conduct an in-depth evaluation\nacross various loss functions, attribution methods, models, and 'guidance\ndepths' on the PASCAL VOC 2007 and MS COCO 2014 datasets, and show that model\nguidance can sometimes even improve model performance. In this context, we\nfurther propose a novel energy loss, show its effectiveness in directing the\nmodel to focus on object features. We also show that these gains can be\nachieved even with a small fraction (e.g. 1%) of bounding box annotations,\nhighlighting the cost effectiveness of this approach. Lastly, we show that this\napproach can also improve generalization under distribution shifts. Code will\nbe made available.\n","authors":["Sukrut Rao","Moritz Böhle","Amin Parchami-Araghi","Bernt Schiele"],"pdf_url":"https://arxiv.org/pdf/2303.11932v1.pdf","comment":"38 pages, 35 figures, 4 tables"},{"id":"http://arxiv.org/abs/2211.14983v2","updated":"2023-03-21T15:18:48Z","published":"2022-11-28T01:11:11Z","title":"Multiagent Reinforcement Learning for Autonomous Routing and Pickup\n  Problem with Adaptation to Variable Demand","summary":"  We derive a learning framework to generate routing/pickup policies for a\nfleet of autonomous vehicles tasked with servicing stochastically appearing\nrequests on a city map. We focus on policies that 1) give rise to coordination\namongst the vehicles, thereby reducing wait times for servicing requests, 2)\nare non-myopic, and consider a-priori potential future requests, 3) can adapt\nto changes in the underlying demand distribution. Specifically, we are\ninterested in policies that are adaptive to fluctuations of actual demand\nconditions in urban environments, such as on-peak vs. off-peak hours. We\nachieve this through a combination of (i) an online play algorithm that\nimproves the performance of an offline-trained policy, and (ii) an offline\napproximation scheme that allows for adapting to changes in the underlying\ndemand model. In particular, we achieve adaptivity of our learned policy to\ndifferent demand distributions by quantifying a region of validity using the\nq-valid radius of a Wasserstein Ambiguity Set. We propose a mechanism for\nswitching the originally trained offline approximation when the current demand\nis outside the original validity region. In this case, we propose to use an\noffline architecture, trained on a historical demand model that is closer to\nthe current demand in terms of Wasserstein distance. We learn routing and\npickup policies over real taxicab requests in San Francisco with high\nvariability between on-peak and off-peak hours, demonstrating the ability of\nour method to adapt to real fluctuation in demand distributions. Our numerical\nresults demonstrate that our method outperforms alternative rollout-based\nreinforcement learning schemes, as well as other classical methods from\noperations research.\n","authors":["Daniel Garces","Sushmita Bhattacharya","Stephanie Gil","Dimitri Bertsekas"],"pdf_url":"https://arxiv.org/pdf/2211.14983v2.pdf","comment":"8 pages, 6 figures, 3 tables, accepted to ICRA 2023"},{"id":"http://arxiv.org/abs/2303.11920v1","updated":"2023-03-21T15:12:01Z","published":"2023-03-21T15:12:01Z","title":"Do intermediate feature coalitions aid explainability of black-box\n  models?","summary":"  This work introduces the notion of intermediate concepts based on levels\nstructure to aid explainability for black-box models. The levels structure is a\nhierarchical structure in which each level corresponds to features of a dataset\n(i.e., a player-set partition). The level of coarseness increases from the\ntrivial set, which only comprises singletons, to the set, which only contains\nthe grand coalition. In addition, it is possible to establish meronomies, i.e.,\npart-whole relationships, via a domain expert that can be utilised to generate\nexplanations at an abstract level. We illustrate the usability of this approach\nin a real-world car model example and the Titanic dataset, where intermediate\nconcepts aid in explainability at different levels of abstraction.\n","authors":["Minal Suresh Patil","Kary Främling"],"pdf_url":"https://arxiv.org/pdf/2303.11920v1.pdf","comment":"14 pages"},{"id":"http://arxiv.org/abs/2208.09189v3","updated":"2023-03-21T15:06:37Z","published":"2022-08-19T07:28:31Z","title":"Cross-Domain Evaluation of a Deep Learning-Based Type Inference System","summary":"  Optional type annotations allow for enriching dynamic programming languages\nwith static typing features like better Integrated Development Environment\n(IDE) support, more precise program analysis, and early detection and\nprevention of type-related runtime errors. Machine learning-based type\ninference promises interesting results for automating this task. However, the\npractical usage of such systems depends on their ability to generalize across\ndifferent domains, as they are often applied outside their training domain. In\nthis work, we investigate Type4Py as a representative of state-of-the-art deep\nlearning-based type inference systems, by conducting extensive cross-domain\nexperiments. Thereby, we address the following problems: class imbalances,\nout-of-vocabulary words, dataset shifts, and unknown classes. To perform such\nexperiments, we use the datasets ManyTypes4Py and CrossDomainTypes4Py. The\nlatter we introduce in this paper. Our dataset enables the evaluation of type\ninference systems in different domains of software projects and has over\n1,000,000 type annotations mined on the platforms GitHub and Libraries. It\nconsists of data from the two domains web development and scientific\ncalculation. Through our experiments, we detect that the shifts in the dataset\nand the long-tailed distribution with many rare and unknown data types decrease\nthe performance of the deep learning-based type inference system drastically.\nIn this context, we test unsupervised domain adaptation methods and fine-tuning\nto overcome these issues. Moreover, we investigate the impact of\nout-of-vocabulary words.\n","authors":["Bernd Gruner","Tim Sonnekalb","Thomas S. Heinze","Clemens-Alexander Brust"],"pdf_url":"https://arxiv.org/pdf/2208.09189v3.pdf","comment":"Preprint for the MSR'23 technical track"},{"id":"http://arxiv.org/abs/2303.11911v1","updated":"2023-03-21T15:02:50Z","published":"2023-03-21T15:02:50Z","title":"Time Series Contrastive Learning with Information-Aware Augmentations","summary":"  Various contrastive learning approaches have been proposed in recent years\nand achieve significant empirical success. While effective and prevalent,\ncontrastive learning has been less explored for time series data. A key\ncomponent of contrastive learning is to select appropriate augmentations\nimposing some priors to construct feasible positive samples, such that an\nencoder can be trained to learn robust and discriminative representations.\nUnlike image and language domains where ``desired'' augmented samples can be\ngenerated with the rule of thumb guided by prefabricated human priors, the\nad-hoc manual selection of time series augmentations is hindered by their\ndiverse and human-unrecognizable temporal structures. How to find the desired\naugmentations of time series data that are meaningful for given contrastive\nlearning tasks and datasets remains an open question. In this work, we address\nthe problem by encouraging both high \\textit{fidelity} and \\textit{variety}\nbased upon information theory. A theoretical analysis leads to the criteria for\nselecting feasible data augmentations. On top of that, we propose a new\ncontrastive learning approach with information-aware augmentations, InfoTS,\nthat adaptively selects optimal augmentations for time series representation\nlearning. Experiments on various datasets show highly competitive performance\nwith up to 12.0\\% reduction in MSE on forecasting tasks and up to 3.7\\%\nrelative improvement in accuracy on classification tasks over the leading\nbaselines.\n","authors":["Dongsheng Luo","Wei Cheng","Yingheng Wang","Dongkuan Xu","Jingchao Ni","Wenchao Yu","Xuchao Zhang","Yanchi Liu","Yuncong Chen","Haifeng Chen","Xiang Zhang"],"pdf_url":"https://arxiv.org/pdf/2303.11911v1.pdf","comment":"Published in AAAI 2023"},{"id":"http://arxiv.org/abs/2303.11908v1","updated":"2023-03-21T14:58:16Z","published":"2023-03-21T14:58:16Z","title":"Non-Asymptotic Pointwise and Worst-Case Bounds for Classical Spectrum\n  Estimators","summary":"  Spectrum estimation is a fundamental methodology in the analysis of\ntime-series data, with applications including medicine, speech analysis, and\ncontrol design. The asymptotic theory of spectrum estimation is\nwell-understood, but the theory is limited when the number of samples is fixed\nand finite. This paper gives non-asymptotic error bounds for a broad class of\nspectral estimators, both pointwise (at specific frequencies) and in the worst\ncase over all frequencies. The general method is used to derive error bounds\nfor the classical Blackman-Tukey, Bartlett, and Welch estimators.\n","authors":["Andrew Lamperski"],"pdf_url":"https://arxiv.org/pdf/2303.11908v1.pdf","comment":"12 pages, under review in IEEE Transactions on Signal Processing"},{"id":"http://arxiv.org/abs/2205.00701v3","updated":"2023-03-21T14:39:49Z","published":"2022-05-02T07:45:51Z","title":"DeepGraviLens: a Multi-Modal Architecture for Classifying Gravitational\n  Lensing Data","summary":"  Gravitational lensing is the relativistic effect generated by massive bodies,\nwhich bend the space-time surrounding them. It is a deeply investigated topic\nin astrophysics and allows validating theoretical relativistic results and\nstudying faint astrophysical objects that would not be visible otherwise. In\nrecent years Machine Learning methods have been applied to support the analysis\nof the gravitational lensing phenomena by detecting lensing effects in data\nsets consisting of images associated with brightness variation time series.\nHowever, the state-of-art approaches either consider only images and neglect\ntime-series data or achieve relatively low accuracy on the most difficult data\nsets. This paper introduces DeepGraviLens, a novel multi-modal network that\nclassifies spatio-temporal data belonging to one non-lensed system type and\nthree lensed system types. It surpasses the current state of the art accuracy\nresults by $\\approx$ 19% to $\\approx$ 43%, depending on the considered data\nset. Such an improvement will enable the acceleration of the analysis of lensed\nobjects in upcoming astrophysical surveys, which will exploit the petabytes of\ndata collected, e.g., from the Vera C. Rubin Observatory.\n","authors":["Nicolò Oreste Pinciroli Vago","Piero Fraternali"],"pdf_url":"https://arxiv.org/pdf/2205.00701v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.11884v1","updated":"2023-03-21T14:24:58Z","published":"2023-03-21T14:24:58Z","title":"Better Understanding Differences in Attribution Methods via Systematic\n  Evaluations","summary":"  Deep neural networks are very successful on many vision tasks, but hard to\ninterpret due to their black box nature. To overcome this, various post-hoc\nattribution methods have been proposed to identify image regions most\ninfluential to the models' decisions. Evaluating such methods is challenging\nsince no ground truth attributions exist. We thus propose three novel\nevaluation schemes to more reliably measure the faithfulness of those methods,\nto make comparisons between them more fair, and to make visual inspection more\nsystematic. To address faithfulness, we propose a novel evaluation setting\n(DiFull) in which we carefully control which parts of the input can influence\nthe output in order to distinguish possible from impossible attributions. To\naddress fairness, we note that different methods are applied at different\nlayers, which skews any comparison, and so evaluate all methods on the same\nlayers (ML-Att) and discuss how this impacts their performance on quantitative\nmetrics. For more systematic visualizations, we propose a scheme (AggAtt) to\nqualitatively evaluate the methods on complete datasets. We use these\nevaluation schemes to study strengths and shortcomings of some widely used\nattribution methods over a wide range of models. Finally, we propose a\npost-processing smoothing step that significantly improves the performance of\nsome attribution methods, and discuss its applicability.\n","authors":["Sukrut Rao","Moritz Böhle","Bernt Schiele"],"pdf_url":"https://arxiv.org/pdf/2303.11884v1.pdf","comment":"35 pages, 37 figures, 2 tables, extended version of arXiv:2205.10435"},{"id":"http://arxiv.org/abs/2303.11881v1","updated":"2023-03-21T14:24:26Z","published":"2023-03-21T14:24:26Z","title":"Protective Self-Adaptive Pruning to Better Compress DNNs","summary":"  Adaptive network pruning approach has recently drawn significant attention\ndue to its excellent capability to identify the importance and redundancy of\nlayers and filters and customize a suitable pruning solution. However, it\nremains unsatisfactory since current adaptive pruning methods rely mostly on an\nadditional monitor to score layer and filter importance, and thus faces high\ncomplexity and weak interpretability. To tackle these issues, we have deeply\nresearched the weight reconstruction process in iterative prune-train process\nand propose a Protective Self-Adaptive Pruning (PSAP) method. First of all,\nPSAP can utilize its own information, weight sparsity ratio, to adaptively\nadjust pruning ratio of layers before each pruning step. Moreover, we propose a\nprotective reconstruction mechanism to prevent important filters from being\npruned through supervising gradients and to avoid unrecoverable information\nloss as well. Our PSAP is handy and explicit because it merely depends on\nweights and gradients of model itself, instead of requiring an additional\nmonitor as in early works. Experiments on ImageNet and CIFAR-10 also\ndemonstrate its superiority to current works in both accuracy and compression\nratio, especially for compressing with a high ratio or pruning from scratch.\n","authors":["Liang Li","Pengfei Zhao"],"pdf_url":"https://arxiv.org/pdf/2303.11881v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.01593v2","updated":"2023-03-21T14:22:00Z","published":"2023-03-02T21:35:15Z","title":"QAID: Question Answering Inspired Few-shot Intent Detection","summary":"  Intent detection with semantically similar fine-grained intents is a\nchallenging task. To address it, we reformulate intent detection as a\nquestion-answering retrieval task by treating utterances and intent names as\nquestions and answers. To that end, we utilize a question-answering retrieval\narchitecture and adopt a two stages training schema with batch contrastive\nloss. In the pre-training stage, we improve query representations through\nself-supervised training. Then, in the fine-tuning stage, we increase\ncontextualized token-level similarity scores between queries and answers from\nthe same intent. Our results on three few-shot intent detection benchmarks\nachieve state-of-the-art performance.\n","authors":["Asaf Yehudai","Matan Vetzler","Yosi Mass","Koren Lazar","Doron Cohen","Boaz Carmeli"],"pdf_url":"https://arxiv.org/pdf/2303.01593v2.pdf","comment":"ICLR paper"},{"id":"http://arxiv.org/abs/2303.11873v1","updated":"2023-03-21T14:17:29Z","published":"2023-03-21T14:17:29Z","title":"A Tale of Two Circuits: Grokking as Competition of Sparse and Dense\n  Subnetworks","summary":"  Grokking is a phenomenon where a model trained on an algorithmic task first\noverfits but, then, after a large amount of additional training, undergoes a\nphase transition to generalize perfectly. We empirically study the internal\nstructure of networks undergoing grokking on the sparse parity task, and find\nthat the grokking phase transition corresponds to the emergence of a sparse\nsubnetwork that dominates model predictions. On an optimization level, we find\nthat this subnetwork arises when a small subset of neurons undergoes rapid norm\ngrowth, whereas the other neurons in the network decay slowly in norm. Thus, we\nsuggest that the grokking phase transition can be understood to emerge from\ncompetition of two largely distinct subnetworks: a dense one that dominates\nbefore the transition and generalizes poorly, and a sparse one that dominates\nafterwards.\n","authors":["William Merrill","Nikolaos Tsilivis","Aman Shukla"],"pdf_url":"https://arxiv.org/pdf/2303.11873v1.pdf","comment":"Published at the Workshop on Understanding Foundation Models at ICLR\n  2023"},{"id":"http://arxiv.org/abs/2302.11577v2","updated":"2023-03-21T14:16:06Z","published":"2023-01-25T10:34:38Z","title":"Explainable AI does not provide the explanations end-users are asking\n  for","summary":"  Explainable Artificial Intelligence (XAI) techniques are frequently required\nby users in many AI systems with the goal of understanding complex models,\ntheir associated predictions, and gaining trust. While suitable for some\nspecific tasks during development, their adoption by organisations to enhance\ntrust in machine learning systems has unintended consequences. In this paper we\ndiscuss XAI's limitations in deployment and conclude that transparency\nalongside with rigorous validation are better suited to gaining trust in AI\nsystems.\n","authors":["Savio Rozario","George Čevora"],"pdf_url":"https://arxiv.org/pdf/2302.11577v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.01254v2","updated":"2023-03-21T14:11:37Z","published":"2023-02-13T10:33:21Z","title":"Privacy-Preserving Tree-Based Inference with Fully Homomorphic\n  Encryption","summary":"  Privacy enhancing technologies (PETs) have been proposed as a way to protect\nthe privacy of data while still allowing for data analysis. In this work, we\nfocus on Fully Homomorphic Encryption (FHE), a powerful tool that allows for\narbitrary computations to be performed on encrypted data. FHE has received lots\nof attention in the past few years and has reached realistic execution times\nand correctness.\n  More precisely, we explain in this paper how we apply FHE to tree-based\nmodels and get state-of-the-art solutions over encrypted tabular data. We show\nthat our method is applicable to a wide range of tree-based models, including\ndecision trees, random forests, and gradient boosted trees, and has been\nimplemented within the Concrete-ML library, which is open-source at\nhttps://github.com/zama-ai/concrete-ml. With a selected set of use-cases, we\ndemonstrate that our FHE version is very close to the unprotected version in\nterms of accuracy.\n","authors":["Jordan Frery","Andrei Stoian","Roman Bredehoft","Luis Montero","Celia Kherfallah","Benoit Chevallier-Mames","Arthur Meyre"],"pdf_url":"https://arxiv.org/pdf/2303.01254v2.pdf","comment":null},{"id":"http://arxiv.org/abs/1911.02621v3","updated":"2023-03-21T14:10:36Z","published":"2019-11-06T20:29:56Z","title":"The Threat of Adversarial Attacks on Machine Learning in Network\n  Security -- A Survey","summary":"  Machine learning models have made many decision support systems to be faster,\nmore accurate, and more efficient. However, applications of machine learning in\nnetwork security face a more disproportionate threat of active adversarial\nattacks compared to other domains. This is because machine learning\napplications in network security such as malware detection, intrusion\ndetection, and spam filtering are by themselves adversarial in nature. In what\ncould be considered an arm's race between attackers and defenders, adversaries\nconstantly probe machine learning systems with inputs that are explicitly\ndesigned to bypass the system and induce a wrong prediction. In this survey, we\nfirst provide a taxonomy of machine learning techniques, tasks, and depth. We\nthen introduce a classification of machine learning in network security\napplications. Next, we examine various adversarial attacks against machine\nlearning in network security and introduce two classification approaches for\nadversarial attacks in network security. First, we classify adversarial attacks\nin network security based on a taxonomy of network security applications.\nSecondly, we categorize adversarial attacks in network security into a problem\nspace vs feature space dimensional classification model. We then analyze the\nvarious defenses against adversarial attacks on machine learning-based network\nsecurity applications. We conclude by introducing an adversarial risk grid map\nand evaluating several existing adversarial attacks against machine learning in\nnetwork security using the risk grid map. We also identify where each attack\nclassification resides within the adversarial risk grid map.\n","authors":["Olakunle Ibitoye","Rana Abou-Khamis","Mohamed el Shehaby","Ashraf Matrawy","M. Omair Shafiq"],"pdf_url":"https://arxiv.org/pdf/1911.02621v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.11863v1","updated":"2023-03-21T14:06:12Z","published":"2023-03-21T14:06:12Z","title":"Continual Learning in the Presence of Spurious Correlation","summary":"  Most continual learning (CL) algorithms have focused on tackling the\nstability-plasticity dilemma, that is, the challenge of preventing the\nforgetting of previous tasks while learning new ones. However, they have\noverlooked the impact of the knowledge transfer when the dataset in a certain\ntask is biased - namely, when some unintended spurious correlations of the\ntasks are learned from the biased dataset. In that case, how would they affect\nlearning future tasks or the knowledge already learned from the past tasks? In\nthis work, we carefully design systematic experiments using one synthetic and\ntwo real-world datasets to answer the question from our empirical findings.\nSpecifically, we first show through two-task CL experiments that standard CL\nmethods, which are unaware of dataset bias, can transfer biases from one task\nto another, both forward and backward, and this transfer is exacerbated\ndepending on whether the CL methods focus on the stability or the plasticity.\nWe then present that the bias transfer also exists and even accumulate in\nlonger sequences of tasks. Finally, we propose a simple, yet strong plug-in\nmethod for debiasing-aware continual learning, dubbed as Group-class Balanced\nGreedy Sampling (BGS). As a result, we show that our BGS can always reduce the\nbias of a CL model, with a slight loss of CL performance at most.\n","authors":["Donggyu Lee","Sangwon Jung","Taesup Moon"],"pdf_url":"https://arxiv.org/pdf/2303.11863v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.11860v1","updated":"2023-03-21T13:59:35Z","published":"2023-03-21T13:59:35Z","title":"Online Transformers with Spiking Neurons for Fast Prosthetic Hand\n  Control","summary":"  Transformers are state-of-the-art networks for most sequence processing\ntasks. However, the self-attention mechanism often used in Transformers\nrequires large time windows for each computation step and thus makes them less\nsuitable for online signal processing compared to Recurrent Neural Networks\n(RNNs). In this paper, instead of the self-attention mechanism, we use a\nsliding window attention mechanism. We show that this mechanism is more\nefficient for continuous signals with finite-range dependencies between input\nand target, and that we can use it to process sequences element-by-element,\nthis making it compatible with online processing. We test our model on a finger\nposition regression dataset (NinaproDB8) with Surface Electromyographic (sEMG)\nsignals measured on the forearm skin to estimate muscle activities. Our\napproach sets the new state-of-the-art in terms of accuracy on this dataset\nwhile requiring only very short time windows of 3.5 ms at each inference step.\nMoreover, we increase the sparsity of the network using Leaky-Integrate and\nFire (LIF) units, a bio-inspired neuron model that activates sparsely in time\nsolely when crossing a threshold. We thus reduce the number of synaptic\noperations up to a factor of $\\times5.3$ without loss of accuracy. Our results\nhold great promises for accurate and fast online processing of sEMG signals for\nsmooth prosthetic hand control and is a step towards Transformers and Spiking\nNeural Networks (SNNs) co-integration for energy efficient temporal signal\nprocessing.\n","authors":["Nathan Leroux","Jan Finkbeiner","Emre Neftci"],"pdf_url":"https://arxiv.org/pdf/2303.11860v1.pdf","comment":"Preprint of 9 pages, 4 figures"},{"id":"http://arxiv.org/abs/2303.11848v1","updated":"2023-03-21T13:48:53Z","published":"2023-03-21T13:48:53Z","title":"Dens-PU: PU Learning with Density-Based Positive Labeled Augmentation","summary":"  This study proposes a novel approach for solving the PU learning problem\nbased on an anomaly-detection strategy. Latent encodings extracted from\npositive-labeled data are linearly combined to acquire new samples. These new\nsamples are used as embeddings to increase the density of positive-labeled data\nand, thus, define a boundary that approximates the positive class. The further\na sample is from the boundary the more it is considered as a negative sample.\nOnce a set of negative samples is obtained, the PU learning problem reduces to\nbinary classification. The approach, named Dens-PU due to its reliance on the\ndensity of positive-labeled data, was evaluated using benchmark image datasets,\nand state-of-the-art results were attained.\n","authors":["Vasileios Sevetlidis","George Pavlidis","Spyridon Mouroutsos","Antonios Gasteratos"],"pdf_url":"https://arxiv.org/pdf/2303.11848v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.09280v2","updated":"2023-03-21T13:48:00Z","published":"2023-03-13T12:44:32Z","title":"Topology optimization with physics-informed neural networks: application\n  to noninvasive detection of hidden geometries","summary":"  Detecting hidden geometrical structures from surface measurements under\nelectromagnetic, acoustic, or mechanical loading is the goal of noninvasive\nimaging techniques in medical and industrial applications. Solving the inverse\nproblem can be challenging due to the unknown topology and geometry, the\nsparsity of the data, and the complexity of the physical laws. Physics-informed\nneural networks (PINNs) have shown promise as a simple-yet-powerful tool for\nproblem inversion, but they have yet to be applied to general problems with a\npriori unknown topology. Here, we introduce a topology optimization framework\nbased on PINNs that solves geometry detection problems without prior knowledge\nof the number or types of shapes. We allow for arbitrary solution topology by\nrepresenting the geometry using a material density field that approaches binary\nvalues thanks to a novel eikonal regularization. We validate our framework by\ndetecting the number, locations, and shapes of hidden voids and inclusions in\nlinear and nonlinear elastic bodies using measurements of outer surface\ndisplacement from a single mechanical loading experiment. Our methodology opens\na pathway for PINNs to solve various engineering problems targeting geometry\noptimization.\n","authors":["Saviz Mowlavi","Ken Kamrin"],"pdf_url":"https://arxiv.org/pdf/2303.09280v2.pdf","comment":"24 pages, 16 figures including supplementary information. Added\n  supplementary movies"},{"id":"http://arxiv.org/abs/2303.11844v1","updated":"2023-03-21T13:42:43Z","published":"2023-03-21T13:42:43Z","title":"Doubly Regularized Entropic Wasserstein Barycenters","summary":"  We study a general formulation of regularized Wasserstein barycenters that\nenjoys favorable regularity, approximation, stability and (grid-free)\noptimization properties. This barycenter is defined as the unique probability\nmeasure that minimizes the sum of entropic optimal transport (EOT) costs with\nrespect to a family of given probability measures, plus an entropy term. We\ndenote it $(\\lambda,\\tau)$-barycenter, where $\\lambda$ is the inner\nregularization strength and $\\tau$ the outer one. This formulation recovers\nseveral previously proposed EOT barycenters for various choices of\n$\\lambda,\\tau \\geq 0$ and generalizes them. First, in spite of -- and in fact\nowing to -- being \\emph{doubly} regularized, we show that our formulation is\ndebiased for $\\tau=\\lambda/2$: the suboptimality in the (unregularized)\nWasserstein barycenter objective is, for smooth densities, of the order of the\nstrength $\\lambda^2$ of entropic regularization, instead of\n$\\max\\{\\lambda,\\tau\\}$ in general. We discuss this phenomenon for isotropic\nGaussians where all $(\\lambda,\\tau)$-barycenters have closed form. Second, we\nshow that for $\\lambda,\\tau>0$, this barycenter has a smooth density and is\nstrongly stable under perturbation of the marginals. In particular, it can be\nestimated efficiently: given $n$ samples from each of the probability measures,\nit converges in relative entropy to the population barycenter at a rate\n$n^{-1/2}$. And finally, this formulation lends itself naturally to a grid-free\noptimization algorithm: we propose a simple \\emph{noisy particle gradient\ndescent} which, in the mean-field limit, converges globally at an exponential\nrate to the barycenter.\n","authors":["Lénaïc Chizat"],"pdf_url":"https://arxiv.org/pdf/2303.11844v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2211.08326v2","updated":"2023-03-21T13:37:04Z","published":"2022-11-14T10:07:30Z","title":"Contrastive learning for regression in multi-site brain age prediction","summary":"  Building accurate Deep Learning (DL) models for brain age prediction is a\nvery relevant topic in neuroimaging, as it could help better understand\nneurodegenerative disorders and find new biomarkers. To estimate accurate and\ngeneralizable models, large datasets have been collected, which are often\nmulti-site and multi-scanner. This large heterogeneity negatively affects the\ngeneralization performance of DL models since they are prone to overfit\nsite-related noise. Recently, contrastive learning approaches have been shown\nto be more robust against noise in data or labels. For this reason, we propose\na novel contrastive learning regression loss for robust brain age prediction\nusing MRI scans. Our method achieves state-of-the-art performance on the\nOpenBHB challenge, yielding the best generalization capability and robustness\nto site-related noise.\n","authors":["Carlo Alberto Barbano","Benoit Dufumier","Edouard Duchesnay","Marco Grangetto","Pietro Gori"],"pdf_url":"https://arxiv.org/pdf/2211.08326v2.pdf","comment":"5 pages"},{"id":"http://arxiv.org/abs/2303.11833v1","updated":"2023-03-21T13:21:43Z","published":"2023-03-21T13:21:43Z","title":"Materials Discovery with Extreme Properties via AI-Driven Combinatorial\n  Chemistry","summary":"  The goal of most materials discovery is to discover materials that are\nsuperior to those currently known. Fundamentally, this is close to\nextrapolation, which is a weak point for most machine learning models that\nlearn the probability distribution of data. Herein, we develop AI-driven\ncombinatorial chemistry, which is a rule-based inverse molecular designer that\ndoes not rely on data. Since our model has the potential to generate all\npossible molecular structures that can be obtained from combinations of\nmolecular fragments, unknown materials with superior properties can be\ndiscovered. We theoretically and empirically demonstrate that our model is more\nsuitable for discovering better materials than probability\ndistribution-learning models. In an experiment aimed at discovering molecules\nthat hit seven target properties, our model discovered 1,315 of all\ntarget-hitting molecules and 7,629 of five target-hitting molecules out of\n100,000 trials, whereas the probability distribution-learning models failed. To\nillustrate the performance in actual problems, we also demonstrate that our\nmodels work well on two practical applications: discovering protein docking\nmaterials and HIV inhibitors.\n","authors":["Hyunseung Kim","Haeyeon Choi","Dongju Kang","Won Bo Lee","Jonggeol Na"],"pdf_url":"https://arxiv.org/pdf/2303.11833v1.pdf","comment":"30 pages, 5 figures"},{"id":"http://arxiv.org/abs/2303.11831v1","updated":"2023-03-21T13:19:51Z","published":"2023-03-21T13:19:51Z","title":"GLADE: Gradient Loss Augmented Degradation Enhancement for Unpaired\n  Super-Resolution of Anisotropic MRI","summary":"  We present a novel approach to synthesise high-resolution isotropic 3D\nabdominal MR images, from anisotropic 3D images in an unpaired fashion. Using a\nmodified CycleGAN architecture with a gradient mapping loss, we leverage\ndisjoint patches from the high-resolution (in-plane) data of an anisotropic\nvolume to enforce the network generator to increase the resolution of the\nlow-resolution (through-plane) slices. This will enable accelerated\nwhole-abdomen scanning with high-resolution isotropic images within short\nbreath-hold times.\n","authors":["Michele Pascale","Vivek Muthurangu","Javier Montalt Tordera","Heather E Fitzke","Gauraang Bhatnagar","Stuart Taylor","Jennifer Steeden"],"pdf_url":"https://arxiv.org/pdf/2303.11831v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2203.07120v3","updated":"2023-03-21T13:09:33Z","published":"2022-03-14T14:08:46Z","title":"Neural Message Passing for Objective-Based Uncertainty Quantification\n  and Optimal Experimental Design","summary":"  Various real-world scientific applications involve the mathematical modeling\nof complex uncertain systems with numerous unknown parameters. Accurate\nparameter estimation is often practically infeasible in such systems, as the\navailable training data may be insufficient and the cost of acquiring\nadditional data may be high. In such cases, based on a Bayesian paradigm, we\ncan design robust operators retaining the best overall performance across all\npossible models and design optimal experiments that can effectively reduce\nuncertainty to enhance the performance of such operators maximally. While\nobjective-based uncertainty quantification (objective-UQ) based on MOCU (mean\nobjective cost of uncertainty) provides an effective means for quantifying\nuncertainty in complex systems, the high computational cost of estimating MOCU\nhas been a challenge in applying it to real-world scientific/engineering\nproblems. In this work, we propose a novel scheme to reduce the computational\ncost for objective-UQ via MOCU based on a data-driven approach. We adopt a\nneural message-passing model for surrogate modeling, incorporating a novel\naxiomatic constraint loss that penalizes an increase in the estimated system\nuncertainty. As an illustrative example, we consider the optimal experimental\ndesign (OED) problem for uncertain Kuramoto models, where the goal is to\npredict the experiments that can most effectively enhance robust\nsynchronization performance through uncertainty reduction. We show that our\nproposed approach can accelerate MOCU-based OED by four to five orders of\nmagnitude, without any visible performance loss compared to the\nstate-of-the-art. The proposed approach applies to general OED tasks, beyond\nthe Kuramoto model.\n","authors":["Qihua Chen","Xuejin Chen","Hyun-Myung Woo","Byung-Jun Yoon"],"pdf_url":"https://arxiv.org/pdf/2203.07120v3.pdf","comment":"14 pages, 5 figures, accepted by Engineering Applications of\n  Artificial Intelligence"},{"id":"http://arxiv.org/abs/2302.02119v2","updated":"2023-03-21T13:05:50Z","published":"2023-02-04T07:31:36Z","title":"Diversity Induced Environment Design via Self-Play","summary":"  Recent work on designing an appropriate distribution of environments has\nshown promise for training effective generally capable agents. Its success is\npartly because of a form of adaptive curriculum learning that generates\nenvironment instances (or levels) at the frontier of the agent's capabilities.\nHowever, such an environment design framework often struggles to find effective\nlevels in challenging design spaces and requires costly interactions with the\nenvironment. In this paper, we aim to introduce diversity in the Unsupervised\nEnvironment Design (UED) framework. Specifically, we propose a task-agnostic\nmethod to identify observed/hidden states that are representative of a given\nlevel. The outcome of this method is then utilized to characterize the\ndiversity between two levels, which as we show can be crucial to effective\nperformance. In addition, to improve sampling efficiency, we incorporate the\nself-play technique that allows the environment generator to automatically\ngenerate environments that are of great benefit to the training agent.\nQuantitatively, our approach, Diversity-induced Environment Design via\nSelf-Play (DivSP), shows compelling performance over existing methods.\n","authors":["Dexun Li","Wenjun Li","Pradeep Varakantham"],"pdf_url":"https://arxiv.org/pdf/2302.02119v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2302.08722v3","updated":"2023-03-21T12:59:20Z","published":"2023-02-17T06:33:06Z","title":"GPT4MIA: Utilizing Generative Pre-trained Transformer (GPT-3) as A\n  Plug-and-Play Transductive Model for Medical Image Analysis","summary":"  In this paper, we propose a novel approach (called GPT4MIA) that utilizes\nGenerative Pre-trained Transformer (GPT) as a plug-and-play transductive\ninference tool for medical image analysis (MIA). We provide theoretical\nanalysis on why a large pre-trained language model such as GPT-3 can be used as\na plug-and-play transductive inference model for MIA. At the methodological\nlevel, we develop several technical treatments to improve the efficiency and\neffectiveness of GPT4MIA, including better prompt structure design, sample\nselection, and prompt ordering of representative samples/features. We present\ntwo concrete use cases (with workflow) of GPT4MIA: (1) detecting prediction\nerrors and (2) improving prediction accuracy, working in conjecture with\nwell-established vision-based models for image classification (e.g., ResNet).\nExperiments validate that our proposed method is effective for these two tasks.\nWe further discuss the opportunities and challenges in utilizing\nTransformer-based large language models for broader MIA applications.\n","authors":["Yizhe Zhang","Danny Z. Chen"],"pdf_url":"https://arxiv.org/pdf/2302.08722v3.pdf","comment":"Version 3: Added appendix with more results and visualizations.\n  Questions and suggestions are welcome"},{"id":"http://arxiv.org/abs/2303.11809v1","updated":"2023-03-21T12:50:17Z","published":"2023-03-21T12:50:17Z","title":"Addressing Class Variable Imbalance in Federated Semi-supervised\n  Learning","summary":"  Federated Semi-supervised Learning (FSSL) combines techniques from both\nfields of federated and semi-supervised learning to improve the accuracy and\nperformance of models in a distributed environment by using a small fraction of\nlabeled data and a large amount of unlabeled data. Without the need to\ncentralize all data in one place for training, it collect updates of model\ntraining after devices train models at local, and thus can protect the privacy\nof user data. However, during the federal training process, some of the devices\nfail to collect enough data for local training, while new devices will be\nincluded to the group training. This leads to an unbalanced global data\ndistribution and thus affect the performance of the global model training. Most\nof the current research is focusing on class imbalance with a fixed number of\nclasses, while little attention is paid to data imbalance with a variable\nnumber of classes. Therefore, in this paper, we propose Federated\nSemi-supervised Learning for Class Variable Imbalance (FCVI) to solve class\nvariable imbalance. The class-variable learning algorithm is used to mitigate\nthe data imbalance due to changes of the number of classes. Our scheme is\nproved to be significantly better than baseline methods, while maintaining\nclient privacy.\n","authors":["Zehui Dong","Wenjing Liu","Siyuan Liu","Xingzhi Chen"],"pdf_url":"https://arxiv.org/pdf/2303.11809v1.pdf","comment":"12th International Conference on Cloud Computing: Services and\n  Architecture (CLOUD 2023)"},{"id":"http://arxiv.org/abs/2303.11239v2","updated":"2023-03-21T12:43:11Z","published":"2023-03-20T16:24:06Z","title":"Training Invertible Neural Networks as Autoencoders","summary":"  Autoencoders are able to learn useful data representations in an unsupervised\nmatter and have been widely used in various machine learning and computer\nvision tasks. In this work, we present methods to train Invertible Neural\nNetworks (INNs) as (variational) autoencoders which we call INN (variational)\nautoencoders. Our experiments on MNIST, CIFAR and CelebA show that for low\nbottleneck sizes our INN autoencoder achieves results similar to the classical\nautoencoder. However, for large bottleneck sizes our INN autoencoder\noutperforms its classical counterpart. Based on the empirical results, we\nhypothesize that INN autoencoders might not have any intrinsic information loss\nand thereby are not bounded to a maximal number of layers (depth) after which\nonly suboptimal results can be achieved.\n","authors":["The-Gia Leo Nguyen","Lynton Ardizzone","Ullrich Köthe"],"pdf_url":"https://arxiv.org/pdf/2303.11239v2.pdf","comment":"Conference Paper at GCPR2019"},{"id":"http://arxiv.org/abs/2302.09738v2","updated":"2023-03-21T12:42:29Z","published":"2023-02-20T03:31:11Z","title":"Simplifying Momentum-based Riemannian Submanifold Optimization","summary":"  Riemannian submanifold optimization with momentum is computationally\nchallenging because ensuring iterates remain on the submanifold often requires\nsolving difficult differential equations. We simplify such optimization\nalgorithms for the submanifold of symmetric positive-definite matrices with the\naffine invariant metric. We propose a generalized version of the Riemannian\nnormal coordinates which dynamically trivializes the problem into a Euclidean\nunconstrained problem. We use our approach to explain and simplify existing\napproaches for structured covariances and develop efficient second-order\noptimizers for deep learning without explicit matrix inverses.\n","authors":["Wu Lin","Valentin Duruisseaux","Melvin Leok","Frank Nielsen","Mohammad Emtiyaz Khan","Mark Schmidt"],"pdf_url":"https://arxiv.org/pdf/2302.09738v2.pdf","comment":"updated the main text and added more numerical results"},{"id":"http://arxiv.org/abs/2211.11561v2","updated":"2023-03-21T12:34:27Z","published":"2022-11-18T16:58:23Z","title":"SAMSON: Sharpness-Aware Minimization Scaled by Outlier Normalization for\n  Improving DNN Generalization and Robustness","summary":"  Energy-efficient deep neural network (DNN) accelerators are prone to\nnon-idealities that degrade DNN performance at inference time. To mitigate such\ndegradation, existing methods typically add perturbations to the DNN weights\nduring training to simulate inference on noisy hardware. However, this often\nrequires knowledge about the target hardware and leads to a trade-off between\nDNN performance and robustness, decreasing the former to increase the latter.\nIn this work, we show that applying sharpness-aware training, by optimizing for\nboth the loss value and loss sharpness, significantly improves robustness to\nnoisy hardware at inference time without relying on any assumptions about the\ntarget hardware. In particular, we propose a new adaptive sharpness-aware\nmethod that conditions the worst-case perturbation of a given weight not only\non its magnitude but also on the range of the weight distribution. This is\nachieved by performing sharpness-aware minimization scaled by outlier\nminimization (SAMSON). Our approach outperforms existing sharpness-aware\ntraining methods both in terms of model generalization performance in noiseless\nregimes and robustness in noisy settings, as measured on several architectures\nand datasets.\n","authors":["Gonçalo Mordido","Sébastien Henwood","Sarath Chandar","François Leduc-Primeau"],"pdf_url":"https://arxiv.org/pdf/2211.11561v2.pdf","comment":"Preprint"},{"id":"http://arxiv.org/abs/2303.11774v1","updated":"2023-03-21T11:45:27Z","published":"2023-03-21T11:45:27Z","title":"Exact Non-Oblivious Performance of Rademacher Random Embeddings","summary":"  This paper revisits the performance of Rademacher random projections,\nestablishing novel statistical guarantees that are numerically sharp and\nnon-oblivious with respect to the input data. More specifically, the central\nresult is the Schur-concavity property of Rademacher random projections with\nrespect to the inputs. This offers a novel geometric perspective on the\nperformance of random projections, while improving quantitatively on bounds\nfrom previous works. As a corollary of this broader result, we obtained the\nimproved performance on data which is sparse or is distributed with small\nspread. This non-oblivious analysis is a novelty compared to techniques from\nprevious work, and bridges the frequently observed gap between theory and\npractise. The main result uses an algebraic framework for proving\nSchur-concavity properties, which is a contribution of independent interest and\nan elegant alternative to derivative-based criteria.\n","authors":["Maciej Skorski","Alessandro Temperoni"],"pdf_url":"https://arxiv.org/pdf/2303.11774v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2301.05599v3","updated":"2023-03-21T11:42:11Z","published":"2023-01-13T15:04:32Z","title":"Short-length SSVEP data extension by a novel generative adversarial\n  networks based framework","summary":"  Steady-state visual evoked potentials (SSVEPs) based brain-computer interface\n(BCI) has received considerable attention due to its high information transfer\nrate (ITR) and available quantity of targets. However, the performance of\nfrequency identification methods heavily hinges on the amount of user\ncalibration data and data length, which hinders the deployment in real-world\napplications. Recently, generative adversarial networks (GANs)-based data\ngeneration methods have been widely adopted to create synthetic\nelectroencephalography (EEG) data, holds promise to address these issues. In\nthis paper, we proposed a GAN-based end-to-end signal transformation network\nfor data length extension, termed as TEGAN. TEGAN transforms short-length SSVEP\nsignals into long-length artificial SSVEP signals. By incorporating a novel\nU-Net generator architecture and an auxiliary classifier into the network\narchitecture, the TEGAN could produce conditioned features in the synthetic\ndata. Additionally, we introduced a two-stage training strategy and the\nLeCam-divergence regularization term to regularize the training process of GAN\nduring the network implementation. The proposed TEGAN was evaluated on two\npublic SSVEP datasets (a 4-class dataset and a 12-class dataset). With the\nassistance of TEGAN, the performance of traditional frequency recognition\nmethods and deep learning-based methods have been significantly improved under\nlimited calibration data. And the classification performance gap of various\nfrequency recognition methods has been narrowed. This study substantiates the\nfeasibility of the proposed method to extend the data length for short-time\nSSVEP signals for developing a high-performance BCI system. The proposed\nGAN-based methods have the great potential of shortening the calibration time\nand cutting down the budget for various real-world BCI-based applications.\n","authors":["Yudong Pan","Ning Li","Yangsong Zhang","Peng Xu","Dezhong Yao"],"pdf_url":"https://arxiv.org/pdf/2301.05599v3.pdf","comment":"16 pages, 10 figures, 2 tables"},{"id":"http://arxiv.org/abs/2207.01955v3","updated":"2023-03-21T11:41:14Z","published":"2022-07-05T10:58:11Z","title":"Ask-AC: An Initiative Advisor-in-the-Loop Actor-Critic Framework","summary":"  Despite the promising results achieved, state-of-the-art interactive\nreinforcement learning schemes rely on passively receiving supervision signals\nfrom advisor experts, in the form of either continuous monitoring or\npre-defined rules, which inevitably result in a cumbersome and expensive\nlearning process. In this paper, we introduce a novel initiative\nadvisor-in-the-loop actor-critic framework, termed as Ask-AC, that replaces the\nunilateral advisor-guidance mechanism with a bidirectional learner-initiative\none, and thereby enables a customized and efficacious message exchange between\nlearner and advisor. At the heart of Ask-AC are two complementary components,\nnamely action requester and adaptive state selector, that can be readily\nincorporated into various discrete actor-critic architectures. The former\ncomponent allows the agent to initiatively seek advisor intervention in the\npresence of uncertain states, while the latter identifies the unstable states\npotentially missed by the former especially when environment changes, and then\nlearns to promote the ask action on such states. Experimental results on both\nstationary and non-stationary environments and across different actor-critic\nbackbones demonstrate that the proposed framework significantly improves the\nlearning efficiency of the agent, and achieves the performances on par with\nthose obtained by continuous advisor monitoring.\n","authors":["Shunyu Liu","Na Yu","Jie Song","Kaixuan Chen","Zunlei Feng","Mingli Song"],"pdf_url":"https://arxiv.org/pdf/2207.01955v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2301.07389v2","updated":"2023-03-21T11:40:47Z","published":"2023-01-18T09:36:41Z","title":"Towards Models that Can See and Read","summary":"  Visual Question Answering (VQA) and Image Captioning (CAP), which are among\nthe most popular vision-language tasks, have analogous scene-text versions that\nrequire reasoning from the text in the image. Despite their obvious\nresemblance, the two are treated independently and, as we show, yield\ntask-specific methods that can either see or read, but not both. In this work,\nwe conduct an in-depth analysis of this phenomenon and propose UniTNT, a\nUnified Text-Non-Text approach, which grants existing multimodal architectures\nscene-text understanding capabilities. Specifically, we treat scene-text\ninformation as an additional modality, fusing it with any pretrained\nencoder-decoder-based architecture via designated modules. Thorough experiments\nreveal that UniTNT leads to the first single model that successfully handles\nboth task types. Moreover, we show that scene-text understanding capabilities\ncan boost vision-language models' performance on general VQA and CAP by up to\n2.69% and 0.6 CIDEr, respectively.\n","authors":["Roy Ganz","Oren Nuriel","Aviad Aberdam","Yair Kittenplon","Shai Mazor","Ron Litman"],"pdf_url":"https://arxiv.org/pdf/2301.07389v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.11761v1","updated":"2023-03-21T11:28:09Z","published":"2023-03-21T11:28:09Z","title":"Reasonable Scale Machine Learning with Open-Source Metaflow","summary":"  As Machine Learning (ML) gains adoption across industries and new use cases,\npractitioners increasingly realize the challenges around effectively developing\nand iterating on ML systems: reproducibility, debugging, scalability, and\ndocumentation are elusive goals for real-world pipelines outside tech-first\ncompanies. In this paper, we review the nature of ML-oriented workloads and\nargue that re-purposing existing tools won't solve the current productivity\nissues, as ML peculiarities warrant specialized development tooling. We then\nintroduce Metaflow, an open-source framework for ML projects explicitly\ndesigned to boost the productivity of data practitioners by abstracting away\nthe execution of ML code from the definition of the business logic. We show how\nour design addresses the main challenges in ML operations (MLOps), and document\nthrough examples, interviews and use cases its practical impact on the field.\n","authors":["Jacopo Tagliabue","Hugo Bowne-Anderson","Ville Tuulos","Savin Goyal","Romain Cledat","David Berg"],"pdf_url":"https://arxiv.org/pdf/2303.11761v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.11756v1","updated":"2023-03-21T11:21:31Z","published":"2023-03-21T11:21:31Z","title":"Improving Deep Dynamics Models for Autonomous Vehicles with Multimodal\n  Latent Mapping of Surfaces","summary":"  The safe deployment of autonomous vehicles relies on their ability to\neffectively react to environmental changes. This can require maneuvering on\nvarying surfaces which is still a difficult problem, especially for slippery\nterrains. To address this issue we propose a new approach that learns a\nsurface-aware dynamics model by conditioning it on a latent variable vector\nstoring surface information about the current location. A latent mapper is\ntrained to update these latent variables during inference from multiple\nmodalities on every traversal of the corresponding locations and stores them in\na map. By training everything end-to-end with the loss of the dynamics model,\nwe enforce the latent mapper to learn an update rule for the latent map that is\nuseful for the subsequent dynamics model. We implement and evaluate our\napproach on a real miniature electric car. The results show that the latent map\nis updated to allow more accurate predictions of the dynamics model compared to\na model without this information. We further show that by using this model, the\ndriving performance can be improved on varying and challenging surfaces.\n","authors":["Johan Vertens","Nicolai Dorka","Tim Welschehold","Michael Thompson","Wolfram Burgard"],"pdf_url":"https://arxiv.org/pdf/2303.11756v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.11754v1","updated":"2023-03-21T11:20:22Z","published":"2023-03-21T11:20:22Z","title":"Projections of Model Spaces for Latent Graph Inference","summary":"  Graph Neural Networks leverage the connectivity structure of graphs as an\ninductive bias. Latent graph inference focuses on learning an adequate graph\nstructure to diffuse information on and improve the downstream performance of\nthe model. In this work we employ stereographic projections of the hyperbolic\nand spherical model spaces, as well as products of Riemannian manifolds, for\nthe purpose of latent graph inference. Stereographically projected model spaces\nachieve comparable performance to their non-projected counterparts, while\nproviding theoretical guarantees that avoid divergence of the spaces when the\ncurvature tends to zero. We perform experiments on both homophilic and\nheterophilic graphs.\n","authors":["Haitz Sáez de Ocáriz Borde","Álvaro Arroyo","Ingmar Posner"],"pdf_url":"https://arxiv.org/pdf/2303.11754v1.pdf","comment":"Accepted at the ICLR 2023 Workshop on Physics for Machine Learning"},{"id":"http://arxiv.org/abs/2211.15377v3","updated":"2023-03-21T11:19:03Z","published":"2022-11-23T09:57:17Z","title":"Whose Emotion Matters? Speaking Activity Localisation without Prior\n  Knowledge","summary":"  The task of emotion recognition in conversations (ERC) benefits from the\navailability of multiple modalities, as provided, for example, in the\nvideo-based Multimodal EmotionLines Dataset (MELD). However, only a few\nresearch approaches use both acoustic and visual information from the MELD\nvideos. There are two reasons for this: First, label-to-video alignments in\nMELD are noisy, making those videos an unreliable source of emotional speech\ndata. Second, conversations can involve several people in the same scene, which\nrequires the localisation of the utterance source. In this paper, we introduce\nMELD with Fixed Audiovisual Information via Realignment (MELD-FAIR) by using\nrecent active speaker detection and automatic speech recognition models, we are\nable to realign the videos of MELD and capture the facial expressions from\nspeakers in 96.92% of the utterances provided in MELD. Experiments with a\nself-supervised voice recognition model indicate that the realigned MELD-FAIR\nvideos more closely match the transcribed utterances given in the MELD dataset.\nFinally, we devise a model for emotion recognition in conversations trained on\nthe realigned MELD-FAIR videos, which outperforms state-of-the-art models for\nERC based on vision alone. This indicates that localising the source of\nspeaking activities is indeed effective for extracting facial expressions from\nthe uttering speakers and that faces provide more informative visual cues than\nthe visual features state-of-the-art models have been using so far. The\nMELD-FAIR realignment data, and the code of the realignment procedure and of\nthe emotional recognition, are available at\nhttps://github.com/knowledgetechnologyuhh/MELD-FAIR.\n","authors":["Hugo Carneiro","Cornelius Weber","Stefan Wermter"],"pdf_url":"https://arxiv.org/pdf/2211.15377v3.pdf","comment":"17 pages, 8 figures, 7 tables"},{"id":"http://arxiv.org/abs/2303.11746v1","updated":"2023-03-21T11:13:01Z","published":"2023-03-21T11:13:01Z","title":"Recommendation Systems in Libraries: an Application with Heterogeneous\n  Data Sources","summary":"  The Reading&Machine project exploits the support of digitalization to\nincrease the attractiveness of libraries and improve the users' experience. The\nproject implements an application that helps the users in their decision-making\nprocess, providing recommendation system (RecSys)-generated lists of books the\nusers might be interested in, and showing them through an interactive Virtual\nReality (VR)-based Graphical User Interface (GUI). In this paper, we focus on\nthe design and testing of the recommendation system, employing data about all\nusers' loans over the past 9 years from the network of libraries located in\nTurin, Italy. In addition, we use data collected by the Anobii online social\ncommunity of readers, who share their feedback and additional information about\nbooks they read. Armed with this heterogeneous data, we build and evaluate\nContent Based (CB) and Collaborative Filtering (CF) approaches. Our results\nshow that the CF outperforms the CB approach, improving by up to 47\\% the\nrelevant recommendations provided to a reader. However, the performance of the\nCB approach is heavily dependent on the number of books the reader has already\nread, and it can work even better than CF for users with a large history.\nFinally, our evaluations highlight that the performances of both approaches are\nsignificantly improved if the system integrates and leverages the information\nfrom the Anobii dataset, which allows us to include more user readings (for CF)\nand richer book metadata (for CB).\n","authors":["Alessandro Speciale","Greta Vallero","Luca Vassio","Marco Mellia"],"pdf_url":"https://arxiv.org/pdf/2303.11746v1.pdf","comment":"Accepted at 7th International workshop on Data Analytics solutions\n  for Real-LIfe APplications - 28th March-31st March, 2023, Ioannina, Greece.\n  The paper will be published in the Proceedings of EDBT/ICDT 2023 Joint\n  Conference"},{"id":"http://arxiv.org/abs/2303.11742v1","updated":"2023-03-21T11:09:31Z","published":"2023-03-21T11:09:31Z","title":"Beam Management Driven by Radio Environment Maps in O-RAN Architecture","summary":"  The Massive Multiple-Input Multiple-Output (M-MIMO) is considered as one of\nthe key technologies in 5G, and future 6G networks. From the perspective of,\ne.g., channel estimation, especially for high-speed users it is easier to\nimplement an M-MIMO network exploiting a static set of beams, i.e., Grid of\nBeams (GoB). While considering GoB it is important to properly assign users to\nthe beams, i.e., to perform Beam Management (BM). BM can be enhanced by taking\ninto account historical knowledge about the radio environment, e.g., to avoid\nradio link failures. The aim of this paper is to propose such a BM algorithm,\nthat utilizes location-dependent data stored in a Radio Environment Map (REM).\nIt utilizes received power maps, and user mobility patterns to optimize the BM\nprocess in terms of Reinforcement Learning (RL) by using the Policy Iteration\nmethod under different goal functions, e.g., maximization of received power or\nminimization of beam reselections while avoiding radio link failures. The\nproposed solution is compliant with the Open Radio Access Network (O-RAN)\narchitecture, enabling its practical implementation. Simulation studies have\nshown that the proposed BM algorithm can significantly reduce the number of\nbeam reselections or radio link failures compared to the baseline algorithm.\n","authors":["Marcin Hoffmann","Pawel Kryszkiewicz"],"pdf_url":"https://arxiv.org/pdf/2303.11742v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2302.14115v2","updated":"2023-03-21T11:01:09Z","published":"2023-02-27T19:53:49Z","title":"Vid2Seq: Large-Scale Pretraining of a Visual Language Model for Dense\n  Video Captioning","summary":"  In this work, we introduce Vid2Seq, a multi-modal single-stage dense event\ncaptioning model pretrained on narrated videos which are readily-available at\nscale. The Vid2Seq architecture augments a language model with special time\ntokens, allowing it to seamlessly predict event boundaries and textual\ndescriptions in the same output sequence. Such a unified model requires\nlarge-scale training data, which is not available in current annotated\ndatasets. We show that it is possible to leverage unlabeled narrated videos for\ndense video captioning, by reformulating sentence boundaries of transcribed\nspeech as pseudo event boundaries, and using the transcribed speech sentences\nas pseudo event captions. The resulting Vid2Seq model pretrained on the\nYT-Temporal-1B dataset improves the state of the art on a variety of dense\nvideo captioning benchmarks including YouCook2, ViTT and ActivityNet Captions.\nVid2Seq also generalizes well to the tasks of video paragraph captioning and\nvideo clip captioning, and to few-shot settings. Our code is publicly available\nat https://antoyang.github.io/vid2seq.html.\n","authors":["Antoine Yang","Arsha Nagrani","Paul Hongsuck Seo","Antoine Miech","Jordi Pont-Tuset","Ivan Laptev","Josef Sivic","Cordelia Schmid"],"pdf_url":"https://arxiv.org/pdf/2302.14115v2.pdf","comment":"CVPR 2023 Camera-Ready; Project Webpage:\n  https://antoyang.github.io/vid2seq.html ; 18 pages; 6 figures"},{"id":"http://arxiv.org/abs/2303.11735v1","updated":"2023-03-21T10:46:56Z","published":"2023-03-21T10:46:56Z","title":"Tensor networks for quantum machine learning","summary":"  Once developed for quantum theory, tensor networks have been established as a\nsuccessful machine learning paradigm. Now, they have been ported back to the\nquantum realm in the emerging field of quantum machine learning to assess\nproblems that classical computers are unable to solve efficiently. Their nature\nat the interface between physics and machine learning makes tensor networks\neasily deployable on quantum computers. In this review article, we shed light\non one of the major architectures considered to be predestined for variational\nquantum machine learning. In particular, we discuss how layouts like MPS, PEPS,\nTTNs and MERA can be mapped to a quantum computer, how they can be used for\nmachine learning and data encoding and which implementation techniques improve\ntheir performance.\n","authors":["Hans-Martin Rieser","Frank Köster","Arne Peter Raulf"],"pdf_url":"https://arxiv.org/pdf/2303.11735v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.11734v1","updated":"2023-03-21T10:46:34Z","published":"2023-03-21T10:46:34Z","title":"Unlocking Layer-wise Relevance Propagation for Autoencoders","summary":"  Autoencoders are a powerful and versatile tool often used for various\nproblems such as anomaly detection, image processing and machine translation.\nHowever, their reconstructions are not always trivial to explain. Therefore, we\npropose a fast explainability solution by extending the Layer-wise Relevance\nPropagation method with the help of Deep Taylor Decomposition framework.\nFurthermore, we introduce a novel validation technique for comparing our\nexplainability approach with baseline methods in the case of missing\nground-truth data. Our results highlight computational as well as qualitative\nadvantages of the proposed explainability solution with respect to existing\nmethods.\n","authors":["Kenyu Kobayashi","Renata Khasanova","Arno Schneuwly","Felix Schmidt","Matteo Casserini"],"pdf_url":"https://arxiv.org/pdf/2303.11734v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.11733v1","updated":"2023-03-21T10:43:41Z","published":"2023-03-21T10:43:41Z","title":"DIPPM: a Deep Learning Inference Performance Predictive Model using\n  Graph Neural Networks","summary":"  Deep Learning (DL) has developed to become a corner-stone in many everyday\napplications that we are now relying on. However, making sure that the DL model\nuses the underlying hardware efficiently takes a lot of effort. Knowledge about\ninference characteristics can help to find the right match so that enough\nresources are given to the model, but not too much. We have developed a DL\nInference Performance Predictive Model (DIPPM) that predicts the inference\nlatency, energy, and memory usage of a given input DL model on the NVIDIA A100\nGPU. We also devised an algorithm to suggest the appropriate A100\nMulti-Instance GPU profile from the output of DIPPM. We developed a methodology\nto convert DL models expressed in multiple frameworks to a generalized graph\nstructure that is used in DIPPM. It means DIPPM can parse input DL models from\nvarious frameworks. Our DIPPM can be used not only helps to find suitable\nhardware configurations but also helps to perform rapid design-space\nexploration for the inference performance of a model. We constructed a graph\nmulti-regression dataset consisting of 10,508 different DL models to train and\nevaluate the performance of DIPPM, and reached a resulting Mean Absolute\nPercentage Error (MAPE) as low as 1.9%.\n","authors":["Karthick Panner Selvam","Mats Brorsson"],"pdf_url":"https://arxiv.org/pdf/2303.11733v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2204.07075v3","updated":"2023-03-21T10:41:12Z","published":"2022-04-14T16:13:06Z","title":"Learning and controlling the source-filter representation of speech with\n  a variational autoencoder","summary":"  Understanding and controlling latent representations in deep generative\nmodels is a challenging yet important problem for analyzing, transforming and\ngenerating various types of data. In speech processing, inspiring from the\nanatomical mechanisms of phonation, the source-filter model considers that\nspeech signals are produced from a few independent and physically meaningful\ncontinuous latent factors, among which the fundamental frequency $f_0$ and the\nformants are of primary importance. In this work, we start from a variational\nautoencoder (VAE) trained in an unsupervised manner on a large dataset of\nunlabeled natural speech signals, and we show that the source-filter model of\nspeech production naturally arises as orthogonal subspaces of the VAE latent\nspace. Using only a few seconds of labeled speech signals generated with an\nartificial speech synthesizer, we propose a method to identify the latent\nsubspaces encoding $f_0$ and the first three formant frequencies, we show that\nthese subspaces are orthogonal, and based on this orthogonality, we develop a\nmethod to accurately and independently control the source-filter speech factors\nwithin the latent subspaces. Without requiring additional information such as\ntext or human-labeled data, this results in a deep generative model of speech\nspectrograms that is conditioned on $f_0$ and the formant frequencies, and\nwhich is applied to the transformation speech signals. Finally, we also propose\na robust $f_0$ estimation method that exploits the projection of a speech\nsignal onto the learned latent subspace associated with $f_0$.\n","authors":["Samir Sadok","Simon Leglaive","Laurent Girin","Xavier Alameda-Pineda","Renaud Séguier"],"pdf_url":"https://arxiv.org/pdf/2204.07075v3.pdf","comment":"23 pages, 7 figures, companion website:\n  https://samsad35.github.io/site-sfvae/"},{"id":"http://arxiv.org/abs/2111.12273v5","updated":"2023-03-21T10:31:46Z","published":"2021-11-24T05:16:41Z","title":"Sharpness-aware Quantization for Deep Neural Networks","summary":"  Network quantization is a dominant paradigm of model compression. However,\nthe abrupt changes in quantized weights during training often lead to severe\nloss fluctuations and result in a sharp loss landscape, making the gradients\nunstable and thus degrading the performance. Recently, Sharpness-Aware\nMinimization (SAM) has been proposed to smooth the loss landscape and improve\nthe generalization performance of the models. Nevertheless, directly applying\nSAM to the quantized models can lead to perturbation mismatch or diminishment\nissues, resulting in suboptimal performance. In this paper, we propose a novel\nmethod, dubbed Sharpness-Aware Quantization (SAQ), to explore the effect of SAM\nin model compression, particularly quantization for the first time.\nSpecifically, we first provide a unified view of quantization and SAM by\ntreating them as introducing quantization noises and adversarial perturbations\nto the model weights, respectively. According to whether the noise and\nperturbation terms depend on each other, SAQ can be formulated into three\ncases, which are analyzed and compared comprehensively. Furthermore, by\nintroducing an efficient training strategy, SAQ only incurs a little additional\ntraining overhead compared with the default optimizer (e.g., SGD or AdamW).\nExtensive experiments on both convolutional neural networks and Transformers\nacross various datasets (i.e., ImageNet, CIFAR-10/100, Oxford Flowers-102,\nOxford-IIIT Pets) show that SAQ improves the generalization performance of the\nquantized models, yielding the SOTA results in uniform quantization. For\nexample, on ImageNet, SAQ outperforms AdamW by 1.2% on the Top-1 accuracy for\n4-bit ViT-B/16. Our 4-bit ResNet-50 surpasses the previous SOTA method by 0.9%\non the Top-1 accuracy.\n","authors":["Jing Liu","Jianfei Cai","Bohan Zhuang"],"pdf_url":"https://arxiv.org/pdf/2111.12273v5.pdf","comment":"Tech report"},{"id":"http://arxiv.org/abs/2303.11724v1","updated":"2023-03-21T10:29:30Z","published":"2023-03-21T10:29:30Z","title":"Task-based Generation of Optimized Projection Sets using Differentiable\n  Ranking","summary":"  We present a method for selecting valuable projections in computed tomography\n(CT) scans to enhance image reconstruction and diagnosis. The approach\nintegrates two important factors, projection-based detectability and data\ncompleteness, into a single feed-forward neural network. The network evaluates\nthe value of projections, processes them through a differentiable ranking\nfunction and makes the final selection using a straight-through estimator. Data\ncompleteness is ensured through the label provided during training. The\napproach eliminates the need for heuristically enforcing data completeness,\nwhich may exclude valuable projections. The method is evaluated on simulated\ndata in a non-destructive testing scenario, where the aim is to maximize the\nreconstruction quality within a specified region of interest. We achieve\ncomparable results to previous methods, laying the foundation for using\nreconstruction-based loss functions to learn the selection of projections.\n","authors":["Linda-Sophie Schneider","Mareike Thies","Christopher Syben","Richard Schielein","Mathias Unberath","Andreas Maier"],"pdf_url":"https://arxiv.org/pdf/2303.11724v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.11720v1","updated":"2023-03-21T10:14:11Z","published":"2023-03-21T10:14:11Z","title":"Lidar Line Selection with Spatially-Aware Shapley Value for\n  Cost-Efficient Depth Completion","summary":"  Lidar is a vital sensor for estimating the depth of a scene. Typical spinning\nlidars emit pulses arranged in several horizontal lines and the monetary cost\nof the sensor increases with the number of these lines. In this work, we\npresent the new problem of optimizing the positioning of lidar lines to find\nthe most effective configuration for the depth completion task. We propose a\nsolution to reduce the number of lines while retaining the up-to-the-mark\nquality of depth completion. Our method consists of two components, (1) line\nselection based on the marginal contribution of a line computed via the Shapley\nvalue and (2) incorporating line position spread to take into account its need\nto arrive at image-wide depth completion. Spatially-aware Shapley values (SaS)\nsucceed in selecting line subsets that yield a depth accuracy comparable to the\nfull lidar input while using just half of the lines.\n","authors":["Kamil Adamczewski","Christos Sakaridis","Vaishakh Patil","Luc Van Gool"],"pdf_url":"https://arxiv.org/pdf/2303.11720v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.11717v1","updated":"2023-03-21T10:09:47Z","published":"2023-03-21T10:09:47Z","title":"A Complete Survey on Generative AI (AIGC): Is ChatGPT from GPT-4 to\n  GPT-5 All You Need?","summary":"  As ChatGPT goes viral, generative AI (AIGC, a.k.a AI-generated content) has\nmade headlines everywhere because of its ability to analyze and create text,\nimages, and beyond. With such overwhelming media coverage, it is almost\nimpossible for us to miss the opportunity to glimpse AIGC from a certain angle.\nIn the era of AI transitioning from pure analysis to creation, it is worth\nnoting that ChatGPT, with its most recent language model GPT-4, is just a tool\nout of numerous AIGC tasks. Impressed by the capability of the ChatGPT, many\npeople are wondering about its limits: can GPT-5 (or other future GPT variants)\nhelp ChatGPT unify all AIGC tasks for diversified content creation? Toward\nanswering this question, a comprehensive review of existing AIGC tasks is\nneeded. As such, our work comes to fill this gap promptly by offering a first\nlook at AIGC, ranging from its techniques to applications. Modern generative AI\nrelies on various technical foundations, ranging from model architecture and\nself-supervised pretraining to generative modeling methods (like GAN and\ndiffusion models). After introducing the fundamental techniques, this work\nfocuses on the technological development of various AIGC tasks based on their\noutput type, including text, images, videos, 3D content, etc., which depicts\nthe full potential of ChatGPT's future. Moreover, we summarize their\nsignificant applications in some mainstream industries, such as education and\ncreativity content. Finally, we discuss the challenges currently faced and\npresent an outlook on how generative AI might evolve in the near future.\n","authors":["Chaoning Zhang","Chenshuang Zhang","Sheng Zheng","Yu Qiao","Chenghao Li","Mengchun Zhang","Sumit Kumar Dam","Chu Myaet Thwal","Ye Lin Tun","Le Luang Huy","Donguk kim","Sung-Ho Bae","Lik-Hang Lee","Yang Yang","Heng Tao Shen","In So Kweon","Choong Seon Hong"],"pdf_url":"https://arxiv.org/pdf/2303.11717v1.pdf","comment":"56 pages, 548 citations"},{"id":"http://arxiv.org/abs/2303.11716v1","updated":"2023-03-21T10:08:42Z","published":"2023-03-21T10:08:42Z","title":"Style Miner: Find Significant and Stable Explanatory Factors in Time\n  Series with Constrained Reinforcement Learning","summary":"  In high-dimensional time-series analysis, it is essential to have a set of\nkey factors (namely, the style factors) that explain the change of the observed\nvariable. For example, volatility modeling in finance relies on a set of risk\nfactors, and climate change studies in climatology rely on a set of causal\nfactors. The ideal low-dimensional style factors should balance significance\n(with high explanatory power) and stability (consistent, no significant\nfluctuations). However, previous supervised and unsupervised feature extraction\nmethods can hardly address the tradeoff. In this paper, we propose Style Miner,\na reinforcement learning method to generate style factors. We first formulate\nthe problem as a Constrained Markov Decision Process with explanatory power as\nthe return and stability as the constraint. Then, we design fine-grained\nimmediate rewards and costs and use a Lagrangian heuristic to balance them\nadaptively. Experiments on real-world financial data sets show that Style Miner\noutperforms existing learning-based methods by a large margin and achieves a\nrelatively 10% gain in R-squared explanatory power compared to the\nindustry-renowned factors proposed by human experts.\n","authors":["Dapeng Li","Feiyang Pan","Jia He","Zhiwei Xu","Dandan Tu","Guoliang Fan"],"pdf_url":"https://arxiv.org/pdf/2303.11716v1.pdf","comment":"9 pages, 6 figures"},{"id":"http://arxiv.org/abs/2203.03186v2","updated":"2023-03-21T10:04:25Z","published":"2022-03-07T07:44:05Z","title":"Bandits Corrupted by Nature: Lower Bounds on Regret and Robust\n  Optimistic Algorithm","summary":"  We study the corrupted bandit problem, i.e. a stochastic multi-armed bandit\nproblem with $k$ unknown reward distributions, which are heavy-tailed and\ncorrupted by a history-independent adversary or Nature. To be specific, the\nreward obtained by playing an arm comes from corresponding heavy-tailed reward\ndistribution with probability $1-\\varepsilon \\in (0.5,1]$ and an arbitrary\ncorruption distribution of unbounded support with probability $\\varepsilon \\in\n[0,0.5)$.\n  First, we provide $\\textit{a problem-dependent lower bound on the regret}$ of\nany corrupted bandit algorithm. The lower bounds indicate that the corrupted\nbandit problem is harder than the classical stochastic bandit problem with\nsub-Gaussian or heavy-tail rewards.\n  Following that, we propose a novel UCB-type algorithm for corrupted bandits,\nnamely HubUCB, that builds on Huber's estimator for robust mean estimation.\nLeveraging a novel concentration inequality of Huber's estimator, we prove that\nHubUCB achieves a near-optimal regret upper bound.\n  Since computing Huber's estimator has quadratic complexity, we further\nintroduce a sequential version of Huber's estimator that exhibits linear\ncomplexity. We leverage this sequential estimator to design SeqHubUCB that\nenjoys similar regret guarantees while reducing the computational burden.\n  Finally, we experimentally illustrate the efficiency of HubUCB and SeqHubUCB\nin solving corrupted bandits for different reward distributions and different\nlevels of corruptions.\n","authors":["Debabrota Basu","Odalric-Ambrym Maillard","Timothée Mathieu"],"pdf_url":"https://arxiv.org/pdf/2203.03186v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.11705v1","updated":"2023-03-21T09:51:19Z","published":"2023-03-21T09:51:19Z","title":"A Single-Step Multiclass SVM based on Quantum Annealing for Remote\n  Sensing Data Classification","summary":"  In recent years, the development of quantum annealers has enabled\nexperimental demonstrations and has increased research interest in applications\nof quantum annealing, such as in quantum machine learning and in particular for\nthe popular quantum SVM. Several versions of the quantum SVM have been\nproposed, and quantum annealing has been shown to be effective in them.\nExtensions to multiclass problems have also been made, which consist of an\nensemble of multiple binary classifiers. This work proposes a novel quantum SVM\nformulation for direct multiclass classification based on quantum annealing,\ncalled Quantum Multiclass SVM (QMSVM). The multiclass classification problem is\nformulated as a single Quadratic Unconstrained Binary Optimization (QUBO)\nproblem solved with quantum annealing. The main objective of this work is to\nevaluate the feasibility, accuracy, and time performance of this approach.\nExperiments have been performed on the D-Wave Advantage quantum annealer for a\nclassification problem on remote sensing data. The results indicate that,\ndespite the memory demands of the quantum annealer, QMSVM can achieve accuracy\nthat is comparable to standard SVM methods and, more importantly, it scales\nmuch more efficiently with the number of training examples, resulting in nearly\nconstant time. This work shows an approach for bringing together classical and\nquantum computation, solving practical problems in remote sensing with current\nhardware.\n","authors":["Amer Delilbasic","Bertrand Le Saux","Morris Riedel","Kristel Michielsen","Gabriele Cavallaro"],"pdf_url":"https://arxiv.org/pdf/2303.11705v1.pdf","comment":"12 pages, 10 figures, 3 tables. Submitted to IEEE JSTARS"},{"id":"http://arxiv.org/abs/2303.11702v1","updated":"2023-03-21T09:42:27Z","published":"2023-03-21T09:42:27Z","title":"Linking generative semi-supervised learning and generative open-set\n  recognition","summary":"  This study investigates the relationship between semi-supervised learning\n(SSL) and open-set recognition (OSR) in the context of generative adversarial\nnetworks (GANs). Although no previous study has formally linked SSL and OSR,\ntheir respective methods share striking similarities. Specifically, SSL-GANs\nand OSR-GANs require generator to produce samples in the complementary space.\nSubsequently, by regularising networks with generated samples, both SSL and OSR\nclassifiers generalize the open space. To demonstrate the connection between\nSSL and OSR, we theoretically and experimentally compare state-of-the-art\nSSL-GAN methods with state-of-the-art OSR-GAN methods. Our results indicate\nthat the SSL optimised margin-GANs, which have a stronger foundation in\nliterature, set the new standard for the combined SSL-OSR task and achieves new\nstate-of-other art results in certain general OSR experiments. However, the OSR\noptimised adversarial reciprocal point (ARP)-GANs still slightly out-performed\nmargin-GANs at other OSR experiments. This result indicates unique insights for\nthe combined optimisation task of SSL-OSR.\n","authors":["Emile Reyn Engelbrecht","Johan du Preez"],"pdf_url":"https://arxiv.org/pdf/2303.11702v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.11701v1","updated":"2023-03-21T09:41:13Z","published":"2023-03-21T09:41:13Z","title":"A High-Frequency Focused Network for Lightweight Single Image\n  Super-Resolution","summary":"  Lightweight neural networks for single-image super-resolution (SISR) tasks\nhave made substantial breakthroughs in recent years. Compared to low-frequency\ninformation, high-frequency detail is much more difficult to reconstruct. Most\nSISR models allocate equal computational resources for low-frequency and\nhigh-frequency information, which leads to redundant processing of simple\nlow-frequency information and inadequate recovery of more challenging\nhigh-frequency information. We propose a novel High-Frequency Focused Network\n(HFFN) through High-Frequency Focused Blocks (HFFBs) that selectively enhance\nhigh-frequency information while minimizing redundant feature computation of\nlow-frequency information. The HFFB effectively allocates more computational\nresources to the more challenging reconstruction of high-frequency information.\nMoreover, we propose a Local Feature Fusion Block (LFFB) effectively fuses\nfeatures from multiple HFFBs in a local region, utilizing complementary\ninformation across layers to enhance feature representativeness and reduce\nartifacts in reconstructed images. We assess the efficacy of our proposed HFFN\non five benchmark datasets and show that it significantly enhances the\nsuper-resolution performance of the network. Our experimental results\ndemonstrate state-of-the-art performance in reconstructing high-frequency\ninformation while using a low number of parameters.\n","authors":["Xiaotian Weng","Yi Chen","Zhichao Zheng","Yanhui Gu","Junsheng Zhou","Yudong Zhang"],"pdf_url":"https://arxiv.org/pdf/2303.11701v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.11699v1","updated":"2023-03-21T09:37:29Z","published":"2023-03-21T09:37:29Z","title":"Neural networks trained on synthetically generated crystals can extract\n  structural information from ICSD powder X-ray diffractograms","summary":"  Machine learning techniques have successfully been used to extract structural\ninformation such as the crystal space group from powder X-ray diffractograms.\nHowever, training directly on simulated diffractograms from databases such as\nthe ICSD is challenging due to its limited size, class-inhomogeneity, and bias\ntoward certain structure types. We propose an alternative approach of\ngenerating synthetic crystals with random coordinates by using the symmetry\noperations of each space group. Based on this approach, we demonstrate online\ntraining of deep ResNet-like models on up to a few million unique on-the-fly\ngenerated synthetic diffractograms per hour. For our chosen task of space group\nclassification, we achieved a test accuracy of 79.9% on unseen ICSD structure\ntypes from most space groups. This surpasses the 56.1% accuracy of the current\nstate-of-the-art approach of training on ICSD crystals directly. Our results\ndemonstrate that synthetically generated crystals can be used to extract\nstructural information from ICSD powder diffractograms, which makes it possible\nto apply very large state-of-the-art machine learning models in the area of\npowder X-ray diffraction. We further show first steps toward applying our\nmethodology to experimental data, where automated XRD data analysis is crucial,\nespecially in high-throughput settings. While we focused on the prediction of\nthe space group, our approach has the potential to be extended to related tasks\nin the future.\n","authors":["Henrik Schopmans","Patrick Reiser","Pascal Friederich"],"pdf_url":"https://arxiv.org/pdf/2303.11699v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.11698v1","updated":"2023-03-21T09:36:58Z","published":"2023-03-21T09:36:58Z","title":"Data Augmentation For Label Enhancement","summary":"  Label distribution (LD) uses the description degree to describe instances,\nwhich provides more fine-grained supervision information when learning with\nlabel ambiguity. Nevertheless, LD is unavailable in many real-world\napplications. To obtain LD, label enhancement (LE) has emerged to recover LD\nfrom logical label. Existing LE approach have the following problems:\n(\\textbf{i}) They use logical label to train mappings to LD, but the\nsupervision information is too loose, which can lead to inaccurate model\nprediction; (\\textbf{ii}) They ignore feature redundancy and use the collected\nfeatures directly. To solve (\\textbf{i}), we use the topology of the feature\nspace to generate more accurate label-confidence. To solve (\\textbf{ii}), we\nproposed a novel supervised LE dimensionality reduction approach, which\nprojects the original data into a lower dimensional feature space. Combining\nthe above two, we obtain the augmented data for LE. Further, we proposed a\nnovel nonlinear LE model based on the label-confidence and reduced features.\nExtensive experiments on 12 real-world datasets are conducted and the results\nshow that our method consistently outperforms the other five comparing\napproaches.\n","authors":["Zhiqiang Kou","Yuheng Jia","Jing Wang","Boyu Shi","Xin Geng"],"pdf_url":"https://arxiv.org/pdf/2303.11698v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.11695v1","updated":"2023-03-21T09:32:31Z","published":"2023-03-21T09:32:31Z","title":"Transcriptomics-based matching of drugs to diseases with deep learning","summary":"  In this work we present a deep learning approach to conduct hypothesis-free,\ntranscriptomics-based matching of drugs for diseases. Our proposed neural\nnetwork architecture is trained on approved drug-disease indications, taking as\ninput the relevant disease and drug differential gene expression profiles, and\nlearns to identify novel indications. We assemble an evaluation dataset of\ndisease-drug indications spanning 68 diseases and evaluate in silico our\napproach against the most widely used transcriptomics-based matching baselines,\nCMap and the Characteristic Direction. Our results show a more than 200%\nimprovement over both baselines in terms of standard retrieval metrics. We\nfurther showcase our model's ability to capture different genes' expressions\ninteractions among drugs and diseases. We provide our trained models, data and\ncode to predict with them at https://github.com/healx/dgem-nn-public.\n","authors":["Yannis Papanikolaou","Francesco Tuveri","Misa Ogura","Daniel O'Donovan"],"pdf_url":"https://arxiv.org/pdf/2303.11695v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2212.07356v2","updated":"2023-03-21T09:26:58Z","published":"2022-12-14T17:33:01Z","title":"Scheduling and Aggregation Design for Asynchronous Federated Learning\n  over Wireless Networks","summary":"  Federated Learning (FL) is a collaborative machine learning (ML) framework\nthat combines on-device training and server-based aggregation to train a common\nML model among distributed agents. In this work, we propose an asynchronous FL\ndesign with periodic aggregation to tackle the straggler issue in FL systems.\nConsidering limited wireless communication resources, we investigate the effect\nof different scheduling policies and aggregation designs on the convergence\nperformance. Driven by the importance of reducing the bias and variance of the\naggregated model updates, we propose a scheduling policy that jointly considers\nthe channel quality and training data representation of user devices. The\neffectiveness of our channel-aware data-importance-based scheduling policy,\ncompared with state-of-the-art methods proposed for synchronous FL, is\nvalidated through simulations. Moreover, we show that an ``age-aware''\naggregation weighting design can significantly improve the learning performance\nin an asynchronous FL setting.\n","authors":["Chung-Hsuan Hu","Zheng Chen","Erik G. Larsson"],"pdf_url":"https://arxiv.org/pdf/2212.07356v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.11674v1","updated":"2023-03-21T08:36:34Z","published":"2023-03-21T08:36:34Z","title":"ALOFT: A Lightweight MLP-like Architecture with Dynamic Low-frequency\n  Transform for Domain Generalization","summary":"  Domain generalization (DG) aims to learn a model that generalizes well to\nunseen target domains utilizing multiple source domains without re-training.\nMost existing DG works are based on convolutional neural networks (CNNs).\nHowever, the local operation of the convolution kernel makes the model focus\ntoo much on local representations (e.g., texture), which inherently causes the\nmodel more prone to overfit to the source domains and hampers its\ngeneralization ability. Recently, several MLP-based methods have achieved\npromising results in supervised learning tasks by learning global interactions\namong different patches of the image. Inspired by this, in this paper, we first\nanalyze the difference between CNN and MLP methods in DG and find that MLP\nmethods exhibit a better generalization ability because they can better capture\nthe global representations (e.g., structure) than CNN methods. Then, based on a\nrecent lightweight MLP method, we obtain a strong baseline that outperforms\nmost state-of-the-art CNN-based methods. The baseline can learn global\nstructure representations with a filter to suppress structure irrelevant\ninformation in the frequency space. Moreover, we propose a dynAmic\nLOw-Frequency spectrum Transform (ALOFT) that can perturb local texture\nfeatures while preserving global structure features, thus enabling the filter\nto remove structure-irrelevant information sufficiently. Extensive experiments\non four benchmarks have demonstrated that our method can achieve great\nperformance improvement with a small number of parameters compared to SOTA\nCNN-based DG methods. Our code is available at\nhttps://github.com/lingeringlight/ALOFT/.\n","authors":["Jintao Guo","Na Wang","Lei Qi","Yinghuan Shi"],"pdf_url":"https://arxiv.org/pdf/2303.11674v1.pdf","comment":"Accepted by CVPR2023. The code is available at\n  https://github.com/lingeringlight/ALOFT/"},{"id":"http://arxiv.org/abs/2303.11673v1","updated":"2023-03-21T08:34:23Z","published":"2023-03-21T08:34:23Z","title":"A Survey on Class Imbalance in Federated Learning","summary":"  Federated learning, which allows multiple client devices in a network to\njointly train a machine learning model without direct exposure of clients'\ndata, is an emerging distributed learning technique due to its nature of\nprivacy preservation. However, it has been found that models trained with\nfederated learning usually have worse performance than their counterparts\ntrained in the standard centralized learning mode, especially when the training\ndata is imbalanced. In the context of federated learning, data imbalance may\noccur either locally one one client device, or globally across many devices.\nThe complexity of different types of data imbalance has posed challenges to the\ndevelopment of federated learning technique, especially considering the need of\nrelieving data imbalance issue and preserving data privacy at the same time.\nTherefore, in the literature, many attempts have been made to handle class\nimbalance in federated learning. In this paper, we present a detailed review of\nrecent advancements along this line. We first introduce various types of class\nimbalance in federated learning, after which we review existing methods for\nestimating the extent of class imbalance without the need of knowing the actual\ndata to preserve data privacy. After that, we discuss existing methods for\nhandling class imbalance in FL, where the advantages and disadvantages of the\nthese approaches are discussed. We also summarize common evaluation metrics for\nclass imbalanced tasks, and point out potential future directions.\n","authors":["Jing Zhang","Chuanwen Li","Jianzgong Qi","Jiayuan He"],"pdf_url":"https://arxiv.org/pdf/2303.11673v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2210.06640v2","updated":"2023-03-21T08:24:01Z","published":"2022-10-13T00:40:04Z","title":"Compute-Efficient Deep Learning: Algorithmic Trends and Opportunities","summary":"  Although deep learning has made great progress in recent years, the exploding\neconomic and environmental costs of training neural networks are becoming\nunsustainable. To address this problem, there has been a great deal of research\non *algorithmically-efficient deep learning*, which seeks to reduce training\ncosts not at the hardware or implementation level, but through changes in the\nsemantics of the training program. In this paper, we present a structured and\ncomprehensive overview of the research in this field. First, we formalize the\n*algorithmic speedup* problem, then we use fundamental building blocks of\nalgorithmically efficient training to develop a taxonomy. Our taxonomy\nhighlights commonalities of seemingly disparate methods and reveals current\nresearch gaps. Next, we present evaluation best practices to enable\ncomprehensive, fair, and reliable comparisons of speedup techniques. To further\naid research and applications, we discuss common bottlenecks in the training\npipeline (illustrated via experiments) and offer taxonomic mitigation\nstrategies for them. Finally, we highlight some unsolved research challenges\nand present promising future directions.\n","authors":["Brian R. Bartoldson","Bhavya Kailkhura","Davis Blalock"],"pdf_url":"https://arxiv.org/pdf/2210.06640v2.pdf","comment":"77 pages"},{"id":"http://arxiv.org/abs/2303.11669v1","updated":"2023-03-21T08:23:37Z","published":"2023-03-21T08:23:37Z","title":"Universal Smoothed Score Functions for Generative Modeling","summary":"  We consider the problem of generative modeling based on smoothing an unknown\ndensity of interest in $\\mathbb{R}^d$ using factorial kernels with $M$\nindependent Gaussian channels with equal noise levels introduced by Saremi and\nSrivastava (2022). First, we fully characterize the time complexity of learning\nthe resulting smoothed density in $\\mathbb{R}^{Md}$, called M-density, by\nderiving a universal form for its parametrization in which the score function\nis by construction permutation equivariant. Next, we study the time complexity\nof sampling an M-density by analyzing its condition number for Gaussian\ndistributions. This spectral analysis gives a geometric insight on the \"shape\"\nof M-densities as one increases $M$. Finally, we present results on the sample\nquality in this class of generative models on the CIFAR-10 dataset where we\nreport Fr\\'echet inception distances (14.15), notably obtained with a single\nnoise level on long-run fast-mixing MCMC chains.\n","authors":["Saeed Saremi","Rupesh Kumar Srivastava","Francis Bach"],"pdf_url":"https://arxiv.org/pdf/2303.11669v1.pdf","comment":"Technical Report"},{"id":"http://arxiv.org/abs/2303.11650v1","updated":"2023-03-21T07:51:52Z","published":"2023-03-21T07:51:52Z","title":"Uniform Risk Bounds for Learning with Dependent Data Sequences","summary":"  This paper extends standard results from learning theory with independent\ndata to sequences of dependent data. Contrary to most of the literature, we do\nnot rely on mixing arguments or sequential measures of complexity and derive\nuniform risk bounds with classical proof patterns and capacity measures. In\nparticular, we show that the standard classification risk bounds based on the\nVC-dimension hold in the exact same form for dependent data, and further\nprovide Rademacher complexity-based bounds, that remain unchanged compared to\nthe standard results for the identically and independently distributed case.\nFinally, we show how to apply these results in the context of scenario-based\noptimization in order to compute the sample complexity of random programs with\ndependent constraints.\n","authors":["Fabien Lauer"],"pdf_url":"https://arxiv.org/pdf/2303.11650v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.11648v1","updated":"2023-03-21T07:46:57Z","published":"2023-03-21T07:46:57Z","title":"Improving Content Retrievability in Search with Controllable Query\n  Generation","summary":"  An important goal of online platforms is to enable content discovery, i.e.\nallow users to find a catalog entity they were not familiar with. A\npre-requisite to discover an entity, e.g. a book, with a search engine is that\nthe entity is retrievable, i.e. there are queries for which the system will\nsurface such entity in the top results. However, machine-learned search engines\nhave a high retrievability bias, where the majority of the queries return the\nsame entities. This happens partly due to the predominance of narrow intent\nqueries, where users create queries using the title of an already known entity,\ne.g. in book search 'harry potter'. The amount of broad queries where users\nwant to discover new entities, e.g. in music search 'chill lyrical electronica\nwith an atmospheric feeling to it', and have a higher tolerance to what they\nmight find, is small in comparison. We focus here on two factors that have a\nnegative impact on the retrievability of the entities (I) the training data\nused for dense retrieval models and (II) the distribution of narrow and broad\nintent queries issued in the system. We propose CtrlQGen, a method that\ngenerates queries for a chosen underlying intent-narrow or broad. We can use\nCtrlQGen to improve factor (I) by generating training data for dense retrieval\nmodels comprised of diverse synthetic queries. CtrlQGen can also be used to\ndeal with factor (II) by suggesting queries with broader intents to users. Our\nresults on datasets from the domains of music, podcasts, and books reveal that\nwe can significantly decrease the retrievability bias of a dense retrieval\nmodel when using CtrlQGen. First, by using the generated queries as training\ndata for dense models we make 9% of the entities retrievable (go from zero to\nnon-zero retrievability). Second, by suggesting broader queries to users, we\ncan make 12% of the entities retrievable in the best case.\n","authors":["Gustavo Penha","Enrico Palumbo","Maryam Aziz","Alice Wang","Hugues Bouchard"],"pdf_url":"https://arxiv.org/pdf/2303.11648v1.pdf","comment":"Accepted for publication in the International World Wide Web\n  Conference 2023"},{"id":"http://arxiv.org/abs/2303.11647v1","updated":"2023-03-21T07:46:28Z","published":"2023-03-21T07:46:28Z","title":"Are uGLAD? Time will tell!","summary":"  We frequently encounter multiple series that are temporally correlated in our\nsurroundings, such as EEG data to examine alterations in brain activity or\nsensors to monitor body movements. Segmentation of multivariate time series\ndata is a technique for identifying meaningful patterns or changes in the time\nseries that can signal a shift in the system's behavior. However, most\nsegmentation algorithms have been designed primarily for univariate time\nseries, and their performance on multivariate data remains largely\nunsatisfactory, making this a challenging problem. In this work, we introduce a\nnovel approach for multivariate time series segmentation using conditional\nindependence (CI) graphs. CI graphs are probabilistic graphical models that\nrepresents the partial correlations between the nodes. We propose a domain\nagnostic multivariate segmentation framework `$\\texttt{tGLAD}$' which draws a\nparallel between the CI graph nodes and the variables of the time series.\nConsider applying a graph recovery model $\\texttt{uGLAD}$ to a short interval\nof the time series, it will result in a CI graph that shows partial\ncorrelations among the variables. We extend this idea to the entire time series\nby utilizing a sliding window to create a batch of time intervals and then run\na single $\\texttt{uGLAD}$ model in multitask learning mode to recover all the\nCI graphs simultaneously. As a result, we obtain a corresponding temporal CI\ngraphs representation. We then designed a first-order and second-order based\ntrajectory tracking algorithms to study the evolution of these graphs across\ndistinct intervals. Finally, an `Allocation' algorithm is used to determine a\nsuitable segmentation of the temporal graph sequence. $\\texttt{tGLAD}$ provides\na competitive time complexity of $O(N)$ for settings where number of variables\n$D<<N$. We demonstrate successful empirical results on a Physical Activity\nMonitoring data.\n","authors":["Shima Imani","Harsh Shrivastava"],"pdf_url":"https://arxiv.org/pdf/2303.11647v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2302.12465v2","updated":"2023-03-21T07:46:02Z","published":"2023-02-24T05:43:47Z","title":"PaGE-Link: Path-based Graph Neural Network Explanation for Heterogeneous\n  Link Prediction","summary":"  Transparency and accountability have become major concerns for black-box\nmachine learning (ML) models. Proper explanations for the model behavior\nincrease model transparency and help researchers develop more accountable\nmodels. Graph neural networks (GNN) have recently shown superior performance in\nmany graph ML problems than traditional methods, and explaining them has\nattracted increased interest. However, GNN explanation for link prediction (LP)\nis lacking in the literature. LP is an essential GNN task and corresponds to\nweb applications like recommendation and sponsored search on web. Given\nexisting GNN explanation methods only address node/graph-level tasks, we\npropose Path-based GNN Explanation for heterogeneous Link prediction\n(PaGE-Link) that generates explanations with connection interpretability,\nenjoys model scalability, and handles graph heterogeneity. Qualitatively,\nPaGE-Link can generate explanations as paths connecting a node pair, which\nnaturally captures connections between the two nodes and easily transfer to\nhuman-interpretable explanations. Quantitatively, explanations generated by\nPaGE-Link improve AUC for recommendation on citation and user-item graphs by 9\n- 35% and are chosen as better by 78.79% of responses in human evaluation.\n","authors":["Shichang Zhang","Jiani Zhang","Xiang Song","Soji Adeshina","Da Zheng","Christos Faloutsos","Yizhou Sun"],"pdf_url":"https://arxiv.org/pdf/2302.12465v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2201.01652v3","updated":"2023-03-21T07:35:09Z","published":"2022-01-05T15:17:35Z","title":"Stochastic regularized majorization-minimization with weakly convex and\n  multi-convex surrogates","summary":"  Stochastic majorization-minimization (SMM) is a class of stochastic\noptimization algorithms that proceed by sampling new data points and minimizing\na recursive average of surrogate functions of an objective function. The\nsurrogates are required to be strongly convex and convergence rate analysis for\nthe general non-convex setting was not available. In this paper, we propose an\nextension of SMM where surrogates are allowed to be only weakly convex or block\nmulti-convex, and the averaged surrogates are approximately minimized with\nproximal regularization or block-minimized within diminishing radii,\nrespectively. For the general nonconvex constrained setting with non-i.i.d.\ndata samples, we show that the first-order optimality gap of the proposed\nalgorithm decays at the rate $O((\\log n)^{1+\\epsilon}/n^{1/2})$ for the\nempirical loss and $O((\\log n)^{1+\\epsilon}/n^{1/4})$ for the expected loss,\nwhere $n$ denotes the number of data samples processed. Under some additional\nassumption, the latter convergence rate can be improved to $O((\\log\nn)^{1+\\epsilon}/n^{1/2})$. As a corollary, we obtain the first convergence rate\nbounds for various optimization methods under general nonconvex dependent data\nsetting: Double-averaging projected gradient descent and its generalizations,\nproximal point empirical risk minimization, and online matrix/tensor\ndecomposition algorithms. We also provide experimental validation of our\nresults.\n","authors":["Hanbaek Lyu"],"pdf_url":"https://arxiv.org/pdf/2201.01652v3.pdf","comment":"64 pages, 5 figures, 1 table"},{"id":"http://arxiv.org/abs/2303.11643v1","updated":"2023-03-21T07:32:32Z","published":"2023-03-21T07:32:32Z","title":"Manipulating Transfer Learning for Property Inference","summary":"  Transfer learning is a popular method for tuning pretrained (upstream) models\nfor different downstream tasks using limited data and computational resources.\nWe study how an adversary with control over an upstream model used in transfer\nlearning can conduct property inference attacks on a victim's tuned downstream\nmodel. For example, to infer the presence of images of a specific individual in\nthe downstream training set. We demonstrate attacks in which an adversary can\nmanipulate the upstream model to conduct highly effective and specific property\ninference attacks (AUC score $> 0.9$), without incurring significant\nperformance loss on the main task. The main idea of the manipulation is to make\nthe upstream model generate activations (intermediate features) with different\ndistributions for samples with and without a target property, thus enabling the\nadversary to distinguish easily between downstream models trained with and\nwithout training examples that have the target property. Our code is available\nat https://github.com/yulongt23/Transfer-Inference.\n","authors":["Yulong Tian","Fnu Suya","Anshuman Suri","Fengyuan Xu","David Evans"],"pdf_url":"https://arxiv.org/pdf/2303.11643v1.pdf","comment":"Accepted to CVPR 2023"},{"id":"http://arxiv.org/abs/2201.04207v5","updated":"2023-03-21T07:13:27Z","published":"2022-01-11T21:31:18Z","title":"Fighting Money Laundering with Statistics and Machine Learning","summary":"  Money laundering is a profound global problem. Nonetheless, there is little\nscientific literature on statistical and machine learning methods for\nanti-money laundering. In this paper, we focus on anti-money laundering in\nbanks and provide an introduction and review of the literature. We propose a\nunifying terminology with two central elements: (i) client risk profiling and\n(ii) suspicious behavior flagging. We find that client risk profiling is\ncharacterized by diagnostics, i.e., efforts to find and explain risk factors.\nOn the other hand, suspicious behavior flagging is characterized by\nnon-disclosed features and hand-crafted risk indices. Finally, we discuss\ndirections for future research. One major challenge is the need for more public\ndata sets. This may potentially be addressed by synthetic data generation.\nOther possible research directions include semi-supervised and deep learning,\ninterpretability, and fairness of the results.\n","authors":["Rasmus Jensen","Alexandros Iosifidis"],"pdf_url":"https://arxiv.org/pdf/2201.04207v5.pdf","comment":"Accepted for publication in IEEE Access, vol. 11, pp. 8889-8903,\n  doi:10.1109/ACCESS.2023.3239549"},{"id":"http://arxiv.org/abs/2303.11634v1","updated":"2023-03-21T07:01:22Z","published":"2023-03-21T07:01:22Z","title":"Deep Q-Network Based Decision Making for Autonomous Driving","summary":"  Currently decision making is one of the biggest challenges in autonomous\ndriving. This paper introduces a method for safely navigating an autonomous\nvehicle in highway scenarios by combining deep Q-Networks and insight from\ncontrol theory. A Deep Q-Network is trained in simulation to serve as a central\ndecision-making unit by proposing targets for a trajectory planner. The\ngenerated trajectories in combination with a controller for longitudinal\nmovement are used to execute lane change maneuvers. In order to prove the\nfunctionality of this approach it is evaluated on two different highway traffic\nscenarios. Furthermore, the impact of different state representations on the\nperformance and training process is analyzed. The results show that the\nproposed system can produce efficient and safe driving behavior.\n","authors":["Max Peter Ronecker","Yuan Zhu"],"pdf_url":"https://arxiv.org/pdf/2303.11634v1.pdf","comment":"Accepted at 2019 International Conference on Robotics and Automation\n  Sciences (ICRAS)"},{"id":"http://arxiv.org/abs/2303.11632v1","updated":"2023-03-21T07:00:13Z","published":"2023-03-21T07:00:13Z","title":"An Embarrassingly Simple Approach for Wafer Feature Extraction and\n  Defect Pattern Recognition","summary":"  Identifying defect patterns in a wafer map during manufacturing is crucial to\nfind the root cause of the underlying issue and provides valuable insights on\nimproving yield in the foundry. Currently used methods use deep neural networks\nto identify the defects. These methods are generally very huge and have\nsignificant inference time. They also require GPU support to efficiently\noperate. All these issues make these models not fit for on-line prediction in\nthe manufacturing foundry. In this paper, we propose an extremely simple yet\neffective technique to extract features from wafer images. The proposed method\nis extremely fast, intuitive, and non-parametric while being explainable. The\nexperiment results show that the proposed pipeline outperforms conventional\ndeep learning models. Our feature extraction requires no training or\nfine-tuning while preserving the relative shape and location of data points as\nrevealed by our interpretability analysis.\n","authors":["Nitish Shukla"],"pdf_url":"https://arxiv.org/pdf/2303.11632v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.11624v1","updated":"2023-03-21T06:45:14Z","published":"2023-03-21T06:45:14Z","title":"Assessor-Guided Learning for Continual Environments","summary":"  This paper proposes an assessor-guided learning strategy for continual\nlearning where an assessor guides the learning process of a base learner by\ncontrolling the direction and pace of the learning process thus allowing an\nefficient learning of new environments while protecting against the\ncatastrophic interference problem. The assessor is trained in a meta-learning\nmanner with a meta-objective to boost the learning process of the base learner.\nIt performs a soft-weighting mechanism of every sample accepting positive\nsamples while rejecting negative samples. The training objective of a base\nlearner is to minimize a meta-weighted combination of the cross entropy loss\nfunction, the dark experience replay (DER) loss function and the knowledge\ndistillation loss function whose interactions are controlled in such a way to\nattain an improved performance. A compensated over-sampling (COS) strategy is\ndeveloped to overcome the class imbalanced problem of the episodic memory due\nto limited memory budgets. Our approach, Assessor-Guided Learning Approach\n(AGLA), has been evaluated in the class-incremental and task-incremental\nlearning problems. AGLA achieves improved performances compared to its\ncompetitors while the theoretical analysis of the COS strategy is offered.\nSource codes of AGLA, baseline algorithms and experimental logs are shared\npublicly in \\url{https://github.com/anwarmaxsum/AGLA} for further study.\n","authors":["Muhammad Anwar Ma'sum","Mahardhika Pratama","Edwin Lughofer","Weiping Ding","Wisnu Jatmiko"],"pdf_url":"https://arxiv.org/pdf/2303.11624v1.pdf","comment":"Submitted for publication to Information Sciences"},{"id":"http://arxiv.org/abs/2303.11619v1","updated":"2023-03-21T06:40:06Z","published":"2023-03-21T06:40:06Z","title":"Blow-up Algorithm for Sum-of-Products Polynomials and Real Log Canonical\n  Thresholds","summary":"  When considering a real log canonical threshold (RLCT) that gives a Bayesian\ngeneralization error, in general, papers replace a mean error function with a\nrelatively simple polynomial whose RLCT corresponds to that of the mean error\nfunction, and obtain its RLCT by resolving its singularities through an\nalgebraic operation called blow-up. Though it is known that the singularities\nof any polynomial can be resolved by a finite number of blow-up iterations, it\nis not clarified whether or not it is possible to resolve singularities of a\nspecific polynomial by applying a specific blow-up algorithm. Therefore this\npaper considers the blow-up algorithm for the polynomials called\nsum-of-products (sop) polynomials and its RLCT.\n","authors":["Joe Hirose"],"pdf_url":"https://arxiv.org/pdf/2303.11619v1.pdf","comment":"47 pages, 5 figure"},{"id":"http://arxiv.org/abs/2212.07740v2","updated":"2023-03-21T06:06:45Z","published":"2022-12-15T11:44:11Z","title":"Sim-to-Real Transfer for Quadrupedal Locomotion via Terrain Transformer","summary":"  Deep reinforcement learning has recently emerged as an appealing alternative\nfor legged locomotion over multiple terrains by training a policy in physical\nsimulation and then transferring it to the real world (i.e., sim-to-real\ntransfer). Despite considerable progress, the capacity and scalability of\ntraditional neural networks are still limited, which may hinder their\napplications in more complex environments. In contrast, the Transformer\narchitecture has shown its superiority in a wide range of large-scale sequence\nmodeling tasks, including natural language processing and decision-making\nproblems. In this paper, we propose Terrain Transformer (TERT), a high-capacity\nTransformer model for quadrupedal locomotion control on various terrains.\nFurthermore, to better leverage Transformer in sim-to-real scenarios, we\npresent a novel two-stage training framework consisting of an offline\npretraining stage and an online correction stage, which can naturally integrate\nTransformer with privileged training. Extensive experiments in simulation\ndemonstrate that TERT outperforms state-of-the-art baselines on different\nterrains in terms of return, energy consumption and control smoothness. In\nfurther real-world validation, TERT successfully traverses nine challenging\nterrains, including sand pit and stair down, which can not be accomplished by\nstrong baselines.\n","authors":["Hang Lai","Weinan Zhang","Xialin He","Chen Yu","Zheng Tian","Yong Yu","Jun Wang"],"pdf_url":"https://arxiv.org/pdf/2212.07740v2.pdf","comment":"Accepted by ICRA2023"},{"id":"http://arxiv.org/abs/2206.04959v4","updated":"2023-03-21T06:05:16Z","published":"2022-06-10T09:15:48Z","title":"Merak: An Efficient Distributed DNN Training Framework with Automated 3D\n  Parallelism for Giant Foundation Models","summary":"  Foundation models are becoming the dominant deep learning technologies.\nPretraining a foundation model is always time-consumed due to the large scale\nof both the model parameter and training dataset. Besides being\ncomputing-intensive, the training process is extremely memory-intensive and\ncommunication-intensive. These features make it necessary to apply 3D\nparallelism, which integrates data parallelism, pipeline model parallelism and\ntensor model parallelism, to achieve high training efficiency.\n  To achieve this goal, some custom software frameworks such as Megatron-LM and\nDeepSpeed are developed. However, current 3D parallelism frameworks still meet\ntwo issues: i) they are not transparent to model developers, which need to\nmanually modify the model to parallelize training. ii) their utilization of\ncomputation, GPU memory and network bandwidth are not sufficient. We propose\nMerak, an automated 3D parallelism deep learning training framework with high\nresource utilization. Merak automatically deploys with an automatic model\npartitioner, which uses a graph sharding algorithm on a proxy representation of\nthe model. Merak also presents the non-intrusive API for scaling out foundation\nmodel training with minimal code modification. In addition, we design a\nhigh-performance 3D parallel runtime engine in Merak. It uses several\ntechniques to exploit available training resources, including shifted critical\npath pipeline schedule that brings a higher computation utilization,\nstage-aware recomputation that makes use of idle worker memory, and\nsub-pipelined tensor model parallelism that overlaps communication and\ncomputation. Experiments on 64 GPUs show Merak can speedup the training\nperformance over the state-of-the-art 3D parallelism frameworks of models with\n1.5, 2.5, 8.3, and 20 billion parameters by up to 1.42X, 1.39X, 1.43X, and\n1.61X, respectively.\n","authors":["Zhiquan Lai","Shengwei Li","Xudong Tang","Keshi Ge","Weijie Liu","Yabo Duan","Linbo Qiao","Dongsheng Li"],"pdf_url":"https://arxiv.org/pdf/2206.04959v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.05382v2","updated":"2023-03-21T05:47:11Z","published":"2023-03-06T16:36:17Z","title":"ChatGPT Is on the Horizon: Could a Large Language Model Be All We Need\n  for Intelligent Transportation?","summary":"  ChatGPT, developed by OpenAI, is one of the milestone large language models\n(LLMs) with 6 billion parameters. ChatGPT has demonstrated the impressive\nlanguage understanding capability of LLM, particularly in generating\nconversational response. As LLMs start to gain more attention in various\nresearch or engineering domains, it is time to envision how LLM may\nrevolutionize the way we approach intelligent transportation systems. This\npaper explores the future applications of LLM in addressing key transportation\nproblems. By leveraging LLM with cross-modal encoder, an intelligent system can\nalso process traffic data from different modalities and execute transportation\noperations through an LLM. We present and validate these potential\ntransportation applications equipped by LLM. To further demonstrate this\npotential, we also provide a concrete smartphone-based crash report\nauto-generation and analysis framework as a use case. Despite the potential\nbenefits, challenges related to data privacy, data quality, and model bias must\nbe considered. Overall, the use of LLM in intelligent transport systems holds\npromise for more efficient, intelligent, and sustainable transportation systems\nthat further improve daily life around the world.\n","authors":["Ou Zheng","Mohamed Abdel-Aty","Dongdong Wang","Zijin Wang","Shengxuan Ding"],"pdf_url":"https://arxiv.org/pdf/2303.05382v2.pdf","comment":"Submitted to Nature - Machine Intelligence (13 Pages, 8 Figures)"},{"id":"http://arxiv.org/abs/2303.11602v1","updated":"2023-03-21T05:41:24Z","published":"2023-03-21T05:41:24Z","title":"Convergence of stochastic gradient descent on parameterized sphere with\n  applications to variational Monte Carlo simulation","summary":"  We analyze stochastic gradient descent (SGD) type algorithms on a\nhigh-dimensional sphere which is parameterized by a neural network up to a\nnormalization constant. We provide a new algorithm for the setting of\nsupervised learning and show its convergence both theoretically and\nnumerically. We also provide the first proof of convergence for the\nunsupervised setting, which corresponds to the widely used variational Monte\nCarlo (VMC) method in quantum physics.\n","authors":["Nilin Abrahamsen","Zhiyan Ding","Gil Goldshlager","Lin Lin"],"pdf_url":"https://arxiv.org/pdf/2303.11602v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2208.04319v2","updated":"2023-03-21T05:28:26Z","published":"2022-08-07T13:33:34Z","title":"PhyGNNet: Solving spatiotemporal PDEs with Physics-informed Graph Neural\n  Network","summary":"  Solving partial differential equations (PDEs) is an important research means\nin the fields of physics, biology, and chemistry. As an approximate alternative\nto numerical methods, PINN has received extensive attention and played an\nimportant role in many fields. However, PINN uses a fully connected network as\nits model, which has limited fitting ability and limited extrapolation ability\nin both time and space. In this paper, we propose PhyGNNet for solving partial\ndifferential equations on the basics of a graph neural network which consists\nof encoder, processer, and decoder blocks. In particular, we divide the\ncomputing area into regular grids, define partial differential operators on the\ngrids, then construct pde loss for the network to optimize to build PhyGNNet\nmodel. What's more, we conduct comparative experiments on Burgers equation and\nheat equation to validate our approach, the results show that our method has\nbetter fit ability and extrapolation ability both in time and spatial areas\ncompared with PINN.\n","authors":["Longxiang Jiang","Liyuan Wang","Xinkun Chu","Yonghao Xiao","Hao Zhang"],"pdf_url":"https://arxiv.org/pdf/2208.04319v2.pdf","comment":"there some errors in method describtion"},{"id":"http://arxiv.org/abs/2111.09543v3","updated":"2023-03-21T05:17:08Z","published":"2021-11-18T06:48:00Z","title":"DeBERTaV3: Improving DeBERTa using ELECTRA-Style Pre-Training with\n  Gradient-Disentangled Embedding Sharing","summary":"  This paper presents a new pre-trained language model, DeBERTaV3, which\nimproves the original DeBERTa model by replacing mask language modeling (MLM)\nwith replaced token detection (RTD), a more sample-efficient pre-training task.\nOur analysis shows that vanilla embedding sharing in ELECTRA hurts training\nefficiency and model performance. This is because the training losses of the\ndiscriminator and the generator pull token embeddings in different directions,\ncreating the \"tug-of-war\" dynamics. We thus propose a new gradient-disentangled\nembedding sharing method that avoids the tug-of-war dynamics, improving both\ntraining efficiency and the quality of the pre-trained model. We have\npre-trained DeBERTaV3 using the same settings as DeBERTa to demonstrate its\nexceptional performance on a wide range of downstream natural language\nunderstanding (NLU) tasks. Taking the GLUE benchmark with eight tasks as an\nexample, the DeBERTaV3 Large model achieves a 91.37% average score, which is\n1.37% over DeBERTa and 1.91% over ELECTRA, setting a new state-of-the-art\n(SOTA) among the models with a similar structure. Furthermore, we have\npre-trained a multi-lingual model mDeBERTa and observed a larger improvement\nover strong baselines compared to English models. For example, the mDeBERTa\nBase achieves a 79.8% zero-shot cross-lingual accuracy on XNLI and a 3.6%\nimprovement over XLM-R Base, creating a new SOTA on this benchmark. We have\nmade our pre-trained models and inference code publicly available at\nhttps://github.com/microsoft/DeBERTa.\n","authors":["Pengcheng He","Jianfeng Gao","Weizhu Chen"],"pdf_url":"https://arxiv.org/pdf/2111.09543v3.pdf","comment":"16 pages, 10 tables, 2 Figures. The DeBERTaV3 model significantly\n  improves performance of the downstream NLU tasks over models with a similar\n  structure, e.g. DeBERTaV3 large achieves 91.37% average GLUE score which is\n  1.37% over DeBERTa large. XSmall has only 22M backbone parameters, but\n  significantly outperforms RoBERTa/XLNet-base. Paper is published as a\n  conference paper at ICLR 2023"},{"id":"http://arxiv.org/abs/2212.09010v2","updated":"2023-03-21T04:48:24Z","published":"2022-12-18T04:44:38Z","title":"Risk-Sensitive Reinforcement Learning with Exponential Criteria","summary":"  While risk-neutral reinforcement learning has shown experimental success in a\nnumber of applications, it is well-known to be non-robust with respect to noise\nand perturbations in the parameters of the system. For this reason,\nrisk-sensitive reinforcement learning algorithms have been studied to introduce\nrobustness and sample efficiency, and lead to better real-life performance. In\nthis work, we introduce new model-free risk-sensitive reinforcement learning\nalgorithms as variations of widely-used Policy Gradient algorithms with similar\nimplementation properties. In particular, we study the effect of exponential\ncriteria on the risk-sensitivity of the policy of a reinforcement learning\nagent, and develop variants of the Monte Carlo Policy Gradient algorithm and\nthe online (temporal-difference) Actor-Critic algorithm. Analytical results\nshowcase that the use of exponential criteria generalize commonly used ad-hoc\nregularization approaches. The implementation, performance, and robustness\nproperties of the proposed methods are evaluated in simulated experiments.\n","authors":["Erfaun Noorani","Christos Mavridis","John Baras"],"pdf_url":"https://arxiv.org/pdf/2212.09010v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.11593v1","updated":"2023-03-21T04:47:45Z","published":"2023-03-21T04:47:45Z","title":"Difficulty in learning chirality for Transformer fed with SMILES","summary":"  Recent years have seen development of descriptor generation based on\nrepresentation learning of extremely diverse molecules, especially those that\napply natural language processing (NLP) models to SMILES, a literal\nrepresentation of molecular structure. However, little research has been done\non how these models understand chemical structure. To address this, we\ninvestigated the relationship between the learning progress of SMILES and\nchemical structure using a representative NLP model, the Transformer. The\nresults suggest that while the Transformer learns partial structures of\nmolecules quickly, it requires extended training to understand overall\nstructures. Consistently, the accuracy of molecular property predictions using\ndescriptors generated from models at different learning steps was similar from\nthe beginning to the end of training. Furthermore, we found that the\nTransformer requires particularly long training to learn chirality and\nsometimes stagnates with low translation accuracy due to misunderstanding of\nenantiomers. These findings are expected to deepen understanding of NLP models\nin chemistry.\n","authors":["Yasuhiro Yoshikai","Tadahaya Mizuno","Shumpei Nemoto","Hiroyuki Kusuhara"],"pdf_url":"https://arxiv.org/pdf/2303.11593v1.pdf","comment":"20 pages, 6 figures"},{"id":"http://arxiv.org/abs/2201.12243v2","updated":"2023-03-21T04:41:42Z","published":"2022-01-28T16:53:56Z","title":"Joint Differentiable Optimization and Verification for Certified\n  Reinforcement Learning","summary":"  In model-based reinforcement learning for safety-critical control systems, it\nis important to formally certify system properties (e.g., safety, stability)\nunder the learned controller. However, as existing methods typically apply\nformal verification \\emph{after} the controller has been learned, it is\nsometimes difficult to obtain any certificate, even after many iterations\nbetween learning and verification. To address this challenge, we propose a\nframework that jointly conducts reinforcement learning and formal verification\nby formulating and solving a novel bilevel optimization problem, which is\ndifferentiable by the gradients from the value function and certificates.\nExperiments on a variety of examples demonstrate the significant advantages of\nour framework over the model-based stochastic value gradient (SVG) method and\nthe model-free proximal policy optimization (PPO) method in finding feasible\ncontrollers with barrier functions and Lyapunov functions that ensure system\nsafety and stability.\n","authors":["Yixuan Wang","Simon Zhan","Zhilu Wang","Chao Huang","Zhaoran Wang","Zhuoran Yang","Qi Zhu"],"pdf_url":"https://arxiv.org/pdf/2201.12243v2.pdf","comment":"This paper is accepted to International Conference on Cyber-Physical\n  Systems"},{"id":"http://arxiv.org/abs/2303.10880v2","updated":"2023-03-21T04:41:28Z","published":"2023-03-20T05:38:30Z","title":"Rotating without Seeing: Towards In-hand Dexterity through Touch","summary":"  Tactile information plays a critical role in human dexterity. It reveals\nuseful contact information that may not be inferred directly from vision. In\nfact, humans can even perform in-hand dexterous manipulation without using\nvision. Can we enable the same ability for the multi-finger robot hand? In this\npaper, we present Touch Dexterity, a new system that can perform in-hand object\nrotation using only touching without seeing the object. Instead of relying on\nprecise tactile sensing in a small region, we introduce a new system design\nusing dense binary force sensors (touch or no touch) overlaying one side of the\nwhole robot hand (palm, finger links, fingertips). Such a design is low-cost,\ngiving a larger coverage of the object, and minimizing the Sim2Real gap at the\nsame time. We train an in-hand rotation policy using Reinforcement Learning on\ndiverse objects in simulation. Relying on touch-only sensing, we can directly\ndeploy the policy in a real robot hand and rotate novel objects that are not\npresented in training. Extensive ablations are performed on how tactile\ninformation help in-hand manipulation.Our project is available at\nhttps://touchdexterity.github.io.\n","authors":["Zhao-Heng Yin","Binghao Huang","Yuzhe Qin","Qifeng Chen","Xiaolong Wang"],"pdf_url":"https://arxiv.org/pdf/2303.10880v2.pdf","comment":"Project page: https://touchdexterity.github.io"},{"id":"http://arxiv.org/abs/2303.11582v1","updated":"2023-03-21T04:17:03Z","published":"2023-03-21T04:17:03Z","title":"Adaptive Experimentation at Scale: Bayesian Algorithms for Flexible\n  Batches","summary":"  Standard bandit algorithms that assume continual reallocation of measurement\neffort are challenging to implement due to delayed feedback and\ninfrastructural/organizational difficulties. Motivated by practical instances\ninvolving a handful of reallocation epochs in which outcomes are measured in\nbatches, we develop a new adaptive experimentation framework that can flexibly\nhandle any batch size. Our main observation is that normal approximations\nuniversal in statistical inference can also guide the design of scalable\nadaptive designs. By deriving an asymptotic sequential experiment, we formulate\na dynamic program that can leverage prior information on average rewards. State\ntransitions of the dynamic program are differentiable with respect to the\nsampling allocations, allowing the use of gradient-based methods for planning\nand policy optimization. We propose a simple iterative planning method,\nResidual Horizon Optimization, which selects sampling allocations by optimizing\na planning objective via stochastic gradient-based methods. Our method\nsignificantly improves statistical power over standard adaptive policies, even\nwhen compared to Bayesian bandit algorithms (e.g., Thompson sampling) that\nrequire full distributional knowledge of individual rewards. Overall, we expand\nthe scope of adaptive experimentation to settings which are difficult for\nstandard adaptive policies, including problems with a small number of\nreallocation epochs, low signal-to-noise ratio, and unknown reward\ndistributions.\n","authors":["Ethan Che","Hongseok Namkoong"],"pdf_url":"https://arxiv.org/pdf/2303.11582v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2212.08189v3","updated":"2023-03-21T04:13:48Z","published":"2022-12-15T23:21:49Z","title":"Multi-Resolution Online Deterministic Annealing: A Hierarchical and\n  Progressive Learning Architecture","summary":"  Hierarchical learning algorithms that gradually approximate a solution to a\ndata-driven optimization problem are essential to decision-making systems,\nespecially under limitations on time and computational resources. In this\nstudy, we introduce a general-purpose hierarchical learning architecture that\nis based on the progressive partitioning of a possibly multi-resolution data\nspace. The optimal partition is gradually approximated by solving a sequence of\noptimization sub-problems that yield a sequence of partitions with increasing\nnumber of subsets. We show that the solution of each optimization problem can\nbe estimated online using gradient-free stochastic approximation updates. As a\nconsequence, a function approximation problem can be defined within each subset\nof the partition and solved using the theory of two-timescale stochastic\napproximation algorithms. This simulates an annealing process and defines a\nrobust and interpretable heuristic method to gradually increase the complexity\nof the learning architecture in a task-agnostic manner, giving emphasis to\nregions of the data space that are considered more important according to a\npredefined criterion. Finally, by imposing a tree structure in the progression\nof the partitions, we provide a means to incorporate potential multi-resolution\nstructure of the data space into this approach, significantly reducing its\ncomplexity, while introducing hierarchical variable-rate feature extraction\nproperties similar to certain classes of deep learning architectures.\nAsymptotic convergence analysis and experimental results are provided for\nsupervised and unsupervised learning problems.\n","authors":["Christos Mavridis","John Baras"],"pdf_url":"https://arxiv.org/pdf/2212.08189v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.11580v1","updated":"2023-03-21T04:01:55Z","published":"2023-03-21T04:01:55Z","title":"Efficient Multi-stage Inference on Tabular Data","summary":"  Many ML applications and products train on medium amounts of input data but\nget bottlenecked in real-time inference. When implementing ML systems,\nconventional wisdom favors segregating ML code into services queried by product\ncode via Remote Procedure Call (RPC) APIs. This approach clarifies the overall\nsoftware architecture and simplifies product code by abstracting away ML\ninternals. However, the separation adds network latency and entails additional\nCPU overhead. Hence, we simplify inference algorithms and embed them into the\nproduct code to reduce network communication. For public datasets and a\nhigh-performance real-time platform that deals with tabular data, we show that\nover half of the inputs are often amenable to such optimization, while the\nremainder can be handled by the original model. By applying our optimization\nwith AutoML to both training and inference, we reduce inference latency by\n1.3x, CPU resources by 30%, and network communication between application\nfront-end and ML back-end by about 50% for a commercial end-to-end ML platform\nthat serves millions of real-time decisions per second.\n","authors":["Daniel S Johnson","Igor L Markov"],"pdf_url":"https://arxiv.org/pdf/2303.11580v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2210.04366v2","updated":"2023-03-21T03:52:17Z","published":"2022-10-09T22:46:55Z","title":"Computational Choreography using Human Motion Synthesis","summary":"  Should deep learning models be trained to analyze human performance art? To\nhelp answer this question, we explore an application of deep neural networks to\nsynthesize artistic human motion. Problem tasks in human motion synthesis can\ninclude predicting the motions of humans in-the-wild, as well as generating new\nsequences of motions based on said predictions. We will discuss the potential\nof a less traditional application, where learning models are applied to\npredicting dance movements. There have been notable, recent efforts to analyze\ndance movements in a computational light, such as the Everybody Dance Now (EDN)\nlearning model and a Cal Poly master's thesis, Take The Lead (TTL). We have\neffectively combined these two works along with our own deep neural network to\nproduce a new system for dance motion prediction, image-to-image translation,\nand video generation.\n","authors":["Patrick Perrine","Trevor Kirkby"],"pdf_url":"https://arxiv.org/pdf/2210.04366v2.pdf","comment":"6 pages, 7 figures, to be submitted to AIVR 2023"},{"id":"http://arxiv.org/abs/2303.11577v1","updated":"2023-03-21T03:51:15Z","published":"2023-03-21T03:51:15Z","title":"Feature-adjacent multi-fidelity physics-informed machine learning for\n  partial differential equations","summary":"  Physics-informed neural networks have emerged as an alternative method for\nsolving partial differential equations. However, for complex problems, the\ntraining of such networks can still require high-fidelity data which can be\nexpensive to generate. To reduce or even eliminate the dependency on\nhigh-fidelity data, we propose a novel multi-fidelity architecture which is\nbased on a feature space shared by the low- and high-fidelity solutions. In the\nfeature space, the projections of the low-fidelity and high-fidelity solutions\nare adjacent by constraining their relative distance. The feature space is\nrepresented with an encoder and its mapping to the original solution space is\neffected through a decoder. The proposed multi-fidelity approach is validated\non forward and inverse problems for steady and unsteady problems described by\npartial differential equations.\n","authors":["Wenqian Chen","Panos Stinis"],"pdf_url":"https://arxiv.org/pdf/2303.11577v1.pdf","comment":"12 figures"},{"id":"http://arxiv.org/abs/2303.07397v2","updated":"2023-03-21T03:27:35Z","published":"2023-03-13T18:25:46Z","title":"Fast exploration and learning of latent graphs with aliased observations","summary":"  Consider this scenario: an agent navigates a latent graph by performing\nactions that take it from one node to another. The chosen action determines the\nprobability distribution over the next visited node. At each node, the agent\nreceives an observation, but this observation is not unique, so it does not\nidentify the node, making the problem aliased. The purpose of this work is to\nprovide a policy that approximately maximizes exploration efficiency (i.e., how\nwell the graph is recovered for a given exploration budget). In the unaliased\ncase, we show improved performance w.r.t. state-of-the-art reinforcement\nlearning baselines. For the aliased case we are not aware of suitable baselines\nand instead show faster recovery w.r.t. a random policy for a wide variety of\ntopologies, and exponentially faster recovery than a random policy for\nchallenging topologies. We dub the algorithm eFeX (from eFficient eXploration).\n","authors":["Miguel Lazaro-Gredilla","Ishan Deshpande","Sivaramakrishnan Swaminathan","Meet Dave","Dileep George"],"pdf_url":"https://arxiv.org/pdf/2303.07397v2.pdf","comment":"v2: Added extra figure and fixed typos"},{"id":"http://arxiv.org/abs/2303.11563v1","updated":"2023-03-21T03:14:15Z","published":"2023-03-21T03:14:15Z","title":"Dynamic Healthcare Embeddings for Improving Patient Care","summary":"  As hospitals move towards automating and integrating their computing systems,\nmore fine-grained hospital operations data are becoming available. These data\ninclude hospital architectural drawings, logs of interactions between patients\nand healthcare professionals, prescription data, procedures data, and data on\npatient admission, discharge, and transfers. This has opened up many\nfascinating avenues for healthcare-related prediction tasks for improving\npatient care. However, in order to leverage off-the-shelf machine learning\nsoftware for these tasks, one needs to learn structured representations of\nentities involved from heterogeneous, dynamic data streams. Here, we propose\nDECENT, an auto-encoding heterogeneous co-evolving dynamic neural network, for\nlearning heterogeneous dynamic embeddings of patients, doctors, rooms, and\nmedications from diverse data streams. These embeddings capture similarities\namong doctors, rooms, patients, and medications based on static attributes and\ndynamic interactions. DECENT enables several applications in healthcare\nprediction, such as predicting mortality risk and case severity of patients,\nadverse events (e.g., transfer back into an intensive care unit), and future\nhealthcare-associated infections. The results of using the learned patient\nembeddings in predictive modeling show that DECENT has a gain of up to 48.1% on\nthe mortality risk prediction task, 12.6% on the case severity prediction task,\n6.4% on the medical intensive care unit transfer task, and 3.8% on the\nClostridioides difficile (C.diff) Infection (CDI) prediction task over the\nstate-of-the-art baselines. In addition, case studies on the learned doctor,\nmedication, and room embeddings show that our approach learns meaningful and\ninterpretable embeddings.\n","authors":["Hankyu Jang","Sulyun Lee","D. M. Hasibul Hasan","Philip M. Polgreen","Sriram V. Pemmaraju","Bijaya Adhikari"],"pdf_url":"https://arxiv.org/pdf/2303.11563v1.pdf","comment":"To be published in IEEE/ACM ASONAM 2022"},{"id":"http://arxiv.org/abs/2303.11562v1","updated":"2023-03-21T03:05:21Z","published":"2023-03-21T03:05:21Z","title":"Dynamic-Aware Loss for Learning with Label Noise","summary":"  Label noise poses a serious threat to deep neural networks (DNNs). Employing\nrobust loss function which reconciles fitting ability with robustness is a\nsimple but effective strategy to handle this problem. However, the widely-used\nstatic trade-off between these two factors contradicts the dynamic nature of\nDNNs learning with label noise, leading to inferior performance. Therefore, we\npropose a dynamics-aware loss (DAL) to solve this problem. Considering that\nDNNs tend to first learn generalized patterns, then gradually overfit label\nnoise, DAL strengthens the fitting ability initially, then gradually increases\nthe weight of robustness. Moreover, at the later stage, we let DNNs put more\nemphasis on easy examples which are more likely to be correctly labeled than\nhard ones and introduce a bootstrapping term to further reduce the negative\nimpact of label noise. Both the detailed theoretical analyses and extensive\nexperimental results demonstrate the superiority of our method.\n","authors":["Xiu-Chuan Li","Xiaobo Xia","Fei Zhu","Tongliang Liu","Xu-Yao Zhang","Cheng-Lin Liu"],"pdf_url":"https://arxiv.org/pdf/2303.11562v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.04386v2","updated":"2023-03-21T02:50:48Z","published":"2023-03-08T05:19:08Z","title":"Policy Mirror Descent Inherently Explores Action Space","summary":"  Explicit exploration in the action space was assumed to be indispensable for\nonline policy gradient methods to avoid a drastic degradation in sample\ncomplexity, for solving general reinforcement learning problems over finite\nstate and action spaces. In this paper, we establish for the first time an\n$\\tilde{\\mathcal{O}}(1/\\epsilon^2)$ sample complexity for online policy\ngradient methods without incorporating any exploration strategies. The\nessential development consists of two new on-policy evaluation operators and a\nnovel analysis of the stochastic policy mirror descent method (SPMD). SPMD with\nthe first evaluation operator, called value-based estimation, tailors to the\nKullback-Leibler divergence. Provided the Markov chains on the state space of\ngenerated policies are uniformly mixing with non-diminishing minimal visitation\nmeasure, an $\\tilde{\\mathcal{O}}(1/\\epsilon^2)$ sample complexity is obtained\nwith a linear dependence on the size of the action space. SPMD with the second\nevaluation operator, namely truncated on-policy Monte Carlo (TOMC), attains an\n$\\tilde{\\mathcal{O}}(\\mathcal{H}_{\\mathcal{D}}/\\epsilon^2)$ sample complexity,\nwhere $\\mathcal{H}_{\\mathcal{D}}$ mildly depends on the effective horizon and\nthe size of the action space with properly chosen Bregman divergence (e.g.,\nTsallis divergence). SPMD with TOMC also exhibits stronger convergence\nproperties in that it controls the optimality gap with high probability rather\nthan in expectation. In contrast to explicit exploration, these new policy\ngradient methods can prevent repeatedly committing to potentially high-risk\nactions when searching for optimal policies.\n","authors":["Yan Li","Guanghui Lan"],"pdf_url":"https://arxiv.org/pdf/2303.04386v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.11553v1","updated":"2023-03-21T02:44:15Z","published":"2023-03-21T02:44:15Z","title":"Dynamic Vertex Replacement Grammars","summary":"  Context-free graph grammars have shown a remarkable ability to model\nstructures in real-world relational data. However, graph grammars lack the\nability to capture time-changing phenomena since the left-to-right transitions\nof a production rule do not represent temporal change. In the present work, we\ndescribe dynamic vertex-replacement grammars (DyVeRG), which generalize vertex\nreplacement grammars in the time domain by providing a formal framework for\nupdating a learned graph grammar in accordance with modifications to its\nunderlying data. We show that DyVeRG grammars can be learned from, and used to\ngenerate, real-world dynamic graphs faithfully while remaining\nhuman-interpretable. We also demonstrate their ability to forecast by computing\ndyvergence scores, a novel graph similarity measurement exposed by this\nframework.\n","authors":["Daniel Gonzalez Cedre","Justus Isaiah Hibshman","Timothy La Fond","Grant Boquet","Tim Weninger"],"pdf_url":"https://arxiv.org/pdf/2303.11553v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.11551v1","updated":"2023-03-21T02:37:46Z","published":"2023-03-21T02:37:46Z","title":"ModEFormer: Modality-Preserving Embedding for Audio-Video\n  Synchronization using Transformers","summary":"  Lack of audio-video synchronization is a common problem during television\nbroadcasts and video conferencing, leading to an unsatisfactory viewing\nexperience. A widely accepted paradigm is to create an error detection\nmechanism that identifies the cases when audio is leading or lagging. We\npropose ModEFormer, which independently extracts audio and video embeddings\nusing modality-specific transformers. Different from the other\ntransformer-based approaches, ModEFormer preserves the modality of the input\nstreams which allows us to use a larger batch size with more negative audio\nsamples for contrastive learning. Further, we propose a trade-off between the\nnumber of negative samples and number of unique samples in a batch to\nsignificantly exceed the performance of previous methods. Experimental results\nshow that ModEFormer achieves state-of-the-art performance, 94.5% for LRS2 and\n90.9% for LRS3. Finally, we demonstrate how ModEFormer can be used for offset\ndetection for test clips.\n","authors":["Akash Gupta","Rohun Tripathi","Wondong Jang"],"pdf_url":"https://arxiv.org/pdf/2303.11551v1.pdf","comment":"Paper accepted at ICASSP 2023"},{"id":"http://arxiv.org/abs/2303.11545v1","updated":"2023-03-21T02:19:48Z","published":"2023-03-21T02:19:48Z","title":"Fix the Noise: Disentangling Source Feature for Controllable Domain\n  Translation","summary":"  Recent studies show strong generative performance in domain translation\nespecially by using transfer learning techniques on the unconditional\ngenerator. However, the control between different domain features using a\nsingle model is still challenging. Existing methods often require additional\nmodels, which is computationally demanding and leads to unsatisfactory visual\nquality. In addition, they have restricted control steps, which prevents a\nsmooth transition. In this paper, we propose a new approach for high-quality\ndomain translation with better controllability. The key idea is to preserve\nsource features within a disentangled subspace of a target feature space. This\nallows our method to smoothly control the degree to which it preserves source\nfeatures while generating images from an entirely new domain using only a\nsingle model. Our extensive experiments show that the proposed method can\nproduce more consistent and realistic images than previous works and maintain\nprecise controllability over different levels of transformation. The code is\navailable at https://github.com/LeeDongYeun/FixNoise.\n","authors":["Dongyeun Lee","Jae Young Lee","Doyeon Kim","Jaehyun Choi","Jaejun Yoo","Junmo Kim"],"pdf_url":"https://arxiv.org/pdf/2303.11545v1.pdf","comment":"Accepted by CVPR 2023. The code is available at\n  https://github.com/LeeDongYeun/FixNoise. Extended from arXiv:2204.14079 (AICC\n  workshop at CVPR 2022)"},{"id":"http://arxiv.org/abs/2303.09065v3","updated":"2023-03-21T02:15:48Z","published":"2023-03-16T03:45:46Z","title":"Maximum margin learning of t-SPNs for cell classification with filtered\n  input","summary":"  An algorithm based on a deep probabilistic architecture referred to as a\ntree-structured sum-product network (t-SPN) is considered for cell\nclassification. The t-SPN is constructed such that the unnormalized probability\nis represented as conditional probabilities of a subset of most similar cell\nclasses. The constructed t-SPN architecture is learned by maximizing the\nmargin, which is the difference in the conditional probability between the true\nand the most competitive false label. To enhance the generalization ability of\nthe architecture, L2-regularization (REG) is considered along with the maximum\nmargin (MM) criterion in the learning process. To highlight cell features, this\npaper investigates the effectiveness of two generic high-pass filters: ideal\nhigh-pass filtering and the Laplacian of Gaussian (LOG) filtering. On both\nHEp-2 and Feulgen benchmark datasets, the t-SPN architecture learned based on\nthe max-margin criterion with regularization produced the highest accuracy rate\ncompared to other state-of-the-art algorithms that include convolutional neural\nnetwork (CNN) based algorithms. The ideal high-pass filter was more effective\non the HEp-2 dataset, which is based on immunofluorescence staining, while the\nLOG was more effective on the Feulgen dataset, which is based on Feulgen\nstaining.\n","authors":["Haeyong Kang","Chang D. Yoo","Yongcheon Na"],"pdf_url":"https://arxiv.org/pdf/2303.09065v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.11540v1","updated":"2023-03-21T02:11:37Z","published":"2023-03-21T02:11:37Z","title":"MSTFormer: Motion Inspired Spatial-temporal Transformer with\n  Dynamic-aware Attention for long-term Vessel Trajectory Prediction","summary":"  Incorporating the dynamics knowledge into the model is critical for achieving\naccurate trajectory prediction while considering the spatial and temporal\ncharacteristics of the vessel. However, existing methods rarely consider the\nunderlying dynamics knowledge and directly use machine learning algorithms to\npredict the trajectories. Intuitively, the vessel's motions are following the\nlaws of dynamics, e.g., the speed of a vessel decreases when turning a corner.\nYet, it is challenging to combine dynamic knowledge and neural networks due to\ntheir inherent heterogeneity. Against this background, we propose MSTFormer, a\nmotion inspired vessel trajectory prediction method based on Transformer. The\ncontribution of this work is threefold. First, we design a data augmentation\nmethod to describe the spatial features and motion features of the trajectory.\nSecond, we propose a Multi-headed Dynamic-aware Self-attention mechanism to\nfocus on trajectory points with frequent motion transformations. Finally, we\nconstruct a knowledge-inspired loss function to further boost the performance\nof the model. Experimental results on real-world datasets show that our\nstrategy not only effectively improves long-term predictive capability but also\noutperforms backbones on cornering data.The ablation analysis further confirms\nthe efficacy of the proposed method. To the best of our knowledge, MSTFormer is\nthe first neural network model for trajectory prediction fused with vessel\nmotion dynamics, providing a worthwhile direction for future research.The\nsource code is available at https://github.com/simple316/MSTFormer.\n","authors":["Huimin Qiang","Zhiyuan Guo","Shiyuan Xie","Xiaodong Peng"],"pdf_url":"https://arxiv.org/pdf/2303.11540v1.pdf","comment":"11 pages"},{"id":"http://arxiv.org/abs/2303.11536v1","updated":"2023-03-21T01:57:40Z","published":"2023-03-21T01:57:40Z","title":"Indeterminate Probability Neural Network","summary":"  We propose a new general model called IPNN - Indeterminate Probability Neural\nNetwork, which combines neural network and probability theory together. In the\nclassical probability theory, the calculation of probability is based on the\noccurrence of events, which is hardly used in current neural networks. In this\npaper, we propose a new general probability theory, which is an extension of\nclassical probability theory, and makes classical probability theory a special\ncase to our theory. Besides, for our proposed neural network framework, the\noutput of neural network is defined as probability events, and based on the\nstatistical analysis of these events, the inference model for classification\ntask is deduced. IPNN shows new property: It can perform unsupervised\nclustering while doing classification. Besides, IPNN is capable of making very\nlarge classification with very small neural network, e.g. model with 100 output\nnodes can classify 10 billion categories. Theoretical advantages are reflected\nin experimental results.\n","authors":["Tao Yang","Chuang Liu","Xiaofeng Ma","Weijia Lu","Ning Wu","Bingyang Li","Zhifei Yang","Peng Liu","Lin Sun","Xiaodong Zhang","Can Zhang"],"pdf_url":"https://arxiv.org/pdf/2303.11536v1.pdf","comment":"13 pages"},{"id":"http://arxiv.org/abs/2205.14230v2","updated":"2023-03-21T01:55:06Z","published":"2022-05-27T20:50:36Z","title":"Semi-supervised Semantics-guided Adversarial Training for Trajectory\n  Prediction","summary":"  Predicting the trajectories of surrounding objects is a critical task for\nself-driving vehicles and many other autonomous systems. Recent works\ndemonstrate that adversarial attacks on trajectory prediction, where small\ncrafted perturbations are introduced to history trajectories, may significantly\nmislead the prediction of future trajectories and induce unsafe planning.\nHowever, few works have addressed enhancing the robustness of this important\nsafety-critical task.In this paper, we present a novel adversarial training\nmethod for trajectory prediction. Compared with typical adversarial training on\nimage tasks, our work is challenged by more random input with rich context and\na lack of class labels. To address these challenges, we propose a method based\non a semi-supervised adversarial autoencoder, which models disentangled\nsemantic features with domain knowledge and provides additional latent labels\nfor the adversarial training. Extensive experiments with different types of\nattacks demonstrate that our Semisupervised Semantics-guided Adversarial\nTraining (SSAT) method can effectively mitigate the impact of adversarial\nattacks by up to 73% and outperform other popular defense methods. In addition,\nexperiments show that our method can significantly improve the system's robust\ngeneralization to unseen patterns of attacks. We believe that such\nsemantics-guided architecture and advancement on robust generalization is an\nimportant step for developing robust prediction models and enabling safe\ndecision-making.\n","authors":["Ruochen Jiao","Xiangguo Liu","Takami Sato","Qi Alfred Chen","Qi Zhu"],"pdf_url":"https://arxiv.org/pdf/2205.14230v2.pdf","comment":"11 pages, adversarial training for trajectory prediction"},{"id":"http://arxiv.org/abs/2302.12426v3","updated":"2023-03-21T01:49:46Z","published":"2023-02-24T03:13:12Z","title":"Statistical Analysis of Karcher Means for Random Restricted PSD Matrices","summary":"  Non-asymptotic statistical analysis is often missing for modern\ngeometry-aware machine learning algorithms due to the possibly intricate\nnon-linear manifold structure. This paper studies an intrinsic mean model on\nthe manifold of restricted positive semi-definite matrices and provides a\nnon-asymptotic statistical analysis of the Karcher mean. We also consider a\ngeneral extrinsic signal-plus-noise model, under which a deterministic error\nbound of the Karcher mean is provided. As an application, we show that the\ndistributed principal component analysis algorithm, LRC-dPCA, achieves the same\nperformance as the full sample PCA algorithm. Numerical experiments lend strong\nsupport to our theories.\n","authors":["Hengchao Chen","Xiang Li","Qiang Sun"],"pdf_url":"https://arxiv.org/pdf/2302.12426v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.11529v1","updated":"2023-03-21T01:28:23Z","published":"2023-03-21T01:28:23Z","title":"Counterfactually Fair Regression with Double Machine Learning","summary":"  Counterfactual fairness is an approach to AI fairness that tries to make\ndecisions based on the outcomes that an individual with some kind of sensitive\nstatus would have had without this status. This paper proposes Double Machine\nLearning (DML) Fairness which analogises this problem of counterfactual\nfairness in regression problems to that of estimating counterfactual outcomes\nin causal inference under the Potential Outcomes framework. It uses arbitrary\nmachine learning methods to partial out the effect of sensitive variables on\nnonsensitive variables and outcomes. Assuming that the effects of the two sets\nof variables are additively separable, outcomes will be approximately equalised\nand individual-level outcomes will be counterfactually fair. This paper\ndemonstrates the approach in a simulation study pertaining to discrimination in\nworkplace hiring and an application on real data estimating the GPAs of law\nschool students. It then discusses when it is appropriate to apply such a\nmethod to problems of real-world discrimination where constructs are\nconceptually complex and finally, whether DML Fairness can achieve justice in\nthese settings.\n","authors":["Patrick Rehill"],"pdf_url":"https://arxiv.org/pdf/2303.11529v1.pdf","comment":"21 pages, 16 figures"},{"id":"http://arxiv.org/abs/1703.01347v3","updated":"2023-03-21T01:19:15Z","published":"2017-03-03T21:39:56Z","title":"Contextual Linear Bandits under Noisy Features: Towards Bayesian Oracles","summary":"  We study contextual linear bandit problems under feature uncertainty; they\nare noisy with missing entries. To address the challenges of the noise, we\nanalyze Bayesian oracles given observed noisy features. Our Bayesian analysis\nfinds that the optimal hypothesis can be far from the underlying realizability\nfunction, depending on the noise characteristics, which are highly\nnon-intuitive and do not occur for classical noiseless setups. This implies\nthat classical approaches cannot guarantee a non-trivial regret bound.\nTherefore, we propose an algorithm that aims at the Bayesian oracle from\nobserved information under this model, achieving $\\tilde{O}(d\\sqrt{T})$ regret\nbound when there is a large number of arms. We demonstrate the proposed\nalgorithm using synthetic and real-world datasets.\n","authors":["Jung-hun Kim","Se-Young Yun","Minchan Jeong","Jun Hyun Nam","Jinwoo Shin","Richard Combes"],"pdf_url":"https://arxiv.org/pdf/1703.01347v3.pdf","comment":"30 pages; accepted at AISTATS2023; minor corrections to Bayesian\n  features"},{"id":"http://arxiv.org/abs/2303.11525v1","updated":"2023-03-21T01:06:37Z","published":"2023-03-21T01:06:37Z","title":"SIFT: Sparse Iso-FLOP Transformations for Maximizing Training Efficiency","summary":"  Recent works have explored the use of weight sparsity to improve the training\nefficiency (test accuracy w.r.t training FLOPs) of deep neural networks (DNNs).\nThese works aim to reduce training FLOPs but training with sparse weights often\nleads to accuracy loss or requires longer train schedules, making the resulting\ntraining efficiency less clear. In contrast, we focus on using sparsity to\nincrease accuracy while using the same FLOPS as the dense model and show\ntraining efficiency gains through higher accuracy. In this work, we introduce\nSIFT, a family of Sparse Iso-FLOP Transformations which are used as drop-in\nreplacements for dense layers to improve their representational capacity and\nFLOP efficiency. Each transformation is parameterized by a single parameter\n(sparsity level) and provides a larger search space to find optimal sparse\nmasks. Without changing any training hyperparameters, replacing dense layers\nwith SIFT leads to significant improvements across computer vision (CV) and\nnatural language processing (NLP) tasks, including ResNet-18 on ImageNet\n(+3.5%) and GPT-3 Small on WikiText-103 (-0.4 PPL), both matching larger dense\nmodel variants with 2x or more FLOPs. To the best of our knowledge, this is the\nfirst work to demonstrate the use of sparsity for improving accuracy of dense\nmodels via a simple-to-use set of sparse transformations. Code is available at:\nhttps://github.com/CerebrasResearch/SIFT.\n","authors":["Shreyas Saxena","Vithursan Thangarasa","Abhay Gupta","Sean Lie"],"pdf_url":"https://arxiv.org/pdf/2303.11525v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2211.10381v3","updated":"2023-03-21T00:53:55Z","published":"2022-11-18T17:25:14Z","title":"Environmental Sensor Placement with Convolutional Gaussian Neural\n  Processes","summary":"  Environmental sensors are crucial for monitoring weather conditions and the\nimpacts of climate change. However, it is challenging to maximise measurement\ninformativeness and place sensors efficiently, particularly in remote regions\nlike Antarctica. Probabilistic machine learning models can evaluate placement\ninformativeness by predicting the uncertainty reduction provided by a new\nsensor. Gaussian process (GP) models are widely used for this purpose, but they\nstruggle with capturing complex non-stationary behaviour and scaling to large\ndatasets. This paper proposes using a convolutional Gaussian neural process\n(ConvGNP) to address these issues. A ConvGNP uses neural networks to\nparameterise a joint Gaussian distribution at arbitrary target locations,\nenabling flexibility and scalability. Using simulated surface air temperature\nanomaly over Antarctica as ground truth, the ConvGNP learns spatial and\nseasonal non-stationarities, outperforming a non-stationary GP baseline. In a\nsimulated sensor placement experiment, the ConvGNP better predicts the\nperformance boost obtained from new observations than GP baselines, leading to\nmore informative sensor placements. We connect our work with similar machine\nlearning and physics-based approaches and discuss steps towards an operational\nsensor placement recommendation system.\n","authors":["Tom R. Andersson","Wessel P. Bruinsma","Stratis Markou","James Requeima","Alejandro Coca-Castro","Anna Vaughan","Anna-Louise Ellis","Matthew Lazzara","Daniel C. Jones","J. Scott Hosking","Richard E. Turner"],"pdf_url":"https://arxiv.org/pdf/2211.10381v3.pdf","comment":"In review for the Climate Informatics 2023 special issue of\n  Environmental Data Science"},{"id":"http://arxiv.org/abs/2303.11522v1","updated":"2023-03-21T00:53:37Z","published":"2023-03-21T00:53:37Z","title":"Online Learning for Equilibrium Pricing in Markets under Incomplete\n  Information","summary":"  The study of market equilibria is central to economic theory, particularly in\nefficiently allocating scarce resources. However, the computation of\nequilibrium prices at which the supply of goods matches their demand typically\nrelies on having access to complete information on private attributes of\nagents, e.g., suppliers' cost functions, which are often unavailable in\npractice. Motivated by this practical consideration, we consider the problem of\nsetting equilibrium prices in the incomplete information setting wherein a\nmarket operator seeks to satisfy the customer demand for a commodity by\npurchasing the required amount from competing suppliers with privately known\ncost functions unknown to the market operator. In this incomplete information\nsetting, we consider the online learning problem of learning equilibrium prices\nover time while jointly optimizing three performance metrics -- unmet demand,\ncost regret, and payment regret -- pertinent in the context of equilibrium\npricing over a horizon of $T$ periods. We first consider the setting when\nsuppliers' cost functions are fixed and develop algorithms that achieve a\nregret of $O(\\log \\log T)$ when the customer demand is constant over time, or\n$O(\\sqrt{T} \\log \\log T)$ when the demand is variable over time. Next, we\nconsider the setting when the suppliers' cost functions can vary over time and\nillustrate that no online algorithm can achieve sublinear regret on all three\nmetrics when the market operator has no information about how the cost\nfunctions change over time. Thus, we consider an augmented setting wherein the\noperator has access to hints/contexts that, without revealing the complete\nspecification of the cost functions, reflect the variation in the cost\nfunctions over time and propose an algorithm with sublinear regret in this\naugmented setting.\n","authors":["Devansh Jalota","Haoyuan Sun","Navid Azizan"],"pdf_url":"https://arxiv.org/pdf/2303.11522v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.06075v2","updated":"2023-03-21T00:36:17Z","published":"2023-03-10T16:53:51Z","title":"Long-tailed Classification from a Bayesian-decision-theory Perspective","summary":"  Long-tailed classification poses a challenge due to its heavy imbalance in\nclass probabilities and tail-sensitivity risks with asymmetric misprediction\ncosts. Recent attempts have used re-balancing loss and ensemble methods, but\nthey are largely heuristic and depend heavily on empirical results, lacking\ntheoretical explanation. Furthermore, existing methods overlook the decision\nloss, which characterizes different costs associated with tailed classes. This\npaper presents a general and principled framework from a\nBayesian-decision-theory perspective, which unifies existing techniques\nincluding re-balancing and ensemble methods, and provides theoretical\njustifications for their effectiveness. From this perspective, we derive a\nnovel objective based on the integrated risk and a Bayesian deep-ensemble\napproach to improve the accuracy of all classes, especially the \"tail\".\nBesides, our framework allows for task-adaptive decision loss which provides\nprovably optimal decisions in varying task scenarios, along with the capability\nto quantify uncertainty. Finally, We conduct comprehensive experiments,\nincluding standard classification, tail-sensitive classification with a new\nFalse Head Rate metric, calibration, and ablation studies. Our framework\nsignificantly improves the current SOTA even on large-scale real-world datasets\nlike ImageNet.\n","authors":["Bolian Li","Ruqi Zhang"],"pdf_url":"https://arxiv.org/pdf/2303.06075v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2208.05949v2","updated":"2023-03-21T00:28:23Z","published":"2022-08-11T17:40:45Z","title":"Valid Inference after Causal Discovery","summary":"  Causal discovery and causal effect estimation are two fundamental tasks in\ncausal inference. While many methods have been developed for each task\nindividually, statistical challenges arise when applying these methods jointly:\nestimating causal effects after running causal discovery algorithms on the same\ndata leads to \"double dipping,\" invalidating the coverage guarantees of\nclassical confidence intervals. To this end, we develop tools for valid\npost-causal-discovery inference. Across empirical studies, we show that a naive\ncombination of causal discovery and subsequent inference algorithms leads to\nhighly inflated miscoverage rates; on the other hand, applying our method\nprovides reliable coverage while achieving more accurate causal discovery than\ndata splitting.\n","authors":["Paula Gradu","Tijana Zrnic","Yixin Wang","Michael I. Jordan"],"pdf_url":"https://arxiv.org/pdf/2208.05949v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.11511v1","updated":"2023-03-21T00:15:53Z","published":"2023-03-21T00:15:53Z","title":"STDLens: Model Hijacking-resilient Federated Learning for Object\n  Detection","summary":"  Federated Learning (FL) has been gaining popularity as a collaborative\nlearning framework to train deep learning-based object detection models over a\ndistributed population of clients. Despite its advantages, FL is vulnerable to\nmodel hijacking. The attacker can control how the object detection system\nshould misbehave by implanting Trojaned gradients using only a small number of\ncompromised clients in the collaborative learning process. This paper\nintroduces STDLens, a principled approach to safeguarding FL against such\nattacks. We first investigate existing mitigation mechanisms and analyze their\nfailures caused by the inherent errors in spatial clustering analysis on\ngradients. Based on the insights, we introduce a three-tier forensic framework\nto identify and expel Trojaned gradients and reclaim the performance over the\ncourse of FL. We consider three types of adaptive attacks and demonstrate the\nrobustness of STDLens against advanced adversaries. Extensive experiments show\nthat STDLens can protect FL against different model hijacking attacks and\noutperform existing methods in identifying and removing Trojaned gradients with\nsignificantly higher precision and much lower false-positive rates.\n","authors":["Ka-Ho Chow","Ling Liu","Wenqi Wei","Fatih Ilhan","Yanzhao Wu"],"pdf_url":"https://arxiv.org/pdf/2303.11511v1.pdf","comment":"CVPR 2023. Source Code: https://github.com/git-disl/STDLens"},{"id":"http://arxiv.org/abs/2303.11508v1","updated":"2023-03-21T00:04:33Z","published":"2023-03-21T00:04:33Z","title":"AI-in-the-Loop -- The impact of HMI in AI-based Application","summary":"  Artificial intelligence (AI) and human-machine interaction (HMI) are two\nkeywords that usually do not fit embedded applications. Within the steps needed\nbefore applying AI to solve a specific task, HMI is usually missing during the\nAI architecture design and the training of an AI model. The human-in-the-loop\nconcept is prevalent in all other steps of developing AI, from data analysis\nvia data selection and cleaning to performance evaluation. During AI\narchitecture design, HMI can immediately highlight unproductive layers of the\narchitecture so that lightweight network architecture for embedded applications\ncan be created easily. We show that by using this HMI, users can instantly\ndistinguish which AI architecture should be trained and evaluated first since a\nhigh accuracy on the task could be expected. This approach reduces the\nresources needed for AI development by avoiding training and evaluating AI\narchitectures with unproductive layers and leads to lightweight AI\narchitectures. These resulting lightweight AI architectures will enable HMI\nwhile running the AI on an edge device. By enabling HMI during an AI uses\ninference, we will introduce the AI-in-the-loop concept that combines AI's and\nhumans' strengths. In our AI-in-the-loop approach, the AI remains the working\nhorse and primarily solves the task. If the AI is unsure whether its inference\nsolves the task correctly, it asks the user to use an appropriate HMI.\nConsequently, AI will become available in many applications soon since HMI will\nmake AI more reliable and explainable.\n","authors":["Julius Schöning","Clemens Westerkamp"],"pdf_url":"https://arxiv.org/pdf/2303.11508v1.pdf","comment":"12 pages; 9 figures; 1 table;"},{"id":"http://arxiv.org/abs/2302.10873v2","updated":"2023-03-21T00:02:34Z","published":"2023-02-21T18:42:24Z","title":"Context-Aware Timewise VAEs for Real-Time Vehicle Trajectory Prediction","summary":"  Real-time, accurate prediction of human steering behaviors has wide\napplications, from developing intelligent traffic systems to deploying\nautonomous driving systems in both real and simulated worlds. In this paper, we\npresent ContextVAE, a context-aware approach for multi-modal vehicle trajectory\nprediction. Built upon the backbone architecture of a timewise variational\nautoencoder, ContextVAE employs a dual attention mechanism for observation\nencoding that accounts for the environmental context information and the\ndynamic agents' states in a unified way. By utilizing features extracted from\nsemantic maps during agent state encoding, our approach takes into account both\nthe social features exhibited by agents on the scene and the physical\nenvironment constraints to generate map-compliant and socially-aware\ntrajectories. We perform extensive testing on the nuScenes prediction\nchallenge, Lyft Level 5 dataset and Waymo Open Motion Dataset to show the\neffectiveness of our approach and its state-of-the-art performance. In all\ntested datasets, ContextVAE models are fast to train and provide high-quality\nmulti-modal predictions in real-time.\n","authors":["Pei Xu","Jean-Bernard Hayet","Ioannis Karamouzas"],"pdf_url":"https://arxiv.org/pdf/2302.10873v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.12238v1","updated":"2023-03-21T23:44:09Z","published":"2023-03-21T23:44:09Z","title":"DG-Trans: Dual-level Graph Transformer for Spatiotemporal Incident\n  Impact Prediction on Traffic Networks","summary":"  The prompt estimation of traffic incident impacts can guide commuters in\ntheir trip planning and improve the resilience of transportation agencies'\ndecision-making on resilience. However, it is more challenging than node-level\nand graph-level forecasting tasks, as it requires extracting the anomaly\nsubgraph or sub-time-series from dynamic graphs. In this paper, we propose\nDG-Trans, a novel traffic incident impact prediction framework, to foresee the\nimpact of traffic incidents through dynamic graph learning. The proposed\nframework contains a dual-level spatial transformer and an\nimportance-score-based temporal transformer, and the performance of this\nframework is justified by two newly constructed benchmark datasets. The\ndual-level spatial transformer removes unnecessary edges between nodes to\nisolate the affected subgraph from the other nodes. Meanwhile, the\nimportance-score-based temporal transformer identifies abnormal changes in node\nfeatures, causing the predictions to rely more on measurement changes after the\nincident occurs. Therefore, DG-Trans is equipped with dual abilities that\nextract spatiotemporal dependency and identify anomaly nodes affected by\nincidents while removing noise introduced by benign nodes. Extensive\nexperiments on real-world datasets verify that DG-Trans outperforms the\nexisting state-of-the-art methods, especially in extracting spatiotemporal\ndependency patterns and predicting traffic accident impacts. It offers\npromising potential for traffic incident management systems.\n","authors":["Yanshen Sun","Kaiqun Fu","Chang-Tien Lu"],"pdf_url":"https://arxiv.org/pdf/2303.12238v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.12233v1","updated":"2023-03-21T23:29:35Z","published":"2023-03-21T23:29:35Z","title":"Secure Aggregation in Federated Learning is not Private: Leaking User\n  Data at Large Scale through Model Modification","summary":"  Security and privacy are important concerns in machine learning. End user\ndevices often contain a wealth of data and this information is sensitive and\nshould not be shared with servers or enterprises. As a result, federated\nlearning was introduced to enable machine learning over large decentralized\ndatasets while promising privacy by eliminating the need for data sharing.\nHowever, prior work has shown that shared gradients often contain private\ninformation and attackers can gain knowledge either through malicious\nmodification of the architecture and parameters or by using optimization to\napproximate user data from the shared gradients. Despite this, most attacks\nhave so far been limited in scale of number of clients, especially failing when\nclient gradients are aggregated together using secure model aggregation. The\nattacks that still function are strongly limited in the number of clients\nattacked, amount of training samples they leak, or number of iterations they\ntake to be trained. In this work, we introduce MANDRAKE, an attack that\novercomes previous limitations to directly leak large amounts of client data\neven under secure aggregation across large numbers of clients. Furthermore, we\nbreak the anonymity of aggregation as the leaked data is identifiable and\ndirectly tied back to the clients they come from. We show that by sending\nclients customized convolutional parameters, the weight gradients of data\npoints between clients will remain separate through aggregation. With an\naggregation across many clients, prior work could only leak less than 1% of\nimages. With the same number of non-zero parameters, and using only a single\ntraining iteration, MANDRAKE leaks 70-80% of data samples.\n","authors":["Joshua C. Zhao","Atul Sharma","Ahmed Roushdy Elkordy","Yahya H. Ezzeldin","Salman Avestimehr","Saurabh Bagchi"],"pdf_url":"https://arxiv.org/pdf/2303.12233v1.pdf","comment":"20 pages"},{"id":"http://arxiv.org/abs/2212.09668v2","updated":"2023-03-21T23:01:40Z","published":"2022-12-19T17:54:36Z","title":"Task-Oriented Communications for NextG: End-to-End Deep Learning and AI\n  Security Aspects","summary":"  Communications systems to date are primarily designed with the goal of\nreliable transfer of digital sequences (bits). Next generation (NextG)\ncommunication systems are beginning to explore shifting this design paradigm to\nreliably executing a given task such as in task-oriented communications. In\nthis paper, wireless signal classification is considered as the task for the\nNextG Radio Access Network (RAN), where edge devices collect wireless signals\nfor spectrum awareness and communicate with the NextG base station (gNodeB)\nthat needs to identify the signal label. Edge devices may not have sufficient\nprocessing power and may not be trusted to perform the signal classification\ntask, whereas the transfer of signals to the gNodeB may not be feasible due to\nstringent delay, rate, and energy restrictions. Task-oriented communications is\nconsidered by jointly training the transmitter, receiver and classifier\nfunctionalities as an encoder-decoder pair for the edge device and the gNodeB.\nThis approach improves the accuracy compared to the separated case of signal\ntransfer followed by classification. Adversarial machine learning poses a major\nsecurity threat to the use of deep learning for task-oriented communications. A\nmajor performance loss is shown when backdoor (Trojan) and adversarial\n(evasion) attacks target the training and test processes of task-oriented\ncommunications.\n","authors":["Yalin E. Sagduyu","Sennur Ulukus","Aylin Yener"],"pdf_url":"https://arxiv.org/pdf/2212.09668v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.12224v1","updated":"2023-03-21T22:55:51Z","published":"2023-03-21T22:55:51Z","title":"Infrastructure-based End-to-End Learning and Prevention of Driver\n  Failure","summary":"  Intelligent intersection managers can improve safety by detecting dangerous\ndrivers or failure modes in autonomous vehicles, warning oncoming vehicles as\nthey approach an intersection. In this work, we present FailureNet, a recurrent\nneural network trained end-to-end on trajectories of both nominal and reckless\ndrivers in a scaled miniature city. FailureNet observes the poses of vehicles\nas they approach an intersection and detects whether a failure is present in\nthe autonomy stack, warning cross-traffic of potentially dangerous drivers.\nFailureNet can accurately identify control failures, upstream perception\nerrors, and speeding drivers, distinguishing them from nominal driving. The\nnetwork is trained and deployed with autonomous vehicles in the MiniCity.\nCompared to speed or frequency-based predictors, FailureNet's recurrent neural\nnetwork structure provides improved predictive power, yielding upwards of 84%\naccuracy when deployed on hardware.\n","authors":["Noam Buckman","Shiva Sreeram","Mathias Lechner","Yutong Ban","Ramin Hasani","Sertac Karaman","Daniela Rus"],"pdf_url":"https://arxiv.org/pdf/2303.12224v1.pdf","comment":"8 pages. Accepted to ICRA 2023"},{"id":"http://arxiv.org/abs/2303.00848v2","updated":"2023-03-21T22:23:51Z","published":"2023-03-01T22:36:05Z","title":"Understanding the Diffusion Objective as a Weighted Integral of ELBOs","summary":"  Diffusion models in the literature are optimized with various objectives that\nare special cases of a weighted loss, where the weighting function specifies\nthe weight per noise level. Uniform weighting corresponds to maximizing the\nELBO, a principled approximation of maximum likelihood. In current practice\ndiffusion models are optimized with non-uniform weighting due to better results\nin terms of sample quality. In this work we expose a direct relationship\nbetween the weighted loss (with any weighting) and the ELBO objective.\n  We show that the weighted loss can be written as a weighted integral of\nELBOs, with one ELBO per noise level. If the weighting function is monotonic,\nthen the weighted loss is a likelihood-based objective: it maximizes the ELBO\nunder simple data augmentation, namely Gaussian noise perturbation. Our main\ncontribution is a deeper theoretical understanding of the diffusion objective,\nbut we also performed some experiments comparing monotonic with non-monotonic\nweightings, finding that monotonic weighting performs competitively with the\nbest published results.\n","authors":["Diederik P. Kingma","Ruiqi Gao"],"pdf_url":"https://arxiv.org/pdf/2303.00848v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2208.11173v3","updated":"2023-03-21T22:20:47Z","published":"2022-08-23T20:02:09Z","title":"The Alberta Plan for AI Research","summary":"  Herein we describe our approach to artificial intelligence research, which we\ncall the Alberta Plan. The Alberta Plan is pursued within our research groups\nin Alberta and by others who are like minded throughout the world. We welcome\nall who would join us in this pursuit.\n","authors":["Richard S. Sutton","Michael Bowling","Patrick M. Pilarski"],"pdf_url":"https://arxiv.org/pdf/2208.11173v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.12212v1","updated":"2023-03-21T22:12:53Z","published":"2023-03-21T22:12:53Z","title":"Community detection in complex networks via node similarity, graph\n  representation learning, and hierarchical clustering","summary":"  Community detection is a critical challenge in the analysis of real-world\ngraphs and complex networks, including social, transportation, citation,\ncybersecurity networks, and food webs. Motivated by many similarities between\ncommunity detection and clustering in Euclidean spaces, we propose three\nalgorithm frameworks to apply hierarchical clustering methods for community\ndetection in graphs. We show that using our methods, it is possible to apply\nvarious linkage-based (single-, complete-, average- linkage, Ward, Genie)\nclustering algorithms to find communities based on vertex similarity matrices,\neigenvector matrices thereof, and Euclidean vector representations of nodes. We\nconvey a comprehensive analysis of choices for each framework, including\nstate-of-the-art graph representation learning algorithms, such as Deep Neural\nGraph Representation, and a vertex proximity matrix known to yield high-quality\nresults in machine learning -- Positive Pointwise Mutual Information. Overall,\nwe test over a hundred combinations of framework components and show that some\n-- including Wasserman-Faust and PPMI proximity, DNGR representation -- can\ncompete with algorithms such as state-of-the-art Leiden and Louvain and easily\noutperform other known community detection algorithms. Notably, our algorithms\nremain hierarchical and allow the user to specify any number of clusters a\npriori.\n","authors":["Łukasz Brzozowski","Grzegorz Siudem","Marek Gagolewski"],"pdf_url":"https://arxiv.org/pdf/2303.12212v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.12210v1","updated":"2023-03-21T21:58:59Z","published":"2023-03-21T21:58:59Z","title":"A Random Projection k Nearest Neighbours Ensemble for Classification via\n  Extended Neighbourhood Rule","summary":"  Ensembles based on k nearest neighbours (kNN) combine a large number of base\nlearners, each constructed on a sample taken from a given training data.\nTypical kNN based ensembles determine the k closest observations in the\ntraining data bounded to a test sample point by a spherical region to predict\nits class. In this paper, a novel random projection extended neighbourhood rule\n(RPExNRule) ensemble is proposed where bootstrap samples from the given\ntraining data are randomly projected into lower dimensions for additional\nrandomness in the base models and to preserve features information. It uses the\nextended neighbourhood rule (ExNRule) to fit kNN as base learners on randomly\nprojected bootstrap samples.\n","authors":["Amjad Ali","Muhammad Hamraz","Dost Muhammad Khan","Wajdan Deebani","Zardad Khan"],"pdf_url":"https://arxiv.org/pdf/2303.12210v1.pdf","comment":"23 pages, 8 diagrams, 69 references"},{"id":"http://arxiv.org/abs/2303.12208v1","updated":"2023-03-21T21:49:39Z","published":"2023-03-21T21:49:39Z","title":"MAGVLT: Masked Generative Vision-and-Language Transformer","summary":"  While generative modeling on multimodal image-text data has been actively\ndeveloped with large-scale paired datasets, there have been limited attempts to\ngenerate both image and text data by a single model rather than a generation of\none fixed modality conditioned on the other modality. In this paper, we explore\na unified generative vision-and-language (VL) model that can produce both\nimages and text sequences. Especially, we propose a generative VL transformer\nbased on the non-autoregressive mask prediction, named MAGVLT, and compare it\nwith an autoregressive generative VL transformer (ARGVLT). In comparison to\nARGVLT, the proposed MAGVLT enables bidirectional context encoding, fast\ndecoding by parallel token predictions in an iterative refinement, and extended\nediting capabilities such as image and text infilling. For rigorous training of\nour MAGVLT with image-text pairs from scratch, we combine the image-to-text,\ntext-to-image, and joint image-and-text mask prediction tasks. Moreover, we\ndevise two additional tasks based on the step-unrolled mask prediction and the\nselective prediction on the mixture of two image-text pairs. Experimental\nresults on various downstream generation tasks of VL benchmarks show that our\nMAGVLT outperforms ARGVLT by a large margin even with significant inference\nspeedup. Particularly, MAGVLT achieves competitive results on both zero-shot\nimage-to-text and text-to-image generation tasks from MS-COCO by one\nmoderate-sized model (fewer than 500M parameters) even without the use of\nmonomodal data and networks.\n","authors":["Sungwoong Kim","Daejin Jo","Donghoon Lee","Jongmin Kim"],"pdf_url":"https://arxiv.org/pdf/2303.12208v1.pdf","comment":"CVPR 2023"},{"id":"http://arxiv.org/abs/2303.12206v1","updated":"2023-03-21T21:42:03Z","published":"2023-03-21T21:42:03Z","title":"Policy Optimization for Personalized Interventions in Behavioral Health","summary":"  Problem definition: Behavioral health interventions, delivered through\ndigital platforms, have the potential to significantly improve health outcomes,\nthrough education, motivation, reminders, and outreach. We study the problem of\noptimizing personalized interventions for patients to maximize some long-term\noutcome, in a setting where interventions are costly and capacity-constrained.\n  Methodology/results: This paper provides a model-free approach to solving\nthis problem. We find that generic model-free approaches from the reinforcement\nlearning literature are too data intensive for healthcare applications, while\nsimpler bandit approaches make progress at the expense of ignoring long-term\npatient dynamics. We present a new algorithm we dub DecompPI that approximates\none step of policy iteration. Implementing DecompPI simply consists of a\nprediction task from offline data, alleviating the need for online\nexperimentation. Theoretically, we show that under a natural set of structural\nassumptions on patient dynamics, DecompPI surprisingly recovers at least 1/2 of\nthe improvement possible between a naive baseline policy and the optimal\npolicy. At the same time, DecompPI is both robust to estimation errors and\ninterpretable. Through an empirical case study on a mobile health platform for\nimproving treatment adherence for tuberculosis, we find that DecompPI can\nprovide the same efficacy as the status quo with approximately half the\ncapacity of interventions.\n  Managerial implications: DecompPI is general and is easily implementable for\norganizations aiming to improve long-term behavior through targeted\ninterventions. Our case study suggests that the platform's costs of deploying\ninterventions can potentially be cut by 50%, which facilitates the ability to\nscale up the system in a cost-efficient fashion.\n","authors":["Jackie Baek","Justin J. Boutilier","Vivek F. Farias","Jonas Oddur Jonasson","Erez Yoeli"],"pdf_url":"https://arxiv.org/pdf/2303.12206v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.12199v1","updated":"2023-03-21T21:12:29Z","published":"2023-03-21T21:12:29Z","title":"Analytical Conjugate Priors for Subclasses of Generalized Pareto\n  Distributions","summary":"  This article is written for pedagogical purposes aiming at practitioners\ntrying to estimate the finite support of continuous probability distributions,\ni.e., the minimum and the maximum of a distribution defined on a finite domain.\nGeneralized Pareto distribution GP({\\theta}, {\\sigma}, {\\xi}) is a\nthree-parameter distribution which plays a key role in Peaks-Over-Threshold\nframework for tail estimation in Extreme Value Theory. Estimators for GP often\nlack analytical solutions and the best known Bayesian methods for GP involves\nnumerical methods. Moreover, existing literature focuses on estimating the\nscale {\\sigma} and the shape {\\xi}, lacking discussion of the estimation of the\nlocation {\\theta} which is the lower support of (minimum value possible in) a\nGP. To fill the gap, we analyze four two-parameter subclasses of GP whose\nconjugate priors can be obtained analytically, although some of the results are\nknown. Namely, we prove the conjugacy for {\\xi} > 0 (Pareto), {\\xi} = 0\n(Shifted Exponential), {\\xi} < 0 (Power), and {\\xi} = -1 (Two-parameter\nUniform).\n","authors":["Masataro Asai"],"pdf_url":"https://arxiv.org/pdf/2303.12199v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2301.13349v2","updated":"2023-03-21T21:01:46Z","published":"2023-01-31T00:52:14Z","title":"Unconstrained Dynamic Regret via Sparse Coding","summary":"  Motivated by time series forecasting, we study Online Linear Optimization\n(OLO) under the coupling of two problem structures: the domain is unbounded,\nand the performance of an algorithm is measured by its dynamic regret. Handling\neither of them requires the regret bound to depend on certain complexity\nmeasure of the comparator sequence -- specifically, the comparator norm in\nunconstrained OLO, and the path length in dynamic regret. In contrast to a\nrecent work (Jacobsen & Cutkosky, 2022) that adapts to the combination of these\ntwo complexity measures, we propose an alternative complexity measure by\nrecasting the problem into sparse coding. Adaptivity can be achieved by a\nsimple modular framework, which naturally exploits more intricate prior\nknowledge of the environment. Along the way, we also present a new gradient\nadaptive algorithm for static unconstrained OLO, designed using novel\ncontinuous time machinery. This could be of independent interest.\n","authors":["Zhiyu Zhang","Ashok Cutkosky","Ioannis Ch. Paschalidis"],"pdf_url":"https://arxiv.org/pdf/2301.13349v2.pdf","comment":"Added experiments"},{"id":"http://arxiv.org/abs/2208.00277v4","updated":"2023-03-21T20:05:37Z","published":"2022-07-30T17:14:14Z","title":"MobileNeRF: Exploiting the Polygon Rasterization Pipeline for Efficient\n  Neural Field Rendering on Mobile Architectures","summary":"  Neural Radiance Fields (NeRFs) have demonstrated amazing ability to\nsynthesize images of 3D scenes from novel views. However, they rely upon\nspecialized volumetric rendering algorithms based on ray marching that are\nmismatched to the capabilities of widely deployed graphics hardware. This paper\nintroduces a new NeRF representation based on textured polygons that can\nsynthesize novel images efficiently with standard rendering pipelines. The NeRF\nis represented as a set of polygons with textures representing binary opacities\nand feature vectors. Traditional rendering of the polygons with a z-buffer\nyields an image with features at every pixel, which are interpreted by a small,\nview-dependent MLP running in a fragment shader to produce a final pixel color.\nThis approach enables NeRFs to be rendered with the traditional polygon\nrasterization pipeline, which provides massive pixel-level parallelism,\nachieving interactive frame rates on a wide range of compute platforms,\nincluding mobile phones.\n","authors":["Zhiqin Chen","Thomas Funkhouser","Peter Hedman","Andrea Tagliasacchi"],"pdf_url":"https://arxiv.org/pdf/2208.00277v4.pdf","comment":"CVPR 2023. Project page: https://mobile-nerf.github.io, code:\n  https://github.com/google-research/jax3d/tree/main/jax3d/projects/mobilenerf"},{"id":"http://arxiv.org/abs/2211.15387v2","updated":"2023-03-21T19:47:52Z","published":"2022-11-24T10:02:38Z","title":"AIREPAIR: A Repair Platform for Neural Networks","summary":"  We present AIREPAIR, a platform for repairing neural networks. It features\nthe integration of existing network repair tools. Based on AIREPAIR, one can\nrun different repair methods on the same model, thus enabling the fair\ncomparison of different repair techniques. We evaluate AIREPAIR with three\nstate-of-the-art repair tools on popular deep-learning datasets and models. Our\nevaluation confirms the utility of AIREPAIR, by comparing and analyzing the\nresults from different repair techniques. A demonstration is available at\nhttps://youtu.be/UkKw5neeWhw.\n","authors":["Xidan Song","Youcheng Sun","Mustafa A. Mustafa","Lucas Cordeiro"],"pdf_url":"https://arxiv.org/pdf/2211.15387v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.12164v1","updated":"2023-03-21T19:45:59Z","published":"2023-03-21T19:45:59Z","title":"Viscoelastic Constitutive Artificial Neural Networks (vCANNs) $-$ a\n  framework for data-driven anisotropic nonlinear finite viscoelasticity","summary":"  The constitutive behavior of polymeric materials is often modeled by finite\nlinear viscoelastic (FLV) or quasi-linear viscoelastic (QLV) models. These\npopular models are simplifications that typically cannot accurately capture the\nnonlinear viscoelastic behavior of materials. For example, the success of\nattempts to capture strain rate-dependent behavior has been limited so far. To\novercome this problem, we introduce viscoelastic Constitutive Artificial Neural\nNetworks (vCANNs), a novel physics-informed machine learning framework for\nanisotropic nonlinear viscoelasticity at finite strains. vCANNs rely on the\nconcept of generalized Maxwell models enhanced with nonlinear strain\n(rate)-dependent properties represented by neural networks. The flexibility of\nvCANNs enables them to automatically identify accurate and sparse constitutive\nmodels of a broad range of materials. To test vCANNs, we trained them on\nstress-strain data from Polyvinyl Butyral, the electro-active polymers VHB 4910\nand 4905, and a biological tissue, the rectus abdominis muscle. Different\nloading conditions were considered, including relaxation tests, cyclic\ntension-compression tests, and blast loads. We demonstrate that vCANNs can\nlearn to capture the behavior of all these materials accurately and\ncomputationally efficiently without human guidance.\n","authors":["Kian P. Abdolazizi","Kevin Linka","Christian J. Cyron"],"pdf_url":"https://arxiv.org/pdf/2303.12164v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.12157v1","updated":"2023-03-21T19:34:20Z","published":"2023-03-21T19:34:20Z","title":"Learning a Depth Covariance Function","summary":"  We propose learning a depth covariance function with applications to\ngeometric vision tasks. Given RGB images as input, the covariance function can\nbe flexibly used to define priors over depth functions, predictive\ndistributions given observations, and methods for active point selection. We\nleverage these techniques for a selection of downstream tasks: depth\ncompletion, bundle adjustment, and monocular dense visual odometry.\n","authors":["Eric Dexheimer","Andrew J. Davison"],"pdf_url":"https://arxiv.org/pdf/2303.12157v1.pdf","comment":"CVPR 2023. Project page: https://edexheim.github.io/depth_cov/"},{"id":"http://arxiv.org/abs/2210.12239v3","updated":"2023-03-21T19:33:46Z","published":"2022-10-21T20:41:44Z","title":"Auto-Encoder Neural Network Incorporating X-Ray Fluorescence Fundamental\n  Parameters with Machine Learning","summary":"  We consider energy-dispersive X-ray Fluorescence (EDXRF) applications where\nthe fundamental parameters method is impractical such as when instrument\nparameters are unavailable. For example, on a mining shovel or conveyor belt,\nrocks are constantly moving (leading to varying angles of incidence and\ndistances) and there may be other factors not accounted for (like dust). Neural\nnetworks do not require instrument and fundamental parameters but training\nneural networks requires XRF spectra labelled with elemental composition, which\nis often limited because of its expense. We develop a neural network model that\nlearns from limited labelled data and also benefits from domain knowledge by\nlearning to invert a forward model. The forward model uses transition energies\nand probabilities of all elements and parameterized distributions to\napproximate other fundamental and instrument parameters. We evaluate the model\nand baseline models on a rock dataset from a lithium mineral exploration\nproject. Our model works particularly well for some low-Z elements (Li, Mg, Al,\nand K) as well as some high-Z elements (Sn and Pb) despite these elements being\noutside the suitable range for common spectrometers to directly measure, likely\nowing to the ability of neural networks to learn correlations and non-linear\nrelationships.\n","authors":["Matthew Dirks","David Poole"],"pdf_url":"https://arxiv.org/pdf/2210.12239v3.pdf","comment":"X-Ray Spectrometry 2023"},{"id":"http://arxiv.org/abs/2302.05185v3","updated":"2023-03-21T19:25:54Z","published":"2023-02-10T11:30:19Z","title":"On Penalty-based Bilevel Gradient Descent Method","summary":"  Bilevel optimization enjoys a wide range of applications in hyper-parameter\noptimization, meta-learning and reinforcement learning. However, bilevel\noptimization problems are difficult to solve. Recent progress on scalable\nbilevel algorithms mainly focuses on bilevel optimization problems where the\nlower-level objective is either strongly convex or unconstrained. In this work,\nwe tackle the bilevel problem through the lens of the penalty method. We show\nthat under certain conditions, the penalty reformulation recovers the solutions\nof the original bilevel problem. Further, we propose the penalty-based bilevel\ngradient descent (PBGD) algorithm and establish its finite-time convergence for\nthe constrained bilevel problem without lower-level strong convexity.\nExperiments showcase the efficiency of the proposed PBGD algorithm.\n","authors":["Han Shen","Quan Xiao","Tianyi Chen"],"pdf_url":"https://arxiv.org/pdf/2302.05185v3.pdf","comment":"Improved Section 4 by removing a critical assumption; Added citations"},{"id":"http://arxiv.org/abs/2303.12148v1","updated":"2023-03-21T19:10:21Z","published":"2023-03-21T19:10:21Z","title":"Neural Pre-Processing: A Learning Framework for End-to-end Brain MRI\n  Pre-processing","summary":"  Head MRI pre-processing involves converting raw images to an\nintensity-normalized, skull-stripped brain in a standard coordinate space. In\nthis paper, we propose an end-to-end weakly supervised learning approach,\ncalled Neural Pre-processing (NPP), for solving all three sub-tasks\nsimultaneously via a neural network, trained on a large dataset without\nindividual sub-task supervision. Because the overall objective is highly\nunder-constrained, we explicitly disentangle geometric-preserving intensity\nmapping (skull-stripping and intensity normalization) and spatial\ntransformation (spatial normalization). Quantitative results show that our\nmodel outperforms state-of-the-art methods which tackle only a single sub-task.\nOur ablation experiments demonstrate the importance of the architecture design\nwe chose for NPP. Furthermore, NPP affords the user the flexibility to control\neach of these tasks at inference time. The code and model are freely-available\nat \\url{https://github.com/Novestars/Neural-Pre-processing}.\n","authors":["Xinzi He","Alan Wang","Mert R. Sabuncu"],"pdf_url":"https://arxiv.org/pdf/2303.12148v1.pdf","comment":"8"},{"id":"http://arxiv.org/abs/2303.12147v1","updated":"2023-03-21T19:10:09Z","published":"2023-03-21T19:10:09Z","title":"Universal Approximation Property of Hamiltonian Deep Neural Networks","summary":"  This paper investigates the universal approximation capabilities of\nHamiltonian Deep Neural Networks (HDNNs) that arise from the discretization of\nHamiltonian Neural Ordinary Differential Equations. Recently, it has been shown\nthat HDNNs enjoy, by design, non-vanishing gradients, which provide numerical\nstability during training. However, although HDNNs have demonstrated\nstate-of-the-art performance in several applications, a comprehensive study to\nquantify their expressivity is missing. In this regard, we provide a universal\napproximation theorem for HDNNs and prove that a portion of the flow of HDNNs\ncan approximate arbitrary well any continuous function over a compact domain.\nThis result provides a solid theoretical foundation for the practical use of\nHDNNs.\n","authors":["Muhammad Zakwan","Massimiliano d'Angelo","Giancarlo Ferrari-Trecate"],"pdf_url":"https://arxiv.org/pdf/2303.12147v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.12136v1","updated":"2023-03-21T18:51:17Z","published":"2023-03-21T18:51:17Z","title":"Improving Fabrication Fidelity of Integrated Nanophotonic Devices Using\n  Deep Learning","summary":"  Next-generation integrated nanophotonic device designs leverage advanced\noptimization techniques such as inverse design and topology optimization which\nachieve high performance and extreme miniaturization by optimizing a massively\ncomplex design space enabled by small feature sizes. However, unless the\noptimization is heavily constrained, the generated small features are not\nreliably fabricated, leading to optical performance degradation. Even for\nsimpler, conventional designs, fabrication-induced performance degradation\nstill occurs. The degree of deviation from the original design not only depends\non the size and shape of its features, but also on the distribution of features\nand the surrounding environment, presenting complex, proximity-dependent\nbehavior. Without proprietary fabrication process specifications, design\ncorrections can only be made after calibrating fabrication runs take place. In\nthis work, we introduce a general deep machine learning model that\nautomatically corrects photonic device design layouts prior to first\nfabrication. Only a small set of scanning electron microscopy images of\nengineered training features are required to create the deep learning model.\nWith correction, the outcome of the fabricated layout is closer to what is\nintended, and thus so too is the performance of the design. Without modifying\nthe nanofabrication process, adding significant computation in design, or\nrequiring proprietary process specifications, we believe our model opens the\ndoor to new levels of reliability and performance in next-generation photonic\ncircuits.\n","authors":["Dusan Gostimirovic","Yuri Grinberg","Dan-Xia Xu","Odile Liboiron-Ladouceur"],"pdf_url":"https://arxiv.org/pdf/2303.12136v1.pdf","comment":"13 pages, 8 figures"},{"id":"http://arxiv.org/abs/2303.12132v1","updated":"2023-03-21T18:45:09Z","published":"2023-03-21T18:45:09Z","title":"Fundamentals of Generative Large Language Models and Perspectives in\n  Cyber-Defense","summary":"  Generative Language Models gained significant attention in late 2022 / early\n2023, notably with the introduction of models refined to act consistently with\nusers' expectations of interactions with AI (conversational models). Arguably\nthe focal point of public attention has been such a refinement of the GPT3\nmodel -- the ChatGPT and its subsequent integration with auxiliary\ncapabilities, including search as part of Microsoft Bing. Despite extensive\nprior research invested in their development, their performance and\napplicability to a range of daily tasks remained unclear and niche. However,\ntheir wider utilization without a requirement for technical expertise, made in\nlarge part possible through conversational fine-tuning, revealed the extent of\ntheir true capabilities in a real-world environment. This has garnered both\npublic excitement for their potential applications and concerns about their\ncapabilities and potential malicious uses. This review aims to provide a brief\noverview of the history, state of the art, and implications of Generative\nLanguage Models in terms of their principles, abilities, limitations, and\nfuture prospects -- especially in the context of cyber-defense, with a focus on\nthe Swiss operational environment.\n","authors":["Andrei Kucharavy","Zachary Schillaci","Loïc Maréchal","Maxime Würsch","Ljiljana Dolamic","Remi Sabonnadiere","Dimitri Percia David","Alain Mermoud","Vincent Lenders"],"pdf_url":"https://arxiv.org/pdf/2303.12132v1.pdf","comment":"41 pages (without references), 13 figures; public report of\n  Cyber-Defence Campus"},{"id":"http://arxiv.org/abs/2302.11953v2","updated":"2023-03-21T18:38:10Z","published":"2023-02-23T12:02:49Z","title":"MFBE: Leveraging Multi-Field Information of FAQs for Efficient Dense\n  Retrieval","summary":"  In the domain of question-answering in NLP, the retrieval of Frequently Asked\nQuestions (FAQ) is an important sub-area which is well researched and has been\nworked upon for many languages. Here, in response to a user query, a retrieval\nsystem typically returns the relevant FAQs from a knowledge-base. The efficacy\nof such a system depends on its ability to establish semantic match between the\nquery and the FAQs in real-time. The task becomes challenging due to the\ninherent lexical gap between queries and FAQs, lack of sufficient context in\nFAQ titles, scarcity of labeled data and high retrieval latency. In this work,\nwe propose a bi-encoder-based query-FAQ matching model that leverages multiple\ncombinations of FAQ fields (like, question, answer, and category) both during\nmodel training and inference. Our proposed Multi-Field Bi-Encoder (MFBE) model\nbenefits from the additional context resulting from multiple FAQ fields and\nperforms well even with minimal labeled data. We empirically support this claim\nthrough experiments on proprietary as well as open-source public datasets in\nboth unsupervised and supervised settings. Our model achieves around 27% and\n20% better top-1 accuracy for the FAQ retrieval task on internal and open\ndatasets, respectively over the best performing baseline.\n","authors":["Debopriyo Banerjee","Mausam Jain","Ashish Kulkarni"],"pdf_url":"https://arxiv.org/pdf/2302.11953v2.pdf","comment":"The first two authors contributed equally to this work. 12 pages, 3\n  figures, 5 tables. Accepted at the 2023 Pacific-Asia Conference On Knowledge\n  Discovery And Data Mining (PAKDD)"},{"id":"http://arxiv.org/abs/2206.14057v3","updated":"2023-03-21T18:26:14Z","published":"2022-06-28T15:00:45Z","title":"Safe Exploration Incurs Nearly No Additional Sample Complexity for\n  Reward-free RL","summary":"  Reward-free reinforcement learning (RF-RL), a recently introduced RL\nparadigm, relies on random action-taking to explore the unknown environment\nwithout any reward feedback information. While the primary goal of the\nexploration phase in RF-RL is to reduce the uncertainty in the estimated model\nwith minimum number of trajectories, in practice, the agent often needs to\nabide by certain safety constraint at the same time. It remains unclear how\nsuch safe exploration requirement would affect the corresponding sample\ncomplexity in order to achieve the desired optimality of the obtained policy in\nplanning. In this work, we make a first attempt to answer this question. In\nparticular, we consider the scenario where a safe baseline policy is known\nbeforehand, and propose a unified Safe reWard-frEe ExploraTion (SWEET)\nframework. We then particularize the SWEET framework to the tabular and the\nlow-rank MDP settings, and develop algorithms coined Tabular-SWEET and\nLow-rank-SWEET, respectively. Both algorithms leverage the concavity and\ncontinuity of the newly introduced truncated value functions, and are\nguaranteed to achieve zero constraint violation during exploration with high\nprobability. Furthermore, both algorithms can provably find a near-optimal\npolicy subject to any constraint in the planning phase. Remarkably, the sample\ncomplexities under both algorithms match or even outperform the state of the\nart in their constraint-free counterparts up to some constant factors, proving\nthat safety constraint hardly increases the sample complexity for RF-RL.\n","authors":["Ruiquan Huang","Jing Yang","Yingbin Liang"],"pdf_url":"https://arxiv.org/pdf/2206.14057v3.pdf","comment":"Accepted by ICLR 2023"},{"id":"http://arxiv.org/abs/2107.02266v3","updated":"2023-03-21T18:18:30Z","published":"2021-07-05T21:05:11Z","title":"Near-optimal inference in adaptive linear regression","summary":"  When data is collected in an adaptive manner, even simple methods like\nordinary least squares can exhibit non-normal asymptotic behavior. As an\nundesirable consequence, hypothesis tests and confidence intervals based on\nasymptotic normality can lead to erroneous results. We propose a family of\nonline debiasing estimators to correct these distributional anomalies in least\nsquares estimation. Our proposed methods take advantage of the covariance\nstructure present in the dataset and provide sharper estimates in directions\nfor which more information has accrued. We establish an asymptotic normality\nproperty for our proposed online debiasing estimators under mild conditions on\nthe data collection process and provide asymptotically exact confidence\nintervals. We additionally prove a minimax lower bound for the adaptive linear\nregression problem, thereby providing a baseline by which to compare\nestimators. There are various conditions under which our proposed estimators\nachieve the minimax lower bound. We demonstrate the usefulness of our theory\nvia applications to multi-armed bandit, autoregressive time series estimation,\nand active learning with exploration.\n","authors":["Koulik Khamaru","Yash Deshpande","Tor Lattimore","Lester Mackey","Martin J. Wainwright"],"pdf_url":"https://arxiv.org/pdf/2107.02266v3.pdf","comment":"51 pages, 7 figures"},{"id":"http://arxiv.org/abs/2303.12513v1","updated":"2023-03-21T17:30:40Z","published":"2023-03-21T17:30:40Z","title":"Is BERT Blind? Exploring the Effect of Vision-and-Language Pretraining\n  on Visual Language Understanding","summary":"  Most humans use visual imagination to understand and reason about language,\nbut models such as BERT reason about language using knowledge acquired during\ntext-only pretraining. In this work, we investigate whether vision-and-language\npretraining can improve performance on text-only tasks that involve implicit\nvisual reasoning, focusing primarily on zero-shot probing methods. We propose a\nsuite of visual language understanding (VLU) tasks for probing the visual\nreasoning abilities of text encoder models, as well as various non-visual\nnatural language understanding (NLU) tasks for comparison. We also contribute a\nnovel zero-shot knowledge probing method, Stroop probing, for applying models\nsuch as CLIP to text-only tasks without needing a prediction head such as the\nmasked language modelling head of models like BERT. We show that SOTA\nmultimodally trained text encoders outperform unimodally trained text encoders\non the VLU tasks while being underperformed by them on the NLU tasks, lending\nnew context to previously mixed results regarding the NLU capabilities of\nmultimodal models. We conclude that exposure to images during pretraining\naffords inherent visual reasoning knowledge that is reflected in language-only\ntasks that require implicit visual reasoning. Our findings bear importance in\nthe broader context of multimodal learning, providing principled guidelines for\nthe choice of text encoders used in such contexts.\n","authors":["Morris Alper","Michael Fiman","Hadar Averbuch-Elor"],"pdf_url":"https://arxiv.org/pdf/2303.12513v1.pdf","comment":"To be presented in CVPR 2023. Project webpage:\n  https://isbertblind.github.io/"},{"id":"http://arxiv.org/abs/2303.12097v1","updated":"2023-03-21T15:57:46Z","published":"2023-03-21T15:57:46Z","title":"CLSA: Contrastive Learning-based Survival Analysis for Popularity\n  Prediction in MEC Networks","summary":"  Mobile Edge Caching (MEC) integrated with Deep Neural Networks (DNNs) is an\ninnovative technology with significant potential for the future generation of\nwireless networks, resulting in a considerable reduction in users' latency. The\nMEC network's effectiveness, however, heavily relies on its capacity to predict\nand dynamically update the storage of caching nodes with the most popular\ncontents. To be effective, a DNN-based popularity prediction model needs to\nhave the ability to understand the historical request patterns of content,\nincluding their temporal and spatial correlations. Existing state-of-the-art\ntime-series DNN models capture the latter by simultaneously inputting the\nsequential request patterns of multiple contents to the network, considerably\nincreasing the size of the input sample. This motivates us to address this\nchallenge by proposing a DNN-based popularity prediction framework based on the\nidea of contrasting input samples against each other, designed for the Unmanned\nAerial Vehicle (UAV)-aided MEC networks. Referred to as the Contrastive\nLearning-based Survival Analysis (CLSA), the proposed architecture consists of\na self-supervised Contrastive Learning (CL) model, where the temporal\ninformation of sequential requests is learned using a Long Short Term Memory\n(LSTM) network as the encoder of the CL architecture. Followed by a Survival\nAnalysis (SA) network, the output of the proposed CLSA architecture is\nprobabilities for each content's future popularity, which are then sorted in\ndescending order to identify the Top-K popular contents. Based on the\nsimulation results, the proposed CLSA architecture outperforms its counterparts\nacross the classification accuracy and cache-hit ratio.\n","authors":["Zohreh Hajiakhondi-Meybodi","Arash Mohammadi","Jamshid Abouei","Konstantinos N. Plataniotis"],"pdf_url":"https://arxiv.org/pdf/2303.12097v1.pdf","comment":null}],"Multimedia":[{"id":"http://arxiv.org/abs/2303.11879v1","updated":"2023-03-21T14:23:46Z","published":"2023-03-21T14:23:46Z","title":"Multimodal Pre-training Framework for Sequential Recommendation via\n  Contrastive Learning","summary":"  Sequential recommendation systems utilize the sequential interactions of\nusers with items as their main supervision signals in learning users'\npreferences. However, existing methods usually generate unsatisfactory results\ndue to the sparsity of user behavior data. To address this issue, we propose a\nnovel pre-training framework, named Multimodal Sequence Mixup for Sequential\nRecommendation (MSM4SR), which leverages both users' sequential behaviors and\nitems' multimodal content (\\ie text and images) for effectively recommendation.\nSpecifically, MSM4SR tokenizes each item image into multiple textual keywords\nand uses the pre-trained BERT model to obtain initial textual and visual\nfeatures of items, for eliminating the discrepancy between the text and image\nmodalities. A novel backbone network, \\ie Multimodal Mixup Sequence Encoder\n(M$^2$SE), is proposed to bridge the gap between the item multimodal content\nand the user behavior, using a complementary sequence mixup strategy. In\naddition, two contrastive learning tasks are developed to assist M$^2$SE in\nlearning generalized multimodal representations of the user behavior sequence.\nExtensive experiments on real-world datasets demonstrate that MSM4SR\noutperforms state-of-the-art recommendation methods. Moreover, we further\nverify the effectiveness of MSM4SR on other challenging tasks including\ncold-start and cross-domain recommendation.\n","authors":["Lingzi Zhang","Xin Zhou","Zhiqi Shen"],"pdf_url":"https://arxiv.org/pdf/2303.11879v1.pdf","comment":"10 pages, 4 figures"},{"id":"http://arxiv.org/abs/2302.08722v3","updated":"2023-03-21T12:59:20Z","published":"2023-02-17T06:33:06Z","title":"GPT4MIA: Utilizing Generative Pre-trained Transformer (GPT-3) as A\n  Plug-and-Play Transductive Model for Medical Image Analysis","summary":"  In this paper, we propose a novel approach (called GPT4MIA) that utilizes\nGenerative Pre-trained Transformer (GPT) as a plug-and-play transductive\ninference tool for medical image analysis (MIA). We provide theoretical\nanalysis on why a large pre-trained language model such as GPT-3 can be used as\na plug-and-play transductive inference model for MIA. At the methodological\nlevel, we develop several technical treatments to improve the efficiency and\neffectiveness of GPT4MIA, including better prompt structure design, sample\nselection, and prompt ordering of representative samples/features. We present\ntwo concrete use cases (with workflow) of GPT4MIA: (1) detecting prediction\nerrors and (2) improving prediction accuracy, working in conjecture with\nwell-established vision-based models for image classification (e.g., ResNet).\nExperiments validate that our proposed method is effective for these two tasks.\nWe further discuss the opportunities and challenges in utilizing\nTransformer-based large language models for broader MIA applications.\n","authors":["Yizhe Zhang","Danny Z. Chen"],"pdf_url":"https://arxiv.org/pdf/2302.08722v3.pdf","comment":"Version 3: Added appendix with more results and visualizations.\n  Questions and suggestions are welcome"},{"id":"http://arxiv.org/abs/2303.11717v1","updated":"2023-03-21T10:09:47Z","published":"2023-03-21T10:09:47Z","title":"A Complete Survey on Generative AI (AIGC): Is ChatGPT from GPT-4 to\n  GPT-5 All You Need?","summary":"  As ChatGPT goes viral, generative AI (AIGC, a.k.a AI-generated content) has\nmade headlines everywhere because of its ability to analyze and create text,\nimages, and beyond. With such overwhelming media coverage, it is almost\nimpossible for us to miss the opportunity to glimpse AIGC from a certain angle.\nIn the era of AI transitioning from pure analysis to creation, it is worth\nnoting that ChatGPT, with its most recent language model GPT-4, is just a tool\nout of numerous AIGC tasks. Impressed by the capability of the ChatGPT, many\npeople are wondering about its limits: can GPT-5 (or other future GPT variants)\nhelp ChatGPT unify all AIGC tasks for diversified content creation? Toward\nanswering this question, a comprehensive review of existing AIGC tasks is\nneeded. As such, our work comes to fill this gap promptly by offering a first\nlook at AIGC, ranging from its techniques to applications. Modern generative AI\nrelies on various technical foundations, ranging from model architecture and\nself-supervised pretraining to generative modeling methods (like GAN and\ndiffusion models). After introducing the fundamental techniques, this work\nfocuses on the technological development of various AIGC tasks based on their\noutput type, including text, images, videos, 3D content, etc., which depicts\nthe full potential of ChatGPT's future. Moreover, we summarize their\nsignificant applications in some mainstream industries, such as education and\ncreativity content. Finally, we discuss the challenges currently faced and\npresent an outlook on how generative AI might evolve in the near future.\n","authors":["Chaoning Zhang","Chenshuang Zhang","Sheng Zheng","Yu Qiao","Chenghao Li","Mengchun Zhang","Sumit Kumar Dam","Chu Myaet Thwal","Ye Lin Tun","Le Luang Huy","Donguk kim","Sung-Ho Bae","Lik-Hang Lee","Yang Yang","Heng Tao Shen","In So Kweon","Choong Seon Hong"],"pdf_url":"https://arxiv.org/pdf/2303.11717v1.pdf","comment":"56 pages, 548 citations"},{"id":"http://arxiv.org/abs/2303.11599v1","updated":"2023-03-21T05:34:04Z","published":"2023-03-21T05:34:04Z","title":"Low-complexity Deep Video Compression with A Distributed Coding\n  Architecture","summary":"  Prevalent predictive coding-based video compression methods rely on a heavy\nencoder to reduce the temporal redundancy, which makes it challenging to deploy\nthem on resource-constrained devices. Meanwhile, as early as the 1970s,\ndistributed source coding theory has indicated that independent encoding and\njoint decoding with side information (SI) can achieve high-efficient\ncompression of correlated sources. This has inspired a distributed coding\narchitecture aiming at reducing the encoding complexity. However, traditional\ndistributed coding methods suffer from a substantial performance gap to\npredictive coding ones. Inspired by the great success of learning-based\ncompression, we propose the first end-to-end distributed deep video compression\nframework to improve the rate-distortion performance. A key ingredient is an\neffective SI generation module at the decoder, which helps to effectively\nexploit inter-frame correlations without computation-intensive encoder-side\nmotion estimation and compensation. Experiments show that our method\nsignificantly outperforms conventional distributed video coding and H.264.\nMeanwhile, it enjoys 6-7x encoding speedup against DVC [1] with comparable\ncompression performance. Code is released at\nhttps://github.com/Xinjie-Q/Distributed-DVC.\n","authors":["Xinjie Zhang","Jiawei Shao","Jun Zhang"],"pdf_url":"https://arxiv.org/pdf/2303.11599v1.pdf","comment":"Accepted by ICME 2023"},{"id":"http://arxiv.org/abs/2303.11591v1","updated":"2023-03-21T04:42:39Z","published":"2023-03-21T04:42:39Z","title":"SVCNet: Scribble-based Video Colorization Network with Temporal\n  Aggregation","summary":"  In this paper, we propose a scribble-based video colorization network with\ntemporal aggregation called SVCNet. It can colorize monochrome videos based on\ndifferent user-given color scribbles. It addresses three common issues in the\nscribble-based video colorization area: colorization vividness, temporal\nconsistency, and color bleeding. To improve the colorization quality and\nstrengthen the temporal consistency, we adopt two sequential sub-networks in\nSVCNet for precise colorization and temporal smoothing, respectively. The first\nstage includes a pyramid feature encoder to incorporate color scribbles with a\ngrayscale frame, and a semantic feature encoder to extract semantics. The\nsecond stage finetunes the output from the first stage by aggregating the\ninformation of neighboring colorized frames (as short-range connections) and\nthe first colorized frame (as a long-range connection). To alleviate the color\nbleeding artifacts, we learn video colorization and segmentation\nsimultaneously. Furthermore, we set the majority of operations on a fixed small\nimage resolution and use a Super-resolution Module at the tail of SVCNet to\nrecover original sizes. It allows the SVCNet to fit different image resolutions\nat the inference. Finally, we evaluate the proposed SVCNet on DAVIS and Videvo\nbenchmarks. The experimental results demonstrate that SVCNet produces both\nhigher-quality and more temporally consistent videos than other well-known\nvideo colorization approaches. The codes and models can be found at\nhttps://github.com/zhaoyuzhi/SVCNet.\n","authors":["Yuzhi Zhao","Lai-Man Po","Kangcheng Liu","Xuehui Wang","Wing-Yin Yu","Pengfei Xian","Yujia Zhang","Mengyang Liu"],"pdf_url":"https://arxiv.org/pdf/2303.11591v1.pdf","comment":"under revision of IEEE Transactions on Image Processing"},{"id":"http://arxiv.org/abs/2303.12112v1","updated":"2023-03-21T18:03:14Z","published":"2023-03-21T18:03:14Z","title":"Positive-Augmented Constrastive Learning for Image and Video Captioning\n  Evaluation","summary":"  The CLIP model has been recently proven to be very effective for a variety of\ncross-modal tasks, including the evaluation of captions generated from\nvision-and-language architectures. In this paper, we propose a new recipe for a\ncontrastive-based evaluation metric for image captioning, namely\nPositive-Augmented Contrastive learning Score (PAC-S), that in a novel way\nunifies the learning of a contrastive visual-semantic space with the addition\nof generated images and text on curated data. Experiments spanning several\ndatasets demonstrate that our new metric achieves the highest correlation with\nhuman judgments on both images and videos, outperforming existing\nreference-based metrics like CIDEr and SPICE and reference-free metrics like\nCLIP-Score. Finally, we test the system-level correlation of the proposed\nmetric when considering popular image captioning approaches, and assess the\nimpact of employing different cross-modal features. Our source code and trained\nmodels are publicly available at: https://github.com/aimagelab/pacscore.\n","authors":["Sara Sarto","Manuele Barraco","Marcella Cornia","Lorenzo Baraldi","Rita Cucchiara"],"pdf_url":"https://arxiv.org/pdf/2303.12112v1.pdf","comment":"CVPR 2023 (highlight paper)"},{"id":"http://arxiv.org/abs/2303.13550v1","updated":"2023-03-21T22:46:22Z","published":"2023-03-21T22:46:22Z","title":"Augmented reality as a Thirdspace: Simultaneous experience of the\n  physical and virtual","summary":"  With the proliferation of devices that display augmented reality (AR), now is\nthe time for scholars and practitioners to evaluate and engage critically with\nemerging applications of the medium. AR mediates the way users see their\nbodies, hear their environment and engage with places. Applied in various\nforms, including social media, e-commerce, gaming, enterprise and art, the\nmedium facilitates a hybrid experience of physical and digital spaces. This\narticle employs a model of real-and-imagined space from geographer Edward Soja\nto examine how the user of an AR app navigates the two intertwined spaces of\nphysical and digital, experiencing what Soja calls a 'Third-space'. The article\nillustrates the potential for headset-based AR to engender such a Thirdspace\nthrough the author's practice-led research project, the installation Through\nthe Wardrobe. This installation demonstrates how AR has the potential to shift\nthe way that users view and interact with their world with artistic\napplications providing an opportunity to question assumptions of social norms,\nidentity and uses of physical space.\n","authors":["Rob Eagle"],"pdf_url":"https://arxiv.org/pdf/2303.13550v1.pdf","comment":"Preprint of chapter published in Proceedings of the 3rd International\n  and Interdisciplinary Conference on Images and Imagination, edited by D.\n  Villa and F. Zuccoli, 2023, Springer Nature, reproduced with permission of\n  Springer Nature"}]},"2023-03-22T00:00:00Z":{"Computation and Language":[{"id":"http://arxiv.org/abs/2303.12788v1","updated":"2023-03-22T17:57:47Z","published":"2023-03-22T17:57:47Z","title":"Open-source Frame Semantic Parsing","summary":"  While the state-of-the-art for frame semantic parsing has progressed\ndramatically in recent years, it is still difficult for end-users to apply\nstate-of-the-art models in practice. To address this, we present Frame Semantic\nTransformer, an open-source Python library which achieves near state-of-the-art\nperformance on FrameNet 1.7, while focusing on ease-of-use. We use a T5 model\nfine-tuned on Propbank and FrameNet exemplars as a base, and improve\nperformance by using FrameNet lexical units to provide hints to T5 at inference\ntime. We enhance robustness to real-world data by using textual data\naugmentations during training.\n","authors":["David Chanin"],"pdf_url":"https://arxiv.org/pdf/2303.12788v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.11117v2","updated":"2023-03-22T17:48:31Z","published":"2023-03-20T13:58:35Z","title":"EmotionIC: Emotional Inertia and Contagion-driven Dependency Modelling\n  for Emotion Recognition in Conversation","summary":"  Emotion Recognition in Conversation (ERC) has attracted growing attention in\nrecent years as a result of the advancement and implementation of\nhuman-computer interface technologies. However, previous approaches to modeling\nglobal and local context dependencies lost the diversity of dependency\ninformation and do not take the context dependency into account at the\nclassification level. In this paper, we propose a novel approach to dependency\nmodeling driven by Emotional Inertia and Contagion (EmotionIC) for\nconversational emotion recognition at the feature extraction and classification\nlevels. At the feature extraction level, our designed Identity Masked\nMulti-head Attention (IM-MHA) captures the identity-based long-distant context\nin the dialogue to contain the diverse influence of different participants and\nconstruct the global emotional atmosphere, while the devised Dialogue-based\nGate Recurrent Unit (DialogGRU) that aggregates the emotional tendencies of\ndyadic dialogue is applied to refine the contextual features with inter- and\nintra-speaker dependencies. At the classification level, by introducing skip\nconnections in Conditional Random Field (CRF), we elaborate the Skip-chain CRF\n(SkipCRF) to capture the high-order dependencies within and between speakers,\nand to emulate the emotional flow of distant participants. Experimental results\nshow that our method can significantly outperform the state-of-the-art models\non four benchmark datasets. The ablation studies confirm that our modules can\neffectively model emotional inertia and contagion.\n","authors":["Yingjian Liu","Jiang Li","Xiaoping Wang","Zhigang Zeng"],"pdf_url":"https://arxiv.org/pdf/2303.11117v2.pdf","comment":"15 pages"},{"id":"http://arxiv.org/abs/2303.12057v2","updated":"2023-03-22T17:35:51Z","published":"2023-03-21T17:48:00Z","title":"Large Language Models Can Be Used to Estimate the Ideologies of\n  Politicians in a Zero-Shot Learning Setting","summary":"  The mass aggregation of knowledge embedded in large language models (LLMs)\nholds the promise of new solutions to problems of observability and measurement\nin the social sciences. We examine the utility of one such model for a\nparticularly difficult measurement task: measuring the latent ideology of\nlawmakers, which allows us to better understand functions that are core to\ndemocracy, such as how politics shape policy and how political actors represent\ntheir constituents. We scale the senators of the 116th United States Congress\nalong the liberal-conservative spectrum by prompting ChatGPT to select the more\nliberal (or conservative) senator in pairwise comparisons. We show that the LLM\nproduced stable answers across repeated iterations, did not hallucinate, and\nwas not simply regurgitating information from a single source. This new scale\nstrongly correlates with pre-existing liberal-conservative scales such as\nNOMINATE, but also differs in several important ways, such as correctly placing\nsenators who vote against their party for far-left or far-right ideological\nreasons on the extreme ends. The scale also highly correlates with ideological\nmeasures based on campaign giving and political activists' perceptions of these\nsenators. In addition to the potential for better-automated data collection and\ninformation retrieval, our results suggest LLMs are likely to open new avenues\nfor measuring latent constructs like ideology that rely on aggregating large\nquantities of data from public sources.\n","authors":["Patrick Y. Wu","Joshua A. Tucker","Jonathan Nagler","Solomon Messing"],"pdf_url":"https://arxiv.org/pdf/2303.12057v2.pdf","comment":"18 pages, 4 figures; V2: fixed graphical error on Figure 2"},{"id":"http://arxiv.org/abs/2303.12772v1","updated":"2023-03-22T17:35:35Z","published":"2023-03-22T17:35:35Z","title":"Interpretable Bangla Sarcasm Detection using BERT and Explainable AI","summary":"  A positive phrase or a sentence with an underlying negative motive is usually\ndefined as sarcasm that is widely used in today's social media platforms such\nas Facebook, Twitter, Reddit, etc. In recent times active users in social media\nplatforms are increasing dramatically which raises the need for an automated\nNLP-based system that can be utilized in various tasks such as determining\nmarket demand, sentiment analysis, threat detection, etc. However, since\nsarcasm usually implies the opposite meaning and its detection is frequently a\nchallenging issue, data meaning extraction through an NLP-based model becomes\nmore complicated. As a result, there has been a lot of study on sarcasm\ndetection in English over the past several years, and there's been a noticeable\nimprovement and yet sarcasm detection in the Bangla language's state remains\nthe same. In this article, we present a BERT-based system that can achieve\n99.60\\% while the utilized traditional machine learning algorithms are only\ncapable of achieving 89.93\\%. Additionally, we have employed Local\nInterpretable Model-Agnostic Explanations that introduce explainability to our\nsystem. Moreover, we have utilized a newly collected bangla sarcasm dataset,\nBanglaSarc that was constructed specifically for the evaluation of this study.\nThis dataset consists of fresh records of sarcastic and non-sarcastic comments,\nthe majority of which are acquired from Facebook and YouTube comment sections.\n","authors":["Ramisa Anan","Tasnim Sakib Apon","Zeba Tahsin Hossain","Elizabeth Antora Modhu","Sudipta Mondal","MD. Golam Rabiul Alam"],"pdf_url":"https://arxiv.org/pdf/2303.12772v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.12767v1","updated":"2023-03-22T17:32:56Z","published":"2023-03-22T17:32:56Z","title":"Can we trust the evaluation on ChatGPT?","summary":"  ChatGPT, the first large language model (LLM) with mass adoption, has\ndemonstrated remarkable performance in numerous natural language tasks. Despite\nits evident usefulness, evaluating ChatGPT's performance in diverse problem\ndomains remains challenging due to the closed nature of the model and its\ncontinuous updates via Reinforcement Learning from Human Feedback (RLHF). We\nhighlight the issue of data contamination in ChatGPT evaluations, with a case\nstudy of the task of stance detection. We discuss the challenge of preventing\ndata contamination and ensuring fair model evaluation in the age of closed and\ncontinuously trained models.\n","authors":["Rachith Aiyappa","Jisun An","Haewoon Kwak","Yong-Yeol Ahn"],"pdf_url":"https://arxiv.org/pdf/2303.12767v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.12712v1","updated":"2023-03-22T16:51:28Z","published":"2023-03-22T16:51:28Z","title":"Sparks of Artificial General Intelligence: Early experiments with GPT-4","summary":"  Artificial intelligence (AI) researchers have been developing and refining\nlarge language models (LLMs) that exhibit remarkable capabilities across a\nvariety of domains and tasks, challenging our understanding of learning and\ncognition. The latest model developed by OpenAI, GPT-4, was trained using an\nunprecedented scale of compute and data. In this paper, we report on our\ninvestigation of an early version of GPT-4, when it was still in active\ndevelopment by OpenAI. We contend that (this early version of) GPT-4 is part of\na new cohort of LLMs (along with ChatGPT and Google's PaLM for example) that\nexhibit more general intelligence than previous AI models. We discuss the\nrising capabilities and implications of these models. We demonstrate that,\nbeyond its mastery of language, GPT-4 can solve novel and difficult tasks that\nspan mathematics, coding, vision, medicine, law, psychology and more, without\nneeding any special prompting. Moreover, in all of these tasks, GPT-4's\nperformance is strikingly close to human-level performance, and often vastly\nsurpasses prior models such as ChatGPT. Given the breadth and depth of GPT-4's\ncapabilities, we believe that it could reasonably be viewed as an early (yet\nstill incomplete) version of an artificial general intelligence (AGI) system.\nIn our exploration of GPT-4, we put special emphasis on discovering its\nlimitations, and we discuss the challenges ahead for advancing towards deeper\nand more comprehensive versions of AGI, including the possible need for\npursuing a new paradigm that moves beyond next-word prediction. We conclude\nwith reflections on societal influences of the recent technological leap and\nfuture research directions.\n","authors":["Sébastien Bubeck","Varun Chandrasekaran","Ronen Eldan","Johannes Gehrke","Eric Horvitz","Ece Kamar","Peter Lee","Yin Tat Lee","Yuanzhi Li","Scott Lundberg","Harsha Nori","Hamid Palangi","Marco Tulio Ribeiro","Yi Zhang"],"pdf_url":"https://arxiv.org/pdf/2303.12712v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.12671v1","updated":"2023-03-22T15:49:33Z","published":"2023-03-22T15:49:33Z","title":"Integrating Image Features with Convolutional Sequence-to-sequence\n  Network for Multilingual Visual Question Answering","summary":"  Visual Question Answering (VQA) is a task that requires computers to give\ncorrect answers for the input questions based on the images. This task can be\nsolved by humans with ease but is a challenge for computers. The\nVLSP2022-EVJVQA shared task carries the Visual Question Answering task in the\nmultilingual domain on a newly released dataset: UIT-EVJVQA, in which the\nquestions and answers are written in three different languages: English,\nVietnamese and Japanese. We approached the challenge as a sequence-to-sequence\nlearning task, in which we integrated hints from pre-trained state-of-the-art\nVQA models and image features with Convolutional Sequence-to-Sequence network\nto generate the desired answers. Our results obtained up to 0.3442 by F1 score\non the public test set, 0.4210 on the private test set, and placed 3rd in the\ncompetition.\n","authors":["Triet Minh Thai","Son T. Luu"],"pdf_url":"https://arxiv.org/pdf/2303.12671v1.pdf","comment":"VLSP2022-EVJVQA"},{"id":"http://arxiv.org/abs/2303.12665v1","updated":"2023-03-22T15:44:15Z","published":"2023-03-22T15:44:15Z","title":"Evaluating the Role of Target Arguments in Rumour Stance Classification","summary":"  Considering a conversation thread, stance classification aims to identify the\nopinion (e.g. agree or disagree) of replies towards a given target. The target\nof the stance is expected to be an essential component in this task, being one\nof the main factors that make it different from sentiment analysis. However, a\nrecent study shows that a target-oblivious model outperforms target-aware\nmodels, suggesting that targets are not useful when predicting stance. This\npaper re-examines this phenomenon for rumour stance classification (RSC) on\nsocial media, where a target is a rumour story implied by the source tweet in\nthe conversation. We propose adversarial attacks in the test data, aiming to\nassess the models robustness and evaluate the role of the data in the models\nperformance. Results show that state-of-the-art models, including approaches\nthat use the entire conversation thread, overly relying on superficial signals.\nOur hypothesis is that the naturally high occurrence of target-independent\ndirect replies in RSC (e.g. \"this is fake\" or just \"fake\") results in the\nimpressive performance of target-oblivious models, highlighting the risk of\ntarget instances being treated as noise during training.\n","authors":["Yue Li","Carolina Scarton"],"pdf_url":"https://arxiv.org/pdf/2303.12665v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2205.00363v3","updated":"2023-03-22T15:42:50Z","published":"2022-04-30T23:03:49Z","title":"Visual Spatial Reasoning","summary":"  Spatial relations are a basic part of human cognition. However, they are\nexpressed in natural language in a variety of ways, and previous work has\nsuggested that current vision-and-language models (VLMs) struggle to capture\nrelational information. In this paper, we present Visual Spatial Reasoning\n(VSR), a dataset containing more than 10k natural text-image pairs with 66\ntypes of spatial relations in English (such as: under, in front of, and\nfacing). While using a seemingly simple annotation format, we show how the\ndataset includes challenging linguistic phenomena, such as varying reference\nframes. We demonstrate a large gap between human and model performance: the\nhuman ceiling is above 95%, while state-of-the-art models only achieve around\n70%. We observe that VLMs' by-relation performances have little correlation\nwith the number of training examples and the tested models are in general\nincapable of recognising relations concerning the orientations of objects.\n","authors":["Fangyu Liu","Guy Emerson","Nigel Collier"],"pdf_url":"https://arxiv.org/pdf/2205.00363v3.pdf","comment":"TACL camera-ready version; code and data available at\n  https://github.com/cambridgeltl/visual-spatial-reasoning"},{"id":"http://arxiv.org/abs/2303.10659v2","updated":"2023-03-22T15:27:08Z","published":"2023-03-19T13:47:56Z","title":"COVID-19 event extraction from Twitter via extractive question answering\n  with continuous prompts","summary":"  As COVID-19 ravages the world, social media analytics could augment\ntraditional surveys in assessing how the pandemic evolves and capturing\nconsumer chatter that could help healthcare agencies in addressing it. This\ntypically involves mining disclosure events that mention testing positive for\nthe disease or discussions surrounding perceptions and beliefs in preventative\nor treatment options. The 2020 shared task on COVID-19 event extraction\n(conducted as part of the W-NUT workshop during the EMNLP conference)\nintroduced a new Twitter dataset for benchmarking event extraction from\nCOVID-19 tweets. In this paper, we cast the problem of event extraction as\nextractive question answering using recent advances in continuous prompting in\nlanguage models. On the shared task test dataset, our approach leads to over 5%\nabsolute micro-averaged F1-score improvement over prior best results, across\nall COVID-19 event slots. Our ablation study shows that continuous prompts have\na major impact on the eventual performance.\n","authors":["Yuhang Jiang","Ramakanth Kavuluru"],"pdf_url":"https://arxiv.org/pdf/2303.10659v2.pdf","comment":"Accepted to appear in MEDINFO 2023. Code:\n  https://github.com/bionlproc/twitter-covid-QA-extraction"},{"id":"http://arxiv.org/abs/2303.11331v2","updated":"2023-03-22T14:10:37Z","published":"2023-03-20T17:59:59Z","title":"EVA-02: A Visual Representation for Neon Genesis","summary":"  We launch EVA-02, a next-generation Transformer-based visual representation\npre-trained to reconstruct strong and robust language-aligned vision features\nvia masked image modeling. With an updated plain Transformer architecture as\nwell as extensive pre-training from an open & accessible giant CLIP vision\nencoder, EVA-02 demonstrates superior performance compared to prior\nstate-of-the-art approaches across various representative vision tasks, while\nutilizing significantly fewer parameters and compute budgets. Notably, using\nexclusively publicly accessible training data, EVA-02 with only 304M parameters\nachieves a phenomenal 90.0 fine-tuning top-1 accuracy on ImageNet-1K val set.\nAdditionally, our EVA-02-CLIP can reach up to 80.4 zero-shot top-1 on\nImageNet-1K, outperforming the previous largest & best open-sourced CLIP with\nonly ~1/6 parameters and ~1/6 image-text training data. We offer four EVA-02\nvariants in various model sizes, ranging from 6M to 304M parameters, all with\nimpressive performance. To facilitate open access and open research, we release\nthe complete suite of EVA-02 to the community at\nhttps://github.com/baaivision/EVA/tree/master/EVA-02.\n","authors":["Yuxin Fang","Quan Sun","Xinggang Wang","Tiejun Huang","Xinlong Wang","Yue Cao"],"pdf_url":"https://arxiv.org/pdf/2303.11331v2.pdf","comment":"v2: Fix some known issues & typos. v1: To Asuka. Code & Models:\n  https://github.com/baaivision/EVA/tree/master/EVA-02"},{"id":"http://arxiv.org/abs/2303.12582v1","updated":"2023-03-22T14:09:20Z","published":"2023-03-22T14:09:20Z","title":"AfroDigits: A Community-Driven Spoken Digit Dataset for African\n  Languages","summary":"  The advancement of speech technologies has been remarkable, yet its\nintegration with African languages remains limited due to the scarcity of\nAfrican speech corpora. To address this issue, we present AfroDigits, a\nminimalist, community-driven dataset of spoken digits for African languages,\ncurrently covering 38 African languages. As a demonstration of the practical\napplications of AfroDigits, we conduct audio digit classification experiments\non six African languages [Igbo (ibo), Yoruba (yor), Rundi (run), Oshiwambo\n(kua), Shona (sna), and Oromo (gax)] using the Wav2Vec2.0-Large and XLS-R\nmodels. Our experiments reveal a useful insight on the effect of mixing African\nspeech corpora during finetuning. AfroDigits is the first published audio digit\ndataset for African languages and we believe it will, among other things, pave\nthe way for Afro-centric speech applications such as the recognition of\ntelephone numbers, and street numbers. We release the dataset and platform\npublicly at https://huggingface.co/datasets/chrisjay/crowd-speech-africa and\nhttps://huggingface.co/spaces/chrisjay/afro-speech respectively.\n","authors":["Chris Chinenye Emezue","Sanchit Gandhi","Lewis Tunstall","Abubakar Abid","Joshua Meyer","Quentin Lhoest","Pete Allen","Patrick Von Platen","Douwe Kiela","Yacine Jernite","Julien Chaumond","Merve Noyan","Omar Sanseviero"],"pdf_url":"https://arxiv.org/pdf/2303.12582v1.pdf","comment":"Accepted to the AfricaNLP Workshop at ICLR 2023"},{"id":"http://arxiv.org/abs/2303.12570v1","updated":"2023-03-22T13:54:46Z","published":"2023-03-22T13:54:46Z","title":"RepoCoder: Repository-Level Code Completion Through Iterative Retrieval\n  and Generation","summary":"  The task of repository-level code completion is to continue writing the\nunfinished code based on a broader context of the repository. While for\nautomated code completion tools, it is difficult to utilize the useful\ninformation scattered in different files. We propose RepoCoder, a simple,\ngeneric, and effective framework to address the challenge. It streamlines the\nrepository-level code completion process by incorporating a similarity-based\nretriever and a pre-trained code language model, which allows for the effective\nutilization of repository-level information for code completion and grants the\nability to generate code at various levels of granularity. Furthermore,\nRepoCoder utilizes a novel iterative retrieval-generation paradigm that bridges\nthe gap between retrieval context and the intended completion target. We also\npropose a new benchmark RepoEval, which consists of the latest and high-quality\nreal-world repositories covering line, API invocation, and function body\ncompletion scenarios. We test the performance of RepoCoder by using various\ncombinations of code retrievers and generators. Experimental results indicate\nthat RepoCoder significantly improves the zero-shot code completion baseline by\nover 10% in all settings and consistently outperforms the vanilla\nretrieval-augmented code completion approach. Furthermore, we validate the\neffectiveness of RepoCoder through comprehensive analysis, providing valuable\ninsights for future research.\n","authors":["Fengji Zhang","Bei Chen","Yue Zhang","Jin Liu","Daoguang Zan","Yi Mao","Jian-Guang Lou","Weizhu Chen"],"pdf_url":"https://arxiv.org/pdf/2303.12570v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.12528v1","updated":"2023-03-22T13:03:10Z","published":"2023-03-22T13:03:10Z","title":"MEGA: Multilingual Evaluation of Generative AI","summary":"  Generative AI models have impressive performance on many Natural Language\nProcessing tasks such as language understanding, reasoning and language\ngeneration. One of the most important questions that is being asked by the AI\ncommunity today is about the capabilities and limits of these models, and it is\nclear that evaluating generative AI is very challenging. Most studies on\ngenerative Large Language Models (LLMs) are restricted to English and it is\nunclear how capable these models are at understanding and generating other\nlanguages. We present the first comprehensive benchmarking of generative LLMs -\nMEGA, which evaluates models on standard NLP benchmarks, covering 8 diverse\ntasks and 33 typologically diverse languages. We also compare the performance\nof generative LLMs to State of the Art (SOTA) non-autoregressive models on\nthese tasks to determine how well generative models perform compared to the\nprevious generation of LLMs. We present a thorough analysis of the performance\nof models across languages and discuss some of the reasons why generative LLMs\nare currently not optimal for all languages. We create a framework for\nevaluating generative LLMs in the multilingual setting and provide directions\nfor future progress in the field.\n","authors":["Kabir Ahuja","Rishav Hada","Millicent Ochieng","Prachi Jain","Harshita Diddee","Samuel Maina","Tanuja Ganu","Sameer Segal","Maxamed Axmed","Kalika Bali","Sunayana Sitaram"],"pdf_url":"https://arxiv.org/pdf/2303.12528v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.09461v2","updated":"2023-03-22T11:30:41Z","published":"2023-03-08T15:46:14Z","title":"ChatGPT Participates in a Computer Science Exam","summary":"  We asked ChatGPT to participate in an undergraduate computer science exam on\n''Algorithms and Data Structures''. The program was evaluated on the entire\nexam as posed to the students. We hand-copied its answers onto an exam sheet,\nwhich was subsequently graded in a blind setup alongside those of 200\nparticipating students. We find that ChatGPT narrowly passed the exam,\nobtaining 20.5 out of 40 points. This impressive performance indicates that\nChatGPT can indeed succeed in challenging tasks like university exams. At the\nsame time, the questions in our exam are structurally similar to those of other\nexams, solved homework problems, and teaching materials that can be found\nonline and might have been part of ChatGPT's training data. Therefore, it would\nbe inadequate to conclude from this experiment that ChatGPT has any\nunderstanding of computer science. We also assess the improvements brought by\nGPT-4. We find that GPT-4 would have obtained about 17\\% more exam points than\nGPT-3.5, reaching the performance of the average student. The transcripts of\nour conversations with ChatGPT are available at\n\\url{https://github.com/tml-tuebingen/chatgpt-algorithm-exam}, and the entire\ngraded exam is in the appendix of this paper.\n","authors":["Sebastian Bordt","Ulrike von Luxburg"],"pdf_url":"https://arxiv.org/pdf/2303.09461v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.08518v2","updated":"2023-03-22T11:29:48Z","published":"2023-03-15T10:53:49Z","title":"UPRISE: Universal Prompt Retrieval for Improving Zero-Shot Evaluation","summary":"  Large Language Models (LLMs) are popular for their impressive abilities, but\nthe need for model-specific fine-tuning or task-specific prompt engineering can\nhinder their generalization. We propose UPRISE (Universal Prompt Retrieval for\nImproving zero-Shot Evaluation), which tunes a lightweight and versatile\nretriever that automatically retrieves prompts for a given zero-shot task\ninput. Specifically, we demonstrate universality in a cross-task and\ncross-model scenario: the retriever is tuned on a diverse set of tasks, but\ntested on unseen task types; we use a small frozen LLM, GPT-Neo-2.7B, for\ntuning the retriever, but test the retriever on different LLMs of much larger\nscales, such as BLOOM-7.1B, OPT-66B and GPT3-175B. Additionally, we show that\nUPRISE mitigates the hallucination problem in our experiments with ChatGPT,\nsuggesting its potential to improve even the strongest LLMs. Our model and code\nare available at https://github.com/microsoft/LMOps.\n","authors":["Daixuan Cheng","Shaohan Huang","Junyu Bi","Yuefeng Zhan","Jianfeng Liu","Yujing Wang","Hao Sun","Furu Wei","Denvy Deng","Qi Zhang"],"pdf_url":"https://arxiv.org/pdf/2303.08518v2.pdf","comment":"Update link to our model and code"},{"id":"http://arxiv.org/abs/2206.06807v2","updated":"2023-03-22T11:22:29Z","published":"2022-06-14T12:56:34Z","title":"The Causal Structure of Semantic Ambiguities","summary":"  Ambiguity is a natural language phenomenon occurring at different levels of\nsyntax, semantics, and pragmatics. It is widely studied; in Psycholinguistics,\nfor instance, we have a variety of competing studies for the human\ndisambiguation processes. These studies are empirical and based on eyetracking\nmeasurements. Here we take first steps towards formalizing these processes for\nsemantic ambiguities where we identified the presence of two features: (1)\njoint plausibility degrees of different possible interpretations, (2) causal\nstructures according to which certain words play a more substantial role in the\nprocesses. The novel sheaf-theoretic model of definite causality developed by\nGogioso and Pinzani in QPL 2021 offers tools to model and reason about these\nfeatures. We applied this theory to a dataset of ambiguous phrases extracted\nfrom Psycholinguistics literature and their human plausibility judgements\ncollected by us using the Amazon Mechanical Turk engine. We measured the causal\nfractions of different disambiguation orders within the phrases and discovered\ntwo prominent orders: from subject to verb in the subject-verb and from object\nto verb in the verb object phrases. We also found evidence for delay in the\ndisambiguation of polysemous vs homonymous verbs, again compatible with\nPsycholinguistic findings.\n","authors":["Daphne Wang","Mehrnoosh Sadrzadeh"],"pdf_url":"https://arxiv.org/pdf/2206.06807v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2208.00690v3","updated":"2023-03-22T07:20:37Z","published":"2022-08-01T08:58:02Z","title":"Generative Bias for Robust Visual Question Answering","summary":"  The task of Visual Question Answering (VQA) is known to be plagued by the\nissue of VQA models exploiting biases within the dataset to make its final\nprediction. Various previous ensemble based debiasing methods have been\nproposed where an additional model is purposefully trained to be biased in\norder to train a robust target model. However, these methods compute the bias\nfor a model simply from the label statistics of the training data or from\nsingle modal branches. In this work, in order to better learn the bias a target\nVQA model suffers from, we propose a generative method to train the bias model\ndirectly from the target model, called GenB. In particular, GenB employs a\ngenerative network to learn the bias in the target model through a combination\nof the adversarial objective and knowledge distillation. We then debias our\ntarget model with GenB as a bias model, and show through extensive experiments\nthe effects of our method on various VQA bias datasets including VQA-CP2,\nVQA-CP1, GQA-OOD, and VQA-CE, and show state-of-the-art results with the LXMERT\narchitecture on VQA-CP2.\n","authors":["Jae Won Cho","Dong-jin Kim","Hyeonggon Ryu","In So Kweon"],"pdf_url":"https://arxiv.org/pdf/2208.00690v3.pdf","comment":"CVPR 2023"},{"id":"http://arxiv.org/abs/2211.12824v2","updated":"2023-03-22T07:20:06Z","published":"2022-11-23T10:14:12Z","title":"Tell Me What Happened: Unifying Text-guided Video Completion via\n  Multimodal Masked Video Generation","summary":"  Generating a video given the first several static frames is challenging as it\nanticipates reasonable future frames with temporal coherence. Besides video\nprediction, the ability to rewind from the last frame or infilling between the\nhead and tail is also crucial, but they have rarely been explored for video\ncompletion. Since there could be different outcomes from the hints of just a\nfew frames, a system that can follow natural language to perform video\ncompletion may significantly improve controllability. Inspired by this, we\nintroduce a novel task, text-guided video completion (TVC), which requests the\nmodel to generate a video from partial frames guided by an instruction. We then\npropose Multimodal Masked Video Generation (MMVG) to address this TVC task.\nDuring training, MMVG discretizes the video frames into visual tokens and masks\nmost of them to perform video completion from any time point. At inference\ntime, a single MMVG model can address all 3 cases of TVC, including video\nprediction, rewind, and infilling, by applying corresponding masking\nconditions. We evaluate MMVG in various video scenarios, including egocentric,\nanimation, and gaming. Extensive experimental results indicate that MMVG is\neffective in generating high-quality visual appearances with text guidance for\nTVC.\n","authors":["Tsu-Jui Fu","Licheng Yu","Ning Zhang","Cheng-Yang Fu","Jong-Chyi Su","William Yang Wang","Sean Bell"],"pdf_url":"https://arxiv.org/pdf/2211.12824v2.pdf","comment":"CVPR'23"},{"id":"http://arxiv.org/abs/2303.12320v1","updated":"2023-03-22T05:35:29Z","published":"2023-03-22T05:35:29Z","title":"GrapeQA: GRaph Augmentation and Pruning to Enhance Question-Answering","summary":"  Commonsense question-answering (QA) methods combine the power of pre-trained\nLanguage Models (LM) with the reasoning provided by Knowledge Graphs (KG). A\ntypical approach collects nodes relevant to the QA pair from a KG to form a\nWorking Graph (WG) followed by reasoning using Graph Neural Networks(GNNs).\nThis faces two major challenges: (i) it is difficult to capture all the\ninformation from the QA in the WG, and (ii) the WG contains some irrelevant\nnodes from the KG. To address these, we propose GrapeQA with two simple\nimprovements on the WG: (i) Prominent Entities for Graph Augmentation\nidentifies relevant text chunks from the QA pair and augments the WG with\ncorresponding latent representations from the LM, and (ii) Context-Aware Node\nPruning removes nodes that are less relevant to the QA pair. We evaluate our\nresults on OpenBookQA, CommonsenseQA and MedQA-USMLE and see that GrapeQA shows\nconsistent improvements over its LM + KG predecessor (QA-GNN in particular) and\nlarge improvements on OpenBookQA.\n","authors":["Dhaval Taunk","Lakshya Khanna","Pavan Kandru","Vasudeva Varma","Charu Sharma","Makarand Tapaswi"],"pdf_url":"https://arxiv.org/pdf/2303.12320v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.12314v1","updated":"2023-03-22T05:04:21Z","published":"2023-03-22T05:04:21Z","title":"Self-supervised Meta-Prompt Learning with Meta-Gradient Regularization\n  for Few-shot Generalization","summary":"  Prompt tuning is a parameter-efficient method, which learns soft prompts and\nconditions frozen language models to perform specific downstream tasks. Though\neffective, prompt tuning under few-shot settings on the one hand heavily relies\non a good initialization of soft prompts. On the other hand, it can easily\nresult in overfitting. Existing works leverage pre-training or supervised\nmeta-learning to initialize soft prompts but they cannot data-efficiently\ngeneralize to unseen downstream tasks. To address the above problems, this\npaper proposes a novel Self-sUpervised meta-Prompt learning framework with\nmeta-gradient Regularization for few-shot generalization (SUPMER). We first\ndesign a set of self-supervised anchor meta-training tasks with different task\nformats and further enrich the task distribution with curriculum-based task\naugmentation. Then a novel meta-gradient regularization method is integrated\ninto meta-prompt learning. It meta-learns to transform the raw gradients during\nfew-shot learning into a domain-generalizable direction, thus alleviating the\nproblem of overfitting. Extensive experiments show that SUPMER achieves better\nperformance for different few-shot downstream tasks, and also exhibits a\nstronger domain generalization ability.\n","authors":["Kaihang Pan","Juncheng Li","Hongye Song","Jun Lin","Xiaozhong Liu","Siliang Tang"],"pdf_url":"https://arxiv.org/pdf/2303.12314v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.12308v1","updated":"2023-03-22T04:52:43Z","published":"2023-03-22T04:52:43Z","title":"XWikiGen: Cross-lingual Summarization for Encyclopedic Text Generation\n  in Low Resource Languages","summary":"  Lack of encyclopedic text contributors, especially on Wikipedia, makes\nautomated text generation for \\emph{low resource (LR) languages} a critical\nproblem. Existing work on Wikipedia text generation has focused on\n\\emph{English only} where English reference articles are summarized to generate\nEnglish Wikipedia pages. But, for low-resource languages, the scarcity of\nreference articles makes monolingual summarization ineffective in solving this\nproblem. Hence, in this work, we propose \\task{}, which is the task of\ncross-lingual multi-document summarization of text from multiple reference\narticles, written in various languages, to generate Wikipedia-style text.\nAccordingly, we contribute a benchmark dataset, \\data{}, spanning $\\sim$69K\nWikipedia articles covering five domains and eight languages. We harness this\ndataset to train a two-stage system where the input is a set of citations and a\nsection title and the output is a section-specific LR summary. The proposed\nsystem is based on a novel idea of neural unsupervised extractive summarization\nto coarsely identify salient information followed by a neural abstractive model\nto generate the section-specific text. Extensive experiments show that\nmulti-domain training is better than the multi-lingual setup on average.\n","authors":["Dhaval Taunk","Shivprasad Sagare","Anupam Patil","Shivansh Subramanian","Manish Gupta","Vasudeva Varma"],"pdf_url":"https://arxiv.org/pdf/2303.12308v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.12300v1","updated":"2023-03-22T04:11:35Z","published":"2023-03-22T04:11:35Z","title":"Exploring Turkish Speech Recognition via Hybrid CTC/Attention\n  Architecture and Multi-feature Fusion Network","summary":"  In recent years, End-to-End speech recognition technology based on deep\nlearning has developed rapidly. Due to the lack of Turkish speech data, the\nperformance of Turkish speech recognition system is poor. Firstly, this paper\nstudies a series of speech recognition tuning technologies. The results show\nthat the performance of the model is the best when the data enhancement\ntechnology combining speed perturbation with noise addition is adopted and the\nbeam search width is set to 16. Secondly, to maximize the use of effective\nfeature information and improve the accuracy of feature extraction, this paper\nproposes a new feature extractor LSPC. LSPC and LiGRU network are combined to\nform a shared encoder structure, and model compression is realized. The results\nshow that the performance of LSPC is better than MSPC and VGGnet when only\nusing Fbank features, and the WER is improved by 1.01% and 2.53% respectively.\nFinally, based on the above two points, a new multi-feature fusion network is\nproposed as the main structure of the encoder. The results show that the WER of\nthe proposed feature fusion network based on LSPC is improved by 0.82% and\n1.94% again compared with the single feature (Fbank feature and Spectrogram\nfeature) extraction using LSPC. Our model achieves performance comparable to\nthat of advanced End-to-End models.\n","authors":["Zeyu Ren","Nurmement Yolwas","Huiru Wang","Wushour Slamu"],"pdf_url":"https://arxiv.org/pdf/2303.12300v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.12294v1","updated":"2023-03-22T03:58:38Z","published":"2023-03-22T03:58:38Z","title":"Evaluating Transformer Models and Human Behaviors on Chinese Character\n  Naming","summary":"  Neural network models have been proposed to explain the grapheme-phoneme\nmapping process in humans for many alphabet languages. These models not only\nsuccessfully learned the correspondence of the letter strings and their\npronunciation, but also captured human behavior in nonce word naming tasks. How\nwould the neural models perform for a non-alphabet language (e.g., Chinese)\nunknown character task? How well would the model capture human behavior? In\nthis study, we evaluate a set of transformer models and compare their\nperformances with human behaviors on an unknown Chinese character naming task.\nWe found that the models and humans behaved very similarly, that they had\nsimilar accuracy distribution for each character, and had a substantial overlap\nin answers. In addition, the models' answers are highly correlated with humans'\nanswers. These results suggested that the transformer models can well capture\nhuman's character naming behavior.\n","authors":["Xiaomeng Ma","Lingyu Gao"],"pdf_url":"https://arxiv.org/pdf/2303.12294v1.pdf","comment":"Accepted by TACL"},{"id":"http://arxiv.org/abs/2303.10893v2","updated":"2023-03-22T03:20:27Z","published":"2023-03-20T06:20:03Z","title":"Character, Word, or Both? Revisiting the Segmentation Granularity for\n  Chinese Pre-trained Language Models","summary":"  Pretrained language models (PLMs) have shown marvelous improvements across\nvarious NLP tasks. Most Chinese PLMs simply treat an input text as a sequence\nof characters, and completely ignore word information. Although Whole Word\nMasking can alleviate this, the semantics in words is still not well\nrepresented. In this paper, we revisit the segmentation granularity of Chinese\nPLMs. We propose a mixed-granularity Chinese BERT (MigBERT) by considering both\ncharacters and words. To achieve this, we design objective functions for\nlearning both character and word-level representations. We conduct extensive\nexperiments on various Chinese NLP tasks to evaluate existing PLMs as well as\nthe proposed MigBERT. Experimental results show that MigBERT achieves new SOTA\nperformance on all these tasks. Further analysis demonstrates that words are\nsemantically richer than characters. More interestingly, we show that MigBERT\nalso works with Japanese. Our code and model have been released\nhere~\\footnote{https://github.com/xnliang98/MigBERT}.\n","authors":["Xinnian Liang","Zefan Zhou","Hui Huang","Shuangzhi Wu","Tong Xiao","Muyun Yang","Zhoujun Li","Chao Bian"],"pdf_url":"https://arxiv.org/pdf/2303.10893v2.pdf","comment":"preprint"},{"id":"http://arxiv.org/abs/2211.12764v3","updated":"2023-03-22T02:36:52Z","published":"2022-11-23T08:20:29Z","title":"VoP: Text-Video Co-operative Prompt Tuning for Cross-Modal Retrieval","summary":"  Many recent studies leverage the pre-trained CLIP for text-video cross-modal\nretrieval by tuning the backbone with additional heavy modules, which not only\nbrings huge computational burdens with much more parameters, but also leads to\nthe knowledge forgetting from upstream models. In this work, we propose the\nVoP: Text-Video Co-operative Prompt Tuning for efficient tuning on the\ntext-video retrieval task. The proposed VoP is an end-to-end framework with\nboth video & text prompts introducing, which can be regarded as a powerful\nbaseline with only 0.1% trainable parameters. Further, based on the\nspatio-temporal characteristics of videos, we develop three novel video prompt\nmechanisms to improve the performance with different scales of trainable\nparameters. The basic idea of the VoP enhancement is to model the frame\nposition, frame context, and layer function with specific trainable prompts,\nrespectively. Extensive experiments show that compared to full fine-tuning, the\nenhanced VoP achieves a 1.4% average R@1 gain across five text-video retrieval\nbenchmarks with 6x less parameter overhead. The code will be available at\nhttps://github.com/bighuang624/VoP.\n","authors":["Siteng Huang","Biao Gong","Yulin Pan","Jianwen Jiang","Yiliang Lv","Yuyuan Li","Donglin Wang"],"pdf_url":"https://arxiv.org/pdf/2211.12764v3.pdf","comment":"Accepted by CVPR 2023"},{"id":"http://arxiv.org/abs/2303.12810v1","updated":"2023-03-22T22:53:44Z","published":"2023-03-22T22:53:44Z","title":"Are LLMs the Master of All Trades? : Exploring Domain-Agnostic Reasoning\n  Skills of LLMs","summary":"  The potential of large language models (LLMs) to reason like humans has been\na highly contested topic in Machine Learning communities. However, the\nreasoning abilities of humans are multifaceted and can be seen in various\nforms, including analogical, spatial and moral reasoning, among others. This\nfact raises the question whether LLMs can perform equally well across all these\ndifferent domains. This research work aims to investigate the performance of\nLLMs on different reasoning tasks by conducting experiments that directly use\nor draw inspirations from existing datasets on analogical and spatial\nreasoning. Additionally, to evaluate the ability of LLMs to reason like human,\ntheir performance is evaluted on more open-ended, natural language questions.\nMy findings indicate that LLMs excel at analogical and moral reasoning, yet\nstruggle to perform as proficiently on spatial reasoning tasks. I believe these\nexperiments are crucial for informing the future development of LLMs,\nparticularly in contexts that require diverse reasoning proficiencies. By\nshedding light on the reasoning abilities of LLMs, this study aims to push\nforward our understanding of how they can better emulate the cognitive\nabilities of humans.\n","authors":["Shrivats Agrawal"],"pdf_url":"https://arxiv.org/pdf/2303.12810v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.12936v1","updated":"2023-03-22T22:31:09Z","published":"2023-03-22T22:31:09Z","title":"Analyzing the Generalizability of Deep Contextualized Language\n  Representations For Text Classification","summary":"  This study evaluates the robustness of two state-of-the-art deep contextual\nlanguage representations, ELMo and DistilBERT, on supervised learning of binary\nprotest news classification and sentiment analysis of product reviews. A\n\"cross-context\" setting is enabled using test sets that are distinct from the\ntraining data. Specifically, in the news classification task, the models are\ndeveloped on local news from India and tested on the local news from China. In\nthe sentiment analysis task, the models are trained on movie reviews and tested\non customer reviews. This comparison is aimed at exploring the limits of the\nrepresentative power of today's Natural Language Processing systems on the path\nto the systems that are generalizable to real-life scenarios. The models are\nfine-tuned and fed into a Feed-Forward Neural Network and a Bidirectional Long\nShort Term Memory network. Multinomial Naive Bayes and Linear Support Vector\nMachine are used as traditional baselines. The results show that, in binary\ntext classification, DistilBERT is significantly better than ELMo on\ngeneralizing to the cross-context setting. ELMo is observed to be significantly\nmore robust to the cross-context test data than both baselines. On the other\nhand, the baselines performed comparably well to ELMo when the training and\ntest data are subsets of the same corpus (no cross-context). DistilBERT is also\nfound to be 30% smaller and 83% faster than ELMo. The results suggest that\nDistilBERT can transfer generic semantic knowledge to other domains better than\nELMo. DistilBERT is also favorable in incorporating into real-life systems for\nit requires a smaller computational training budget. When generalization is not\nthe utmost preference and test domain is similar to the training domain, the\ntraditional ML algorithms can still be considered as more economic alternatives\nto deep language representations.\n","authors":["Berfu Buyukoz"],"pdf_url":"https://arxiv.org/pdf/2303.12936v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2111.03837v2","updated":"2023-03-22T22:18:01Z","published":"2021-11-06T09:04:16Z","title":"Focusing on Potential Named Entities During Active Label Acquisition","summary":"  Named entity recognition (NER) aims to identify mentions of named entities in\nan unstructured text and classify them into predefined named entity classes.\nWhile deep learning-based pre-trained language models help to achieve good\npredictive performances in NER, many domain-specific NER applications still\ncall for a substantial amount of labeled data. Active learning (AL), a general\nframework for the label acquisition problem, has been used for NER tasks to\nminimize the annotation cost without sacrificing model performance. However,\nthe heavily imbalanced class distribution of tokens introduces challenges in\ndesigning effective AL querying methods for NER. We propose several AL sentence\nquery evaluation functions that pay more attention to potential positive\ntokens, and evaluate these proposed functions with both sentence-based and\ntoken-based cost evaluation strategies. We also propose a better data-driven\nnormalization approach to penalize sentences that are too long or too short.\nOur experiments on three datasets from different domains reveal that the\nproposed approach reduces the number of annotated tokens while achieving better\nor comparable prediction performance with conventional methods.\n","authors":["Ali Osman Berk Sapci","Oznur Tastan","Reyyan Yeniterzi"],"pdf_url":"https://arxiv.org/pdf/2111.03837v2.pdf","comment":"20 pages, 8 figures"},{"id":"http://arxiv.org/abs/2301.09209v2","updated":"2023-03-22T21:35:42Z","published":"2023-01-22T21:30:12Z","title":"Summarize the Past to Predict the Future: Natural Language Descriptions\n  of Context Boost Multimodal Object Interaction","summary":"  We study object interaction anticipation in egocentric videos. This task\nrequires an understanding of the spatiotemporal context formed by past actions\non objects, coined action context. We propose TransFusion, a multimodal\ntransformer-based architecture. It exploits the representational power of\nlanguage by summarising the action context. TransFusion leverages pre-trained\nimage captioning and vision-language models to extract the action context from\npast video frames. This action context together with the next video frame is\nprocessed by the multimodal fusion module to forecast the next object\ninteraction. Our model enables more efficient end-to-end learning. The large\npre-trained language models add common sense and a generalisation capability.\nExperiments on Ego4D and EPIC-KITCHENS-100 show the effectiveness of our\nmultimodal fusion model. They also highlight the benefits of using\nlanguage-based context summaries in a task where vision seems to suffice. Our\nmethod outperforms state-of-the-art approaches by 40.4% in relative terms in\noverall mAP on the Ego4D test set. We validate the effectiveness of TransFusion\nvia experiments on EPIC-KITCHENS-100. Video and code are available at:\nhttps://eth-ait.github.io/transfusion-proj/.\n","authors":["Razvan-George Pasca","Alexey Gavryushin","Yen-Ling Kuo","Luc Van Gool","Otmar Hilliges","Xi Wang"],"pdf_url":"https://arxiv.org/pdf/2301.09209v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2205.09229v3","updated":"2023-03-22T21:10:42Z","published":"2022-05-18T22:15:20Z","title":"PromptDA: Label-guided Data Augmentation for Prompt-based Few-shot\n  Learners","summary":"  Recent advances in large pre-trained language models (PLMs) lead to\nimpressive gains in natural language understanding (NLU) tasks with\ntask-specific fine-tuning. However, directly fine-tuning PLMs heavily relies on\nsufficient labeled training instances, which are usually hard to obtain.\nPrompt-based tuning on PLMs has shown to be powerful for various downstream\nfew-shot tasks. Existing works studying prompt-based tuning for few-shot NLU\ntasks mainly focus on deriving proper label words with a verbalizer or\ngenerating prompt templates to elicit semantics from PLMs. In addition,\nconventional data augmentation strategies such as synonym substitution, though\nwidely adopted in low-resource scenarios, only bring marginal improvements for\nprompt-based few-shot learning. Thus, an important research question arises:\nhow to design effective data augmentation methods for prompt-based few-shot\ntuning? To this end, considering the label semantics are essential in\nprompt-based tuning, we propose a novel label-guided data augmentation\nframework PromptDA, which exploits the enriched label semantic information for\ndata augmentation. Extensive experiment results on few-shot text classification\ntasks demonstrate the superior performance of the proposed framework by\neffectively leveraging label semantics and data augmentation for natural\nlanguage understanding. Our code is available at\nhttps://github.com/canyuchen/PromptDA.\n","authors":["Canyu Chen","Kai Shu"],"pdf_url":"https://arxiv.org/pdf/2205.09229v3.pdf","comment":"Accepted to Proceedings of EACL 2023 main conference. Code is\n  available at https://github.com/canyuchen/PromptDA"},{"id":"http://arxiv.org/abs/2303.12898v1","updated":"2023-03-22T20:26:30Z","published":"2023-03-22T20:26:30Z","title":"Towards Understanding the Generalization of Medical Text-to-SQL Models\n  and Datasets","summary":"  Electronic medical records (EMRs) are stored in relational databases. It can\nbe challenging to access the required information if the user is unfamiliar\nwith the database schema or general database fundamentals. Hence, researchers\nhave explored text-to-SQL generation methods that provide healthcare\nprofessionals direct access to EMR data without needing a database expert.\nHowever, currently available datasets have been essentially \"solved\" with\nstate-of-the-art models achieving accuracy greater than or near 90%. In this\npaper, we show that there is still a long way to go before solving text-to-SQL\ngeneration in the medical domain. To show this, we create new splits of the\nexisting medical text-to-SQL dataset MIMICSQL that better measure the\ngeneralizability of the resulting models. We evaluate state-of-the-art language\nmodels on our new split showing substantial drops in performance with accuracy\ndropping from up to 92% to 28%, thus showing substantial room for improvement.\nMoreover, we introduce a novel data augmentation approach to improve the\ngeneralizability of the language models. Overall, this paper is the first step\ntowards developing more robust text-to-SQL models in the medical\ndomain.\\footnote{The dataset and code will be released upon acceptance.\n","authors":["Richard Tarbell","Kim-Kwang Raymond Choo","Glenn Dietrich","Anthony Rios"],"pdf_url":"https://arxiv.org/pdf/2303.12898v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.12024v2","updated":"2023-03-22T20:16:47Z","published":"2023-03-21T17:04:44Z","title":"cTBL: Augmenting Large Language Models for Conversational Tables","summary":"  An open challenge in multimodal conversational AI requires augmenting large\nlanguage models with information from textual and non-textual sources for\nmulti-turn dialogue. To address this problem, this paper introduces\nConversational Tables (cTBL), a three-step encoder-decoder approach to retrieve\ntabular information and generate dialogue responses grounded on the retrieved\ninformation. cTBL uses Transformer encoder embeddings for Dense Table Retrieval\nand obtains up to 5% relative improvement in Top-1 and Top-3 accuracy over\nsparse retrieval on the HyrbiDialogue dataset. Additionally, cTBL performs\ntabular knowledge retrieval using both encoder and decoder models, resulting in\nup to 46% relative improvement in ROUGE scores and better human evaluation for\nresponse generation on HyrbiDialogue.\n","authors":["Anirudh S Sundar","Larry Heck"],"pdf_url":"https://arxiv.org/pdf/2303.12024v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.12892v1","updated":"2023-03-22T20:10:29Z","published":"2023-03-22T20:10:29Z","title":"A Small-Scale Switch Transformer and NLP-based Model for Clinical\n  Narratives Classification","summary":"  In recent years, Transformer-based models such as the Switch Transformer have\nachieved remarkable results in natural language processing tasks. However,\nthese models are often too complex and require extensive pre-training, which\nlimits their effectiveness for small clinical text classification tasks with\nlimited data. In this study, we propose a simplified Switch Transformer\nframework and train it from scratch on a small French clinical text\nclassification dataset at CHU Sainte-Justine hospital. Our results demonstrate\nthat the simplified small-scale Transformer models outperform pre-trained\nBERT-based models, including DistillBERT, CamemBERT, FlauBERT, and FrALBERT.\nAdditionally, using a mixture of expert mechanisms from the Switch Transformer\nhelps capture diverse patterns; hence, the proposed approach achieves better\nresults than a conventional Transformer with the self-attention mechanism.\nFinally, our proposed framework achieves an accuracy of 87\\%, precision at\n87\\%, and recall at 85\\%, compared to the third-best pre-trained BERT-based\nmodel, FlauBERT, which achieved an accuracy of 84\\%, precision at 84\\%, and\nrecall at 84\\%. However, Switch Transformers have limitations, including a\ngeneralization gap and sharp minima. We compare it with a multi-layer\nperceptron neural network for small French clinical narratives classification\nand show that the latter outperforms all other models.\n","authors":["Thanh-Dung Le","Philippe Jouvet","Rita Noumeir"],"pdf_url":"https://arxiv.org/pdf/2303.12892v1.pdf","comment":"Submitted to IEEE Journal of Biomedical and Health Informatics"},{"id":"http://arxiv.org/abs/2210.15387v2","updated":"2023-03-22T19:38:02Z","published":"2022-10-27T12:48:10Z","title":"Automatic Severity Assessment of Dysarthric speech by using\n  Self-supervised Model with Multi-task Learning","summary":"  Automatic assessment of dysarthric speech is essential for sustained\ntreatments and rehabilitation. However, obtaining atypical speech is\nchallenging, often leading to data scarcity issues. To tackle the problem, we\npropose a novel automatic severity assessment method for dysarthric speech,\nusing the self-supervised model in conjunction with multi-task learning.\nWav2vec 2.0 XLS-R is jointly trained for two different tasks: severity\nclassification and auxiliary automatic speech recognition (ASR). For the\nbaseline experiments, we employ hand-crafted acoustic features and machine\nlearning classifiers such as SVM, MLP, and XGBoost. Explored on the Korean\ndysarthric speech QoLT database, our model outperforms the traditional baseline\nmethods, with a relative percentage increase of 1.25% for F1-score. In\naddition, the proposed model surpasses the model trained without ASR head,\nachieving 10.61% relative percentage improvements. Furthermore, we present how\nmulti-task learning affects the severity classification performance by\nanalyzing the latent representations and regularization effect.\n","authors":["Eun Jung Yeo","Kwanghee Choi","Sunhee Kim","Minhwa Chung"],"pdf_url":"https://arxiv.org/pdf/2210.15387v2.pdf","comment":"Accepted to ICASSP 2023"},{"id":"http://arxiv.org/abs/2303.12869v1","updated":"2023-03-22T19:01:25Z","published":"2023-03-22T19:01:25Z","title":"JaCoText: A Pretrained Model for Java Code-Text Generation","summary":"  Pretrained transformer-based models have shown high performance in natural\nlanguage generation task. However, a new wave of interest has surged: automatic\nprogramming language generation. This task consists of translating natural\nlanguage instructions to a programming code. Despite the fact that well-known\npretrained models on language generation have achieved good performance in\nlearning programming languages, effort is still needed in automatic code\ngeneration. In this paper, we introduce JaCoText, a model based on Transformers\nneural network. It aims to generate java source code from natural language\ntext. JaCoText leverages advantages of both natural language and code\ngeneration models. More specifically, we study some findings from the state of\nthe art and use them to (1) initialize our model from powerful pretrained\nmodels, (2) explore additional pretraining on our java dataset, (3) carry out\nexperiments combining the unimodal and bimodal data in the training, and (4)\nscale the input and output length during the fine-tuning of the model.\nConducted experiments on CONCODE dataset show that JaCoText achieves new\nstate-of-the-art results.\n","authors":["Jessica López Espejel","Mahaman Sanoussi Yahaya Alassan","Walid Dahhane","El Hassane Ettifouri"],"pdf_url":"https://arxiv.org/pdf/2303.12869v1.pdf","comment":"International Conference on Code Generation and Implementation\n  Volume: 17"},{"id":"http://arxiv.org/abs/2303.12860v1","updated":"2023-03-22T18:49:43Z","published":"2023-03-22T18:49:43Z","title":"Salient Span Masking for Temporal Understanding","summary":"  Salient Span Masking (SSM) has shown itself to be an effective strategy to\nimprove closed-book question answering performance. SSM extends general masked\nlanguage model pretraining by creating additional unsupervised training\nsentences that mask a single entity or date span, thus oversampling factual\ninformation. Despite the success of this paradigm, the span types and sampling\nstrategies are relatively arbitrary and not widely studied for other tasks.\nThus, we investigate SSM from the perspective of temporal tasks, where learning\na good representation of various temporal expressions is important. To that\nend, we introduce Temporal Span Masking (TSM) intermediate training. First, we\nfind that SSM alone improves the downstream performance on three temporal tasks\nby an avg. +5.8 points. Further, we are able to achieve additional improvements\n(avg. +0.29 points) by adding the TSM task. These comprise the new best\nreported results on the targeted tasks. Our analysis suggests that the\neffectiveness of SSM stems from the sentences chosen in the training data\nrather than the mask choice: sentences with entities frequently also contain\ntemporal expressions. Nonetheless, the additional targeted spans of TSM can\nstill improve performance, especially in a zero-shot context.\n","authors":["Jeremy R. Cole","Aditi Chaudhary","Bhuwan Dhingra","Partha Talukdar"],"pdf_url":"https://arxiv.org/pdf/2303.12860v1.pdf","comment":"5 pages 1 figure, to appear in EACL 2023"},{"id":"http://arxiv.org/abs/2210.14868v2","updated":"2023-03-22T18:37:20Z","published":"2022-10-26T17:17:06Z","title":"Multi-lingual Evaluation of Code Generation Models","summary":"  We present new benchmarks on evaluation code generation models: MBXP and\nMultilingual HumanEval, and MathQA-X. These datasets cover over 10 programming\nlanguages and are generated using a scalable conversion framework that\ntranspiles prompts and test cases from the original Python datasets into the\ncorresponding data in the target language. Using these benchmarks, we are able\nto assess the performance of code generation models in a multi-lingual fashion,\nand discovered generalization ability of language models on out-of-domain\nlanguages, advantages of multi-lingual models over mono-lingual, the ability of\nfew-shot prompting to teach the model new languages, and zero-shot translation\nabilities even on mono-lingual settings. Furthermore, we use our code\ngeneration model to perform large-scale bootstrapping to obtain synthetic\ncanonical solutions in several languages, which can be used for other\ncode-related evaluations such as code insertion, robustness, or summarization\ntasks. Overall, our benchmarks represents a significant step towards a deeper\nunderstanding of language models' code generation abilities. We publicly\nrelease our code and datasets at https://github.com/amazon-research/mxeval.\n","authors":["Ben Athiwaratkun","Sanjay Krishna Gouda","Zijian Wang","Xiaopeng Li","Yuchen Tian","Ming Tan","Wasi Uddin Ahmad","Shiqi Wang","Qing Sun","Mingyue Shang","Sujan Kumar Gonugondla","Hantian Ding","Varun Kumar","Nathan Fulton","Arash Farahani","Siddhartha Jain","Robert Giaquinto","Haifeng Qian","Murali Krishna Ramanathan","Ramesh Nallapati","Baishakhi Ray","Parminder Bhatia","Sudipta Sengupta","Dan Roth","Bing Xiang"],"pdf_url":"https://arxiv.org/pdf/2210.14868v2.pdf","comment":"Code and data release: https://github.com/amazon-research/mxeval"},{"id":"http://arxiv.org/abs/2303.13463v1","updated":"2023-03-22T15:32:40Z","published":"2023-03-22T15:32:40Z","title":"W2KPE: Keyphrase Extraction with Word-Word Relation","summary":"  This paper describes our submission to ICASSP 2023 MUG Challenge Track 4,\nKeyphrase Extraction, which aims to extract keyphrases most relevant to the\nconference theme from conference materials. We model the challenge as a\nsingle-class Named Entity Recognition task and developed techniques for better\nperformance on the challenge: For the data preprocessing, we encode the split\nkeyphrases after word segmentation. In addition, we increase the amount of\ninput information that the model can accept at one time by fusing multiple\npreprocessed sentences into one segment. We replace the loss function with the\nmulti-class focal loss to address the sparseness of keyphrases. Besides, we\nscore each appearance of keyphrases and add an extra output layer to fit the\nscore to rank keyphrases. Exhaustive evaluations are performed to find the best\ncombination of the word segmentation tool, the pre-trained embedding model, and\nthe corresponding hyperparameters. With these proposals, we scored 45.04 on the\nfinal test set.\n","authors":["Wen Cheng","Shichen Dong","Wei Wang"],"pdf_url":"https://arxiv.org/pdf/2303.13463v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.13466v1","updated":"2023-03-22T13:46:16Z","published":"2023-03-22T13:46:16Z","title":"Extracting Physical Rehabilitation Exercise Information from Clinical\n  Notes: a Comparison of Rule-Based and Machine Learning Natural Language\n  Processing Techniques","summary":"  Physical rehabilitation plays a crucial role in the recovery process of\npost-stroke patients. By personalizing therapies for patients leveraging\npredictive modeling and electronic health records (EHRs), healthcare providers\ncan make the rehabilitation process more efficient. Before predictive modeling\ncan provide decision support for the assignment of treatment plans, automated\nmethods are necessary to extract physical rehabilitation exercise information\nfrom unstructured EHRs. We introduce a rule-based natural language processing\nalgorithm to annotate therapeutic procedures for stroke patients and compare it\nto several small machine learning models. We find that our algorithm\noutperforms these models in extracting half of the concepts where sufficient\ndata is available, and individual exercise descriptions can be assigned binary\nlabels with an f-score of no less than 0.75 per concept. More research needs to\nbe done before these algorithms can be deployed on unlabeled documents, but\ncurrent progress gives promise to the potential of precision rehabilitation\nresearch.\n","authors":["Stephen W. Shaffran","Fengyi Gao","Parker E. Denny","Bayan M. Aldhahwani","Allyn Bove","Shyam Visweswaran","Yanshan Wang"],"pdf_url":"https://arxiv.org/pdf/2303.13466v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.13465v1","updated":"2023-03-22T09:29:22Z","published":"2023-03-22T09:29:22Z","title":"Deep RL with Hierarchical Action Exploration for Dialogue Generation","summary":"  Conventionally, since the natural language action space is astronomical,\napproximate dynamic programming applied to dialogue generation involves policy\nimprovement with action sampling. However, such a practice is inefficient for\nreinforcement learning (RL) because the eligible (high action value) responses\nare very sparse, and the greedy policy sustained by the random sampling is\nflabby. This paper shows that the performance of dialogue policy positively\ncorrelated with sampling size by theoretical and experimental. We introduce a\nnovel dual-granularity Q-function to alleviate this limitation by exploring the\nmost promising response category to intervene in the sampling. It extracts the\nactions following the grained hierarchy, which can achieve the optimum with\nfewer policy iterations. Our approach learns in the way of offline RL from\nmultiple reward functions designed to recognize human emotional details.\nEmpirical studies demonstrate that our algorithm outperforms the baseline\nmethods. Further verification presents that ours can generate responses with\nhigher expected rewards and controllability.\n","authors":["Itsugun Cho","Ryota Takahashi","Yusaku Yanase","Hiroaki Saito"],"pdf_url":"https://arxiv.org/pdf/2303.13465v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.12816v1","updated":"2023-03-22T07:34:33Z","published":"2023-03-22T07:34:33Z","title":"From Wide to Deep: Dimension Lifting Network for Parameter-efficient\n  Knowledge Graph Embedding","summary":"  Knowledge graph embedding (KGE) that maps entities and relations into vector\nrepresentations is essential for downstream tasks. Conventional KGE methods\nrequire relatively high-dimensional entity representations to preserve the\nstructural information of knowledge graph, but lead to oversized model\nparameters. Recent methods reduce model parameters by adopting low-dimensional\nentity representations, while developing techniques (e.g., knowledge\ndistillation) to compensate for the reduced dimension. However, such operations\nproduce degraded model accuracy and limited reduction of model parameters.\nSpecifically, we view the concatenation of all entity representations as an\nembedding layer, and then conventional KGE methods that adopt high-dimensional\nentity representations equal to enlarging the width of the embedding layer to\ngain expressiveness. To achieve parameter efficiency without sacrificing\naccuracy, we instead increase the depth and propose a deeper embedding network\nfor entity representations, i.e., a narrow embedding layer and a multi-layer\ndimension lifting network (LiftNet). Experiments on three public datasets show\nthat the proposed method (implemented based on TransE and DistMult) with\n4-dimensional entity representations achieves more accurate link prediction\nresults than counterpart parameter-efficient KGE methods and strong KGE\nbaselines, including TransE and DistMult with 512-dimensional entity\nrepresentations.\n","authors":["Borui Cai","Yong Xiang","Longxiang Gao","Di Wu","He Zhang","Jiong Jin","Tom Luan"],"pdf_url":"https://arxiv.org/pdf/2303.12816v1.pdf","comment":null}],"Computer Vision and Pattern Recognition":[{"id":"http://arxiv.org/abs/2303.12793v1","updated":"2023-03-22T17:59:59Z","published":"2023-03-22T17:59:59Z","title":"CiCo: Domain-Aware Sign Language Retrieval via Cross-Lingual Contrastive\n  Learning","summary":"  This work focuses on sign language retrieval-a recently proposed task for\nsign language understanding. Sign language retrieval consists of two sub-tasks:\ntext-to-sign-video (T2V) retrieval and sign-video-to-text (V2T) retrieval.\nDifferent from traditional video-text retrieval, sign language videos, not only\ncontain visual signals but also carry abundant semantic meanings by themselves\ndue to the fact that sign languages are also natural languages. Considering\nthis character, we formulate sign language retrieval as a cross-lingual\nretrieval problem as well as a video-text retrieval task. Concretely, we take\ninto account the linguistic properties of both sign languages and natural\nlanguages, and simultaneously identify the fine-grained cross-lingual (i.e.,\nsign-to-word) mappings while contrasting the texts and the sign videos in a\njoint embedding space. This process is termed as cross-lingual contrastive\nlearning. Another challenge is raised by the data scarcity issue-sign language\ndatasets are orders of magnitude smaller in scale than that of speech\nrecognition. We alleviate this issue by adopting a domain-agnostic sign encoder\npre-trained on large-scale sign videos into the target domain via\npseudo-labeling. Our framework, termed as domain-aware sign language retrieval\nvia Cross-lingual Contrastive learning or CiCo for short, outperforms the\npioneering method by large margins on various datasets, e.g., +22.4 T2V and\n+28.0 V2T R@1 improvements on How2Sign dataset, and +13.7 T2V and +17.1 V2T R@1\nimprovements on PHOENIX-2014T dataset. Code and models are available at:\nhttps://github.com/FangyunWei/SLRT.\n","authors":["Yiting Cheng","Fangyun Wei","Jianmin Bao","Dong Chen","Wenqiang Zhang"],"pdf_url":"https://arxiv.org/pdf/2303.12793v1.pdf","comment":"Accepted by CVPR 2023. Code and models are available at:\n  https://github.com/FangyunWei/SLRT"},{"id":"http://arxiv.org/abs/2303.12791v1","updated":"2023-03-22T17:59:12Z","published":"2023-03-22T17:59:12Z","title":"SHERF: Generalizable Human NeRF from a Single Image","summary":"  Existing Human NeRF methods for reconstructing 3D humans typically rely on\nmultiple 2D images from multi-view cameras or monocular videos captured from\nfixed camera views. However, in real-world scenarios, human images are often\ncaptured from random camera angles, presenting challenges for high-quality 3D\nhuman reconstruction. In this paper, we propose SHERF, the first generalizable\nHuman NeRF model for recovering animatable 3D humans from a single input image.\nSHERF extracts and encodes 3D human representations in canonical space,\nenabling rendering and animation from free views and poses. To achieve\nhigh-fidelity novel view and pose synthesis, the encoded 3D human\nrepresentations should capture both global appearance and local fine-grained\ntextures. To this end, we propose a bank of 3D-aware hierarchical features,\nincluding global, point-level, and pixel-aligned features, to facilitate\ninformative encoding. Global features enhance the information extracted from\nthe single input image and complement the information missing from the partial\n2D observation. Point-level features provide strong clues of 3D human\nstructure, while pixel-aligned features preserve more fine-grained details. To\neffectively integrate the 3D-aware hierarchical feature bank, we design a\nfeature fusion transformer. Extensive experiments on THuman, RenderPeople,\nZJU_MoCap, and HuMMan datasets demonstrate that SHERF achieves state-of-the-art\nperformance, with better generalizability for novel view and pose synthesis.\n","authors":["Shoukang Hu","Fangzhou Hong","Liang Pan","Haiyi Mei","Lei Yang","Ziwei Liu"],"pdf_url":"https://arxiv.org/pdf/2303.12791v1.pdf","comment":"Project webpage: https://skhu101.github.io/SHERF/"},{"id":"http://arxiv.org/abs/2211.09703v2","updated":"2023-03-22T17:58:09Z","published":"2022-11-17T17:38:55Z","title":"EfficientTrain: Exploring Generalized Curriculum Learning for Training\n  Visual Backbones","summary":"  The superior performance of modern deep networks usually comes with a costly\ntraining procedure. This paper presents a new curriculum learning approach for\nthe efficient training of visual backbones (e.g., vision Transformers). Our\nwork is inspired by the inherent learning dynamics of deep networks: we\nexperimentally show that at an earlier training stage, the model mainly learns\nto recognize some 'easier-to-learn' discriminative patterns within each\nexample, e.g., the lower-frequency components of images and the original\ninformation before data augmentation. Driven by this phenomenon, we propose a\ncurriculum where the model always leverages all the training data at each\nepoch, while the curriculum starts with only exposing the 'easier-to-learn'\npatterns of each example, and introduces gradually more difficult patterns. To\nimplement this idea, we 1) introduce a cropping operation in the Fourier\nspectrum of the inputs, which enables the model to learn from only the\nlower-frequency components efficiently, 2) demonstrate that exposing the\nfeatures of original images amounts to adopting weaker data augmentation, and\n3) integrate 1) and 2) and design a curriculum learning schedule with a\ngreedy-search algorithm. The resulting approach, EfficientTrain, is simple,\ngeneral, yet surprisingly effective. In the absence of hyper-parameter tuning,\nit reduces the training wall-time of a wide variety of popular models (e.g.,\nResNet, ConvNeXt, DeiT, PVT, Swin, and CSWin) by >1.5x on ImageNet-1K/22K\nwithout sacrificing the accuracy. It is also effective for self-supervised\nlearning (e.g., MAE). Code is available at\nhttps://github.com/LeapLabTHU/EfficientTrain.\n","authors":["Yulin Wang","Yang Yue","Rui Lu","Tianjiao Liu","Zhao Zhong","Shiji Song","Gao Huang"],"pdf_url":"https://arxiv.org/pdf/2211.09703v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.12790v1","updated":"2023-03-22T17:58:01Z","published":"2023-03-22T17:58:01Z","title":"Diffuse-Denoise-Count: Accurate Crowd-Counting with Diffusion Models","summary":"  Crowd counting is a key aspect of crowd analysis and has been typically\naccomplished by estimating a crowd-density map and summing over the density\nvalues. However, this approach suffers from background noise accumulation and\nloss of density due to the use of broad Gaussian kernels to create the ground\ntruth density maps. This issue can be overcome by narrowing the Gaussian\nkernel. However, existing approaches perform poorly when trained with such\nground truth density maps. To overcome this limitation, we propose using\nconditional diffusion models to predict density maps, as diffusion models are\nknown to model complex distributions well and show high fidelity to training\ndata during crowd-density map generation. Furthermore, as the intermediate time\nsteps of the diffusion process are noisy, we incorporate a regression branch\nfor direct crowd estimation only during training to improve the feature\nlearning. In addition, owing to the stochastic nature of the diffusion model,\nwe introduce producing multiple density maps to improve the counting\nperformance contrary to the existing crowd counting pipelines. Further, we also\ndiffer from the density summation and introduce contour detection followed by\nsummation as the counting operation, which is more immune to background noise.\nWe conduct extensive experiments on public datasets to validate the\neffectiveness of our method. Specifically, our novel crowd-counting pipeline\nimproves the error of crowd-counting by up to $6\\%$ on JHU-CROWD++ and up to\n$7\\%$ on UCF-QNRF.\n","authors":["Yasiru Ranasinghe","Nithin Gopalakrishnan Nair","Wele Gedara Chaminda Bandara","Vishal M. Patel"],"pdf_url":"https://arxiv.org/pdf/2303.12790v1.pdf","comment":"The project is available at\n  https://github.com/dylran/DiffuseDenoiseCount"},{"id":"http://arxiv.org/abs/2303.12789v1","updated":"2023-03-22T17:57:57Z","published":"2023-03-22T17:57:57Z","title":"Instruct-NeRF2NeRF: Editing 3D Scenes with Instructions","summary":"  We propose a method for editing NeRF scenes with text-instructions. Given a\nNeRF of a scene and the collection of images used to reconstruct it, our method\nuses an image-conditioned diffusion model (InstructPix2Pix) to iteratively edit\nthe input images while optimizing the underlying scene, resulting in an\noptimized 3D scene that respects the edit instruction. We demonstrate that our\nproposed method is able to edit large-scale, real-world scenes, and is able to\naccomplish more realistic, targeted edits than prior work.\n","authors":["Ayaan Haque","Matthew Tancik","Alexei A. Efros","Aleksander Holynski","Angjoo Kanazawa"],"pdf_url":"https://arxiv.org/pdf/2303.12789v1.pdf","comment":"Project website: https://instruct-nerf2nerf.github.io"},{"id":"http://arxiv.org/abs/2303.12787v1","updated":"2023-03-22T17:57:36Z","published":"2023-03-22T17:57:36Z","title":"EPro-PnP: Generalized End-to-End Probabilistic Perspective-n-Points for\n  Monocular Object Pose Estimation","summary":"  Locating 3D objects from a single RGB image via Perspective-n-Point (PnP) is\na long-standing problem in computer vision. Driven by end-to-end deep learning,\nrecent studies suggest interpreting PnP as a differentiable layer, allowing for\npartial learning of 2D-3D point correspondences by backpropagating the\ngradients of pose loss. Yet, learning the entire correspondences from scratch\nis highly challenging, particularly for ambiguous pose solutions, where the\nglobally optimal pose is theoretically non-differentiable w.r.t. the points. In\nthis paper, we propose the EPro-PnP, a probabilistic PnP layer for general\nend-to-end pose estimation, which outputs a distribution of pose with\ndifferentiable probability density on the SE(3) manifold. The 2D-3D coordinates\nand corresponding weights are treated as intermediate variables learned by\nminimizing the KL divergence between the predicted and target pose\ndistribution. The underlying principle generalizes previous approaches, and\nresembles the attention mechanism. EPro-PnP can enhance existing correspondence\nnetworks, closing the gap between PnP-based method and the task-specific\nleaders on the LineMOD 6DoF pose estimation benchmark. Furthermore, EPro-PnP\nhelps to explore new possibilities of network design, as we demonstrate a novel\ndeformable correspondence network with the state-of-the-art pose accuracy on\nthe nuScenes 3D object detection benchmark. Our code is available at\nhttps://github.com/tjiiv-cprg/EPro-PnP-v2.\n","authors":["Hansheng Chen","Wei Tian","Pichao Wang","Fan Wang","Lu Xiong","Hao Li"],"pdf_url":"https://arxiv.org/pdf/2303.12787v1.pdf","comment":"Code available at https://github.com/tjiiv-cprg/EPro-PnP-v2. arXiv\n  admin note: substantial text overlap with arXiv:2203.13254"},{"id":"http://arxiv.org/abs/2303.12786v1","updated":"2023-03-22T17:57:01Z","published":"2023-03-22T17:57:01Z","title":"FeatureNeRF: Learning Generalizable NeRFs by Distilling Foundation\n  Models","summary":"  Recent works on generalizable NeRFs have shown promising results on novel\nview synthesis from single or few images. However, such models have rarely been\napplied on other downstream tasks beyond synthesis such as semantic\nunderstanding and parsing. In this paper, we propose a novel framework named\nFeatureNeRF to learn generalizable NeRFs by distilling pre-trained vision\nfoundation models (e.g., DINO, Latent Diffusion). FeatureNeRF leverages 2D\npre-trained foundation models to 3D space via neural rendering, and then\nextract deep features for 3D query points from NeRF MLPs. Consequently, it\nallows to map 2D images to continuous 3D semantic feature volumes, which can be\nused for various downstream tasks. We evaluate FeatureNeRF on tasks of 2D/3D\nsemantic keypoint transfer and 2D/3D object part segmentation. Our extensive\nexperiments demonstrate the effectiveness of FeatureNeRF as a generalizable 3D\nsemantic feature extractor. Our project page is available at\nhttps://jianglongye.com/featurenerf/ .\n","authors":["Jianglong Ye","Naiyan Wang","Xiaolong Wang"],"pdf_url":"https://arxiv.org/pdf/2303.12786v1.pdf","comment":"Project page: https://jianglongye.com/featurenerf/"},{"id":"http://arxiv.org/abs/2303.12782v1","updated":"2023-03-22T17:52:11Z","published":"2023-03-22T17:52:11Z","title":"Tube-Link: A Flexible Cross Tube Baseline for Universal Video\n  Segmentation","summary":"  The goal of video segmentation is to accurately segment and track every pixel\nin diverse scenarios. In this paper, we present Tube-Link, a versatile\nframework that addresses multiple core tasks of video segmentation with a\nunified architecture. Our framework is a near-online approach that takes a\nshort subclip as input and outputs the corresponding spatial-temporal tube\nmasks. To enhance the modeling of cross-tube relationships, we propose an\neffective way to perform tube-level linking via attention along the queries. In\naddition, we introduce temporal contrastive learning to instance-wise\ndiscriminative features for tube-level association. Our approach offers\nflexibility and efficiency for both short and long video inputs, as the length\nof each subclip can be varied according to the needs of datasets or scenarios.\nTube-Link outperforms existing specialized architectures by a significant\nmargin on five video segmentation datasets. Specifically, it achieves almost\n13% relative improvements on VIPSeg and 4% improvements on KITTI-STEP over the\nstrong baseline Video K-Net. When using a ResNet50 backbone on Youtube-VIS-2019\nand 2021, Tube-Link boosts IDOL by 3% and 4%, respectively. Code will be\navailable.\n","authors":["Xiangtai Li","Haobo Yuan","Wenwei Zhang","Guangliang Cheng","Jiangmiao Pang","Chen Change Loy"],"pdf_url":"https://arxiv.org/pdf/2303.12782v1.pdf","comment":"Project page: https://github.com/lxtGH/Tube-Link"},{"id":"http://arxiv.org/abs/2303.12779v1","updated":"2023-03-22T17:46:27Z","published":"2023-03-22T17:46:27Z","title":"LFM-3D: Learnable Feature Matching Across Wide Baselines Using 3D\n  Signals","summary":"  Finding localized correspondences across different images of the same object\nis crucial to understand its geometry. In recent years, this problem has seen\nremarkable progress with the advent of deep learning based local image features\nand learnable matchers. Still, learnable matchers often underperform when there\nexists only small regions of co-visibility between image pairs (i.e. wide\ncamera baselines). To address this problem, we leverage recent progress in\ncoarse single-view geometry estimation methods. We propose LFM-3D, a Learnable\nFeature Matching framework that uses models based on graph neural networks, and\nenhances their capabilities by integrating noisy, estimated 3D signals to boost\ncorrespondence estimation. When integrating 3D signals into the matcher model,\nwe show that a suitable positional encoding is critical to effectively make use\nof the low-dimensional 3D information. We experiment with two different 3D\nsignals - normalized object coordinates and monocular depth estimates - and\nevaluate our method on large-scale (synthetic and real) datasets containing\nobject-centric image pairs across wide baselines. We observe strong feature\nmatching improvements compared to 2D-only methods, with up to +6% total recall\nand +28% precision at fixed recall. We additionally demonstrate that the\nresulting improved correspondences lead to much higher relative posing accuracy\nfor in-the-wild image pairs, with a more than 8% boost compared to the 2D-only\napproach.\n","authors":["Arjun Karpur","Guilherme Perrotta","Ricardo Martin-Brualla","Howard Zhou","Andre Araujo"],"pdf_url":"https://arxiv.org/pdf/2303.12779v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2302.06514v3","updated":"2023-03-22T17:46:15Z","published":"2023-02-13T16:49:27Z","title":"Multiple Appropriate Facial Reaction Generation in Dyadic Interaction\n  Settings: What, Why and How?","summary":"  According to the Stimulus Organism Response (SOR) theory, all human\nbehavioral reactions are stimulated by context, where people will process the\nreceived stimulus and produce an appropriate reaction. This implies that in a\nspecific context for a given input stimulus, a person can react differently\naccording to their internal state and other contextual factors. Analogously, in\ndyadic interactions, humans communicate using verbal and nonverbal cues, where\na broad spectrum of listeners' non-verbal reactions might be appropriate for\nresponding to a specific speaker behaviour. There already exists a body of work\nthat investigated the problem of automatically generating an appropriate\nreaction for a given input. However, none attempted to automatically generate\nmultiple appropriate reactions in the context of dyadic interactions and\nevaluate the appropriateness of those reactions using objective measures. This\npaper starts by defining the facial Multiple Appropriate Reaction Generation\n(fMARG) task for the first time in the literature and proposes a new set of\nobjective evaluation metrics to evaluate the appropriateness of the generated\nreactions. The paper subsequently introduces a framework to predict, generate,\nand evaluate multiple appropriate facial reactions.\n","authors":["Siyang Song","Micol Spitale","Yiming Luo","Batuhan Bal","Hatice Gunes"],"pdf_url":"https://arxiv.org/pdf/2302.06514v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.12776v1","updated":"2023-03-22T17:42:22Z","published":"2023-03-22T17:42:22Z","title":"Dense Distinct Query for End-to-End Object Detection","summary":"  One-to-one label assignment in object detection has successfully obviated the\nneed for non-maximum suppression (NMS) as postprocessing and makes the pipeline\nend-to-end. However, it triggers a new dilemma as the widely used sparse\nqueries cannot guarantee a high recall, while dense queries inevitably bring\nmore similar queries and encounter optimization difficulties. As both sparse\nand dense queries are problematic, then what are the expected queries in\nend-to-end object detection? This paper shows that the solution should be Dense\nDistinct Queries (DDQ). Concretely, we first lay dense queries like traditional\ndetectors and then select distinct ones for one-to-one assignments. DDQ blends\nthe advantages of traditional and recent end-to-end detectors and significantly\nimproves the performance of various detectors including FCN, R-CNN, and DETRs.\nMost impressively, DDQ-DETR achieves 52.1 AP on MS-COCO dataset within 12\nepochs using a ResNet-50 backbone, outperforming all existing detectors in the\nsame setting. DDQ also shares the benefit of end-to-end detectors in crowded\nscenes and achieves 93.8 AP on CrowdHuman. We hope DDQ can inspire researchers\nto consider the complementarity between traditional methods and end-to-end\ndetectors. The source code can be found at\n\\url{https://github.com/jshilong/DDQ}.\n","authors":["Shilong Zhang","Wang xinjiang","Jiaqi Wang","Jiangmiao Pang","Chengqi Lyu","Wenwei Zhang","Ping Luo","Kai Chen"],"pdf_url":"https://arxiv.org/pdf/2303.12776v1.pdf","comment":"Accepted to CVPR2023. Code has been released at\n  https://github.com/jshilong/DDQ"},{"id":"http://arxiv.org/abs/2303.12766v1","updated":"2023-03-22T17:30:14Z","published":"2023-03-22T17:30:14Z","title":"Spherical Transformer for LiDAR-based 3D Recognition","summary":"  LiDAR-based 3D point cloud recognition has benefited various applications.\nWithout specially considering the LiDAR point distribution, most current\nmethods suffer from information disconnection and limited receptive field,\nespecially for the sparse distant points. In this work, we study the\nvarying-sparsity distribution of LiDAR points and present SphereFormer to\ndirectly aggregate information from dense close points to the sparse distant\nones. We design radial window self-attention that partitions the space into\nmultiple non-overlapping narrow and long windows. It overcomes the\ndisconnection issue and enlarges the receptive field smoothly and dramatically,\nwhich significantly boosts the performance of sparse distant points. Moreover,\nto fit the narrow and long windows, we propose exponential splitting to yield\nfine-grained position encoding and dynamic feature selection to increase model\nrepresentation ability. Notably, our method ranks 1st on both nuScenes and\nSemanticKITTI semantic segmentation benchmarks with 81.9% and 74.8% mIoU,\nrespectively. Also, we achieve the 3rd place on nuScenes object detection\nbenchmark with 72.8% NDS and 68.5% mAP. Code is available at\nhttps://github.com/dvlab-research/SphereFormer.git.\n","authors":["Xin Lai","Yukang Chen","Fanbin Lu","Jianhui Liu","Jiaya Jia"],"pdf_url":"https://arxiv.org/pdf/2303.12766v1.pdf","comment":"Accepted to CVPR 2023. Code is available at\n  https://github.com/dvlab-research/SphereFormer.git"},{"id":"http://arxiv.org/abs/2303.09540v3","updated":"2023-03-22T17:22:35Z","published":"2023-03-16T17:53:24Z","title":"SemDeDup: Data-efficient learning at web-scale through semantic\n  deduplication","summary":"  Progress in machine learning has been driven in large part by massive\nincreases in data. However, large web-scale datasets such as LAION are largely\nuncurated beyond searches for exact duplicates, potentially leaving much\nredundancy. Here, we introduce SemDeDup, a method which leverages embeddings\nfrom pre-trained models to identify and remove semantic duplicates: data pairs\nwhich are semantically similar, but not exactly identical. Removing semantic\nduplicates preserves performance and speeds up learning. Analyzing a subset of\nLAION, we show that SemDeDup can remove 50% of the data with minimal\nperformance loss, effectively halving training time. Moreover, performance\nincreases out of distribution. Also, analyzing language models trained on C4, a\npartially curated dataset, we show that SemDeDup improves over prior approaches\nwhile providing efficiency gains. SemDeDup provides an example of how simple\nways of leveraging quality embeddings can be used to make models learn faster\nwith less data.\n","authors":["Amro Abbas","Kushal Tirumala","Dániel Simig","Surya Ganguli","Ari S. Morcos"],"pdf_url":"https://arxiv.org/pdf/2303.09540v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.12760v1","updated":"2023-03-22T17:14:10Z","published":"2023-03-22T17:14:10Z","title":"Uncertainty Aware Active Learning for Reconfiguration of Pre-trained\n  Deep Object-Detection Networks for New Target Domains","summary":"  Object detection is one of the most important and fundamental aspects of\ncomputer vision tasks, which has been broadly utilized in pose estimation,\nobject tracking and instance segmentation models. To obtain training data for\nobject detection model efficiently, many datasets opt to obtain their\nunannotated data in video format and the annotator needs to draw a bounding box\naround each object in the images. Annotating every frame from a video is costly\nand inefficient since many frames contain very similar information for the\nmodel to learn from. How to select the most informative frames from a video to\nannotate has become a highly practical task to solve but attracted little\nattention in research. In this paper, we proposed a novel active learning\nalgorithm for object detection models to tackle this problem. In the proposed\nactive learning algorithm, both classification and localization informativeness\nof unlabelled data are measured and aggregated. Utilizing the temporal\ninformation from video frames, two novel localization informativeness\nmeasurements are proposed. Furthermore, a weight curve is proposed to avoid\nquerying adjacent frames. Proposed active learning algorithm with multiple\nconfigurations was evaluated on the MuPoTS dataset and FootballPD dataset.\n","authors":["Jiaming Na","Varuna De-Silva"],"pdf_url":"https://arxiv.org/pdf/2303.12760v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.12756v1","updated":"2023-03-22T17:08:31Z","published":"2023-03-22T17:08:31Z","title":"MaskCon: Masked Contrastive Learning for Coarse-Labelled Dataset","summary":"  Deep learning has achieved great success in recent years with the aid of\nadvanced neural network structures and large-scale human-annotated datasets.\nHowever, it is often costly and difficult to accurately and efficiently\nannotate large-scale datasets, especially for some specialized domains where\nfine-grained labels are required. In this setting, coarse labels are much\neasier to acquire as they do not require expert knowledge. In this work, we\npropose a contrastive learning method, called $\\textbf{Mask}$ed\n$\\textbf{Con}$trastive learning~($\\textbf{MaskCon}$) to address the\nunder-explored problem setting, where we learn with a coarse-labelled dataset\nin order to address a finer labelling problem. More specifically, within the\ncontrastive learning framework, for each sample our method generates\nsoft-labels with the aid of coarse labels against other samples and another\naugmented view of the sample in question. By contrast to self-supervised\ncontrastive learning where only the sample's augmentations are considered hard\npositives, and in supervised contrastive learning where only samples with the\nsame coarse labels are considered hard positives, we propose soft labels based\non sample distances, that are masked by the coarse labels. This allows us to\nutilize both inter-sample relations and coarse labels. We demonstrate that our\nmethod can obtain as special cases many existing state-of-the-art works and\nthat it provides tighter bounds on the generalization error. Experimentally,\nour method achieves significant improvement over the current state-of-the-art\nin various datasets, including CIFAR10, CIFAR100, ImageNet-1K, Standford Online\nProducts and Stanford Cars196 datasets. Code and annotations are available at\nhttps://github.com/MrChenFeng/MaskCon_CVPR2023.\n","authors":["Chen Feng","Ioannis Patras"],"pdf_url":"https://arxiv.org/pdf/2303.12756v1.pdf","comment":"CVPR 2023 camera-ready version. Codes are available at\n  https://github.com/MrChenFeng/MaskCon_CVPR2023"},{"id":"http://arxiv.org/abs/2302.02005v2","updated":"2023-03-22T17:03:51Z","published":"2023-02-03T21:20:58Z","title":"DeepAstroUDA: Semi-Supervised Universal Domain Adaptation for\n  Cross-Survey Galaxy Morphology Classification and Anomaly Detection","summary":"  Artificial intelligence methods show great promise in increasing the quality\nand speed of work with large astronomical datasets, but the high complexity of\nthese methods leads to the extraction of dataset-specific, non-robust features.\nTherefore, such methods do not generalize well across multiple datasets. We\npresent a universal domain adaptation method, \\textit{DeepAstroUDA}, as an\napproach to overcome this challenge. This algorithm performs semi-supervised\ndomain adaptation and can be applied to datasets with different data\ndistributions and class overlaps. Non-overlapping classes can be present in any\nof the two datasets (the labeled source domain, or the unlabeled target\ndomain), and the method can even be used in the presence of unknown classes. We\napply our method to three examples of galaxy morphology classification tasks of\ndifferent complexities ($3$-class and $10$-class problems), with anomaly\ndetection: 1) datasets created after different numbers of observing years from\na single survey (LSST mock data of $1$ and $10$ years of observations); 2) data\nfrom different surveys (SDSS and DECaLS); and 3) data from observing fields\nwith different depths within one survey (wide field and Stripe 82 deep field of\nSDSS). For the first time, we demonstrate the successful use of domain\nadaptation between very discrepant observational datasets.\n\\textit{DeepAstroUDA} is capable of bridging the gap between two astronomical\nsurveys, increasing classification accuracy in both domains (up to $40\\%$ on\nthe unlabeled data), and making model performance consistent across datasets.\nFurthermore, our method also performs well as an anomaly detection algorithm\nand successfully clusters unknown class samples even in the unlabeled target\ndataset.\n","authors":["A. Ćiprijanović","A. Lewis","K. Pedro","S. Madireddy","B. Nord","G. N. Perdue","S. M. Wild"],"pdf_url":"https://arxiv.org/pdf/2302.02005v2.pdf","comment":"Accepted in Machine Learning Science and Technology (MLST); 24 pages,\n  14 figures"},{"id":"http://arxiv.org/abs/2201.11097v2","updated":"2023-03-22T17:02:59Z","published":"2022-01-26T18:06:33Z","title":"Adaptive Instance Distillation for Object Detection in Autonomous\n  Driving","summary":"  In recent years, knowledge distillation (KD) has been widely used to derive\nefficient models. Through imitating a large teacher model, a lightweight\nstudent model can achieve comparable performance with more efficiency. However,\nmost existing knowledge distillation methods are focused on classification\ntasks. Only a limited number of studies have applied knowledge distillation to\nobject detection, especially in time-sensitive autonomous driving scenarios. In\nthis paper, we propose Adaptive Instance Distillation (AID) to selectively\nimpart teacher's knowledge to the student to improve the performance of\nknowledge distillation. Unlike previous KD methods that treat all instances\nequally, our AID can attentively adjust the distillation weights of instances\nbased on the teacher model's prediction loss. We verified the effectiveness of\nour AID method through experiments on the KITTI and the COCO traffic datasets.\nThe results show that our method improves the performance of state-of-the-art\nattention-guided and non-local distillation methods and achieves better\ndistillation results on both single-stage and two-stage detectors. Compared to\nthe baseline, our AID led to an average of 2.7% and 2.1% mAP increases for\nsingle-stage and two-stage detectors, respectively. Furthermore, our AID is\nalso shown to be useful for self-distillation to improve the teacher model's\nperformance.\n","authors":["Qizhen Lan","Qing Tian"],"pdf_url":"https://arxiv.org/pdf/2201.11097v2.pdf","comment":"6 pages, 3 figures"},{"id":"http://arxiv.org/abs/2303.12696v1","updated":"2023-03-22T16:42:26Z","published":"2023-03-22T16:42:26Z","title":"Dense Network Expansion for Class Incremental Learning","summary":"  The problem of class incremental learning (CIL) is considered.\nState-of-the-art approaches use a dynamic architecture based on network\nexpansion (NE), in which a task expert is added per task. While effective from\na computational standpoint, these methods lead to models that grow quickly with\nthe number of tasks. A new NE method, dense network expansion (DNE), is\nproposed to achieve a better trade-off between accuracy and model complexity.\nThis is accomplished by the introduction of dense connections between the\nintermediate layers of the task expert networks, that enable the transfer of\nknowledge from old to new tasks via feature sharing and reusing. This sharing\nis implemented with a cross-task attention mechanism, based on a new task\nattention block (TAB), that fuses information across tasks. Unlike traditional\nattention mechanisms, TAB operates at the level of the feature mixing and is\ndecoupled with spatial attentions. This is shown more effective than a joint\nspatial-and-task attention for CIL. The proposed DNE approach can strictly\nmaintain the feature space of old classes while growing the network and feature\nscale at a much slower rate than previous methods. In result, it outperforms\nthe previous SOTA methods by a margin of 4\\% in terms of accuracy, with similar\nor even smaller model scale.\n","authors":["Zhiyuan Hu","Yunsheng Li","Jiancheng Lyu","Dashan Gao","Nuno Vasconcelos"],"pdf_url":"https://arxiv.org/pdf/2303.12696v1.pdf","comment":"Accepted by CVPR2023"},{"id":"http://arxiv.org/abs/2303.12688v1","updated":"2023-03-22T16:36:10Z","published":"2023-03-22T16:36:10Z","title":"Pix2Video: Video Editing using Image Diffusion","summary":"  Image diffusion models, trained on massive image collections, have emerged as\nthe most versatile image generator model in terms of quality and diversity.\nThey support inverting real images and conditional (e.g., text) generation,\nmaking them attractive for high-quality image editing applications. We\ninvestigate how to use such pre-trained image models for text-guided video\nediting. The critical challenge is to achieve the target edits while still\npreserving the content of the source video. Our method works in two simple\nsteps: first, we use a pre-trained structure-guided (e.g., depth) image\ndiffusion model to perform text-guided edits on an anchor frame; then, in the\nkey step, we progressively propagate the changes to the future frames via\nself-attention feature injection to adapt the core denoising step of the\ndiffusion model. We then consolidate the changes by adjusting the latent code\nfor the frame before continuing the process. Our approach is training-free and\ngeneralizes to a wide range of edits. We demonstrate the effectiveness of the\napproach by extensive experimentation and compare it against four different\nprior and parallel efforts (on ArXiv). We demonstrate that realistic\ntext-guided video edits are possible, without any compute-intensive\npreprocessing or video-specific finetuning.\n","authors":["Duygu Ceylan","Chun-Hao Paul Huang","Niloy J. Mitra"],"pdf_url":"https://arxiv.org/pdf/2303.12688v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.09799v2","updated":"2023-03-22T16:34:57Z","published":"2023-03-17T07:02:59Z","title":"Style Transfer for 2D Talking Head Animation","summary":"  Audio-driven talking head animation is a challenging research topic with many\nreal-world applications. Recent works have focused on creating photo-realistic\n2D animation, while learning different talking or singing styles remains an\nopen problem. In this paper, we present a new method to generate talking head\nanimation with learnable style references. Given a set of style reference\nframes, our framework can reconstruct 2D talking head animation based on a\nsingle input image and an audio stream. Our method first produces facial\nlandmarks motion from the audio stream and constructs the intermediate style\npatterns from the style reference images. We then feed both outputs into a\nstyle-aware image generator to generate the photo-realistic and fidelity 2D\nanimation. In practice, our framework can extract the style information of a\nspecific character and transfer it to any new static image for talking head\nanimation. The intensive experimental results show that our method achieves\nbetter results than recent state-of-the-art approaches qualitatively and\nquantitatively.\n","authors":["Trong-Thang Pham","Nhat Le","Tuong Do","Hung Nguyen","Erman Tjiputra","Quang D. Tran","Anh Nguyen"],"pdf_url":"https://arxiv.org/pdf/2303.09799v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.12678v1","updated":"2023-03-22T16:21:44Z","published":"2023-03-22T16:21:44Z","title":"Uni-Fusion: Universal Continuous Mapping","summary":"  We introduce Uni-Fusion, an universal continuous mapping framework for\nsurfaces, surface properties (color, infrared, etc.) and more (latent features\nin CLIP embedding space, etc.). We propose the first Universal Implicit\nEncoding model that supports encoding of both geometry and various types of\nproperties (RGB, infrared, feature and etc.) without the need for any training.\nBased on that, our framework divides the point cloud into regular grid voxels\nand produces a latent feature in each voxel to form a Latent Implicit Map (LIM)\nfor geometries and arbitrary properties. Then, by fusing a Local LIM of new\nframe to Global LIM, an incremental reconstruction is approached. Encoded with\ncorresponding types of data, our Latent Implicit Map is capable to generate\ncontinuous surfaces, surface properties fields, surface feature fields and any\nother possible options. To demonstrate the capabilities of our model, we\nimplement three applications: (1) incremental reconstruction for surfaces and\ncolor (2) 2D-to-3D fabricated properties transfers (3) open-vocabulary scene\nunderstanding by producing a text CLIP feature field on surfaces. We evaluate\nUni-Fusion by comparing in corresponding applications, from which, Uni-Fusion\nshows high flexibility to various of application while performing best or\ncompetitive. The project page of Uni-Fusion is available at\nhttps://jarrome.github.io/Uni-Fusion/\n","authors":["Yijun Yuan","Andreas Nuechter"],"pdf_url":"https://arxiv.org/pdf/2303.12678v1.pdf","comment":"Project page: https://jarrome.github.io/Uni-Fusion/"},{"id":"http://arxiv.org/abs/2303.12675v1","updated":"2023-03-22T16:14:39Z","published":"2023-03-22T16:14:39Z","title":"VecFontSDF: Learning to Reconstruct and Synthesize High-quality Vector\n  Fonts via Signed Distance Functions","summary":"  Font design is of vital importance in the digital content design and modern\nprinting industry. Developing algorithms capable of automatically synthesizing\nvector fonts can significantly facilitate the font design process. However,\nexisting methods mainly concentrate on raster image generation, and only a few\napproaches can directly synthesize vector fonts. This paper proposes an\nend-to-end trainable method, VecFontSDF, to reconstruct and synthesize\nhigh-quality vector fonts using signed distance functions (SDFs). Specifically,\nbased on the proposed SDF-based implicit shape representation, VecFontSDF\nlearns to model each glyph as shape primitives enclosed by several parabolic\ncurves, which can be precisely converted to quadratic B\\'ezier curves that are\nwidely used in vector font products. In this manner, most image generation\nmethods can be easily extended to synthesize vector fonts. Qualitative and\nquantitative experiments conducted on a publicly-available dataset demonstrate\nthat our method obtains high-quality results on several tasks, including vector\nfont reconstruction, interpolation, and few-shot vector font synthesis,\nmarkedly outperforming the state of the art.\n","authors":["Zeqing Xia","Bojun Xiong","Zhouhui Lian"],"pdf_url":"https://arxiv.org/pdf/2303.12675v1.pdf","comment":"Accepted to CVPR 2023"},{"id":"http://arxiv.org/abs/2303.12671v1","updated":"2023-03-22T15:49:33Z","published":"2023-03-22T15:49:33Z","title":"Integrating Image Features with Convolutional Sequence-to-sequence\n  Network for Multilingual Visual Question Answering","summary":"  Visual Question Answering (VQA) is a task that requires computers to give\ncorrect answers for the input questions based on the images. This task can be\nsolved by humans with ease but is a challenge for computers. The\nVLSP2022-EVJVQA shared task carries the Visual Question Answering task in the\nmultilingual domain on a newly released dataset: UIT-EVJVQA, in which the\nquestions and answers are written in three different languages: English,\nVietnamese and Japanese. We approached the challenge as a sequence-to-sequence\nlearning task, in which we integrated hints from pre-trained state-of-the-art\nVQA models and image features with Convolutional Sequence-to-Sequence network\nto generate the desired answers. Our results obtained up to 0.3442 by F1 score\non the public test set, 0.4210 on the private test set, and placed 3rd in the\ncompetition.\n","authors":["Triet Minh Thai","Son T. Luu"],"pdf_url":"https://arxiv.org/pdf/2303.12671v1.pdf","comment":"VLSP2022-EVJVQA"},{"id":"http://arxiv.org/abs/2303.12670v1","updated":"2023-03-22T15:48:23Z","published":"2023-03-22T15:48:23Z","title":"Correlational Image Modeling for Self-Supervised Visual Pre-Training","summary":"  We introduce Correlational Image Modeling (CIM), a novel and surprisingly\neffective approach to self-supervised visual pre-training. Our CIM performs a\nsimple pretext task: we randomly crop image regions (exemplars) from an input\nimage (context) and predict correlation maps between the exemplars and the\ncontext. Three key designs enable correlational image modeling as a nontrivial\nand meaningful self-supervisory task. First, to generate useful\nexemplar-context pairs, we consider cropping image regions with various scales,\nshapes, rotations, and transformations. Second, we employ a bootstrap learning\nframework that involves online and target encoders. During pre-training, the\nformer takes exemplars as inputs while the latter converts the context. Third,\nwe model the output correlation maps via a simple cross-attention block, within\nwhich the context serves as queries and the exemplars offer values and keys. We\nshow that CIM performs on par or better than the current state of the art on\nself-supervised and transfer benchmarks.\n","authors":["Wei Li","Jiahao Xie","Chen Change Loy"],"pdf_url":"https://arxiv.org/pdf/2303.12670v1.pdf","comment":"Accepted by CVPR 2023"},{"id":"http://arxiv.org/abs/2303.12669v1","updated":"2023-03-22T15:47:16Z","published":"2023-03-22T15:47:16Z","title":"An Extended Study of Human-like Behavior under Adversarial Training","summary":"  Neural networks have a number of shortcomings. Amongst the severest ones is\nthe sensitivity to distribution shifts which allows models to be easily fooled\ninto wrong predictions by small perturbations to inputs that are often\nimperceivable to humans and do not have to carry semantic meaning. Adversarial\ntraining poses a partial solution to address this issue by training models on\nworst-case perturbations. Yet, recent work has also pointed out that the\nreasoning in neural networks is different from humans. Humans identify objects\nby shape, while neural nets mainly employ texture cues. Exemplarily, a model\ntrained on photographs will likely fail to generalize to datasets containing\nsketches. Interestingly, it was also shown that adversarial training seems to\nfavorably increase the shift toward shape bias. In this work, we revisit this\nobservation and provide an extensive analysis of this effect on various\narchitectures, the common $\\ell_2$- and $\\ell_\\infty$-training, and\nTransformer-based models. Further, we provide a possible explanation for this\nphenomenon from a frequency perspective.\n","authors":["Paul Gavrikov","Janis Keuper","Margret Keuper"],"pdf_url":"https://arxiv.org/pdf/2303.12669v1.pdf","comment":"6 pages, accepted at the CVPR 2023 Workshop \"The 3rd Workshop of\n  Adversarial Machine Learning on Computer Vision: Art of Robustness\""},{"id":"http://arxiv.org/abs/2205.00363v3","updated":"2023-03-22T15:42:50Z","published":"2022-04-30T23:03:49Z","title":"Visual Spatial Reasoning","summary":"  Spatial relations are a basic part of human cognition. However, they are\nexpressed in natural language in a variety of ways, and previous work has\nsuggested that current vision-and-language models (VLMs) struggle to capture\nrelational information. In this paper, we present Visual Spatial Reasoning\n(VSR), a dataset containing more than 10k natural text-image pairs with 66\ntypes of spatial relations in English (such as: under, in front of, and\nfacing). While using a seemingly simple annotation format, we show how the\ndataset includes challenging linguistic phenomena, such as varying reference\nframes. We demonstrate a large gap between human and model performance: the\nhuman ceiling is above 95%, while state-of-the-art models only achieve around\n70%. We observe that VLMs' by-relation performances have little correlation\nwith the number of training examples and the tested models are in general\nincapable of recognising relations concerning the orientations of objects.\n","authors":["Fangyu Liu","Guy Emerson","Nigel Collier"],"pdf_url":"https://arxiv.org/pdf/2205.00363v3.pdf","comment":"TACL camera-ready version; code and data available at\n  https://github.com/cambridgeltl/visual-spatial-reasoning"},{"id":"http://arxiv.org/abs/2303.12658v1","updated":"2023-03-22T15:36:19Z","published":"2023-03-22T15:36:19Z","title":"Reliable and Efficient Evaluation of Adversarial Robustness for Deep\n  Hashing-Based Retrieval","summary":"  Deep hashing has been extensively applied to massive image retrieval due to\nits efficiency and effectiveness. Recently, several adversarial attacks have\nbeen presented to reveal the vulnerability of deep hashing models against\nadversarial examples. However, existing attack methods suffer from degraded\nperformance or inefficiency because they underutilize the semantic relations\nbetween original samples or spend a lot of time learning these relations with a\ndeep neural network. In this paper, we propose a novel Pharos-guided Attack,\ndubbed PgA, to evaluate the adversarial robustness of deep hashing networks\nreliably and efficiently. Specifically, we design pharos code to represent the\nsemantics of the benign image, which preserves the similarity to semantically\nrelevant samples and dissimilarity to irrelevant ones. It is proven that we can\nquickly calculate the pharos code via a simple math formula. Accordingly, PgA\ncan directly conduct a reliable and efficient attack on deep hashing-based\nretrieval by maximizing the similarity between the hash code of the adversarial\nexample and the pharos code. Extensive experiments on the benchmark datasets\nverify that the proposed algorithm outperforms the prior state-of-the-arts in\nboth attack strength and speed.\n","authors":["Xunguang Wang","Jiawang Bai","Xinyue Xu","Xiaomeng Li"],"pdf_url":"https://arxiv.org/pdf/2303.12658v1.pdf","comment":"arXiv admin note: text overlap with arXiv:2204.10779"},{"id":"http://arxiv.org/abs/2303.12649v1","updated":"2023-03-22T15:30:44Z","published":"2023-03-22T15:30:44Z","title":"MI-SegNet: Mutual Information-Based US Segmentation for Unseen Domain\n  Generalization","summary":"  Generalization capabilities of learning-based medical image segmentation\nacross domains are currently limited by the performance degradation caused by\nthe domain shift, particularly for ultrasound (US) imaging. The quality of US\nimages heavily relies on carefully tuned acoustic parameters, which vary across\nsonographers, machines, and settings. To improve the generalizability on US\nimages across domains, we propose MI-SegNet, a novel mutual information (MI)\nbased framework to explicitly disentangle the anatomical and domain feature\nrepresentations; therefore, robust domain-independent segmentation can be\nexpected. Two encoders are employed to extract the relevant features for the\ndisentanglement. The segmentation only uses the anatomical feature map for its\nprediction. In order to force the encoders to learn meaningful feature\nrepresentations a cross-reconstruction method is used during training.\nTransformations, specific to either domain or anatomy are applied to guide the\nencoders in their respective feature extraction task. Additionally, any MI\npresent in both feature maps is punished to further promote separate feature\nspaces. We validate the generalizability of the proposed domain-independent\nsegmentation approach on several datasets with varying parameters and machines.\nFurthermore, we demonstrate the effectiveness of the proposed MI-SegNet serving\nas a pre-trained model by comparing it with state-of-the-art networks.\n","authors":["Yuan Bi","Zhongliang Jiang","Ricarda Clarenbach","Reza Ghotbi","Angelos Karlas","Nassir Navab"],"pdf_url":"https://arxiv.org/pdf/2303.12649v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.12644v1","updated":"2023-03-22T15:26:22Z","published":"2023-03-22T15:26:22Z","title":"Feature-Conditioned Cascaded Video Diffusion Models for Precise\n  Echocardiogram Synthesis","summary":"  Image synthesis is expected to provide value for the translation of machine\nlearning methods into clinical practice. Fundamental problems like model\nrobustness, domain transfer, causal modelling, and operator training become\napproachable through synthetic data. Especially, heavily operator-dependant\nmodalities like Ultrasound imaging require robust frameworks for image and\nvideo generation. So far, video generation has only been possible by providing\ninput data that is as rich as the output data, e.g., image sequence plus\nconditioning in, video out. However, clinical documentation is usually scarce\nand only single images are reported and stored, thus retrospective\npatient-specific analysis or the generation of rich training data becomes\nimpossible with current approaches. In this paper, we extend elucidated\ndiffusion models for video modelling to generate plausible video sequences from\nsingle images and arbitrary conditioning with clinical parameters. We explore\nthis idea within the context of echocardiograms by looking into the variation\nof the Left Ventricle Ejection Fraction, the most essential clinical metric\ngained from these examinations. We use the publicly available EchoNet-Dynamic\ndataset for all our experiments. Our image to sequence approach achieves an R2\nscore of 93%, which is 38 points higher than recently proposed sequence to\nsequence generation methods. A public demo is available here: bit.ly/3HTskPF.\nCode and models will be available at:\nhttps://github.com/HReynaud/EchoDiffusion.\n","authors":["Hadrien Reynaud","Mengyun Qiao","Mischa Dombrowski","Thomas Day","Reza Razavi","Alberto Gomez","Paul Leeson","Bernhard Kainz"],"pdf_url":"https://arxiv.org/pdf/2303.12644v1.pdf","comment":"Under Review"},{"id":"http://arxiv.org/abs/2303.12641v1","updated":"2023-03-22T15:23:09Z","published":"2023-03-22T15:23:09Z","title":"Reveal to Revise: An Explainable AI Life Cycle for Iterative Bias\n  Correction of Deep Models","summary":"  State-of-the-art machine learning models often learn spurious correlations\nembedded in the training data. This poses risks when deploying these models for\nhigh-stake decision-making, such as in medical applications like skin cancer\ndetection. To tackle this problem, we propose Reveal to Revise (R2R), a\nframework entailing the entire eXplainable Artificial Intelligence (XAI) life\ncycle, enabling practitioners to iteratively identify, mitigate, and\n(re-)evaluate spurious model behavior with a minimal amount of human\ninteraction. In the first step (1), R2R reveals model weaknesses by finding\noutliers in attributions or through inspection of latent concepts learned by\nthe model. Secondly (2), the responsible artifacts are detected and spatially\nlocalized in the input data, which is then leveraged to (3) revise the model\nbehavior. Concretely, we apply the methods of RRR, CDEP and ClArC for model\ncorrection, and (4) (re-)evaluate the model's performance and remaining\nsensitivity towards the artifact. Using two medical benchmark datasets for\nMelanoma detection and bone age estimation, we apply our R2R framework to VGG,\nResNet and EfficientNet architectures and thereby reveal and correct real\ndataset-intrinsic artifacts, as well as synthetic variants in a controlled\nsetting. Completing the XAI life cycle, we demonstrate multiple R2R iterations\nto mitigate different biases. Code is available on\nhttps://github.com/maxdreyer/Reveal2Revise.\n","authors":["Frederik Pahde","Maximilian Dreyer","Wojciech Samek","Sebastian Lapuschkin"],"pdf_url":"https://arxiv.org/pdf/2303.12641v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2102.05368v2","updated":"2023-03-22T15:21:10Z","published":"2021-02-10T10:13:39Z","title":"RoBIC: A benchmark suite for assessing classifiers robustness","summary":"  Many defenses have emerged with the development of adversarial attacks.\nModels must be objectively evaluated accordingly. This paper systematically\ntackles this concern by proposing a new parameter-free benchmark we coin RoBIC.\nRoBIC fairly evaluates the robustness of image classifiers using a new\nhalf-distortion measure. It gauges the robustness of the network against white\nand black box attacks, independently of its accuracy. RoBIC is faster than the\nother available benchmarks. We present the significant differences in the\nrobustness of 16 recent models as assessed by RoBIC.\n","authors":["Thibault Maho","Benoît Bonnet","Teddy Furon","Erwan Le Merrer"],"pdf_url":"https://arxiv.org/pdf/2102.05368v2.pdf","comment":"4 pages, accepted to ICIP 2021"},{"id":"http://arxiv.org/abs/2303.12621v1","updated":"2023-03-22T15:01:20Z","published":"2023-03-22T15:01:20Z","title":"OcTr: Octree-based Transformer for 3D Object Detection","summary":"  A key challenge for LiDAR-based 3D object detection is to capture sufficient\nfeatures from large scale 3D scenes especially for distant or/and occluded\nobjects. Albeit recent efforts made by Transformers with the long sequence\nmodeling capability, they fail to properly balance the accuracy and efficiency,\nsuffering from inadequate receptive fields or coarse-grained holistic\ncorrelations. In this paper, we propose an Octree-based Transformer, named\nOcTr, to address this issue. It first constructs a dynamic octree on the\nhierarchical feature pyramid through conducting self-attention on the top level\nand then recursively propagates to the level below restricted by the octants,\nwhich captures rich global context in a coarse-to-fine manner while maintaining\nthe computational complexity under control. Furthermore, for enhanced\nforeground perception, we propose a hybrid positional embedding, composed of\nthe semantic-aware positional embedding and attention mask, to fully exploit\nsemantic and geometry clues. Extensive experiments are conducted on the Waymo\nOpen Dataset and KITTI Dataset, and OcTr reaches newly state-of-the-art\nresults.\n","authors":["Chao Zhou","Yanan Zhang","Jiaxin Chen","Di Huang"],"pdf_url":"https://arxiv.org/pdf/2303.12621v1.pdf","comment":"Accepted by CVPR 2023"},{"id":"http://arxiv.org/abs/2303.12618v1","updated":"2023-03-22T14:59:49Z","published":"2023-03-22T14:59:49Z","title":"A Perceptual Quality Assessment Exploration for AIGC Images","summary":"  \\underline{AI} \\underline{G}enerated \\underline{C}ontent (\\textbf{AIGC}) has\ngained widespread attention with the increasing efficiency of deep learning in\ncontent creation. AIGC, created with the assistance of artificial intelligence\ntechnology, includes various forms of content, among which the AI-generated\nimages (AGIs) have brought significant impact to society and have been applied\nto various fields such as entertainment, education, social media, etc. However,\ndue to hardware limitations and technical proficiency, the quality of AIGC\nimages (AGIs) varies, necessitating refinement and filtering before practical\nuse. Consequently, there is an urgent need for developing objective models to\nassess the quality of AGIs. Unfortunately, no research has been carried out to\ninvestigate the perceptual quality assessment for AGIs specifically. Therefore,\nin this paper, we first discuss the major evaluation aspects such as technical\nissues, AI artifacts, unnaturalness, discrepancy, and aesthetics for AGI\nquality assessment. Then we present the first perceptual AGI quality assessment\ndatabase, AGIQA-1K, which consists of 1,080 AGIs generated from diffusion\nmodels. A well-organized subjective experiment is followed to collect the\nquality labels of the AGIs. Finally, we conduct a benchmark experiment to\nevaluate the performance of current image quality assessment (IQA) models.\n","authors":["Zicheng Zhang","Chunyi Li","Wei Sun","Xiaohong Liu","Xiongkuo Min","Guangtao Zhai"],"pdf_url":"https://arxiv.org/pdf/2303.12618v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2204.10779v5","updated":"2023-03-22T14:57:41Z","published":"2022-04-18T04:51:08Z","title":"CgAT: Center-Guided Adversarial Training for Deep Hashing-Based\n  Retrieval","summary":"  Deep hashing has been extensively utilized in massive image retrieval because\nof its efficiency and effectiveness. However, deep hashing models are\nvulnerable to adversarial examples, making it essential to develop adversarial\ndefense methods for image retrieval. Existing solutions achieved limited\ndefense performance because of using weak adversarial samples for training and\nlacking discriminative optimization objectives to learn robust features. In\nthis paper, we present a min-max based Center-guided Adversarial Training,\nnamely CgAT, to improve the robustness of deep hashing networks through worst\nadversarial examples. Specifically, we first formulate the center code as a\nsemantically-discriminative representative of the input image content, which\npreserves the semantic similarity with positive samples and dissimilarity with\nnegative examples. We prove that a mathematical formula can calculate the\ncenter code immediately. After obtaining the center codes in each optimization\niteration of the deep hashing network, they are adopted to guide the\nadversarial training process. On the one hand, CgAT generates the worst\nadversarial examples as augmented data by maximizing the Hamming distance\nbetween the hash codes of the adversarial examples and the center codes. On the\nother hand, CgAT learns to mitigate the effects of adversarial samples by\nminimizing the Hamming distance to the center codes. Extensive experiments on\nthe benchmark datasets demonstrate the effectiveness of our adversarial\ntraining algorithm in defending against adversarial attacks for deep\nhashing-based retrieval. Compared with the current state-of-the-art defense\nmethod, we significantly improve the defense performance by an average of\n18.61\\%, 12.35\\%, and 11.56\\% on FLICKR-25K, NUS-WIDE, and MS-COCO,\nrespectively. The code is available at https://github.com/xunguangwang/CgAT.\n","authors":["Xunguang Wang","Yiqun Lin","Xiaomeng Li"],"pdf_url":"https://arxiv.org/pdf/2204.10779v5.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.12615v1","updated":"2023-03-22T14:56:51Z","published":"2023-03-22T14:56:51Z","title":"Multi-view Feature Extraction based on Triple Contrastive Heads","summary":"  Multi-view feature extraction is an efficient approach for alleviating the\nissue of dimensionality in highdimensional multi-view data. Contrastive\nlearning (CL), which is a popular self-supervised learning method, has recently\nattracted considerable attention. In this study, we propose a novel multi-view\nfeature extraction method based on triple contrastive heads, which combines the\nsample-, recovery- , and feature-level contrastive losses to extract the\nsufficient yet minimal subspace discriminative information in compliance with\ninformation bottleneck principle. In MFETCH, we construct the feature-level\ncontrastive loss, which removes the redundent information in the consistency\ninformation to achieve the minimality of the subspace discriminative\ninformation. Moreover, the recovery-level contrastive loss is also constructed\nin MFETCH, which captures the view-specific discriminative information to\nachieve the sufficiency of the subspace discriminative information.The\nnumerical experiments demonstrate that the proposed method offers a strong\nadvantage for multi-view feature extraction.\n","authors":["Hongjie Zhang"],"pdf_url":"https://arxiv.org/pdf/2303.12615v1.pdf","comment":"arXiv admin note: text overlap with arXiv:2302.03932"},{"id":"http://arxiv.org/abs/2211.11277v3","updated":"2023-03-22T14:20:14Z","published":"2022-11-21T09:13:53Z","title":"DrapeNet: Garment Generation and Self-Supervised Draping","summary":"  Recent approaches to drape garments quickly over arbitrary human bodies\nleverage self-supervision to eliminate the need for large training sets.\nHowever, they are designed to train one network per clothing item, which\nseverely limits their generalization abilities. In our work, we rely on\nself-supervision to train a single network to drape multiple garments. This is\nachieved by predicting a 3D deformation field conditioned on the latent codes\nof a generative network, which models garments as unsigned distance fields. Our\npipeline can generate and drape previously unseen garments of any topology,\nwhose shape can be edited by manipulating their latent codes. Being fully\ndifferentiable, our formulation makes it possible to recover accurate 3D models\nof garments from partial observations -- images or 3D scans -- via gradient\ndescent. Our code is publicly available at\nhttps://github.com/liren2515/DrapeNet .\n","authors":["Luca De Luigi","Ren Li","Benoît Guillard","Mathieu Salzmann","Pascal Fua"],"pdf_url":"https://arxiv.org/pdf/2211.11277v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.11331v2","updated":"2023-03-22T14:10:37Z","published":"2023-03-20T17:59:59Z","title":"EVA-02: A Visual Representation for Neon Genesis","summary":"  We launch EVA-02, a next-generation Transformer-based visual representation\npre-trained to reconstruct strong and robust language-aligned vision features\nvia masked image modeling. With an updated plain Transformer architecture as\nwell as extensive pre-training from an open & accessible giant CLIP vision\nencoder, EVA-02 demonstrates superior performance compared to prior\nstate-of-the-art approaches across various representative vision tasks, while\nutilizing significantly fewer parameters and compute budgets. Notably, using\nexclusively publicly accessible training data, EVA-02 with only 304M parameters\nachieves a phenomenal 90.0 fine-tuning top-1 accuracy on ImageNet-1K val set.\nAdditionally, our EVA-02-CLIP can reach up to 80.4 zero-shot top-1 on\nImageNet-1K, outperforming the previous largest & best open-sourced CLIP with\nonly ~1/6 parameters and ~1/6 image-text training data. We offer four EVA-02\nvariants in various model sizes, ranging from 6M to 304M parameters, all with\nimpressive performance. To facilitate open access and open research, we release\nthe complete suite of EVA-02 to the community at\nhttps://github.com/baaivision/EVA/tree/master/EVA-02.\n","authors":["Yuxin Fang","Quan Sun","Xinggang Wang","Tiejun Huang","Xinlong Wang","Yue Cao"],"pdf_url":"https://arxiv.org/pdf/2303.11331v2.pdf","comment":"v2: Fix some known issues & typos. v1: To Asuka. Code & Models:\n  https://github.com/baaivision/EVA/tree/master/EVA-02"},{"id":"http://arxiv.org/abs/2211.05568v3","updated":"2023-03-22T14:08:17Z","published":"2022-11-10T13:44:57Z","title":"Unbiased Supervised Contrastive Learning","summary":"  Many datasets are biased, namely they contain easy-to-learn features that are\nhighly correlated with the target class only in the dataset but not in the true\nunderlying distribution of the data. For this reason, learning unbiased models\nfrom biased data has become a very relevant research topic in the last years.\nIn this work, we tackle the problem of learning representations that are robust\nto biases. We first present a margin-based theoretical framework that allows us\nto clarify why recent contrastive losses (InfoNCE, SupCon, etc.) can fail when\ndealing with biased data. Based on that, we derive a novel formulation of the\nsupervised contrastive loss (epsilon-SupInfoNCE), providing more accurate\ncontrol of the minimal distance between positive and negative samples.\nFurthermore, thanks to our theoretical framework, we also propose FairKL, a new\ndebiasing regularization loss, that works well even with extremely biased data.\nWe validate the proposed losses on standard vision datasets including CIFAR10,\nCIFAR100, and ImageNet, and we assess the debiasing capability of FairKL with\nepsilon-SupInfoNCE, reaching state-of-the-art performance on a number of biased\ndatasets, including real instances of biases in the wild.\n","authors":["Carlo Alberto Barbano","Benoit Dufumier","Enzo Tartaglione","Marco Grangetto","Pietro Gori"],"pdf_url":"https://arxiv.org/pdf/2211.05568v3.pdf","comment":"Accepted at ICLR 2023"},{"id":"http://arxiv.org/abs/2303.09998v2","updated":"2023-03-22T13:58:12Z","published":"2023-03-17T14:20:28Z","title":"TBP-Former: Learning Temporal Bird's-Eye-View Pyramid for Joint\n  Perception and Prediction in Vision-Centric Autonomous Driving","summary":"  Vision-centric joint perception and prediction (PnP) has become an emerging\ntrend in autonomous driving research. It predicts the future states of the\ntraffic participants in the surrounding environment from raw RGB images.\nHowever, it is still a critical challenge to synchronize features obtained at\nmultiple camera views and timestamps due to inevitable geometric distortions\nand further exploit those spatial-temporal features. To address this issue, we\npropose a temporal bird's-eye-view pyramid transformer (TBP-Former) for\nvision-centric PnP, which includes two novel designs. First, a\npose-synchronized BEV encoder is proposed to map raw image inputs with any\ncamera pose at any time to a shared and synchronized BEV space for better\nspatial-temporal synchronization. Second, a spatial-temporal pyramid\ntransformer is introduced to comprehensively extract multi-scale BEV features\nand predict future BEV states with the support of spatial-temporal priors.\nExtensive experiments on nuScenes dataset show that our proposed framework\noverall outperforms all state-of-the-art vision-based prediction methods.\n","authors":["Shaoheng Fang","Zi Wang","Yiqi Zhong","Junhao Ge","Siheng Chen","Yanfeng Wang"],"pdf_url":"https://arxiv.org/pdf/2303.09998v2.pdf","comment":"CVPR 2023"},{"id":"http://arxiv.org/abs/2303.12564v1","updated":"2023-03-22T13:46:15Z","published":"2023-03-22T13:46:15Z","title":"RaBit: Parametric Modeling of 3D Biped Cartoon Characters with a\n  Topological-consistent Dataset","summary":"  Assisting people in efficiently producing visually plausible 3D characters\nhas always been a fundamental research topic in computer vision and computer\ngraphics. Recent learning-based approaches have achieved unprecedented accuracy\nand efficiency in the area of 3D real human digitization. However, none of the\nprior works focus on modeling 3D biped cartoon characters, which are also in\ngreat demand in gaming and filming. In this paper, we introduce 3DBiCar, the\nfirst large-scale dataset of 3D biped cartoon characters, and RaBit, the\ncorresponding parametric model. Our dataset contains 1,500 topologically\nconsistent high-quality 3D textured models which are manually crafted by\nprofessional artists. Built upon the data, RaBit is thus designed with a\nSMPL-like linear blend shape model and a StyleGAN-based neural UV-texture\ngenerator, simultaneously expressing the shape, pose, and texture. To\ndemonstrate the practicality of 3DBiCar and RaBit, various applications are\nconducted, including single-view reconstruction, sketch-based modeling, and 3D\ncartoon animation. For the single-view reconstruction setting, we find a\nstraightforward global mapping from input images to the output UV-based texture\nmaps tends to lose detailed appearances of some local parts (e.g., nose, ears).\nThus, a part-sensitive texture reasoner is adopted to make all important local\nareas perceived. Experiments further demonstrate the effectiveness of our\nmethod both qualitatively and quantitatively. 3DBiCar and RaBit are available\nat gaplab.cuhk.edu.cn/projects/RaBit.\n","authors":["Zhongjin Luo","Shengcai Cai","Jinguo Dong","Ruibo Ming","Liangdong Qiu","Xiaohang Zhan","Xiaoguang Han"],"pdf_url":"https://arxiv.org/pdf/2303.12564v1.pdf","comment":"CVPR 2023, Project page: https://gaplab.cuhk.edu.cn/projects/RaBit/"},{"id":"http://arxiv.org/abs/2303.12557v1","updated":"2023-03-22T13:41:22Z","published":"2023-03-22T13:41:22Z","title":"Q-HyViT: Post-Training Quantization for Hybrid Vision Transformer with\n  Bridge Block Reconstruction","summary":"  Recently, vision transformers (ViT) have replaced convolutional neural\nnetwork models in numerous tasks, including classification, detection, and\nsegmentation. However, the high computational requirements of ViTs hinder their\nwidespread implementation. To address this issue, researchers have proposed\nefficient hybrid transformer architectures that combine convolutional and\ntransformer layers and optimize attention computation for linear complexity.\nAdditionally, post-training quantization has been proposed as a means of\nmitigating computational demands. Combining quantization techniques and\nefficient hybrid transformer structures is crucial to maximize the acceleration\nof vision transformers on mobile devices. However, no prior investigation has\napplied quantization to efficient hybrid transformers. In this paper, at first,\nwe discover that the straightforward manner to apply the existing PTQ methods\nfor ViT to efficient hybrid transformers results in a drastic accuracy drop due\nto the following challenges: (i) highly dynamic ranges, (ii) zero-point\noverflow, (iii) diverse normalization, and (iv) limited model parameters (<5M).\nTo overcome these challenges, we propose a new post-training quantization\nmethod, which is the first to quantize efficient hybrid vision transformers\n(MobileViTv1 and MobileViTv2) with a significant margin (an average improvement\nof 7.75%) compared to existing PTQ methods (EasyQuant, FQ-ViT, and PTQ4ViT). We\nplan to release our code at https://github.com/Q-HyViT.\n","authors":["Jemin Lee","Yongin Kwon","Jeman Park","Misun Yu","Hwanjun Song"],"pdf_url":"https://arxiv.org/pdf/2303.12557v1.pdf","comment":"12 pages, 9 figures"},{"id":"http://arxiv.org/abs/2108.02235v3","updated":"2023-03-22T13:26:59Z","published":"2021-08-04T18:29:42Z","title":"Dynamic Relevance Learning for Few-Shot Object Detection","summary":"  Expensive bounding-box annotations have limited the development of object\ndetection task. Thus, it is necessary to focus on more challenging task of\nfew-shot object detection. It requires the detector to recognize objects of\nnovel classes with only a few training samples. Nowadays, many existing popular\nmethods adopting training way similar to meta-learning have achieved promising\nperformance, such as Meta R-CNN series. However, support data is only used as\nthe class attention to guide the detecting of query images each time. Their\nrelevance to each other remains unexploited. Moreover, a lot of recent works\ntreat the support data and query images as independent branch without\nconsidering the relationship between them. To address this issue, we propose a\ndynamic relevance learning model, which utilizes the relationship between all\nsupport images and Region of Interest (RoI) on the query images to construct a\ndynamic graph convolutional network (GCN). By adjusting the prediction\ndistribution of the base detector using the output of this GCN, the proposed\nmodel serves as a hard auxiliary classification task, which guides the detector\nto improve the class representation implicitly. Comprehensive experiments have\nbeen conducted on Pascal VOC and MS-COCO dataset. The proposed model achieves\nthe best overall performance, which shows its effectiveness of learning more\ngeneralized features. Our code is available at\nhttps://github.com/liuweijie19980216/DRL-for-FSOD.\n","authors":["Weijie Liu","Chong Wang","Haohe Li","Shenghao Yu","Jiafei Wu"],"pdf_url":"https://arxiv.org/pdf/2108.02235v3.pdf","comment":"12 pages, 8 figures, 7 tables"},{"id":"http://arxiv.org/abs/2303.12540v1","updated":"2023-03-22T13:16:37Z","published":"2023-03-22T13:16:37Z","title":"Deployment of Image Analysis Algorithms under Prevalence Shifts","summary":"  Domain gaps are among the most relevant roadblocks in the clinical\ntranslation of machine learning (ML)-based solutions for medical image\nanalysis. While current research focuses on new training paradigms and network\narchitectures, little attention is given to the specific effect of prevalence\nshifts on an algorithm deployed in practice. Such discrepancies between class\nfrequencies in the data used for a method's development/validation and that in\nits deployment environment(s) are of great importance, for example in the\ncontext of artificial intelligence (AI) democratization, as disease prevalences\nmay vary widely across time and location. Our contribution is twofold. First,\nwe empirically demonstrate the potentially severe consequences of missing\nprevalence handling by analyzing (i) the extent of miscalibration, (ii) the\ndeviation of the decision threshold from the optimum, and (iii) the ability of\nvalidation metrics to reflect neural network performance on the deployment\npopulation as a function of the discrepancy between development and deployment\nprevalence. Second, we propose a workflow for prevalence-aware image\nclassification that uses estimated deployment prevalences to adjust a trained\nclassifier to a new environment, without requiring additional annotated\ndeployment data. Comprehensive experiments based on a diverse set of 30 medical\nclassification tasks showcase the benefit of the proposed workflow in\ngenerating better classifier decisions and more reliable performance estimates\ncompared to current practice.\n","authors":["Patrick Godau","Piotr Kalinowski","Evangelia Christodoulou","Annika Reinke","Minu Tizabi","Luciana Ferrer","Paul Jäger","Lena Maier-Hein"],"pdf_url":"https://arxiv.org/pdf/2303.12540v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.12533v1","updated":"2023-03-22T13:06:39Z","published":"2023-03-22T13:06:39Z","title":"Pixel-wise Agricultural Image Time Series Classification: Comparisons\n  and a Deformable Prototype-based Approach","summary":"  Improvements in Earth observation by satellites allow for imagery of ever\nhigher temporal and spatial resolution. Leveraging this data for agricultural\nmonitoring is key for addressing environmental and economic challenges. Current\nmethods for crop segmentation using temporal data either rely on annotated data\nor are heavily engineered to compensate the lack of supervision. In this paper,\nwe present and compare datasets and methods for both supervised and\nunsupervised pixel-wise segmentation of satellite image time series (SITS). We\nalso introduce an approach to add invariance to spectral deformations and\ntemporal shifts to classical prototype-based methods such as K-means and\nNearest Centroid Classifier (NCC). We show this simple and highly interpretable\nmethod leads to meaningful results in both the supervised and unsupervised\nsettings and significantly improves the state of the art for unsupervised\nclassification of agricultural time series on four recent SITS datasets.\n","authors":["Elliot Vincent","Jean Ponce","Mathieu Aubry"],"pdf_url":"https://arxiv.org/pdf/2303.12533v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2211.12501v2","updated":"2023-03-22T13:03:02Z","published":"2022-11-22T18:59:52Z","title":"AeDet: Azimuth-invariant Multi-view 3D Object Detection","summary":"  Recent LSS-based multi-view 3D object detection has made tremendous progress,\nby processing the features in Brid-Eye-View (BEV) via the convolutional\ndetector. However, the typical convolution ignores the radial symmetry of the\nBEV features and increases the difficulty of the detector optimization. To\npreserve the inherent property of the BEV features and ease the optimization,\nwe propose an azimuth-equivariant convolution (AeConv) and an\nazimuth-equivariant anchor. The sampling grid of AeConv is always in the radial\ndirection, thus it can learn azimuth-invariant BEV features. The proposed\nanchor enables the detection head to learn predicting azimuth-irrelevant\ntargets. In addition, we introduce a camera-decoupled virtual depth to unify\nthe depth prediction for the images with different camera intrinsic parameters.\nThe resultant detector is dubbed Azimuth-equivariant Detector (AeDet).\nExtensive experiments are conducted on nuScenes, and AeDet achieves a 62.0%\nNDS, surpassing the recent multi-view 3D object detectors such as PETRv2 and\nBEVDepth by a large margin. Project page: https://fcjian.github.io/aedet.\n","authors":["Chengjian Feng","Zequn Jie","Yujie Zhong","Xiangxiang Chu","Lin Ma"],"pdf_url":"https://arxiv.org/pdf/2211.12501v2.pdf","comment":"CVPR2023"},{"id":"http://arxiv.org/abs/2203.09957v2","updated":"2023-03-22T13:01:43Z","published":"2022-03-18T13:49:25Z","title":"Enhancement of Novel View Synthesis Using Omnidirectional Image\n  Completion","summary":"  In this study, we present a method for synthesizing novel views from a single\n360-degree RGB-D image based on the neural radiance field (NeRF) . Prior\nstudies relied on the neighborhood interpolation capability of multi-layer\nperceptrons to complete missing regions caused by occlusion and zooming, which\nleads to artifacts. In the method proposed in this study, the input image is\nreprojected to 360-degree RGB images at other camera positions, the missing\nregions of the reprojected images are completed by a 2D image generative model,\nand the completed images are utilized to train the NeRF. Because multiple\ncompleted images contain inconsistencies in 3D, we introduce a method to learn\nthe NeRF model using a subset of completed images that cover the target scene\nwith less overlap of completed regions. The selection of such a subset of\nimages can be attributed to the maximum weight independent set problem, which\nis solved through simulated annealing. Experiments demonstrated that the\nproposed method can synthesize plausible novel views while preserving the\nfeatures of the scene for both artificial and real-world data.\n","authors":["Takayuki Hara","Tatsuya Harada"],"pdf_url":"https://arxiv.org/pdf/2203.09957v2.pdf","comment":"19 pages, 20 figures"},{"id":"http://arxiv.org/abs/2303.09556v2","updated":"2023-03-22T12:55:17Z","published":"2023-03-16T17:59:56Z","title":"Efficient Diffusion Training via Min-SNR Weighting Strategy","summary":"  Denoising diffusion models have been a mainstream approach for image\ngeneration, however, training these models often suffers from slow convergence.\nIn this paper, we discovered that the slow convergence is partly due to\nconflicting optimization directions between timesteps. To address this issue,\nwe treat the diffusion training as a multi-task learning problem, and introduce\na simple yet effective approach referred to as Min-SNR-$\\gamma$. This method\nadapts loss weights of timesteps based on clamped signal-to-noise ratios, which\neffectively balances the conflicts among timesteps. Our results demonstrate a\nsignificant improvement in converging speed, 3.4$\\times$ faster than previous\nweighting strategies. It is also more effective, achieving a new record FID\nscore of 2.06 on the ImageNet $256\\times256$ benchmark using smaller\narchitectures than that employed in previous state-of-the-art. The code is\navailable at https://github.com/TiankaiHang/Min-SNR-Diffusion-Training.\n","authors":["Tiankai Hang","Shuyang Gu","Chen Li","Jianmin Bao","Dong Chen","Han Hu","Xin Geng","Baining Guo"],"pdf_url":"https://arxiv.org/pdf/2303.09556v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.12512v1","updated":"2023-03-22T12:46:15Z","published":"2023-03-22T12:46:15Z","title":"Sibling-Attack: Rethinking Transferable Adversarial Attacks against Face\n  Recognition","summary":"  A hard challenge in developing practical face recognition (FR) attacks is due\nto the black-box nature of the target FR model, i.e., inaccessible gradient and\nparameter information to attackers. While recent research took an important\nstep towards attacking black-box FR models through leveraging transferability,\ntheir performance is still limited, especially against online commercial FR\nsystems that can be pessimistic (e.g., a less than 50% ASR--attack success rate\non average). Motivated by this, we present Sibling-Attack, a new FR attack\ntechnique for the first time explores a novel multi-task perspective (i.e.,\nleveraging extra information from multi-correlated tasks to boost attacking\ntransferability). Intuitively, Sibling-Attack selects a set of tasks correlated\nwith FR and picks the Attribute Recognition (AR) task as the task used in\nSibling-Attack based on theoretical and quantitative analysis. Sibling-Attack\nthen develops an optimization framework that fuses adversarial gradient\ninformation through (1) constraining the cross-task features to be under the\nsame space, (2) a joint-task meta optimization framework that enhances the\ngradient compatibility among tasks, and (3) a cross-task gradient stabilization\nmethod which mitigates the oscillation effect during attacking. Extensive\nexperiments demonstrate that Sibling-Attack outperforms state-of-the-art FR\nattack techniques by a non-trivial margin, boosting ASR by 12.61% and 55.77% on\naverage on state-of-the-art pre-trained FR models and two well-known, widely\nused commercial FR systems.\n","authors":["Zexin Li","Bangjie Yin","Taiping Yao","Juefeng Guo","Shouhong Ding","Simin Chen","Cong Liu"],"pdf_url":"https://arxiv.org/pdf/2303.12512v1.pdf","comment":"8 pages, 5 fivures, accepted by CVPR 2023 as a poster paper"},{"id":"http://arxiv.org/abs/2206.10555v2","updated":"2023-03-22T12:43:10Z","published":"2022-06-21T17:35:57Z","title":"LargeKernel3D: Scaling up Kernels in 3D Sparse CNNs","summary":"  Recent advance in 2D CNNs has revealed that large kernels are important.\nHowever, when directly applying large convolutional kernels in 3D CNNs, severe\ndifficulties are met, where those successful module designs in 2D become\nsurprisingly ineffective on 3D networks, including the popular depth-wise\nconvolution. To address this vital challenge, we instead propose the\nspatial-wise partition convolution and its large-kernel module. As a result, it\navoids the optimization and efficiency issues of naive 3D large kernels. Our\nlarge-kernel 3D CNN network, LargeKernel3D, yields notable improvement in 3D\ntasks of semantic segmentation and object detection. It achieves 73.9% mIoU on\nthe ScanNetv2 semantic segmentation and 72.8% NDS nuScenes object detection\nbenchmarks, ranking 1st on the nuScenes LIDAR leaderboard. The performance\nfurther boosts to 74.2% NDS with a simple multi-modal fusion. In addition,\nLargeKernel3D can be scaled to 17x17x17 kernel size on Waymo 3D object\ndetection. For the first time, we show that large kernels are feasible and\nessential for 3D visual tasks.\n","authors":["Yukang Chen","Jianhui Liu","Xiangyu Zhang","Xiaojuan Qi","Jiaya Jia"],"pdf_url":"https://arxiv.org/pdf/2206.10555v2.pdf","comment":"In CVPR 2023. Code is at\n  https://github.com/dvlab-research/LargeKernel3D"},{"id":"http://arxiv.org/abs/2303.08345v2","updated":"2023-03-22T12:41:03Z","published":"2023-03-15T03:54:43Z","title":"Scanning Only Once: An End-to-end Framework for Fast Temporal Grounding\n  in Long Videos","summary":"  Video temporal grounding aims to pinpoint a video segment that matches the\nquery description. Despite the recent advance in short-form videos\n(\\textit{e.g.}, in minutes), temporal grounding in long videos (\\textit{e.g.},\nin hours) is still at its early stage. To address this challenge, a common\npractice is to employ a sliding window, yet can be inefficient and inflexible\ndue to the limited number of frames within the window. In this work, we propose\nan end-to-end framework for fast temporal grounding, which is able to model an\nhours-long video with \\textbf{one-time} network execution. Our pipeline is\nformulated in a coarse-to-fine manner, where we first extract context knowledge\nfrom non-overlapped video clips (\\textit{i.e.}, anchors), and then supplement\nthe anchors that highly response to the query with detailed content knowledge.\nBesides the remarkably high pipeline efficiency, another advantage of our\napproach is the capability of capturing long-range temporal correlation, thanks\nto modeling the entire video as a whole, and hence facilitates more accurate\ngrounding. Experimental results suggest that, on the long-form video datasets\nMAD and Ego4d, our method significantly outperforms state-of-the-arts, and\nachieves \\textbf{14.6$\\times$} / \\textbf{102.8$\\times$} higher efficiency\nrespectively. Project can be found at\n\\url{https://github.com/afcedf/SOONet.git}.\n","authors":["Yulin Pan","Xiangteng He","Biao Gong","Yiliang Lv","Yujun Shen","Yuxin Peng","Deli Zhao"],"pdf_url":"https://arxiv.org/pdf/2303.08345v2.pdf","comment":"11 pages, 8 figures"},{"id":"http://arxiv.org/abs/2303.12501v1","updated":"2023-03-22T12:11:59Z","published":"2023-03-22T12:11:59Z","title":"Cross-Modal Implicit Relation Reasoning and Aligning for Text-to-Image\n  Person Retrieval","summary":"  Text-to-image person retrieval aims to identify the target person based on a\ngiven textual description query. The primary challenge is to learn the mapping\nof visual and textual modalities into a common latent space. Prior works have\nattempted to address this challenge by leveraging separately pre-trained\nunimodal models to extract visual and textual features. However, these\napproaches lack the necessary underlying alignment capabilities required to\nmatch multimodal data effectively. Besides, these works use prior information\nto explore explicit part alignments, which may lead to the distortion of\nintra-modality information. To alleviate these issues, we present IRRA: a\ncross-modal Implicit Relation Reasoning and Aligning framework that learns\nrelations between local visual-textual tokens and enhances global image-text\nmatching without requiring additional prior supervision. Specifically, we first\ndesign an Implicit Relation Reasoning module in a masked language modeling\nparadigm. This achieves cross-modal interaction by integrating the visual cues\ninto the textual tokens with a cross-modal multimodal interaction encoder.\nSecondly, to globally align the visual and textual embeddings, Similarity\nDistribution Matching is proposed to minimize the KL divergence between\nimage-text similarity distributions and the normalized label matching\ndistributions. The proposed method achieves new state-of-the-art results on all\nthree public datasets, with a notable margin of about 3%-9% for Rank-1 accuracy\ncompared to prior methods.\n","authors":["Ding Jiang","Mang Ye"],"pdf_url":"https://arxiv.org/pdf/2303.12501v1.pdf","comment":"Accepted by CVPR 2023. Codes are available at this\n  https://github.com/anosorae/IRRA"},{"id":"http://arxiv.org/abs/2303.12499v1","updated":"2023-03-22T12:10:44Z","published":"2023-03-22T12:10:44Z","title":"On Domain-Specific Pre-Training for Effective Semantic Perception in\n  Agricultural Robotics","summary":"  Agricultural robots have the prospect to enable more efficient and\nsustainable agricultural production of food, feed, and fiber. Perception of\ncrops and weeds is a central component of agricultural robots that aim to\nmonitor fields and assess the plants as well as their growth stage in an\nautomatic manner. Semantic perception mostly relies on deep learning using\nsupervised approaches, which require time and qualified workers to label fairly\nlarge amounts of data. In this paper, we look into the problem of reducing the\namount of labels without compromising the final segmentation performance. For\nrobots operating in the field, pre-training networks in a supervised way is\nalready a popular method to reduce the number of required labeled images. We\ninvestigate the possibility of pre-training in a self-supervised fashion using\ndata from the target domain. To better exploit this data, we propose a set of\ndomain-specific augmentation strategies. We evaluate our pre-training on\nsemantic segmentation and leaf instance segmentation, two important tasks in\nour domain. The experimental results suggest that pre-training with\ndomain-specific data paired with our data augmentation strategy leads to\nsuperior performance compared to commonly used pre-trainings. Furthermore, the\npre-trained networks obtain similar performance to the fully supervised with\nless labeled data.\n","authors":["Gianmarco Roggiolani","Federico Magistri","Tiziano Guadagnino","Jan Weyler","Giorgio Grisetti","Cyrill Stachniss","Jens Behley"],"pdf_url":"https://arxiv.org/pdf/2303.12499v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2301.06429v3","updated":"2023-03-22T12:01:42Z","published":"2023-01-16T13:38:22Z","title":"Linguistic Query-Guided Mask Generation for Referring Image Segmentation","summary":"  Referring image segmentation aims to segment the image region of interest\naccording to the given language expression, which is a typical multi-modal\ntask. Existing methods either adopt the pixel classification-based or the\nlearnable query-based framework for mask generation, both of which are\ninsufficient to deal with various text-image pairs with a fix number of\nparametric prototypes. In this work, we propose an end-to-end framework built\non transformer to perform Linguistic query-Guided mask generation, dubbed\nLGFormer. It views the linguistic features as query to generate a specialized\nprototype for arbitrary input image-text pair, thus generating more consistent\nsegmentation results. Moreover, we design several cross-modal interaction\nmodules (\\eg, vision-language bidirectional attention module, VLBA) in both\nencoder and decoder to achieve better cross-modal alignment.\n","authors":["Zhichao Wei","Xiaohao Chen","Mingqiang Chen","Siyu Zhu"],"pdf_url":"https://arxiv.org/pdf/2301.06429v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.12484v1","updated":"2023-03-22T11:51:49Z","published":"2023-03-22T11:51:49Z","title":"Label-Efficient Deep Learning in Medical Image Analysis: Challenges and\n  Future Directions","summary":"  Deep learning has seen rapid growth in recent years and achieved\nstate-of-the-art performance in a wide range of applications. However, training\nmodels typically requires expensive and time-consuming collection of large\nquantities of labeled data. This is particularly true within the scope of\nmedical imaging analysis (MIA), where data are limited and labels are expensive\nto be acquired. Thus, label-efficient deep learning methods are developed to\nmake comprehensive use of the labeled data as well as the abundance of\nunlabeled and weak-labeled data. In this survey, we extensively investigated\nover 300 recent papers to provide a comprehensive overview of recent progress\non label-efficient learning strategies in MIA. We first present the background\nof label-efficient learning and categorize the approaches into different\nschemes. Next, we examine the current state-of-the-art methods in detail\nthrough each scheme. Specifically, we provide an in-depth investigation,\ncovering not only canonical semi-supervised, self-supervised, and\nmulti-instance learning schemes, but also recently emerged active and\nannotation-efficient learning strategies. Moreover, as a comprehensive\ncontribution to the field, this survey not only elucidates the commonalities\nand unique features of the surveyed methods but also presents a detailed\nanalysis of the current challenges in the field and suggests potential avenues\nfor future research.\n","authors":["Cheng Jin","Zhengrui Guo","Yi Lin","Luyang Luo","Hao Chen"],"pdf_url":"https://arxiv.org/pdf/2303.12484v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.08320v3","updated":"2023-03-22T11:24:34Z","published":"2023-03-15T02:16:39Z","title":"VideoFusion: Decomposed Diffusion Models for High-Quality Video\n  Generation","summary":"  A diffusion probabilistic model (DPM), which constructs a forward diffusion\nprocess by gradually adding noise to data points and learns the reverse\ndenoising process to generate new samples, has been shown to handle complex\ndata distribution. Despite its recent success in image synthesis, applying DPMs\nto video generation is still challenging due to high-dimensional data spaces.\nPrevious methods usually adopt a standard diffusion process, where frames in\nthe same video clip are destroyed with independent noises, ignoring the content\nredundancy and temporal correlation. This work presents a decomposed diffusion\nprocess via resolving the per-frame noise into a base noise that is shared\namong all frames and a residual noise that varies along the time axis. The\ndenoising pipeline employs two jointly-learned networks to match the noise\ndecomposition accordingly. Experiments on various datasets confirm that our\napproach, termed as VideoFusion, surpasses both GAN-based and diffusion-based\nalternatives in high-quality video generation. We further show that our\ndecomposed formulation can benefit from pre-trained image diffusion models and\nwell-support text-conditioned video creation.\n","authors":["Zhengxiong Luo","Dayou Chen","Yingya Zhang","Yan Huang","Liang Wang","Yujun Shen","Deli Zhao","Jingren Zhou","Tieniu Tan"],"pdf_url":"https://arxiv.org/pdf/2303.08320v3.pdf","comment":"Accepted to CVPR2023"},{"id":"http://arxiv.org/abs/2203.16244v3","updated":"2023-03-22T11:19:19Z","published":"2022-03-30T12:22:26Z","title":"CycDA: Unsupervised Cycle Domain Adaptation from Image to Video","summary":"  Although action recognition has achieved impressive results over recent\nyears, both collection and annotation of video training data are still\ntime-consuming and cost intensive. Therefore, image-to-video adaptation has\nbeen proposed to exploit labeling-free web image source for adapting on\nunlabeled target videos. This poses two major challenges: (1) spatial domain\nshift between web images and video frames; (2) modality gap between image and\nvideo data. To address these challenges, we propose Cycle Domain Adaptation\n(CycDA), a cycle-based approach for unsupervised image-to-video domain\nadaptation by leveraging the joint spatial information in images and videos on\nthe one hand and, on the other hand, training an independent spatio-temporal\nmodel to bridge the modality gap. We alternate between the spatial and\nspatio-temporal learning with knowledge transfer between the two in each cycle.\nWe evaluate our approach on benchmark datasets for image-to-video as well as\nfor mixed-source domain adaptation achieving state-of-the-art results and\ndemonstrating the benefits of our cyclic adaptation. Code is available at\n\\url{https://github.com/wlin-at/CycDA}.\n","authors":["Wei Lin","Anna Kukleva","Kunyang Sun","Horst Possegger","Hilde Kuehne","Horst Bischof"],"pdf_url":"https://arxiv.org/pdf/2203.16244v3.pdf","comment":"Accepted at ECCV2022. Supplementary included"},{"id":"http://arxiv.org/abs/2212.06726v2","updated":"2023-03-22T11:17:29Z","published":"2022-12-13T16:54:08Z","title":"Semantic Brain Decoding: from fMRI to conceptually similar image\n  reconstruction of visual stimuli","summary":"  Brain decoding is a field of computational neuroscience that uses measurable\nbrain activity to infer mental states or internal representations of perceptual\ninputs. Therefore, we propose a novel approach to brain decoding that also\nrelies on semantic and contextual similarity. We employ an fMRI dataset of\nnatural image vision and create a deep learning decoding pipeline inspired by\nthe existence of both bottom-up and top-down processes in human vision. We\ntrain a linear brain-to-feature model to map fMRI activity features to visual\nstimuli features, assuming that the brain projects visual information onto a\nspace that is homeomorphic to the latent space represented by the last\nconvolutional layer of a pretrained convolutional neural network, which\ntypically collects a variety of semantic features that summarize and highlight\nsimilarities and differences between concepts. These features are then\ncategorized in the latent space using a nearest-neighbor strategy, and the\nresults are used to condition a generative latent diffusion model to create\nnovel images. From fMRI data only, we produce reconstructions of visual stimuli\nthat match the original content very well on a semantic level, surpassing the\nstate of the art in previous literature. We evaluate our work and obtain good\nresults using a quantitative semantic metric (the Wu-Palmer similarity metric\nover the WordNet lexicon, which had an average value of 0.57) and perform a\nhuman evaluation experiment that resulted in correct evaluation, according to\nthe multiplicity of human criteria in evaluating image similarity, in over 80%\nof the test set.\n","authors":["Matteo Ferrante","Tommaso Boccato","Nicola Toschi"],"pdf_url":"https://arxiv.org/pdf/2212.06726v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.09975v2","updated":"2023-03-22T11:14:08Z","published":"2023-03-17T13:48:17Z","title":"MedNeXt: Transformer-driven Scaling of ConvNets for Medical Image\n  Segmentation","summary":"  There has been exploding interest in embracing Transformer-based\narchitectures for medical image segmentation. However, the lack of large-scale\nannotated medical datasets make achieving performances equivalent to those in\nnatural images challenging. Convolutional networks, in contrast, have higher\ninductive biases and consequently, are easily trainable to high performance.\nRecently, the ConvNeXt architecture attempted to modernize the standard ConvNet\nby mirroring Transformer blocks. In this work, we improve upon this to design a\nmodernized and scalable convolutional architecture customized to challenges of\ndata-scarce medical settings. We introduce MedNeXt, a Transformer-inspired\nlarge kernel segmentation network which introduces - 1) A fully ConvNeXt 3D\nEncoder-Decoder Network for medical image segmentation, 2) Residual ConvNeXt up\nand downsampling blocks to preserve semantic richness across scales, 3) A novel\ntechnique to iteratively increase kernel sizes by upsampling small kernel\nnetworks, to prevent performance saturation on limited medical data, 4)\nCompound scaling at multiple levels (depth, width, kernel size) of MedNeXt.\nThis leads to state-of-the-art performance on 4 tasks on CT and MRI modalities\nand varying dataset sizes, representing a modernized deep architecture for\nmedical image segmentation.\n","authors":["Saikat Roy","Gregor Koehler","Constantin Ulrich","Michael Baumgartner","Jens Petersen","Fabian Isensee","Paul F. Jaeger","Klaus Maier-Hein"],"pdf_url":"https://arxiv.org/pdf/2303.09975v2.pdf","comment":"12 pages"},{"id":"http://arxiv.org/abs/2303.12445v1","updated":"2023-03-22T10:30:43Z","published":"2023-03-22T10:30:43Z","title":"MEDIMP: Medical Images and Prompts for renal transplant representation\n  learning","summary":"  Renal transplantation emerges as the most effective solution for end-stage\nrenal disease. Occurring from complex causes, a substantial risk of transplant\nchronic dysfunction persists and may lead to graft loss. Medical imaging plays\na substantial role in renal transplant monitoring in clinical practice.\nHowever, graft supervision is multi-disciplinary, notably joining nephrology,\nurology, and radiology, while identifying robust biomarkers from such\nhigh-dimensional and complex data for prognosis is challenging. In this work,\ntaking inspiration from the recent success of Large Language Models (LLMs), we\npropose MEDIMP -- Medical Images and Prompts -- a model to learn meaningful\nmulti-modal representations of renal transplant Dynamic Contrast-Enhanced\nMagnetic Resonance Imaging (DCE MRI) by incorporating structural\nclinicobiological data after translating them into text prompts. MEDIMP is\nbased on contrastive learning from joint text-image paired embeddings to\nperform this challenging task. Moreover, we propose a framework that generates\nmedical prompts using automatic textual data augmentations from LLMs. Our goal\nis to learn meaningful manifolds of renal transplant DCE MRI, interesting for\nthe prognosis of the transplant or patient status (2, 3, and 4 years after the\ntransplant), fully exploiting the available multi-modal data in the most\nefficient way. Extensive experiments and comparisons with other renal\ntransplant representation learning methods with limited data prove the\neffectiveness of MEDIMP in a relevant clinical setting, giving new directions\ntoward medical prompts. Our code is available at\nhttps://github.com/leomlck/MEDIMP.\n","authors":["Leo Milecki","Vicky Kalogeiton","Sylvain Bodard","Dany Anglicheau","Jean-Michel Correas","Marc-Olivier Timsit","Maria Vakalopoulou"],"pdf_url":"https://arxiv.org/pdf/2303.12445v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2211.12872v3","updated":"2023-03-22T10:05:15Z","published":"2022-11-23T11:26:24Z","title":"μSplit: efficient image decomposition for microscopy data","summary":"  We present uSplit, a dedicated approach for trained image decomposition in\nthe context of fluorescence microscopy images. We find that best results using\nregular deep architectures are achieved when large image patches are used\nduring training, making memory consumption the limiting factor to further\nimproving performance. We therefore introduce lateral contextualization (LC), a\nmemory efficient way to train powerful networks and show that LC leads to\nconsistent and significant improvements on the task at hand. We integrate LC\nwith U-Nets, Hierarchical AEs, and Hierarchical VAEs, for which we formulate a\nmodified ELBO loss. Additionally, LC enables training deeper hierarchical\nmodels than otherwise possible and, interestingly, helps to reduce tiling\nartefacts that are inherently impossible to avoid when using tiled VAE\npredictions. We apply uSplit to five decomposition tasks, one on a synthetic\ndataset, four others derived from real microscopy data. LC achieves SOTA\nresults (average improvements to the best baseline of 2.36 dB PSNR), while\nsimultaneously requiring considerably less GPU memory.\n","authors":[" Ashesh","Alexander Krull","Moises Di Sante","Francesco Silvio Pasqualini","Florian Jug"],"pdf_url":"https://arxiv.org/pdf/2211.12872v3.pdf","comment":"10 pages, 7 figures, 9 pages supplement, 8 supplementary figures"},{"id":"http://arxiv.org/abs/2303.12424v1","updated":"2023-03-22T09:51:08Z","published":"2023-03-22T09:51:08Z","title":"Unsupervised Domain Adaptation for Training Event-Based Networks Using\n  Contrastive Learning and Uncorrelated Conditioning","summary":"  Event-based cameras offer reliable measurements for preforming computer\nvision tasks in high-dynamic range environments and during fast motion\nmaneuvers. However, adopting deep learning in event-based vision faces the\nchallenge of annotated data scarcity due to recency of event cameras.\nTransferring the knowledge that can be obtained from conventional camera\nannotated data offers a practical solution to this challenge. We develop an\nunsupervised domain adaptation algorithm for training a deep network for\nevent-based data image classification using contrastive learning and\nuncorrelated conditioning of data. Our solution outperforms the existing\nalgorithms for this purpose.\n","authors":["Dayuan Jian","Mohammad Rostami"],"pdf_url":"https://arxiv.org/pdf/2303.12424v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.12423v1","updated":"2023-03-22T09:49:53Z","published":"2023-03-22T09:49:53Z","title":"Text with Knowledge Graph Augmented Transformer for Video Captioning","summary":"  Video captioning aims to describe the content of videos using natural\nlanguage. Although significant progress has been made, there is still much room\nto improve the performance for real-world applications, mainly due to the\nlong-tail words challenge. In this paper, we propose a text with knowledge\ngraph augmented transformer (TextKG) for video captioning. Notably, TextKG is a\ntwo-stream transformer, formed by the external stream and internal stream. The\nexternal stream is designed to absorb additional knowledge, which models the\ninteractions between the additional knowledge, e.g., pre-built knowledge graph,\nand the built-in information of videos, e.g., the salient object regions,\nspeech transcripts, and video captions, to mitigate the long-tail words\nchallenge. Meanwhile, the internal stream is designed to exploit the\nmulti-modality information in videos (e.g., the appearance of video frames,\nspeech transcripts, and video captions) to ensure the quality of caption\nresults. In addition, the cross attention mechanism is also used in between the\ntwo streams for sharing information. In this way, the two streams can help each\nother for more accurate results. Extensive experiments conducted on four\nchallenging video captioning datasets, i.e., YouCookII, ActivityNet Captions,\nMSRVTT, and MSVD, demonstrate that the proposed method performs favorably\nagainst the state-of-the-art methods. Specifically, the proposed TextKG method\noutperforms the best published results by improving 18.7% absolute CIDEr scores\non the YouCookII dataset.\n","authors":["Xin Gu","Guang Chen","Yufei Wang","Libo Zhang","Tiejian Luo","Longyin Wen"],"pdf_url":"https://arxiv.org/pdf/2303.12423v1.pdf","comment":"Accepted by CVPR2023"},{"id":"http://arxiv.org/abs/2303.06919v2","updated":"2023-03-22T09:45:51Z","published":"2023-03-13T08:36:30Z","title":"NeRFLiX: High-Quality Neural View Synthesis by Learning a\n  Degradation-Driven Inter-viewpoint MiXer","summary":"  Neural radiance fields (NeRF) show great success in novel view synthesis.\nHowever, in real-world scenes, recovering high-quality details from the source\nimages is still challenging for the existing NeRF-based approaches, due to the\npotential imperfect calibration information and scene representation\ninaccuracy. Even with high-quality training frames, the synthetic novel views\nproduced by NeRF models still suffer from notable rendering artifacts, such as\nnoise, blur, etc. Towards to improve the synthesis quality of NeRF-based\napproaches, we propose NeRFLiX, a general NeRF-agnostic restorer paradigm by\nlearning a degradation-driven inter-viewpoint mixer. Specially, we design a\nNeRF-style degradation modeling approach and construct large-scale training\ndata, enabling the possibility of effectively removing NeRF-native rendering\nartifacts for existing deep neural networks. Moreover, beyond the degradation\nremoval, we propose an inter-viewpoint aggregation framework that is able to\nfuse highly related high-quality training images, pushing the performance of\ncutting-edge NeRF models to entirely new levels and producing highly\nphoto-realistic synthetic views.\n","authors":["Kun Zhou","Wenbo Li","Yi Wang","Tao Hu","Nianjuan Jiang","Xiaoguang Han","Jiangbo Lu"],"pdf_url":"https://arxiv.org/pdf/2303.06919v2.pdf","comment":"Accepted to CVPR 2023; Project Page: see\n  https://redrock303.github.io/nerflix/"},{"id":"http://arxiv.org/abs/2302.12242v2","updated":"2023-03-22T09:45:35Z","published":"2023-02-23T18:58:28Z","title":"Side Adapter Network for Open-Vocabulary Semantic Segmentation","summary":"  This paper presents a new framework for open-vocabulary semantic segmentation\nwith the pre-trained vision-language model, named Side Adapter Network (SAN).\nOur approach models the semantic segmentation task as a region recognition\nproblem. A side network is attached to a frozen CLIP model with two branches:\none for predicting mask proposals, and the other for predicting attention bias\nwhich is applied in the CLIP model to recognize the class of masks. This\ndecoupled design has the benefit CLIP in recognizing the class of mask\nproposals. Since the attached side network can reuse CLIP features, it can be\nvery light. In addition, the entire network can be trained end-to-end, allowing\nthe side network to be adapted to the frozen CLIP model, which makes the\npredicted mask proposals CLIP-aware. Our approach is fast, accurate, and only\nadds a few additional trainable parameters. We evaluate our approach on\nmultiple semantic segmentation benchmarks. Our method significantly outperforms\nother counterparts, with up to 18 times fewer trainable parameters and 19 times\nfaster inference speed. We hope our approach will serve as a solid baseline and\nhelp ease future research in open-vocabulary semantic segmentation. The code\nwill be available at https://github.com/MendelXu/SAN.\n","authors":["Mengde Xu","Zheng Zhang","Fangyun Wei","Han Hu","Xiang Bai"],"pdf_url":"https://arxiv.org/pdf/2302.12242v2.pdf","comment":"CVPR2023 Highlight"},{"id":"http://arxiv.org/abs/2303.12421v1","updated":"2023-03-22T09:38:34Z","published":"2023-03-22T09:38:34Z","title":"Region-wise matching for image inpainting based on adaptive weighted\n  low-rank decomposition","summary":"  Digital image inpainting is an interpolation problem, inferring the content\nin the missing (unknown) region to agree with the known region data such that\nthe interpolated result fulfills some prior knowledge. Low-rank and nonlocal\nself-similarity are two important priors for image inpainting. Based on the\nnonlocal self-similarity assumption, an image is divided into overlapped square\ntarget patches (submatrices) and the similar patches of any target patch are\nreshaped as vectors and stacked into a patch matrix. Such a patch matrix\nusually enjoys a property of low rank or approximately low rank, and its\nmissing entries are recoveried by low-rank matrix approximation (LRMA)\nalgorithms. Traditionally, $n$ nearest neighbor similar patches are searched\nwithin a local window centered at a target patch. However, for an image with\nmissing lines, the generated patch matrix is prone to having entirely-missing\nrows such that the downstream low-rank model fails to reconstruct it well. To\naddress this problem, we propose a region-wise matching (RwM) algorithm by\ndividing the neighborhood of a target patch into multiple subregions and then\nsearch the most similar one within each subregion. A non-convex weighted\nlow-rank decomposition (NC-WLRD) model for LRMA is also proposed to reconstruct\nall degraded patch matrices grouped by the proposed RwM algorithm. We solve the\nproposed NC-WLRD model by the alternating direction method of multipliers\n(ADMM) and analyze the convergence in detail. Numerous experiments on line\ninpainting (entire-row/column missing) demonstrate the superiority of our\nmethod over other competitive inpainting algorithms. Unlike other\nlow-rank-based matrix completion methods and inpainting algorithms, the\nproposed model NC-WLRD is also effective for removing random-valued impulse\nnoise and structural noise (stripes).\n","authors":["Shenghai Liao","Xuya Liu","Ruyi Han","Shujun Fu","Yuanfeng Zhou","Yuliang Li"],"pdf_url":"https://arxiv.org/pdf/2303.12421v1.pdf","comment":"region-wise matching algorithm, image inpainting, 20 pages, 18\n  figures"},{"id":"http://arxiv.org/abs/2303.12419v1","updated":"2023-03-22T09:33:50Z","published":"2023-03-22T09:33:50Z","title":"BiCro: Noisy Correspondence Rectification for Multi-modality Data via\n  Bi-directional Cross-modal Similarity Consistency","summary":"  As one of the most fundamental techniques in multimodal learning, cross-modal\nmatching aims to project various sensory modalities into a shared feature\nspace. To achieve this, massive and correctly aligned data pairs are required\nfor model training. However, unlike unimodal datasets, multimodal datasets are\nextremely harder to collect and annotate precisely. As an alternative, the\nco-occurred data pairs (e.g., image-text pairs) collected from the Internet\nhave been widely exploited in the area. Unfortunately, the cheaply collected\ndataset unavoidably contains many mismatched data pairs, which have been proven\nto be harmful to the model's performance. To address this, we propose a general\nframework called BiCro (Bidirectional Cross-modal similarity consistency),\nwhich can be easily integrated into existing cross-modal matching models and\nimprove their robustness against noisy data. Specifically, BiCro aims to\nestimate soft labels for noisy data pairs to reflect their true correspondence\ndegree. The basic idea of BiCro is motivated by that -- taking image-text\nmatching as an example -- similar images should have similar textual\ndescriptions and vice versa. Then the consistency of these two similarities can\nbe recast as the estimated soft labels to train the matching model. The\nexperiments on three popular cross-modal matching datasets demonstrate that our\nmethod significantly improves the noise-robustness of various matching models,\nand surpass the state-of-the-art by a clear margin.\n","authors":["Shuo Yang","Zhaopan Xu","Kai Wang","Yang You","Hongxun Yao","Tongliang Liu","Min Xu"],"pdf_url":"https://arxiv.org/pdf/2303.12419v1.pdf","comment":"CVPR 2023"},{"id":"http://arxiv.org/abs/2303.12417v1","updated":"2023-03-22T09:32:45Z","published":"2023-03-22T09:32:45Z","title":"CLIP^2: Contrastive Language-Image-Point Pretraining from Real-World\n  Point Cloud Data","summary":"  Contrastive Language-Image Pre-training, benefiting from large-scale\nunlabeled text-image pairs, has demonstrated great performance in open-world\nvision understanding tasks. However, due to the limited Text-3D data pairs,\nadapting the success of 2D Vision-Language Models (VLM) to the 3D space remains\nan open problem. Existing works that leverage VLM for 3D understanding\ngenerally resort to constructing intermediate 2D representations for the 3D\ndata, but at the cost of losing 3D geometry information. To take a step toward\nopen-world 3D vision understanding, we propose Contrastive Language-Image-Point\nCloud Pretraining (CLIP^2) to directly learn the transferable 3D point cloud\nrepresentation in realistic scenarios with a novel proxy alignment mechanism.\nSpecifically, we exploit naturally-existed correspondences in 2D and 3D\nscenarios, and build well-aligned and instance-based text-image-point proxies\nfrom those complex scenarios. On top of that, we propose a cross-modal\ncontrastive objective to learn semantic and instance-level aligned point cloud\nrepresentation. Experimental results on both indoor and outdoor scenarios show\nthat our learned 3D representation has great transfer ability in downstream\ntasks, including zero-shot and few-shot 3D recognition, which boosts the\nstate-of-the-art methods by large margins. Furthermore, we provide analyses of\nthe capability of different representations in real scenarios and present the\noptional ensemble scheme.\n","authors":["Yihan Zeng","Chenhan Jiang","Jiageng Mao","Jianhua Han","Chaoqiang Ye","Qingqiu Huang","Dit-Yan Yeung","Zhen Yang","Xiaodan Liang","Hang Xu"],"pdf_url":"https://arxiv.org/pdf/2303.12417v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2211.06627v3","updated":"2023-03-22T09:32:26Z","published":"2022-11-12T10:29:05Z","title":"MARLIN: Masked Autoencoder for facial video Representation LearnINg","summary":"  This paper proposes a self-supervised approach to learn universal facial\nrepresentations from videos, that can transfer across a variety of facial\nanalysis tasks such as Facial Attribute Recognition (FAR), Facial Expression\nRecognition (FER), DeepFake Detection (DFD), and Lip Synchronization (LS). Our\nproposed framework, named MARLIN, is a facial video masked autoencoder, that\nlearns highly robust and generic facial embeddings from abundantly available\nnon-annotated web crawled facial videos. As a challenging auxiliary task,\nMARLIN reconstructs the spatio-temporal details of the face from the densely\nmasked facial regions which mainly include eyes, nose, mouth, lips, and skin to\ncapture local and global aspects that in turn help in encoding generic and\ntransferable features. Through a variety of experiments on diverse downstream\ntasks, we demonstrate MARLIN to be an excellent facial video encoder as well as\nfeature extractor, that performs consistently well across a variety of\ndownstream tasks including FAR (1.13% gain over supervised benchmark), FER\n(2.64% gain over unsupervised benchmark), DFD (1.86% gain over unsupervised\nbenchmark), LS (29.36% gain for Frechet Inception Distance), and even in low\ndata regime. Our code and models are available at\nhttps://github.com/ControlNet/MARLIN .\n","authors":["Zhixi Cai","Shreya Ghosh","Kalin Stefanov","Abhinav Dhall","Jianfei Cai","Hamid Rezatofighi","Reza Haffari","Munawar Hayat"],"pdf_url":"https://arxiv.org/pdf/2211.06627v3.pdf","comment":"CVPR 2023"},{"id":"http://arxiv.org/abs/2303.12408v1","updated":"2023-03-22T09:17:01Z","published":"2023-03-22T09:17:01Z","title":"Balanced Spherical Grid for Egocentric View Synthesis","summary":"  We present EgoNeRF, a practical solution to reconstruct large-scale\nreal-world environments for VR assets. Given a few seconds of casually captured\n360 video, EgoNeRF can efficiently build neural radiance fields which enable\nhigh-quality rendering from novel viewpoints. Motivated by the recent\nacceleration of NeRF using feature grids, we adopt spherical coordinate instead\nof conventional Cartesian coordinate. Cartesian feature grid is inefficient to\nrepresent large-scale unbounded scenes because it has a spatially uniform\nresolution, regardless of distance from viewers. The spherical parameterization\nbetter aligns with the rays of egocentric images, and yet enables factorization\nfor performance enhancement. However, the na\\\"ive spherical grid suffers from\nirregularities at two poles, and also cannot represent unbounded scenes. To\navoid singularities near poles, we combine two balanced grids, which results in\na quasi-uniform angular grid. We also partition the radial grid exponentially\nand place an environment map at infinity to represent unbounded scenes.\nFurthermore, with our resampling technique for grid-based methods, we can\nincrease the number of valid samples to train NeRF volume. We extensively\nevaluate our method in our newly introduced synthetic and real-world egocentric\n360 video datasets, and it consistently achieves state-of-the-art performance.\n","authors":["Changwoon Choi","Sang Min Kim","Young Min Kim"],"pdf_url":"https://arxiv.org/pdf/2303.12408v1.pdf","comment":"Accepted to CVPR 2023"},{"id":"http://arxiv.org/abs/2108.11726v3","updated":"2023-03-22T09:13:00Z","published":"2021-08-26T12:04:32Z","title":"Learning to Diversify for Single Domain Generalization","summary":"  Domain generalization (DG) aims to generalize a model trained on multiple\nsource (i.e., training) domains to a distributionally different target (i.e.,\ntest) domain. In contrast to the conventional DG that strictly requires the\navailability of multiple source domains, this paper considers a more realistic\nyet challenging scenario, namely Single Domain Generalization (Single-DG),\nwhere only one source domain is available for training. In this scenario, the\nlimited diversity may jeopardize the model generalization on unseen target\ndomains. To tackle this problem, we propose a style-complement module to\nenhance the generalization power of the model by synthesizing images from\ndiverse distributions that are complementary to the source ones. More\nspecifically, we adopt a tractable upper bound of mutual information (MI)\nbetween the generated and source samples and perform a two-step optimization\niteratively: (1) by minimizing the MI upper bound approximation for each sample\npair, the generated images are forced to be diversified from the source\nsamples; (2) subsequently, we maximize the MI between the samples from the same\nsemantic category, which assists the network to learn discriminative features\nfrom diverse-styled images. Extensive experiments on three benchmark datasets\ndemonstrate the superiority of our approach, which surpasses the\nstate-of-the-art single-DG methods by up to 25.14%.\n","authors":["Zijian Wang","Yadan Luo","Ruihong Qiu","Zi Huang","Mahsa Baktashmotlagh"],"pdf_url":"https://arxiv.org/pdf/2108.11726v3.pdf","comment":"ICCV 2021"},{"id":"http://arxiv.org/abs/2106.10836v2","updated":"2023-03-22T09:12:09Z","published":"2021-06-21T03:55:33Z","title":"Active Learning for Deep Neural Networks on Edge Devices","summary":"  When dealing with deep neural network (DNN) applications on edge devices,\ncontinuously updating the model is important. Although updating a model with\nreal incoming data is ideal, using all of them is not always feasible due to\nlimits, such as labeling and communication costs. Thus, it is necessary to\nfilter and select the data to use for training (i.e., active learning) on the\ndevice. In this paper, we formalize a practical active learning problem for\nDNNs on edge devices and propose a general task-agnostic framework to tackle\nthis problem, which reduces it to a stream submodular maximization. This\nframework is light enough to be run with low computational resources, yet\nprovides solutions whose quality is theoretically guaranteed thanks to the\nsubmodular property. Through this framework, we can configure data selection\ncriteria flexibly, including using methods proposed in previous active learning\nstudies. We evaluate our approach on both classification and object detection\ntasks in a practical setting to simulate a real-life scenario. The results of\nour study show that the proposed framework outperforms all other methods in\nboth tasks, while running at a practical speed on real devices.\n","authors":["Yuya Senzaki","Christian Hamelain"],"pdf_url":"https://arxiv.org/pdf/2106.10836v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.12400v1","updated":"2023-03-22T09:09:02Z","published":"2023-03-22T09:09:02Z","title":"UMC: A Unified Bandwidth-efficient and Multi-resolution based\n  Collaborative Perception Framework","summary":"  Multi-agent collaborative perception (MCP) has recently attracted much\nattention. It includes three key processes: communication for sharing,\ncollaboration for integration, and reconstruction for different downstream\ntasks. Existing methods pursue designing the collaboration process alone,\nignoring their intrinsic interactions and resulting in suboptimal performance.\nIn contrast, we aim to propose a Unified Collaborative perception framework\nnamed UMC, optimizing the communication, collaboration, and reconstruction\nprocesses with the Multi-resolution technique. The communication introduces a\nnovel trainable multi-resolution and selective-region (MRSR) mechanism,\nachieving higher quality and lower bandwidth. Then, a graph-based collaboration\nis proposed, conducting on each resolution to adapt the MRSR. Finally, the\nreconstruction integrates the multi-resolution collaborative features for\ndownstream tasks. Since the general metric can not reflect the performance\nenhancement brought by MCP systematically, we introduce a brand-new evaluation\nmetric that evaluates the MCP from different perspectives. To verify our\nalgorithm, we conducted experiments on the V2X-Sim and OPV2V datasets. Our\nquantitative and qualitative experiments prove that the proposed UMC greatly\noutperforms the state-of-the-art collaborative perception approaches.\n","authors":["Tianhang Wang","Guang Chen","Kai Chen","Zhengfa Liu","Bo Zhang","Alois Knoll","Changjun Jiang"],"pdf_url":"https://arxiv.org/pdf/2303.12400v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2211.12782v2","updated":"2023-03-22T09:08:09Z","published":"2022-11-23T08:50:03Z","title":"Hand Avatar: Free-Pose Hand Animation and Rendering from Monocular Video","summary":"  We present HandAvatar, a novel representation for hand animation and\nrendering, which can generate smoothly compositional geometry and\nself-occlusion-aware texture. Specifically, we first develop a MANO-HD model as\na high-resolution mesh topology to fit personalized hand shapes. Sequentially,\nwe decompose hand geometry into per-bone rigid parts, and then re-compose\npaired geometry encodings to derive an across-part consistent occupancy field.\nAs for texture modeling, we propose a self-occlusion-aware shading field\n(SelF). In SelF, drivable anchors are paved on the MANO-HD surface to record\nalbedo information under a wide variety of hand poses. Moreover, directed soft\noccupancy is designed to describe the ray-to-surface relation, which is\nleveraged to generate an illumination field for the disentanglement of\npose-independent albedo and pose-dependent illumination. Trained from monocular\nvideo data, our HandAvatar can perform free-pose hand animation and rendering\nwhile at the same time achieving superior appearance fidelity. We also\ndemonstrate that HandAvatar provides a route for hand appearance editing.\nProject website: https://seanchenxy.github.io/HandAvatarWeb.\n","authors":["Xingyu Chen","Baoyuan Wang","Heung-Yeung Shum"],"pdf_url":"https://arxiv.org/pdf/2211.12782v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.12398v1","updated":"2023-03-22T09:06:07Z","published":"2023-03-22T09:06:07Z","title":"Multiscale Attention via Wavelet Neural Operators for Vision\n  Transformers","summary":"  Transformers have achieved widespread success in computer vision. At their\nheart, there is a Self-Attention (SA) mechanism, an inductive bias that\nassociates each token in the input with every other token through a weighted\nbasis. The standard SA mechanism has quadratic complexity with the sequence\nlength, which impedes its utility to long sequences appearing in high\nresolution vision. Recently, inspired by operator learning for PDEs, Adaptive\nFourier Neural Operators (AFNO) were introduced for high resolution attention\nbased on global convolution that is efficiently implemented via FFT. However,\nthe AFNO global filtering cannot well represent small and moderate scale\nstructures that commonly appear in natural images. To leverage the\ncoarse-to-fine scale structures we introduce a Multiscale Wavelet Attention\n(MWA) by leveraging wavelet neural operators which incurs linear complexity in\nthe sequence size. We replace the attention in ViT with MWA and our experiments\nwith CIFAR and ImageNet classification demonstrate significant improvement over\nalternative Fourier-based attentions such as AFNO and Global Filter Network\n(GFN).\n","authors":["Anahita Nekoozadeh","Mohammad Reza Ahmadzadeh","Zahra Mardani","Morteza Mardani"],"pdf_url":"https://arxiv.org/pdf/2303.12398v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.12396v1","updated":"2023-03-22T09:02:54Z","published":"2023-03-22T09:02:54Z","title":"Rigidity-Aware Detection for 6D Object Pose Estimation","summary":"  Most recent 6D object pose estimation methods first use object detection to\nobtain 2D bounding boxes before actually regressing the pose. However, the\ngeneral object detection methods they use are ill-suited to handle cluttered\nscenes, thus producing poor initialization to the subsequent pose network. To\naddress this, we propose a rigidity-aware detection method exploiting the fact\nthat, in 6D pose estimation, the target objects are rigid. This lets us\nintroduce an approach to sampling positive object regions from the entire\nvisible object area during training, instead of naively drawing samples from\nthe bounding box center where the object might be occluded. As such, every\nvisible object part can contribute to the final bounding box prediction,\nyielding better detection robustness. Key to the success of our approach is a\nvisibility map, which we propose to build using a minimum barrier distance\nbetween every pixel in the bounding box and the box boundary. Our results on\nseven challenging 6D pose estimation datasets evidence that our method\noutperforms general detection frameworks by a large margin. Furthermore,\ncombined with a pose regression network, we obtain state-of-the-art pose\nestimation results on the challenging BOP benchmark.\n","authors":["Yang Hai","Rui Song","Jiaojiao Li","Mathieu Salzmann","Yinlin Hu"],"pdf_url":"https://arxiv.org/pdf/2303.12396v1.pdf","comment":"CVPR 2023"},{"id":"http://arxiv.org/abs/2303.12394v1","updated":"2023-03-22T08:59:42Z","published":"2023-03-22T08:59:42Z","title":"Road Extraction with Satellite Images and Partial Road Maps","summary":"  Road extraction is a process of automatically generating road maps mainly\nfrom satellite images. Existing models all target to generate roads from the\nscratch despite that a large quantity of road maps, though incomplete, are\npublicly available (e.g. those from OpenStreetMap) and can help with road\nextraction. In this paper, we propose to conduct road extraction based on\nsatellite images and partial road maps, which is new. We then propose a\ntwo-branch Partial to Complete Network (P2CNet) for the task, which has two\nprominent components: Gated Self-Attention Module (GSAM) and Missing Part (MP)\nloss. GSAM leverages a channel-wise self-attention module and a gate module to\ncapture long-range semantics, filter out useless information, and better fuse\nthe features from two branches. MP loss is derived from the partial road maps,\ntrying to give more attention to the road pixels that do not exist in partial\nroad maps. Extensive experiments are conducted to demonstrate the effectiveness\nof our model, e.g. P2CNet achieves state-of-the-art performance with the IoU\nscores of 70.71% and 75.52%, respectively, on the SpaceNet and OSM datasets.\n","authors":["Qianxiong Xu","Cheng Long","Liang Yu","Chen Zhang"],"pdf_url":"https://arxiv.org/pdf/2303.12394v1.pdf","comment":"This paper has been accepted by IEEE Transactions on Geoscience and\n  Remote Sensing"},{"id":"http://arxiv.org/abs/2212.00210v2","updated":"2023-03-22T08:58:15Z","published":"2022-12-01T01:39:28Z","title":"Shape-Guided Diffusion with Inside-Outside Attention","summary":"  When manipulating an object, existing text-to-image diffusion models often\nignore the shape of the object and generate content that is incorrectly scaled,\ncut off, or replaced with background content. We propose a training-free\nmethod, Shape-Guided Diffusion, that modifies pretrained diffusion models to be\nsensitive to shape input specified by a user or automatically inferred from\ntext. We use a novel Inside-Outside Attention mechanism during the inversion\nand generation process to apply this shape constraint to the cross- and\nself-attention maps. Our mechanism designates which spatial region is the\nobject (inside) vs. background (outside) then associates edits specified by\ntext prompts to the correct region. We demonstrate the efficacy of our method\non the shape-guided editing task, where the model must replace an object\naccording to a text prompt and object mask. We curate a new ShapePrompts\nbenchmark derived from MS-COCO and achieve SOTA results in shape faithfulness\nwithout a degradation in text alignment or image realism according to both\nautomatic metrics and annotator ratings. Our data and code will be made\navailable at https://shape-guided-diffusion.github.io.\n","authors":["Dong Huk Park","Grace Luo","Clayton Toste","Samaneh Azadi","Xihui Liu","Maka Karalashvili","Anna Rohrbach","Trevor Darrell"],"pdf_url":"https://arxiv.org/pdf/2212.00210v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.11330v2","updated":"2023-03-22T08:48:15Z","published":"2023-03-20T17:59:58Z","title":"Legs as Manipulator: Pushing Quadrupedal Agility Beyond Locomotion","summary":"  Locomotion has seen dramatic progress for walking or running across\nchallenging terrains. However, robotic quadrupeds are still far behind their\nbiological counterparts, such as dogs, which display a variety of agile skills\nand can use the legs beyond locomotion to perform several basic manipulation\ntasks like interacting with objects and climbing. In this paper, we take a step\ntowards bridging this gap by training quadruped robots not only to walk but\nalso to use the front legs to climb walls, press buttons, and perform object\ninteraction in the real world. To handle this challenging optimization, we\ndecouple the skill learning broadly into locomotion, which involves anything\nthat involves movement whether via walking or climbing a wall, and\nmanipulation, which involves using one leg to interact while balancing on the\nother three legs. These skills are trained in simulation using curriculum and\ntransferred to the real world using our proposed sim2real variant that builds\nupon recent locomotion success. Finally, we combine these skills into a robust\nlong-term plan by learning a behavior tree that encodes a high-level task\nhierarchy from one clean expert demonstration. We evaluate our method in both\nsimulation and real-world showing successful executions of both short as well\nas long-range tasks and how robustness helps confront external perturbations.\nVideos at https://robot-skills.github.io\n","authors":["Xuxin Cheng","Ashish Kumar","Deepak Pathak"],"pdf_url":"https://arxiv.org/pdf/2303.11330v2.pdf","comment":"Accepted at ICRA 2023. Videos at https://robot-skills.github.io"},{"id":"http://arxiv.org/abs/2303.12384v1","updated":"2023-03-22T08:47:37Z","published":"2023-03-22T08:47:37Z","title":"RegFormer: An Efficient Projection-Aware Transformer Network for\n  Large-Scale Point Cloud Registration","summary":"  Although point cloud registration has achieved remarkable advances in\nobject-level and indoor scenes, large-scale registration methods are rarely\nexplored. Challenges mainly arise from the huge point number, complex\ndistribution, and outliers of outdoor LiDAR scans. In addition, most existing\nregistration works generally adopt a two-stage paradigm: They first find\ncorrespondences by extracting discriminative local features, and then leverage\nestimators (eg. RANSAC) to filter outliers, which are highly dependent on\nwell-designed descriptors and post-processing choices. To address these\nproblems, we propose an end-to-end transformer network (RegFormer) for\nlarge-scale point cloud alignment without any further post-processing.\nSpecifically, a projection-aware hierarchical transformer is proposed to\ncapture long-range dependencies and filter outliers by extracting point\nfeatures globally. Our transformer has linear complexity, which guarantees high\nefficiency even for large-scale scenes. Furthermore, to effectively reduce\nmismatches, a bijective association transformer is designed for regressing the\ninitial transformation. Extensive experiments on KITTI and NuScenes datasets\ndemonstrate that our RegFormer achieves state-of-the-art performance in terms\nof both accuracy and efficiency.\n","authors":["Jiuming Liu","Guangming Wang","Zhe Liu","Chaokang Jiang","Marc Pollefeys","Hesheng Wang"],"pdf_url":"https://arxiv.org/pdf/2303.12384v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2302.08175v5","updated":"2023-03-22T08:32:12Z","published":"2023-02-16T09:44:55Z","title":"A numerical approximation method for the Fisher-Rao distance between\n  multivariate normal distributions","summary":"  We present a simple method to approximate Rao's distance between multivariate\nnormal distributions based on discretizing curves joining normal distributions\nand approximating Rao's distances between successive nearby normal\ndistributions on the curves by the square root of Jeffreys divergence, the\nsymmetrized Kullback-Leibler divergence. We consider experimentally the linear\ninterpolation curves in the ordinary, natural and expectation parameterizations\nof the normal distributions, and compare these curves with a curve derived from\nthe Calvo and Oller's isometric embedding of the Fisher-Rao $d$-variate normal\nmanifold into the cone of $(d+1)\\times (d+1)$ symmetric positive-definite\nmatrices [Journal of multivariate analysis 35.2 (1990): 223-242]. We report on\nour experiments and assess the quality of our approximation technique by\ncomparing the numerical approximations with both lower and upper bounds.\nFinally, we present several information-geometric properties of the Calvo and\nOller's isometric embedding.\n","authors":["Frank Nielsen"],"pdf_url":"https://arxiv.org/pdf/2302.08175v5.pdf","comment":"42 pages, 17 figures, 3 tables"},{"id":"http://arxiv.org/abs/2303.12379v1","updated":"2023-03-22T08:28:23Z","published":"2023-03-22T08:28:23Z","title":"VMCML: Video and Music Matching via Cross-Modality Lifting","summary":"  We propose a content-based system for matching video and background music.\nThe system aims to address the challenges in music recommendation for new users\nor new music give short-form videos. To this end, we propose a cross-modal\nframework VMCML that finds a shared embedding space between video and music\nrepresentations. To ensure the embedding space can be effectively shared by\nboth representations, we leverage CosFace loss based on margin-based cosine\nsimilarity loss. Furthermore, we establish a large-scale dataset called MSVD,\nin which we provide 390 individual music and the corresponding matched 150,000\nvideos. We conduct extensive experiments on Youtube-8M and our MSVD datasets.\nOur quantitative and qualitative results demonstrate the effectiveness of our\nproposed framework and achieve state-of-the-art video and music matching\nperformance.\n","authors":["Yi-Shan Lee","Wei-Cheng Tseng","Fu-En Wang","Min Sun"],"pdf_url":"https://arxiv.org/pdf/2303.12379v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.11910v2","updated":"2023-03-22T08:23:28Z","published":"2023-03-21T15:01:02Z","title":"360BEV: Panoramic Semantic Mapping for Indoor Bird's-Eye View","summary":"  Seeing only a tiny part of the whole is not knowing the full circumstance.\nBird's-eye-view (BEV) perception, a process of obtaining allocentric maps from\negocentric views, is restricted when using a narrow Field of View (FoV) alone.\nIn this work, mapping from 360{\\deg} panoramas to BEV semantics, the 360BEV\ntask, is established for the first time to achieve holistic representations of\nindoor scenes in a top-down view. Instead of relying on narrow-FoV image\nsequences, a panoramic image with depth information is sufficient to generate a\nholistic BEV semantic map. To benchmark 360BEV, we present two indoor datasets,\n360BEV-Matterport and 360BEV-Stanford, both of which include egocentric\npanoramic images and semantic segmentation labels, as well as allocentric\nsemantic maps. Besides delving deep into different mapping paradigms, we\npropose a dedicated solution for panoramic semantic mapping, namely 360Mapper.\nThrough extensive experiments, our methods achieve 44.32% and 45.78% in mIoU on\nboth datasets respectively, surpassing previous counterparts with gains of\n+7.60% and +9.70% in mIoU. Code and datasets will be available at:\nhttps://jamycheung.github.io/360BEV.html.\n","authors":["Zhifeng Teng","Jiaming Zhang","Kailun Yang","Kunyu Peng","Hao Shi","Simon Reiß","Ke Cao","Rainer Stiefelhagen"],"pdf_url":"https://arxiv.org/pdf/2303.11910v2.pdf","comment":"Code and datasets will be available at:\n  https://jamycheung.github.io/360BEV.html"},{"id":"http://arxiv.org/abs/2303.12371v1","updated":"2023-03-22T08:14:23Z","published":"2023-03-22T08:14:23Z","title":"$P^{3}O$: Transferring Visual Representations for Reinforcement Learning\n  via Prompting","summary":"  It is important for deep reinforcement learning (DRL) algorithms to transfer\ntheir learned policies to new environments that have different visual inputs.\nIn this paper, we introduce Prompt based Proximal Policy Optimization\n($P^{3}O$), a three-stage DRL algorithm that transfers visual representations\nfrom a target to a source environment by applying prompting. The process of\n$P^{3}O$ consists of three stages: pre-training, prompting, and predicting. In\nparticular, we specify a prompt-transformer for representation conversion and\npropose a two-step training process to train the prompt-transformer for the\ntarget environment, while the rest of the DRL pipeline remains unchanged. We\nimplement $P^{3}O$ and evaluate it on the OpenAI CarRacing video game. The\nexperimental results show that $P^{3}O$ outperforms the state-of-the-art visual\ntransferring schemes. In particular, $P^{3}O$ allows the learned policies to\nperform well in environments with different visual inputs, which is much more\neffective than retraining the policies in these environments.\n","authors":["Guoliang You","Xiaomeng Chu","Yifan Duan","Jie Peng","Jianmin Ji","Yu Zhang","Yanyong Zhang"],"pdf_url":"https://arxiv.org/pdf/2303.12371v1.pdf","comment":"This paper has been accepted to be presented at the upcoming IEEE\n  International Conference on Multimedia & Expo (ICME) in 2023"},{"id":"http://arxiv.org/abs/2303.12370v1","updated":"2023-03-22T08:13:25Z","published":"2023-03-22T08:13:25Z","title":"Weakly Supervised Video Representation Learning with Unaligned Text for\n  Sequential Videos","summary":"  Sequential video understanding, as an emerging video understanding task, has\ndriven lots of researchers' attention because of its goal-oriented nature. This\npaper studies weakly supervised sequential video understanding where the\naccurate time-stamp level text-video alignment is not provided. We solve this\ntask by borrowing ideas from CLIP. Specifically, we use a transformer to\naggregate frame-level features for video representation and use a pre-trained\ntext encoder to encode the texts corresponding to each action and the whole\nvideo, respectively. To model the correspondence between text and video, we\npropose a multiple granularity loss, where the video-paragraph contrastive loss\nenforces matching between the whole video and the complete script, and a\nfine-grained frame-sentence contrastive loss enforces the matching between each\naction and its description. As the frame-sentence correspondence is not\navailable, we propose to use the fact that video actions happen sequentially in\nthe temporal domain to generate pseudo frame-sentence correspondence and\nsupervise the network training with the pseudo labels. Extensive experiments on\nvideo sequence verification and text-to-video matching show that our method\noutperforms baselines by a large margin, which validates the effectiveness of\nour proposed approach. Code is available at https://github.com/svip-lab/WeakSVR\n","authors":["Sixun Dong","Huazhang Hu","Dongze Lian","Weixin Luo","Yicheng Qian","Shenghua Gao"],"pdf_url":"https://arxiv.org/pdf/2303.12370v1.pdf","comment":"CVPR 2023. Code: https://github.com/svip-lab/WeakSVR"},{"id":"http://arxiv.org/abs/2303.12369v1","updated":"2023-03-22T08:11:22Z","published":"2023-03-22T08:11:22Z","title":"Unbiased Multiple Instance Learning for Weakly Supervised Video Anomaly\n  Detection","summary":"  Weakly Supervised Video Anomaly Detection (WSVAD) is challenging because the\nbinary anomaly label is only given on the video level, but the output requires\nsnippet-level predictions. So, Multiple Instance Learning (MIL) is prevailing\nin WSVAD. However, MIL is notoriously known to suffer from many false alarms\nbecause the snippet-level detector is easily biased towards the abnormal\nsnippets with simple context, confused by the normality with the same bias, and\nmissing the anomaly with a different pattern. To this end, we propose a new MIL\nframework: Unbiased MIL (UMIL), to learn unbiased anomaly features that improve\nWSVAD. At each MIL training iteration, we use the current detector to divide\nthe samples into two groups with different context biases: the most confident\nabnormal/normal snippets and the rest ambiguous ones. Then, by seeking the\ninvariant features across the two sample groups, we can remove the variant\ncontext biases. Extensive experiments on benchmarks UCF-Crime and TAD\ndemonstrate the effectiveness of our UMIL. Our code is provided at\nhttps://github.com/ktr-hubrt/UMIL.\n","authors":["Hui Lv","Zhongqi Yue","Qianru Sun","Bin Luo","Zhen Cui","Hanwang Zhang"],"pdf_url":"https://arxiv.org/pdf/2303.12369v1.pdf","comment":"11 pages,10 figures"},{"id":"http://arxiv.org/abs/2303.12368v1","updated":"2023-03-22T08:07:28Z","published":"2023-03-22T08:07:28Z","title":"MAIR: Multi-view Attention Inverse Rendering with 3D Spatially-Varying\n  Lighting Estimation","summary":"  We propose a scene-level inverse rendering framework that uses multi-view\nimages to decompose the scene into geometry, a SVBRDF, and 3D spatially-varying\nlighting. Because multi-view images provide a variety of information about the\nscene, multi-view images in object-level inverse rendering have been taken for\ngranted. However, owing to the absence of multi-view HDR synthetic dataset,\nscene-level inverse rendering has mainly been studied using single-view image.\nWe were able to successfully perform scene-level inverse rendering using\nmulti-view images by expanding OpenRooms dataset and designing efficient\npipelines to handle multi-view images, and splitting spatially-varying\nlighting. Our experiments show that the proposed method not only achieves\nbetter performance than single-view-based methods, but also achieves robust\nperformance on unseen real-world scene. Also, our sophisticated 3D\nspatially-varying lighting volume allows for photorealistic object insertion in\nany 3D location.\n","authors":["JunYong Choi","SeokYeong Lee","Haesol Park","Seung-Won Jung","Ig-Jae Kim","Junghyun Cho"],"pdf_url":"https://arxiv.org/pdf/2303.12368v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.12360v1","updated":"2023-03-22T07:51:32Z","published":"2023-03-22T07:51:32Z","title":"Automatically Predict Material Properties with Microscopic Image Example\n  Polymer Compatibility","summary":"  Many material properties are manifested in the morphological appearance and\ncharacterized with microscopic image, such as scanning electron microscopy\n(SEM). Polymer compatibility is a key physical quantity of polymer material and\ncommonly and intuitively judged by SEM images. However, human observation and\njudgement for the images is time-consuming, labor-intensive and hard to be\nquantified. Computer image recognition with machine learning method can make up\nthe defects of artificial judging, giving accurate and quantitative judgement.\nWe achieve automatic compatibility recognition utilizing convolution neural\nnetwork and transfer learning method, and the model obtains up to 94% accuracy.\nWe also put forward a quantitative criterion for polymer compatibility with\nthis model. The proposed method can be widely applied to the quantitative\ncharacterization of the microstructure and properties of various materials.\n","authors":["Zhilong Liang","Zhenzhi Tan","Ruixin Hong","Wanli Ouyang","Jinying Yuan","Changshui Zhang"],"pdf_url":"https://arxiv.org/pdf/2303.12360v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2201.10981v3","updated":"2023-03-22T07:35:10Z","published":"2022-01-26T14:52:23Z","title":"Joint Liver and Hepatic Lesion Segmentation in MRI using a Hybrid CNN\n  with Transformer Layers","summary":"  Deep learning-based segmentation of the liver and hepatic lesions therein\nsteadily gains relevance in clinical practice due to the increasing incidence\nof liver cancer each year. Whereas various network variants with overall\npromising results in the field of medical image segmentation have been\nsuccessfully developed over the last years, almost all of them struggle with\nthe challenge of accurately segmenting hepatic lesions in magnetic resonance\nimaging (MRI). This led to the idea of combining elements of convolutional and\ntransformer-based architectures to overcome the existing limitations. This work\npresents a hybrid network called SWTR-Unet, consisting of a pretrained ResNet,\ntransformer blocks as well as a common Unet-style decoder path. This network\nwas primarily applied to single-modality non-contrast-enhanced liver MRI and\nadditionally to the publicly available computed tomography (CT) data of the\nliver tumor segmentation (LiTS) challenge to verify the applicability on other\nmodalities. For a broader evaluation, multiple state-of-the-art networks were\nimplemented and applied, ensuring a direct comparability. Furthermore,\ncorrelation analysis and an ablation study were carried out, to investigate\nvarious influencing factors on the segmentation accuracy of the presented\nmethod. With Dice scores of averaged 98+-2% for liver and 81+-28% lesion\nsegmentation on the MRI dataset and 97+-2% and 79+-25%, respectively on the CT\ndataset, the proposed SWTR-Unet proved to be a precise approach for liver and\nhepatic lesion segmentation with state-of-the-art results for MRI and competing\naccuracy in CT imaging. The achieved segmentation accuracy was found to be on\npar with manually performed expert segmentations as indicated by inter-observer\nvariabilities for liver lesion segmentation. In conclusion, the presented\nmethod could save valuable time and resources in clinical practice.\n","authors":["Georg Hille","Shubham Agrawal","Pavan Tummala","Christian Wybranski","Maciej Pech","Alexey Surov","Sylvia Saalfeld"],"pdf_url":"https://arxiv.org/pdf/2201.10981v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2208.00690v3","updated":"2023-03-22T07:20:37Z","published":"2022-08-01T08:58:02Z","title":"Generative Bias for Robust Visual Question Answering","summary":"  The task of Visual Question Answering (VQA) is known to be plagued by the\nissue of VQA models exploiting biases within the dataset to make its final\nprediction. Various previous ensemble based debiasing methods have been\nproposed where an additional model is purposefully trained to be biased in\norder to train a robust target model. However, these methods compute the bias\nfor a model simply from the label statistics of the training data or from\nsingle modal branches. In this work, in order to better learn the bias a target\nVQA model suffers from, we propose a generative method to train the bias model\ndirectly from the target model, called GenB. In particular, GenB employs a\ngenerative network to learn the bias in the target model through a combination\nof the adversarial objective and knowledge distillation. We then debias our\ntarget model with GenB as a bias model, and show through extensive experiments\nthe effects of our method on various VQA bias datasets including VQA-CP2,\nVQA-CP1, GQA-OOD, and VQA-CE, and show state-of-the-art results with the LXMERT\narchitecture on VQA-CP2.\n","authors":["Jae Won Cho","Dong-jin Kim","Hyeonggon Ryu","In So Kweon"],"pdf_url":"https://arxiv.org/pdf/2208.00690v3.pdf","comment":"CVPR 2023"},{"id":"http://arxiv.org/abs/2211.12824v2","updated":"2023-03-22T07:20:06Z","published":"2022-11-23T10:14:12Z","title":"Tell Me What Happened: Unifying Text-guided Video Completion via\n  Multimodal Masked Video Generation","summary":"  Generating a video given the first several static frames is challenging as it\nanticipates reasonable future frames with temporal coherence. Besides video\nprediction, the ability to rewind from the last frame or infilling between the\nhead and tail is also crucial, but they have rarely been explored for video\ncompletion. Since there could be different outcomes from the hints of just a\nfew frames, a system that can follow natural language to perform video\ncompletion may significantly improve controllability. Inspired by this, we\nintroduce a novel task, text-guided video completion (TVC), which requests the\nmodel to generate a video from partial frames guided by an instruction. We then\npropose Multimodal Masked Video Generation (MMVG) to address this TVC task.\nDuring training, MMVG discretizes the video frames into visual tokens and masks\nmost of them to perform video completion from any time point. At inference\ntime, a single MMVG model can address all 3 cases of TVC, including video\nprediction, rewind, and infilling, by applying corresponding masking\nconditions. We evaluate MMVG in various video scenarios, including egocentric,\nanimation, and gaming. Extensive experimental results indicate that MMVG is\neffective in generating high-quality visual appearances with text guidance for\nTVC.\n","authors":["Tsu-Jui Fu","Licheng Yu","Ning Zhang","Cheng-Yang Fu","Jong-Chyi Su","William Yang Wang","Sean Bell"],"pdf_url":"https://arxiv.org/pdf/2211.12824v2.pdf","comment":"CVPR'23"},{"id":"http://arxiv.org/abs/2303.12346v1","updated":"2023-03-22T07:10:09Z","published":"2023-03-22T07:10:09Z","title":"NUWA-XL: Diffusion over Diffusion for eXtremely Long Video Generation","summary":"  In this paper, we propose NUWA-XL, a novel Diffusion over Diffusion\narchitecture for eXtremely Long video generation. Most current work generates\nlong videos segment by segment sequentially, which normally leads to the gap\nbetween training on short videos and inferring long videos, and the sequential\ngeneration is inefficient. Instead, our approach adopts a ``coarse-to-fine''\nprocess, in which the video can be generated in parallel at the same\ngranularity. A global diffusion model is applied to generate the keyframes\nacross the entire time range, and then local diffusion models recursively fill\nin the content between nearby frames. This simple yet effective strategy allows\nus to directly train on long videos (3376 frames) to reduce the\ntraining-inference gap, and makes it possible to generate all segments in\nparallel. To evaluate our model, we build FlintstonesHD dataset, a new\nbenchmark for long video generation. Experiments show that our model not only\ngenerates high-quality long videos with both global and local coherence, but\nalso decreases the average inference time from 7.55min to 26s (by 94.26\\%) at\nthe same hardware setting when generating 1024 frames. The homepage link is\n\\url{https://msra-nuwa.azurewebsites.net/}\n","authors":["Shengming Yin","Chenfei Wu","Huan Yang","Jianfeng Wang","Xiaodong Wang","Minheng Ni","Zhengyuan Yang","Linjie Li","Shuguang Liu","Fan Yang","Jianlong Fu","Gong Ming","Lijuan Wang","Zicheng Liu","Houqiang Li","Nan Duan"],"pdf_url":"https://arxiv.org/pdf/2303.12346v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2211.00288v3","updated":"2023-03-22T07:03:38Z","published":"2022-11-01T05:48:18Z","title":"Self-supervised Character-to-Character Distillation for Text Recognition","summary":"  When handling complicated text images (e.g., irregular structures, low\nresolution, heavy occlusion, and uneven illumination), existing supervised text\nrecognition methods are data-hungry. Although these methods employ large-scale\nsynthetic text images to reduce the dependence on annotated real images, the\ndomain gap still limits the recognition performance. Therefore, exploring the\nrobust text feature representations on unlabeled real images by self-supervised\nlearning is a good solution. However, existing self-supervised text recognition\nmethods conduct sequence-to-sequence representation learning by roughly\nsplitting the visual features along the horizontal axis, which limits the\nflexibility of the augmentations, as large geometric-based augmentations may\nlead to sequence-to-sequence feature inconsistency. Motivated by this, we\npropose a novel self-supervised Character-to-Character Distillation method,\nCCD, which enables versatile augmentations to facilitate general text\nrepresentation learning. Specifically, we delineate the character structures of\nunlabeled real images by designing a self-supervised character segmentation\nmodule. Following this, CCD easily enriches the diversity of local characters\nwhile keeping their pairwise alignment under flexible augmentations, using the\ntransformation matrix between two augmented views from images. Experiments\ndemonstrate that CCD achieves state-of-the-art results, with average\nperformance gains of 1.38% in text recognition, 1.7% in text segmentation, 0.24\ndB (PSNR) and 0.0321 (SSIM) in text super-resolution. Code will be released\nsoon.\n","authors":["Tongkun Guan","Wei Shen","Xue Yang","Qi Feng","Zekun Jiang"],"pdf_url":"https://arxiv.org/pdf/2211.00288v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2211.06597v3","updated":"2023-03-22T07:02:46Z","published":"2022-11-12T07:24:29Z","title":"OpenGait: Revisiting Gait Recognition Toward Better Practicality","summary":"  Gait recognition is one of the most critical long-distance identification\ntechnologies and increasingly gains popularity in both research and industry\ncommunities. Despite the significant progress made in indoor datasets, much\nevidence shows that gait recognition techniques perform poorly in the wild.\nMore importantly, we also find that some conclusions drawn from indoor datasets\ncannot be generalized to real applications. Therefore, the primary goal of this\npaper is to present a comprehensive benchmark study for better practicality\nrather than only a particular model for better performance. To this end, we\nfirst develop a flexible and efficient gait recognition codebase named\nOpenGait. Based on OpenGait, we deeply revisit the recent development of gait\nrecognition by re-conducting the ablative experiments. Encouragingly,we detect\nsome unperfect parts of certain prior woks, as well as new insights. Inspired\nby these discoveries, we develop a structurally simple, empirically powerful,\nand practically robust baseline model, GaitBase. Experimentally, we\ncomprehensively compare GaitBase with many current gait recognition methods on\nmultiple public datasets, and the results reflect that GaitBase achieves\nsignificantly strong performance in most cases regardless of indoor or outdoor\nsituations. Code is available at https://github.com/ShiqiYu/OpenGait.\n","authors":["Chao Fan","Junhao Liang","Chuanfu Shen","Saihui Hou","Yongzhen Huang","Shiqi Yu"],"pdf_url":"https://arxiv.org/pdf/2211.06597v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.12343v1","updated":"2023-03-22T06:55:01Z","published":"2023-03-22T06:55:01Z","title":"LD-ZNet: A Latent Diffusion Approach for Text-Based Image Segmentation","summary":"  We present a technique for segmenting real and AI-generated images using\nlatent diffusion models (LDMs) trained on internet-scale datasets. First, we\nshow that the latent space of LDMs (z-space) is a better input representation\ncompared to other feature representations like RGB images or CLIP encodings for\ntext-based image segmentation. By training the segmentation models on the\nlatent z-space, which creates a compressed representation across several\ndomains like different forms of art, cartoons, illustrations, and photographs,\nwe are also able to bridge the domain gap between real and AI-generated images.\nWe show that the internal features of LDMs contain rich semantic information\nand present a technique in the form of LD-ZNet to further boost the performance\nof text-based segmentation. Overall, we show up to 6% improvement over standard\nbaselines for text-to-image segmentation on natural images. For AI-generated\nimagery, we show close to 20% improvement compared to state-of-the-art\ntechniques.\n","authors":["Koutilya Pnvr","Bharat Singh","Pallabi Ghosh","Behjat Siddiquie","David Jacobs"],"pdf_url":"https://arxiv.org/pdf/2303.12343v1.pdf","comment":"Supplementary material is included in the paper following the\n  references section"},{"id":"http://arxiv.org/abs/2301.09120v2","updated":"2023-03-22T06:41:10Z","published":"2023-01-22T13:07:24Z","title":"Causality-based Dual-Contrastive Learning Framework for Domain\n  Generalization","summary":"  Domain Generalization (DG) is essentially a sub-branch of out-of-distribution\ngeneralization, which trains models from multiple source domains and\ngeneralizes to unseen target domains. Recently, some domain generalization\nalgorithms have emerged, but most of them were designed with non-transferable\ncomplex architecture. Additionally, contrastive learning has become a promising\nsolution for simplicity and efficiency in DG. However, existing contrastive\nlearning neglected domain shifts that caused severe model confusions. In this\npaper, we propose a Dual-Contrastive Learning (DCL) module on feature and\nprototype contrast. Moreover, we design a novel Causal Fusion Attention (CFA)\nmodule to fuse diverse views of a single image to attain prototype.\nFurthermore, we introduce a Similarity-based Hard-pair Mining (SHM) strategy to\nleverage information on diversity shift. Extensive experiments show that our\nmethod outperforms state-of-the-art algorithms on three DG datasets. The\nproposed algorithm can also serve as a plug-and-play module without usage of\ndomain labels.\n","authors":["Zining Chen","Weiqiu Wang","Zhicheng Zhao","Aidong Men"],"pdf_url":"https://arxiv.org/pdf/2301.09120v2.pdf","comment":"Inadequate proof of the effectiveness of the method"},{"id":"http://arxiv.org/abs/2303.12342v1","updated":"2023-03-22T06:41:09Z","published":"2023-03-22T06:41:09Z","title":"One-Step Detection Paradigm for Hyperspectral Anomaly Detection via\n  Spectral Deviation Relationship Learning","summary":"  Hyperspectral anomaly detection (HAD) involves identifying the targets that\ndeviate spectrally from their surroundings, without prior knowledge. Recently,\ndeep learning based methods have become the mainstream HAD methods, due to\ntheir powerful spatial-spectral feature extraction ability. However, the\ncurrent deep detection models are optimized to complete a proxy task (two-step\nparadigm), such as background reconstruction or generation, rather than\nachieving anomaly detection directly. This leads to suboptimal results and poor\ntransferability, which means that the deep model is trained and tested on the\nsame image. In this paper, an unsupervised transferred direct detection (TDD)\nmodel is proposed, which is optimized directly for the anomaly detection task\n(one-step paradigm) and has transferability. Specially, the TDD model is\noptimized to identify the spectral deviation relationship according to the\nanomaly definition. Compared to learning the specific background distribution\nas most models do, the spectral deviation relationship is universal for\ndifferent images and guarantees the model transferability. To train the TDD\nmodel in an unsupervised manner, an anomaly sample simulation strategy is\nproposed to generate numerous pairs of anomaly samples. Furthermore, a global\nself-attention module and a local self-attention module are designed to help\nthe model focus on the \"spectrally deviating\" relationship. The TDD model was\nvalidated on four public HAD datasets. The results show that the proposed TDD\nmodel can successfully overcome the limitation of traditional model training\nand testing on a single image, and the model has a powerful detection ability\nand excellent transferability.\n","authors":["Jingtao Li","Xinyu Wang","Shaoyu Wang","Hengwei Zhao","Liangpei Zhang","Yanfei Zhong"],"pdf_url":"https://arxiv.org/pdf/2303.12342v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2210.06551v2","updated":"2023-03-22T06:34:14Z","published":"2022-10-12T19:46:25Z","title":"Learning Human Motion Representations: A Unified Perspective","summary":"  We present a unified perspective on tackling various human-centric video\ntasks by learning human motion representations from large-scale and\nheterogeneous data resources. Specifically, we propose a pretraining stage in\nwhich a motion encoder is trained to recover the underlying 3D motion from\nnoisy partial 2D observations. The motion representations acquired in this way\nincorporate geometric, kinematic, and physical knowledge about human motion,\nwhich can be easily transferred to multiple downstream tasks. We implement the\nmotion encoder with a Dual-stream Spatio-temporal Transformer (DSTformer)\nneural network. It could capture long-range spatio-temporal relationships among\nthe skeletal joints comprehensively and adaptively, exemplified by the lowest\n3D pose estimation error so far when trained from scratch. Furthermore, our\nproposed framework achieves state-of-the-art performance on all three\ndownstream tasks by simply finetuning the pretrained motion encoder with a\nsimple regression head (1-2 layers), which demonstrates the versatility of the\nlearned motion representations.\n","authors":["Wentao Zhu","Xiaoxuan Ma","Zhaoyang Liu","Libin Liu","Wayne Wu","Yizhou Wang"],"pdf_url":"https://arxiv.org/pdf/2210.06551v2.pdf","comment":"Project page: https://motionbert.github.io/"},{"id":"http://arxiv.org/abs/2303.12337v1","updated":"2023-03-22T06:26:56Z","published":"2023-03-22T06:26:56Z","title":"Music-Driven Group Choreography","summary":"  Music-driven choreography is a challenging problem with a wide variety of\nindustrial applications. Recently, many methods have been proposed to\nsynthesize dance motions from music for a single dancer. However, generating\ndance motion for a group remains an open problem. In this paper, we present\n$\\rm AIOZ-GDANCE$, a new large-scale dataset for music-driven group dance\ngeneration. Unlike existing datasets that only support single dance, our new\ndataset contains group dance videos, hence supporting the study of group\nchoreography. We propose a semi-autonomous labeling method with humans in the\nloop to obtain the 3D ground truth for our dataset. The proposed dataset\nconsists of $16.7$ hours of paired music and 3D motion from in-the-wild videos,\ncovering $7$ dance styles and $16$ music genres. We show that naively applying\nsingle dance generation technique to creating group dance motion may lead to\nunsatisfactory results, such as inconsistent movements and collisions between\ndancers. Based on our new dataset, we propose a new method that takes an input\nmusic sequence and a set of 3D positions of dancers to efficiently produce\nmultiple group-coherent choreographies. We propose new evaluation metrics for\nmeasuring group dance quality and perform intensive experiments to demonstrate\nthe effectiveness of our method.\n","authors":["Nhat Le","Thang Pham","Tuong Do","Erman Tjiputra","Quang D. Tran","Anh Nguyen"],"pdf_url":"https://arxiv.org/pdf/2303.12337v1.pdf","comment":"accepted in cvpr 2023"},{"id":"http://arxiv.org/abs/2303.12332v1","updated":"2023-03-22T06:08:34Z","published":"2023-03-22T06:08:34Z","title":"Weakly-Supervised Temporal Action Localization by Inferring\n  Snippet-Feature Affinity","summary":"  Weakly-supervised temporal action localization aims to locate action regions\nand identify action categories in untrimmed videos, only taking video-level\nlabels as the supervised information. Pseudo label generation is a promising\nstrategy to solve the challenging problem, but most existing methods are\nlimited to employing snippet-wise classification results to guide the\ngeneration, and they ignore that the natural temporal structure of the video\ncan also provide rich information to assist such a generation process. In this\npaper, we propose a novel weakly-supervised temporal action localization method\nby inferring snippet-feature affinity. First, we design an affinity inference\nmodule that exploits the affinity relationship between temporal neighbor\nsnippets to generate initial coarse pseudo labels. Then, we introduce an\ninformation interaction module that refines the coarse labels by enhancing the\ndiscriminative nature of snippet-features through exploring intra- and\ninter-video relationships. Finally, the high-fidelity pseudo labels generated\nfrom the information interaction module are used to supervise the training of\nthe action localization network. Extensive experiments on two publicly\navailable datasets, i.e., THUMOS14 and ActivityNet v1.3, demonstrate our\nproposed method achieves significant improvements compared to the\nstate-of-the-art methods.\n","authors":["Wulian Yun","Mengshi Qi","Chuanming Wang","Huadong Ma"],"pdf_url":"https://arxiv.org/pdf/2303.12332v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.10752v2","updated":"2023-03-22T06:02:06Z","published":"2023-03-19T19:59:48Z","title":"Fully Self-Supervised Depth Estimation from Defocus Clue","summary":"  Depth-from-defocus (DFD), modeling the relationship between depth and defocus\npattern in images, has demonstrated promising performance in depth estimation.\nRecently, several self-supervised works try to overcome the difficulties in\nacquiring accurate depth ground-truth. However, they depend on the all-in-focus\n(AIF) images, which cannot be captured in real-world scenarios. Such limitation\ndiscourages the applications of DFD methods. To tackle this issue, we propose a\ncompletely self-supervised framework that estimates depth purely from a sparse\nfocal stack. We show that our framework circumvents the needs for the depth and\nAIF image ground-truth, and receives superior predictions, thus closing the gap\nbetween the theoretical success of DFD works and their applications in the real\nworld. In particular, we propose (i) a more realistic setting for DFD tasks,\nwhere no depth or AIF image ground-truth is available; (ii) a novel\nself-supervision framework that provides reliable predictions of depth and AIF\nimage under the challenging setting. The proposed framework uses a neural model\nto predict the depth and AIF image, and utilizes an optical model to validate\nand refine the prediction. We verify our framework on three benchmark datasets\nwith rendered focal stacks and real focal stacks. Qualitative and quantitative\nevaluations show that our method provides a strong baseline for self-supervised\nDFD tasks.\n","authors":["Haozhe Si","Bin Zhao","Dong Wang","Yupeng Gao","Mulin Chen","Zhigang Wang","Xuelong Li"],"pdf_url":"https://arxiv.org/pdf/2303.10752v2.pdf","comment":"CVPR 2023 camera-ready version. The code is released at\n  https://github.com/Ehzoahis/DEReD"},{"id":"http://arxiv.org/abs/2206.15083v2","updated":"2023-03-22T05:52:23Z","published":"2022-06-30T07:32:23Z","title":"UniDAformer: Unified Domain Adaptive Panoptic Segmentation Transformer\n  via Hierarchical Mask Calibration","summary":"  Domain adaptive panoptic segmentation aims to mitigate data annotation\nchallenge by leveraging off-the-shelf annotated data in one or multiple related\nsource domains. However, existing studies employ two separate networks for\ninstance segmentation and semantic segmentation which lead to excessive network\nparameters as well as complicated and computationally intensive training and\ninference processes. We design UniDAformer, a unified domain adaptive panoptic\nsegmentation transformer that is simple but can achieve domain adaptive\ninstance segmentation and semantic segmentation simultaneously within a single\nnetwork. UniDAformer introduces Hierarchical Mask Calibration (HMC) that\nrectifies inaccurate predictions at the level of regions, superpixels and\npixels via online self-training on the fly. It has three unique features: 1) it\nenables unified domain adaptive panoptic adaptation; 2) it mitigates false\npredictions and improves domain adaptive panoptic segmentation effectively; 3)\nit is end-to-end trainable with a much simpler training and inference pipeline.\nExtensive experiments over multiple public benchmarks show that UniDAformer\nachieves superior domain adaptive panoptic segmentation as compared with the\nstate-of-the-art.\n","authors":["Jingyi Zhang","Jiaxing Huang","Xiaoqin Zhang","Shijian Lu"],"pdf_url":"https://arxiv.org/pdf/2206.15083v2.pdf","comment":"Accepted to CVPR2023"},{"id":"http://arxiv.org/abs/2303.12326v1","updated":"2023-03-22T05:51:53Z","published":"2023-03-22T05:51:53Z","title":"Make Encoder Great Again in 3D GAN Inversion through Geometry and\n  Occlusion-Aware Encoding","summary":"  3D GAN inversion aims to achieve high reconstruction fidelity and reasonable\n3D geometry simultaneously from a single image input. However, existing 3D GAN\ninversion methods rely on time-consuming optimization for each individual case.\nIn this work, we introduce a novel encoder-based inversion framework based on\nEG3D, one of the most widely-used 3D GAN models. We leverage the inherent\nproperties of EG3D's latent space to design a discriminator and a background\ndepth regularization. This enables us to train a geometry-aware encoder capable\nof converting the input image into corresponding latent code. Additionally, we\nexplore the feature space of EG3D and develop an adaptive refinement stage that\nimproves the representation ability of features in EG3D to enhance the recovery\nof fine-grained textural details. Finally, we propose an occlusion-aware fusion\noperation to prevent distortion in unobserved regions. Our method achieves\nimpressive results comparable to optimization-based methods while operating up\nto 500 times faster. Our framework is well-suited for applications such as\nsemantic editing.\n","authors":["Ziyang Yuan","Yiming Zhu","Yu Li","Hongyu Liu","Chun Yuan"],"pdf_url":"https://arxiv.org/pdf/2303.12326v1.pdf","comment":"Project page: https://eg3d-goae.github.io/"},{"id":"http://arxiv.org/abs/2303.12317v1","updated":"2023-03-22T05:21:21Z","published":"2023-03-22T05:21:21Z","title":"Re-thinking Federated Active Learning based on Inter-class Diversity","summary":"  Although federated learning has made awe-inspiring advances, most studies\nhave assumed that the client's data are fully labeled. However, in a real-world\nscenario, every client may have a significant amount of unlabeled instances.\nAmong the various approaches to utilizing unlabeled data, a federated active\nlearning framework has emerged as a promising solution. In the decentralized\nsetting, there are two types of available query selector models, namely\n'global' and 'local-only' models, but little literature discusses their\nperformance dominance and its causes. In this work, we first demonstrate that\nthe superiority of two selector models depends on the global and local\ninter-class diversity. Furthermore, we observe that the global and local-only\nmodels are the keys to resolving the imbalance of each side. Based on our\nfindings, we propose LoGo, a FAL sampling strategy robust to varying local\nheterogeneity levels and global imbalance ratio, that integrates both models by\ntwo steps of active selection scheme. LoGo consistently outperforms six active\nlearning strategies in the total number of 38 experimental settings.\n","authors":["SangMook Kim","Sangmin Bae","Hwanjun Song","Se-Young Yun"],"pdf_url":"https://arxiv.org/pdf/2303.12317v1.pdf","comment":"CVPR 2023"},{"id":"http://arxiv.org/abs/2303.10598v2","updated":"2023-03-22T05:21:14Z","published":"2023-03-19T08:26:06Z","title":"StyleRF: Zero-shot 3D Style Transfer of Neural Radiance Fields","summary":"  3D style transfer aims to render stylized novel views of a 3D scene with\nmulti-view consistency. However, most existing work suffers from a three-way\ndilemma over accurate geometry reconstruction, high-quality stylization, and\nbeing generalizable to arbitrary new styles. We propose StyleRF (Style Radiance\nFields), an innovative 3D style transfer technique that resolves the three-way\ndilemma by performing style transformation within the feature space of a\nradiance field. StyleRF employs an explicit grid of high-level features to\nrepresent 3D scenes, with which high-fidelity geometry can be reliably restored\nvia volume rendering. In addition, it transforms the grid features according to\nthe reference style which directly leads to high-quality zero-shot style\ntransfer. StyleRF consists of two innovative designs. The first is\nsampling-invariant content transformation that makes the transformation\ninvariant to the holistic statistics of the sampled 3D points and accordingly\nensures multi-view consistency. The second is deferred style transformation of\n2D feature maps which is equivalent to the transformation of 3D points but\ngreatly reduces memory footprint without degrading multi-view consistency.\nExtensive experiments show that StyleRF achieves superior 3D stylization\nquality with precise geometry reconstruction and it can generalize to various\nnew styles in a zero-shot manner.\n","authors":["Kunhao Liu","Fangneng Zhan","Yiwen Chen","Jiahui Zhang","Yingchen Yu","Abdulmotaleb El Saddik","Shijian Lu","Eric Xing"],"pdf_url":"https://arxiv.org/pdf/2303.10598v2.pdf","comment":"Accepted to CVPR 2023. Project website:\n  https://kunhao-liu.github.io/StyleRF/"},{"id":"http://arxiv.org/abs/2211.16312v2","updated":"2023-03-22T05:17:01Z","published":"2022-11-29T15:52:22Z","title":"PLA: Language-Driven Open-Vocabulary 3D Scene Understanding","summary":"  Open-vocabulary scene understanding aims to localize and recognize unseen\ncategories beyond the annotated label space. The recent breakthrough of 2D\nopen-vocabulary perception is largely driven by Internet-scale paired\nimage-text data with rich vocabulary concepts. However, this success cannot be\ndirectly transferred to 3D scenarios due to the inaccessibility of large-scale\n3D-text pairs. To this end, we propose to distill knowledge encoded in\npre-trained vision-language (VL) foundation models through captioning\nmulti-view images from 3D, which allows explicitly associating 3D and\nsemantic-rich captions. Further, to foster coarse-to-fine visual-semantic\nrepresentation learning from captions, we design hierarchical 3D-caption pairs,\nleveraging geometric constraints between 3D scenes and multi-view images.\nFinally, by employing contrastive learning, the model learns language-aware\nembeddings that connect 3D and text for open-vocabulary tasks. Our method not\nonly remarkably outperforms baseline methods by 25.8% $\\sim$ 44.7% hIoU and\n14.5% $\\sim$ 50.4% hAP$_{50}$ in open-vocabulary semantic and instance\nsegmentation, but also shows robust transferability on challenging zero-shot\ndomain transfer tasks. See the project website at\nhttps://dingry.github.io/projects/PLA.\n","authors":["Runyu Ding","Jihan Yang","Chuhui Xue","Wenqing Zhang","Song Bai","Xiaojuan Qi"],"pdf_url":"https://arxiv.org/pdf/2211.16312v2.pdf","comment":"CVPR2023"},{"id":"http://arxiv.org/abs/2103.17084v2","updated":"2023-03-22T05:15:36Z","published":"2021-03-31T13:55:56Z","title":"DA-DETR: Domain Adaptive Detection Transformer with Information Fusion","summary":"  The recent detection transformer (DETR) simplifies the object detection\npipeline by removing hand-crafted designs and hyperparameters as employed in\nconventional two-stage object detectors. However, how to leverage the simple\nyet effective DETR architecture in domain adaptive object detection is largely\nneglected. Inspired by the unique DETR attention mechanisms, we design DA-DETR,\na domain adaptive object detection transformer that introduces information\nfusion for effective transfer from a labeled source domain to an unlabeled\ntarget domain. DA-DETR introduces a novel CNN-Transformer Blender (CTBlender)\nthat fuses the CNN features and Transformer features ingeniously for effective\nfeature alignment and knowledge transfer across domains. Specifically,\nCTBlender employs the Transformer features to modulate the CNN features across\nmultiple scales where the high-level semantic information and the low-level\nspatial information are fused for accurate object identification and\nlocalization. Extensive experiments show that DA-DETR achieves superior\ndetection performance consistently across multiple widely adopted domain\nadaptation benchmarks.\n","authors":["Jingyi Zhang","Jiaxing Huang","Zhipeng Luo","Gongjie Zhang","Xiaoqin Zhang","Shijian Lu"],"pdf_url":"https://arxiv.org/pdf/2103.17084v2.pdf","comment":"Accepted to CVPR2023"},{"id":"http://arxiv.org/abs/2303.12313v1","updated":"2023-03-22T05:03:14Z","published":"2023-03-22T05:03:14Z","title":"Distribution Aligned Diffusion and Prototype-guided network for\n  Unsupervised Domain Adaptive Segmentation","summary":"  The Diffusion Probabilistic Model (DPM) has emerged as a highly effective\ngenerative model in the field of computer vision. Its intermediate latent\nvectors offer rich semantic information, making it an attractive option for\nvarious downstream tasks such as segmentation and detection. In order to\nexplore its potential further, we have taken a step forward and considered a\nmore complex scenario in the medical image domain, specifically, under an\nunsupervised adaptation condition. To this end, we propose a Diffusion-based\nand Prototype-guided network (DP-Net) for unsupervised domain adaptive\nsegmentation. Concretely, our DP-Net consists of two stages: 1) Distribution\nAligned Diffusion (DADiff), which involves training a domain discriminator to\nminimize the difference between the intermediate features generated by the DPM,\nthereby aligning the inter-domain distribution; and 2) Prototype-guided\nConsistency Learning (PCL), which utilizes feature centroids as prototypes and\napplies a prototype-guided loss to ensure that the segmentor learns consistent\ncontent from both source and target domains. Our approach is evaluated on\nfundus datasets through a series of experiments, which demonstrate that the\nperformance of the proposed method is reliable and outperforms state-of-the-art\nmethods. Our work presents a promising direction for using DPM in complex\nmedical image scenarios, opening up new possibilities for further research in\nmedical imaging.\n","authors":["Haipeng Zhou","Lei Zhu","Yuyin Zhou"],"pdf_url":"https://arxiv.org/pdf/2303.12313v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.12307v1","updated":"2023-03-22T04:49:23Z","published":"2023-03-22T04:49:23Z","title":"Curvature-Balanced Feature Manifold Learning for Long-Tailed\n  Classification","summary":"  To address the challenges of long-tailed classification, researchers have\nproposed several approaches to reduce model bias, most of which assume that\nclasses with few samples are weak classes. However, recent studies have shown\nthat tail classes are not always hard to learn, and model bias has been\nobserved on sample-balanced datasets, suggesting the existence of other factors\nthat affect model bias. In this work, we systematically propose a series of\ngeometric measurements for perceptual manifolds in deep neural networks, and\nthen explore the effect of the geometric characteristics of perceptual\nmanifolds on classification difficulty and how learning shapes the geometric\ncharacteristics of perceptual manifolds. An unanticipated finding is that the\ncorrelation between the class accuracy and the separation degree of perceptual\nmanifolds gradually decreases during training, while the negative correlation\nwith the curvature gradually increases, implying that curvature imbalance leads\nto model bias. Therefore, we propose curvature regularization to facilitate the\nmodel to learn curvature-balanced and flatter perceptual manifolds. Evaluations\non multiple long-tailed and non-long-tailed datasets show the excellent\nperformance and exciting generality of our approach, especially in achieving\nsignificant performance improvements based on current state-of-the-art\ntechniques. Our work opens up a geometric analysis perspective on model bias\nand reminds researchers to pay attention to model bias on non-long-tailed and\neven sample-balanced datasets. The code and model will be made public.\n","authors":["Yanbiao Ma","Licheng Jiao","Fang Liu","Shuyuan Yang","Xu Liu","Lingling Li"],"pdf_url":"https://arxiv.org/pdf/2303.12307v1.pdf","comment":"20pages, Accepted by CVPR 2023"},{"id":"http://arxiv.org/abs/2303.11616v2","updated":"2023-03-22T04:36:11Z","published":"2023-03-21T06:26:18Z","title":"HRDFuse: Monocular 360°Depth Estimation by Collaboratively Learning\n  Holistic-with-Regional Depth Distributions","summary":"  Depth estimation from a monocular 360{\\deg} image is a burgeoning problem\nowing to its holistic sensing of a scene. Recently, some methods, \\eg,\nOmniFusion, have applied the tangent projection (TP) to represent a\n360{\\deg}image and predicted depth values via patch-wise regressions, which are\nmerged to get a depth map with equirectangular projection (ERP) format.\nHowever, these methods suffer from 1) non-trivial process of merging plenty of\npatches; 2) capturing less holistic-with-regional contextual information by\ndirectly regressing the depth value of each pixel. In this paper, we propose a\nnovel framework, \\textbf{HRDFuse}, that subtly combines the potential of\nconvolutional neural networks (CNNs) and transformers by collaboratively\nlearning the \\textit{holistic} contextual information from the ERP and the\n\\textit{regional} structural information from the TP. Firstly, we propose a\nspatial feature alignment (\\textbf{SFA}) module that learns feature\nsimilarities between the TP and ERP to aggregate the TP features into a\ncomplete ERP feature map in a pixel-wise manner. Secondly, we propose a\ncollaborative depth distribution classification (\\textbf{CDDC}) module that\nlearns the \\textbf{holistic-with-regional} histograms capturing the ERP and TP\ndepth distributions. As such, the final depth values can be predicted as a\nlinear combination of histogram bin centers. Lastly, we adaptively combine the\ndepth predictions from ERP and TP to obtain the final depth map. Extensive\nexperiments show that our method predicts\\textbf{ more smooth and accurate\ndepth} results while achieving \\textbf{favorably better} results than the SOTA\nmethods.\n","authors":["Hao Ai","Zidong cao","Yan-pei Cao","Ying Shan","Lin Wang"],"pdf_url":"https://arxiv.org/pdf/2303.11616v2.pdf","comment":"To appear at CVPR2023, 20 pages"},{"id":"http://arxiv.org/abs/2303.12304v1","updated":"2023-03-22T04:33:02Z","published":"2023-03-22T04:33:02Z","title":"SiamTHN: Siamese Target Highlight Network for Visual Tracking","summary":"  Siamese network based trackers develop rapidly in the field of visual object\ntracking in recent years. The majority of siamese network based trackers now in\nuse treat each channel in the feature maps generated by the backbone network\nequally, making the similarity response map sensitive to background influence\nand hence challenging to focus on the target region. Additionally, there are no\nstructural links between the classification and regression branches in these\ntrackers, and the two branches are optimized separately during training.\nTherefore, there is a misalignment between the classification and regression\nbranches, which results in less accurate tracking results. In this paper, a\nTarget Highlight Module is proposed to help the generated similarity response\nmaps to be more focused on the target region. To reduce the misalignment and\nproduce more precise tracking results, we propose a corrective loss to train\nthe model. The two branches of the model are jointly tuned with the use of\ncorrective loss to produce more reliable prediction results. Experiments on 5\nchallenging benchmark datasets reveal that the method outperforms current\nmodels in terms of performance, and runs at 38 fps, proving its effectiveness\nand efficiency.\n","authors":["Jiahao Bao","Kaiqiang Chen","Xian Sun","Liangjin Zhao","Wenhui Diao","Menglong Yan"],"pdf_url":"https://arxiv.org/pdf/2303.12304v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2301.00954v3","updated":"2023-03-22T04:27:59Z","published":"2023-01-03T05:30:56Z","title":"PanopticPartFormer++: A Unified and Decoupled View for Panoptic Part\n  Segmentation","summary":"  Panoptic Part Segmentation (PPS) unifies panoptic and part segmentation into\none task. Previous works utilize separate approaches to handle things, stuff,\nand part predictions without shared computation and task association. We aim to\nunify these tasks at the architectural level, designing the first end-to-end\nunified framework, Panoptic-PartFormer. Moreover, we find the previous metric\nPartPQ biases to PQ. To handle both issues, we first design a meta-architecture\nthat decouples part features and things/stuff features, respectively. We model\nthings, stuff, and parts as object queries and directly learn to optimize all\nthree forms of prediction as a unified mask prediction and classification\nproblem. We term our model as Panoptic-PartFormer. Second, we propose a new\nmetric Part-Whole Quality (PWQ), better to measure this task from pixel-region\nand part-whole perspectives. It also decouples the errors for part segmentation\nand panoptic segmentation. Third, inspired by Mask2Former, based on our\nmeta-architecture, we propose Panoptic-PartFormer++ and design a new part-whole\ncross-attention scheme to boost part segmentation qualities further. We design\na new part-whole interaction method using masked cross attention. Finally,\nextensive ablation studies and analysis demonstrate the effectiveness of both\nPanoptic-PartFormer and Panoptic-PartFormer++. Compared with previous\nPanoptic-PartFormer, our Panoptic-PartFormer++ achieves 2% PartPQ and 3% PWQ\nimprovements on the Cityscapes PPS dataset and 5% PartPQ on the Pascal Context\nPPS dataset. On both datasets, Panoptic-PartFormer++ achieves new\nstate-of-the-art results. Our models can serve as a strong baseline and aid\nfuture research in PPS. The source code and trained models will be available\nat~\\url{https://github.com/lxtGH/Panoptic-PartFormer}.\n","authors":["Xiangtai Li","Shilin Xu","Yibo Yang","Haobo Yuan","Guangliang Cheng","Yunhai Tong","Zhouchen Lin","Ming-Hsuan Yang","Dacheng Tao"],"pdf_url":"https://arxiv.org/pdf/2301.00954v3.pdf","comment":"Extension of PanopticPartFormer (ECCV 2022). Code:\n  https://github.com/lxtGH/Panoptic-PartFormer. Update Results"},{"id":"http://arxiv.org/abs/2303.11305v2","updated":"2023-03-22T04:23:56Z","published":"2023-03-20T17:45:02Z","title":"SVDiff: Compact Parameter Space for Diffusion Fine-Tuning","summary":"  Diffusion models have achieved remarkable success in text-to-image\ngeneration, enabling the creation of high-quality images from text prompts or\nother modalities. However, existing methods for customizing these models are\nlimited by handling multiple personalized subjects and the risk of overfitting.\nMoreover, their large number of parameters is inefficient for model storage. In\nthis paper, we propose a novel approach to address these limitations in\nexisting text-to-image diffusion models for personalization. Our method\ninvolves fine-tuning the singular values of the weight matrices, leading to a\ncompact and efficient parameter space that reduces the risk of overfitting and\nlanguage-drifting. We also propose a Cut-Mix-Unmix data-augmentation technique\nto enhance the quality of multi-subject image generation and a simple\ntext-based image editing framework. Our proposed SVDiff method has a\nsignificantly smaller model size (1.7MB for StableDiffusion) compared to\nexisting methods (vanilla DreamBooth 3.66GB, Custom Diffusion 73MB), making it\nmore practical for real-world applications.\n","authors":["Ligong Han","Yinxiao Li","Han Zhang","Peyman Milanfar","Dimitris Metaxas","Feng Yang"],"pdf_url":"https://arxiv.org/pdf/2303.11305v2.pdf","comment":"20 pages, 21 figures"},{"id":"http://arxiv.org/abs/2212.02809v3","updated":"2023-03-22T04:08:48Z","published":"2022-12-06T07:58:21Z","title":"An advanced YOLOv3 method for small object detection","summary":"  Small object detection has important application value in the fields of\nautonomous driving and drone scene analysis. As one of the most advanced object\ndetection algorithms, YOLOv3 suffers some challenges when detecting small\nobjects, such as the problem of detection failure of small objects and occluded\nobjects. To solve these problems, an improved YOLOv3 algorithm for small object\ndetection is proposed. In the proposed method, the dilated convolutions mish\n(DCM) module is introduced into the backbone network of YOLOv3 to improve the\nfeature expression ability by fusing the feature maps of different receptive\nfields. In the neck network of YOLOv3, the convolutional block attention module\n(CBAM) and multi-level fusion module are introduced to select the important\ninformation for small object detection in the shallow network, suppress the\nuncritical information, and use the fusion module to fuse the feature maps of\ndifferent scales, so as to improve the detection accuracy of the algorithm. In\naddition, the Soft-NMS and Complete-IoU (CloU) strategies are applied to\ncandidate frame screening, which improves the accuracy of the algorithm for the\ndetection of occluded objects. The ablation experiment of the MS COCO2017\nobject detection task proves the effectiveness of several modules introduced in\nthis paper for small object detection. The experimental results on the MS\nCOCO2017, VOC2007, and VOC2012 datasets show that the Average Precision (AP) of\nthis method is 16.5%, 8.71%, and 9.68% higher than that of YOLOv3,\nrespectively.\n","authors":["Baokai Liu","Fengjie He","Shiqiang Du","Jiacheng Li","Wenjie Liu"],"pdf_url":"https://arxiv.org/pdf/2212.02809v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2206.05149v3","updated":"2023-03-22T03:47:41Z","published":"2022-06-10T14:44:43Z","title":"Referring Image Matting","summary":"  Different from conventional image matting, which either requires user-defined\nscribbles/trimap to extract a specific foreground object or directly extracts\nall the foreground objects in the image indiscriminately, we introduce a new\ntask named Referring Image Matting (RIM) in this paper, which aims to extract\nthe meticulous alpha matte of the specific object that best matches the given\nnatural language description, thus enabling a more natural and simpler\ninstruction for image matting. First, we establish a large-scale challenging\ndataset RefMatte by designing a comprehensive image composition and expression\ngeneration engine to automatically produce high-quality images along with\ndiverse text attributes based on public datasets. RefMatte consists of 230\nobject categories, 47,500 images, 118,749 expression-region entities, and\n474,996 expressions. Additionally, we construct a real-world test set with 100\nhigh-resolution natural images and manually annotate complex phrases to\nevaluate the out-of-domain generalization abilities of RIM methods.\nFurthermore, we present a novel baseline method CLIPMat for RIM, including a\ncontext-embedded prompt, a text-driven semantic pop-up, and a multi-level\ndetails extractor. Extensive experiments on RefMatte in both keyword and\nexpression settings validate the superiority of CLIPMat over representative\nmethods. We hope this work could provide novel insights into image matting and\nencourage more follow-up studies. The dataset, code and models are available at\nhttps://github.com/JizhiziLi/RIM.\n","authors":["Jizhizi Li","Jing Zhang","Dacheng Tao"],"pdf_url":"https://arxiv.org/pdf/2206.05149v3.pdf","comment":"Accepted to CVPR2023. The dataset, code and models are available at\n  https://github.com/JizhiziLi/RIM"},{"id":"http://arxiv.org/abs/2303.11728v2","updated":"2023-03-22T03:30:47Z","published":"2023-03-21T10:32:27Z","title":"ExtremeNeRF: Few-shot Neural Radiance Fields Under Unconstrained\n  Illumination","summary":"  In this paper, we propose a new challenge that synthesizes a novel view in a\nmore practical environment, where the number of input multi-view images is\nlimited and illumination variations are significant. Despite recent success,\nneural radiance fields (NeRF) require a massive amount of input multi-view\nimages taken under constrained illuminations. To address the problem, we\nsuggest ExtremeNeRF, which utilizes occlusion-aware multiview albedo\nconsistency, supported by geometric alignment and depth consistency. We extract\nintrinsic image components that should be illumination-invariant across\ndifferent views, enabling direct appearance comparison between the input and\nnovel view under unconstrained illumination. We provide extensive experimental\nresults for an evaluation of the task, using the newly built NeRF Extreme\nbenchmark, which is the first in-the-wild novel view synthesis benchmark taken\nunder multiple viewing directions and varying illuminations. The project page\nis at https://seokyeong94.github.io/ExtremeNeRF/\n","authors":["SeokYeong Lee","JunYong Choi","Seungryong Kim","Ig-Jae Kim","Junghyun Cho"],"pdf_url":"https://arxiv.org/pdf/2303.11728v2.pdf","comment":"Project Page: https://seokyeong94.github.io/ExtremeNeRF/"},{"id":"http://arxiv.org/abs/2211.11629v2","updated":"2023-03-22T03:28:46Z","published":"2022-11-21T16:43:33Z","title":"PVT++: A Simple End-to-End Latency-Aware Visual Tracking Framework","summary":"  Visual object tracking is essential to intelligent robots. Most existing\napproaches have ignored the online latency that can cause severe performance\ndegradation during real-world processing. Especially for unmanned aerial\nvehicles (UAVs), where robust tracking is more challenging and onboard\ncomputation is limited, the latency issue can be fatal. In this work, we\npresent a simple framework for end-to-end latency-aware tracking, i.e.,\nend-to-end predictive visual tracking (PVT++). Unlike existing solutions that\nnaively append Kalman Filters after trackers, PVT++ can be jointly optimized,\nso that it takes not only motion information but can also leverage the rich\nvisual knowledge in most pre-trained tracker models for robust prediction.\nBesides, to bridge the training-evaluation domain gap, we propose a relative\nmotion factor, empowering PVT++ to generalize to the challenging and complex\nUAV tracking scenes. These careful designs have made the small-capacity\nlightweight PVT++ a widely effective solution. Additionally, this work presents\nan extended latency-aware evaluation benchmark for assessing an any-speed\ntracker in the online setting. Empirical results on a robotic platform from the\naerial perspective show that PVT++ can achieve significant performance gain on\nvarious trackers and exhibit higher accuracy than prior solutions, largely\nmitigating the degradation brought by latency. Our code will be made public.\n","authors":["Bowen Li","Ziyuan Huang","Junjie Ye","Yiming Li","Sebastian Scherer","Hang Zhao","Changhong Fu"],"pdf_url":"https://arxiv.org/pdf/2211.11629v2.pdf","comment":"18 pages, 10 figures"},{"id":"http://arxiv.org/abs/2303.12280v1","updated":"2023-03-22T03:13:55Z","published":"2023-03-22T03:13:55Z","title":"NLOS-NeuS: Non-line-of-sight Neural Implicit Surface","summary":"  Non-line-of-sight (NLOS) imaging is conducted to infer invisible scenes from\nindirect light on visible objects. The neural transient field (NeTF) was\nproposed for representing scenes as neural radiance fields in NLOS scenes. We\npropose NLOS neural implicit surface (NLOS-NeuS), which extends the NeTF to\nneural implicit surfaces with a signed distance function (SDF) for\nreconstructing three-dimensional surfaces in NLOS scenes. We introduce two\nconstraints as loss functions for correctly learning an SDF to avoid non-zero\nlevel-set surfaces. We also introduce a lower bound constraint of an SDF based\non the geometry of the first-returning photons. The experimental results\nindicate that these constraints are essential for learning a correct SDF in\nNLOS scenes. Compared with previous methods with discretized representation,\nNLOS-NeuS with the neural continuous representation enables us to reconstruct\nsmooth surfaces while preserving fine details in NLOS scenes. To the best of\nour knowledge, this is the first study on neural implicit surfaces with volume\nrendering in NLOS scenes.\n","authors":["Yuki Fujimura","Takahiro Kushida","Takuya Funatomi","Yasuhiro Mukaigawa"],"pdf_url":"https://arxiv.org/pdf/2303.12280v1.pdf","comment":"13 pages"},{"id":"http://arxiv.org/abs/2303.07543v3","updated":"2023-03-22T03:11:28Z","published":"2023-03-14T00:13:57Z","title":"WDiscOOD: Out-of-Distribution Detection via Whitened Linear Discriminant\n  Analysis","summary":"  Deep neural networks are susceptible to generating overconfident yet\nerroneous predictions when presented with data beyond known concepts. This\nchallenge underscores the importance of detecting out-of-distribution (OOD)\nsamples in the open world. In this work, we propose a novel feature-space OOD\ndetection score that jointly reasons with both class-specific and\nclass-agnostic information. Specifically, our approach utilizes Whitened Linear\nDiscriminant Analysis to project features into two subspaces - the\ndiscriminative and residual subspaces - in which the ID classes are maximally\nseparated and closely clustered, respectively. The OOD score is then determined\nby combining the deviation from the input data to the ID distribution in both\nsubspaces. The efficacy of our method, named WDiscOOD, is verified on the\nlarge-scale ImageNet-1k benchmark, with six OOD datasets that covers a variety\nof distribution shifts. WDiscOOD demonstrates superior performance on deep\nclassifiers with diverse backbone architectures, including CNN and vision\ntransformer. Furthermore, we also show that our method can more effectively\ndetect novel concepts in representation space trained with contrastive\nobjectives, including supervised contrastive loss and multi-modality\ncontrastive loss.\n","authors":["Yiye Chen","Yunzhi Lin","Ruinian Xu","Patricio A. Vela"],"pdf_url":"https://arxiv.org/pdf/2303.07543v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.12274v1","updated":"2023-03-22T02:47:42Z","published":"2023-03-22T02:47:42Z","title":"A Hierarchical Hybrid Learning Framework for Multi-agent Trajectory\n  Prediction","summary":"  Accurate and robust trajectory prediction of neighboring agents is critical\nfor autonomous vehicles traversing in complex scenes. Most methods proposed in\nrecent years are deep learning-based due to their strength in encoding complex\ninteractions. However, unplausible predictions are often generated since they\nrely heavily on past observations and cannot effectively capture the transient\nand contingency interactions from sparse samples. In this paper, we propose a\nhierarchical hybrid framework of deep learning (DL) and reinforcement learning\n(RL) for multi-agent trajectory prediction, to cope with the challenge of\npredicting motions shaped by multi-scale interactions. In the DL stage, the\ntraffic scene is divided into multiple intermediate-scale heterogenous graphs\nbased on which Transformer-style GNNs are adopted to encode heterogenous\ninteractions at intermediate and global levels. In the RL stage, we divide the\ntraffic scene into local sub-scenes utilizing the key future points predicted\nin the DL stage. To emulate the motion planning procedure so as to produce\ntrajectory predictions, a Transformer-based Proximal Policy Optimization (PPO)\nincorporated with a vehicle kinematics model is devised to plan motions under\nthe dominant influence of microscopic interactions. A multi-objective reward is\ndesigned to balance between agent-centric accuracy and scene-wise\ncompatibility. Experimental results show that our proposal matches the\nstate-of-the-arts on the Argoverse forecasting benchmark. It's also revealed by\nthe visualized results that the hierarchical learning framework captures the\nmulti-scale interactions and improves the feasibility and compliance of the\npredicted trajectories.\n","authors":["Yujun Jiao","Mingze Miao","Zhishuai Yin","Chunyuan Lei","Xu Zhu","Linzhen Nie","Bo Tao"],"pdf_url":"https://arxiv.org/pdf/2303.12274v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2212.07060v2","updated":"2023-03-22T02:44:57Z","published":"2022-12-14T07:03:23Z","title":"VINet: Lightweight, Scalable, and Heterogeneous Cooperative Perception\n  for 3D Object Detection","summary":"  Utilizing the latest advances in Artificial Intelligence (AI), the computer\nvision community is now witnessing an unprecedented evolution in all kinds of\nperception tasks, particularly in object detection. Based on multiple spatially\nseparated perception nodes, Cooperative Perception (CP) has emerged to\nsignificantly advance the perception of automated driving. However, current\ncooperative object detection methods mainly focus on ego-vehicle efficiency\nwithout considering the practical issues of system-wide costs. In this paper,\nwe introduce VINet, a unified deep learning-based CP network for scalable,\nlightweight, and heterogeneous cooperative 3D object detection. VINet is the\nfirst CP method designed from the standpoint of large-scale system-level\nimplementation and can be divided into three main phases: 1) Global\nPre-Processing and Lightweight Feature Extraction which prepare the data into\nglobal style and extract features for cooperation in a lightweight manner; 2)\nTwo-Stream Fusion which fuses the features from scalable and heterogeneous\nperception nodes; and 3) Central Feature Backbone and 3D Detection Head which\nfurther process the fused features and generate cooperative detection results.\nAn open-source data experimental platform is designed and developed for CP\ndataset acquisition and model evaluation. The experimental analysis shows that\nVINet can reduce 84% system-level computational cost and 94% system-level\ncommunication cost while improving the 3D detection accuracy.\n","authors":["Zhengwei Bai","Guoyuan Wu","Matthew J. Barth","Yongkang Liu","Emrah Akin Sisbot","Kentaro Oguchi"],"pdf_url":"https://arxiv.org/pdf/2212.07060v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2204.12037v8","updated":"2023-03-22T02:41:58Z","published":"2022-04-26T02:22:28Z","title":"Causal Reasoning Meets Visual Representation Learning: A Prospective\n  Study","summary":"  Visual representation learning is ubiquitous in various real-world\napplications, including visual comprehension, video understanding, multi-modal\nanalysis, human-computer interaction, and urban computing. Due to the emergence\nof huge amounts of multi-modal heterogeneous spatial/temporal/spatial-temporal\ndata in big data era, the lack of interpretability, robustness, and\nout-of-distribution generalization are becoming the challenges of the existing\nvisual models. The majority of the existing methods tend to fit the original\ndata/variable distributions and ignore the essential causal relations behind\nthe multi-modal knowledge, which lacks unified guidance and analysis about why\nmodern visual representation learning methods easily collapse into data bias\nand have limited generalization and cognitive abilities. Inspired by the strong\ninference ability of human-level agents, recent years have therefore witnessed\ngreat effort in developing causal reasoning paradigms to realize robust\nrepresentation and model learning with good cognitive ability. In this paper,\nwe conduct a comprehensive review of existing causal reasoning methods for\nvisual representation learning, covering fundamental theories, models, and\ndatasets. The limitations of current methods and datasets are also discussed.\nMoreover, we propose some prospective challenges, opportunities, and future\nresearch directions for benchmarking causal reasoning algorithms in visual\nrepresentation learning. This paper aims to provide a comprehensive overview of\nthis emerging field, attract attention, encourage discussions, bring to the\nforefront the urgency of developing novel causal reasoning methods, publicly\navailable benchmarks, and consensus-building standards for reliable visual\nrepresentation learning and related real-world applications more efficiently.\n","authors":["Yang Liu","Yushen Wei","Hong Yan","Guanbin Li","Liang Lin"],"pdf_url":"https://arxiv.org/pdf/2204.12037v8.pdf","comment":"35 pages, 14 figures. This work has been accepted by Machine\n  Intelligence Research. The arxiv version is kept updating by adding more\n  novel methods, datasets and insights. The official video interpretation of\n  this paper can be referred at https://youtu.be/2lfNaTkcTHI"},{"id":"http://arxiv.org/abs/2211.12764v3","updated":"2023-03-22T02:36:52Z","published":"2022-11-23T08:20:29Z","title":"VoP: Text-Video Co-operative Prompt Tuning for Cross-Modal Retrieval","summary":"  Many recent studies leverage the pre-trained CLIP for text-video cross-modal\nretrieval by tuning the backbone with additional heavy modules, which not only\nbrings huge computational burdens with much more parameters, but also leads to\nthe knowledge forgetting from upstream models. In this work, we propose the\nVoP: Text-Video Co-operative Prompt Tuning for efficient tuning on the\ntext-video retrieval task. The proposed VoP is an end-to-end framework with\nboth video & text prompts introducing, which can be regarded as a powerful\nbaseline with only 0.1% trainable parameters. Further, based on the\nspatio-temporal characteristics of videos, we develop three novel video prompt\nmechanisms to improve the performance with different scales of trainable\nparameters. The basic idea of the VoP enhancement is to model the frame\nposition, frame context, and layer function with specific trainable prompts,\nrespectively. Extensive experiments show that compared to full fine-tuning, the\nenhanced VoP achieves a 1.4% average R@1 gain across five text-video retrieval\nbenchmarks with 6x less parameter overhead. The code will be available at\nhttps://github.com/bighuang624/VoP.\n","authors":["Siteng Huang","Biao Gong","Yulin Pan","Jianwen Jiang","Yiliang Lv","Yuyuan Li","Donglin Wang"],"pdf_url":"https://arxiv.org/pdf/2211.12764v3.pdf","comment":"Accepted by CVPR 2023"},{"id":"http://arxiv.org/abs/2303.12270v1","updated":"2023-03-22T02:36:13Z","published":"2023-03-22T02:36:13Z","title":"EBSR: Enhanced Binary Neural Network for Image Super-Resolution","summary":"  While the performance of deep convolutional neural networks for image\nsuper-resolution (SR) has improved significantly, the rapid increase of memory\nand computation requirements hinders their deployment on resource-constrained\ndevices. Quantized networks, especially binary neural networks (BNN) for SR\nhave been proposed to significantly improve the model inference efficiency but\nsuffer from large performance degradation. We observe the activation\ndistribution of SR networks demonstrates very large pixel-to-pixel,\nchannel-to-channel, and image-to-image variation, which is important for high\nperformance SR but gets lost during binarization. To address the problem, we\npropose two effective methods, including the spatial re-scaling as well as\nchannel-wise shifting and re-scaling, which augments binary convolutions by\nretaining more spatial and channel-wise information. Our proposed models,\ndubbed EBSR, demonstrate superior performance over prior art methods both\nquantitatively and qualitatively across different datasets and different model\nsizes. Specifically, for x4 SR on Set5 and Urban100, EBSRlight improves the\nPSNR by 0.31 dB and 0.28 dB compared to SRResNet-E2FIF, respectively, while\nEBSR outperforms EDSR-E2FIF by 0.29 dB and 0.32 dB PSNR, respectively.\n","authors":["Renjie Wei","Shuwen Zhang","Zechun Liu","Meng Li","Yuchen Fan","Runsheng Wang","Ru Huang"],"pdf_url":"https://arxiv.org/pdf/2303.12270v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.12267v1","updated":"2023-03-22T02:28:54Z","published":"2023-03-22T02:28:54Z","title":"AUTO: Adaptive Outlier Optimization for Online Test-Time OOD Detection","summary":"  Out-of-distribution (OOD) detection is a crucial aspect of deploying machine\nlearning models in open-world applications. Empirical evidence suggests that\ntraining with auxiliary outliers substantially improves OOD detection. However,\nsuch outliers typically exhibit a distribution gap compared to the test OOD\ndata and do not cover all possible test OOD scenarios. Additionally,\nincorporating these outliers introduces additional training burdens. In this\npaper, we introduce a novel paradigm called test-time OOD detection, which\nutilizes unlabeled online data directly at test time to improve OOD detection\nperformance. While this paradigm is efficient, it also presents challenges such\nas catastrophic forgetting. To address these challenges, we propose adaptive\noutlier optimization (AUTO), which consists of an in-out-aware filter, an ID\nmemory bank, and a semantically-consistent objective. AUTO adaptively mines\npseudo-ID and pseudo-OOD samples from test data, utilizing them to optimize\nnetworks in real time during inference. Extensive results on CIFAR-10,\nCIFAR-100, and ImageNet benchmarks demonstrate that AUTO significantly enhances\nOOD detection performance.\n","authors":["Puning Yang","Jian Liang","Jie Cao","Ran He"],"pdf_url":"https://arxiv.org/pdf/2303.12267v1.pdf","comment":"15 pages"},{"id":"http://arxiv.org/abs/2303.04995v2","updated":"2023-03-22T02:21:50Z","published":"2023-03-09T02:38:32Z","title":"Text-Visual Prompting for Efficient 2D Temporal Video Grounding","summary":"  In this paper, we study the problem of temporal video grounding (TVG), which\naims to predict the starting/ending time points of moments described by a text\nsentence within a long untrimmed video. Benefiting from fine-grained 3D visual\nfeatures, the TVG techniques have achieved remarkable progress in recent years.\nHowever, the high complexity of 3D convolutional neural networks (CNNs) makes\nextracting dense 3D visual features time-consuming, which calls for intensive\nmemory and computing resources. Towards efficient TVG, we propose a novel\ntext-visual prompting (TVP) framework, which incorporates optimized\nperturbation patterns (that we call 'prompts') into both visual inputs and\ntextual features of a TVG model. In sharp contrast to 3D CNNs, we show that TVP\nallows us to effectively co-train vision encoder and language encoder in a 2D\nTVG model and improves the performance of crossmodal feature fusion using only\nlow-complexity sparse 2D visual features. Further, we propose a\nTemporal-Distance IoU (TDIoU) loss for efficient learning of TVG. Experiments\non two benchmark datasets, Charades-STA and ActivityNet Captions datasets,\nempirically show that the proposed TVP significantly boosts the performance of\n2D TVG (e.g., 9.79% improvement on Charades-STA and 30.77% improvement on\nActivityNet Captions) and achieves 5x inference acceleration over TVG using 3D\nvisual features. Codes are available at Open.Intel.\n","authors":["Yimeng Zhang","Xin Chen","Jinghan Jia","Sijia Liu","Ke Ding"],"pdf_url":"https://arxiv.org/pdf/2303.04995v2.pdf","comment":"Accepted to the CVPR 2023"},{"id":"http://arxiv.org/abs/2303.12255v1","updated":"2023-03-22T01:45:35Z","published":"2023-03-22T01:45:35Z","title":"Encoding Binary Concepts in the Latent Space of Generative Models for\n  Enhancing Data Representation","summary":"  Binary concepts are empirically used by humans to generalize efficiently. And\nthey are based on Bernoulli distribution which is the building block of\ninformation. These concepts span both low-level and high-level features such as\n\"large vs small\" and \"a neuron is active or inactive\". Binary concepts are\nubiquitous features and can be used to transfer knowledge to improve model\ngeneralization. We propose a novel binarized regularization to facilitate\nlearning of binary concepts to improve the quality of data generation in\nautoencoders. We introduce a binarizing hyperparameter $r$ in data generation\nprocess to disentangle the latent space symmetrically. We demonstrate that this\nmethod can be applied easily to existing variational autoencoder (VAE) variants\nto encourage symmetric disentanglement, improve reconstruction quality, and\nprevent posterior collapse without computation overhead. We also demonstrate\nthat this method can boost existing models to learn more transferable\nrepresentations and generate more representative samples for the input\ndistribution which can alleviate catastrophic forgetting using generative\nreplay under continual learning settings.\n","authors":["Zizhao Hu","Mohammad Rostami"],"pdf_url":"https://arxiv.org/pdf/2303.12255v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.12249v1","updated":"2023-03-22T01:14:52Z","published":"2023-03-22T01:14:52Z","title":"State-of-the-art optical-based physical adversarial attacks for deep\n  learning computer vision systems","summary":"  Adversarial attacks can mislead deep learning models to make false\npredictions by implanting small perturbations to the original input that are\nimperceptible to the human eye, which poses a huge security threat to the\ncomputer vision systems based on deep learning. Physical adversarial attacks,\nwhich is more realistic, as the perturbation is introduced to the input before\nit is being captured and converted to a binary image inside the vision system,\nwhen compared to digital adversarial attacks. In this paper, we focus on\nphysical adversarial attacks and further classify them into invasive and\nnon-invasive. Optical-based physical adversarial attack techniques (e.g. using\nlight irradiation) belong to the non-invasive category. As the perturbations\ncan be easily ignored by humans as the perturbations are very similar to the\neffects generated by a natural environment in the real world. They are highly\ninvisibility and executable and can pose a significant or even lethal threats\nto real systems. This paper focuses on optical-based physical adversarial\nattack techniques for computer vision systems, with emphasis on the\nintroduction and discussion of optical-based physical adversarial attack\ntechniques.\n","authors":["Junbin Fang","You Jiang","Canjian Jiang","Zoe L. Jiang","Siu-Ming Yiu","Chuanyi Liu"],"pdf_url":"https://arxiv.org/pdf/2303.12249v1.pdf","comment":"This work has been submitted to the IEEE for possible publication.\n  Copyright may be transferred without notice, after which this version may no\n  longer be accessible"},{"id":"http://arxiv.org/abs/2203.11437v3","updated":"2023-03-22T01:10:08Z","published":"2022-03-22T03:17:15Z","title":"Representation Uncertainty in Self-Supervised Learning as Variational\n  Inference","summary":"  In this paper, a novel self-supervised learning (SSL) method is proposed,\nwhich learns not only representations but also representations uncertainties by\nconsidering SSL in terms of variational inference. SSL is a method of learning\nrepresentation without labels by maximizing the similarity between image\nrepresentations of different augmented views of the same image. Variational\nautoencoder (VAE) is an unsupervised representation learning method that trains\na probabilistic generative model with variational inference. VAE and SSL can\nlearn representations without labels, but the relationship between VAE and SSL\nhas not been revealed. In this paper, the theoretical relationship between SSL\nand variational inference is clarified. In addition, variational inference\nSimSiam (VI-SimSiam) is proposed, which can predict the representation\nuncertainty by interpreting SimSiam with variational inference and defining the\nlatent space distribution. The experiment qualitatively showed that VISimSiam\ncould learn uncertainty by comparing input images and predicted uncertainties.\nWe also revealed a relationship between estimated uncertainty and\nclassification accuracy.\n","authors":["Hiroki Nakamura","Masashi Okada","Tadahiro Taniguchi"],"pdf_url":"https://arxiv.org/pdf/2203.11437v3.pdf","comment":"15 pages, 12 figures, work in progress"},{"id":"http://arxiv.org/abs/2303.12247v1","updated":"2023-03-22T01:01:14Z","published":"2023-03-22T01:01:14Z","title":"Exploring the Benefits of Visual Prompting in Differential Privacy","summary":"  Visual Prompting (VP) is an emerging and powerful technique that allows\nsample-efficient adaptation to downstream tasks by engineering a well-trained\nfrozen source model. In this work, we explore the benefits of VP in\nconstructing compelling neural network classifiers with differential privacy\n(DP). We explore and integrate VP into canonical DP training methods and\ndemonstrate its simplicity and efficiency. In particular, we discover that VP\nin tandem with PATE, a state-of-the-art DP training method that leverages the\nknowledge transfer from an ensemble of teachers, achieves the state-of-the-art\nprivacy-utility trade-off with minimum expenditure of privacy budget. Moreover,\nwe conduct additional experiments on cross-domain image classification with a\nsufficient domain gap to further unveil the advantage of VP in DP. Lastly, we\nalso conduct extensive ablation studies to validate the effectiveness and\ncontribution of VP under DP consideration.\n","authors":["Yizhe Li","Yu-Lin Tsai","Xuebin Ren","Chia-Mu Yu","Pin-Yu Chen"],"pdf_url":"https://arxiv.org/pdf/2303.12247v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.12246v1","updated":"2023-03-22T00:55:53Z","published":"2023-03-22T00:55:53Z","title":"Object Pose Estimation with Statistical Guarantees: Conformal Keypoint\n  Detection and Geometric Uncertainty Propagation","summary":"  The two-stage object pose estimation paradigm first detects semantic\nkeypoints on the image and then estimates the 6D pose by minimizing\nreprojection errors. Despite performing well on standard benchmarks, existing\ntechniques offer no provable guarantees on the quality and uncertainty of the\nestimation. In this paper, we inject two fundamental changes, namely conformal\nkeypoint detection and geometric uncertainty propagation, into the two-stage\nparadigm and propose the first pose estimator that endows an estimation with\nprovable and computable worst-case error bounds. On one hand, conformal\nkeypoint detection applies the statistical machinery of inductive conformal\nprediction to convert heuristic keypoint detections into circular or elliptical\nprediction sets that cover the groundtruth keypoints with a user-specified\nmarginal probability (e.g., 90%). Geometric uncertainty propagation, on the\nother, propagates the geometric constraints on the keypoints to the 6D object\npose, leading to a Pose UnceRtainty SEt (PURSE) that guarantees coverage of the\ngroundtruth pose with the same probability. The PURSE, however, is a nonconvex\nset that does not directly lead to estimated poses and uncertainties.\nTherefore, we develop RANdom SAmple averaGing (RANSAG) to compute an average\npose and apply semidefinite relaxation to upper bound the worst-case errors\nbetween the average pose and the groundtruth. On the LineMOD Occlusion dataset\nwe demonstrate: (i) the PURSE covers the groundtruth with valid probabilities;\n(ii) the worst-case error bounds provide correct uncertainty quantification;\nand (iii) the average pose achieves better or similar accuracy as\nrepresentative methods based on sparse keypoints.\n","authors":["Heng Yang","Marco Pavone"],"pdf_url":"https://arxiv.org/pdf/2303.12246v1.pdf","comment":"Accepted at CVPR 2023 as a highlight paper"},{"id":"http://arxiv.org/abs/2205.14320v3","updated":"2023-03-22T00:55:32Z","published":"2022-05-28T03:32:56Z","title":"RIAV-MVS: Recurrent-Indexing an Asymmetric Volume for Multi-View Stereo","summary":"  This paper presents a learning-based method for multi-view depth estimation\nfrom posed images. Our core idea is a \"learning-to-optimize\" paradigm that\niteratively indexes a plane-sweeping cost volume and regresses the depth map\nvia a convolutional Gated Recurrent Unit (GRU). Since the cost volume plays a\nparamount role in encoding the multi-view geometry, we aim to improve its\nconstruction both at pixel- and frame- levels. At the pixel level, we propose\nto break the symmetry of the Siamese network (which is typically used in MVS to\nextract image features) by introducing a transformer block to the reference\nimage (but not to the source images). Such an asymmetric volume allows the\nnetwork to extract global features from the reference image to predict its\ndepth map. Given potential inaccuracies in the poses between reference and\nsource images, we propose to incorporate a residual pose network to correct the\nrelative poses. This essentially rectifies the cost volume at the frame level.\nWe conduct extensive experiments on real-world MVS datasets and show that our\nmethod achieves state-of-the-art performance in terms of both within-dataset\nevaluation and cross-dataset generalization.\n","authors":["Changjiang Cai","Pan Ji","Qingan Yan","Yi Xu"],"pdf_url":"https://arxiv.org/pdf/2205.14320v3.pdf","comment":"CVPR 2023"},{"id":"http://arxiv.org/abs/2204.14079v3","updated":"2023-03-22T00:49:31Z","published":"2022-04-29T13:21:14Z","title":"Fix the Noise: Disentangling Source Feature for Transfer Learning of\n  StyleGAN","summary":"  Transfer learning of StyleGAN has recently shown great potential to solve\ndiverse tasks, especially in domain translation. Previous methods utilized a\nsource model by swapping or freezing weights during transfer learning, however,\nthey have limitations on visual quality and controlling source features. In\nother words, they require additional models that are computationally demanding\nand have restricted control steps that prevent a smooth transition. In this\npaper, we propose a new approach to overcome these limitations. Instead of\nswapping or freezing, we introduce a simple feature matching loss to improve\ngeneration quality. In addition, to control the degree of source features, we\ntrain a target model with the proposed strategy, FixNoise, to preserve the\nsource features only in a disentangled subspace of a target feature space.\nOwing to the disentangled feature space, our method can smoothly control the\ndegree of the source features in a single model. Extensive experiments\ndemonstrate that the proposed method can generate more consistent and realistic\nimages than previous works.\n","authors":["Dongyeun Lee","Jae Young Lee","Doyeon Kim","Jaehyun Choi","Junmo Kim"],"pdf_url":"https://arxiv.org/pdf/2204.14079v3.pdf","comment":"Full CVPR 2023 paper is available at arXiv:2303.11545. Best paper of\n  CVPRW AICC 2022 (CVPR 2022 Workshop on AI for Content Creation). The code is\n  available at https://github.com/LeeDongYeun/FixNoise"},{"id":"http://arxiv.org/abs/2212.04493v2","updated":"2023-03-22T00:30:56Z","published":"2022-12-08T18:59:05Z","title":"SDFusion: Multimodal 3D Shape Completion, Reconstruction, and Generation","summary":"  In this work, we present a novel framework built to simplify 3D asset\ngeneration for amateur users. To enable interactive generation, our method\nsupports a variety of input modalities that can be easily provided by a human,\nincluding images, text, partially observed shapes and combinations of these,\nfurther allowing to adjust the strength of each input. At the core of our\napproach is an encoder-decoder, compressing 3D shapes into a compact latent\nrepresentation, upon which a diffusion model is learned. To enable a variety of\nmulti-modal inputs, we employ task-specific encoders with dropout followed by a\ncross-attention mechanism. Due to its flexibility, our model naturally supports\na variety of tasks, outperforming prior works on shape completion, image-based\n3D reconstruction, and text-to-3D. Most interestingly, our model can combine\nall these tasks into one swiss-army-knife tool, enabling the user to perform\nshape generation using incomplete shapes, images, and textual descriptions at\nthe same time, providing the relative weights for each input and facilitating\ninteractivity. Despite our approach being shape-only, we further show an\nefficient method to texture the generated shape using large-scale text-to-image\nmodels.\n","authors":["Yen-Chi Cheng","Hsin-Ying Lee","Sergey Tulyakov","Alexander Schwing","Liangyan Gui"],"pdf_url":"https://arxiv.org/pdf/2212.04493v2.pdf","comment":"In CVPR 2023. Project page and code is available at:\n  https://yccyenchicheng.github.io/SDFusion/. Fix some typos"},{"id":"http://arxiv.org/abs/2212.00937v4","updated":"2023-03-22T00:30:39Z","published":"2022-12-02T02:52:01Z","title":"StructVPR: Distill Structural Knowledge with Weighting Samples for\n  Visual Place Recognition","summary":"  Visual place recognition (VPR) is usually considered as a specific image\nretrieval problem. Limited by existing training frameworks, most deep\nlearning-based works cannot extract sufficiently stable global features from\nRGB images and rely on a time-consuming re-ranking step to exploit spatial\nstructural information for better performance. In this paper, we propose\nStructVPR, a novel training architecture for VPR, to enhance structural\nknowledge in RGB global features and thus improve feature stability in a\nconstantly changing environment. Specifically, StructVPR uses segmentation\nimages as a more definitive source of structural knowledge input into a CNN\nnetwork and applies knowledge distillation to avoid online segmentation and\ninference of seg-branch in testing. Considering that not all samples contain\nhigh-quality and helpful knowledge, and some even hurt the performance of\ndistillation, we partition samples and weigh each sample's distillation loss to\nenhance the expected knowledge precisely. Finally, StructVPR achieves\nimpressive performance on several benchmarks using only global retrieval and\neven outperforms many two-stage approaches by a large margin. After adding\nadditional re-ranking, ours achieves state-of-the-art performance while\nmaintaining a low computational cost.\n","authors":["Yanqing Shen","Sanping Zhou","Jingwen Fu","Ruotong Wang","Shitao Chen","Nanning Zheng"],"pdf_url":"https://arxiv.org/pdf/2212.00937v4.pdf","comment":"accepted by CVPR2023"},{"id":"http://arxiv.org/abs/2303.09915v3","updated":"2023-03-22T00:29:34Z","published":"2023-03-17T12:08:47Z","title":"Privacy-preserving Pedestrian Tracking using Distributed 3D LiDARs","summary":"  The growing demand for intelligent environments unleashes an extraordinary\ncycle of privacy-aware applications that makes individuals' life more\ncomfortable and safe. Examples of these applications include pedestrian\ntracking systems in large areas. Although the ubiquity of camera-based systems,\nthey are not a preferable solution due to the vulnerability of leaking the\nprivacy of pedestrians. In this paper, we introduce a novel privacy-preserving\nsystem for pedestrian tracking in smart environments using multiple distributed\nLiDARs of non-overlapping views. The system is designed to leverage LiDAR\ndevices to track pedestrians in partially covered areas due to practical\nconstraints, e.g., occlusion or cost. Therefore, the system uses the point\ncloud captured by different LiDARs to extract discriminative features that are\nused to train a metric learning model for pedestrian matching purposes. To\nboost the system's robustness, we leverage a probabilistic approach to model\nand adapt the dynamic mobility patterns of individuals and thus connect their\nsub-trajectories. We deployed the system in a large-scale testbed with 70\ncolorless LiDARs and conducted three different experiments. The evaluation\nresult at the entrance hall confirms the system's ability to accurately track\nthe pedestrians with a 0.98 F-measure even with zero-covered areas. This result\nhighlights the promise of the proposed system as the next generation of\nprivacy-preserving tracking means in smart environments.\n","authors":["Masakazu Ohno","Riki Ukyo","Tatsuya Amano","Hamada Rizk","Hirozumi Yamaguchi"],"pdf_url":"https://arxiv.org/pdf/2303.09915v3.pdf","comment":"Accepted to the 21st International Conference on Pervasive Computing\n  and Communications (PerCom 2023)"},{"id":"http://arxiv.org/abs/2212.07593v3","updated":"2023-03-22T00:23:39Z","published":"2022-12-15T02:45:57Z","title":"Enhanced Training of Query-Based Object Detection via Selective Query\n  Recollection","summary":"  This paper investigates a phenomenon where query-based object detectors\nmispredict at the last decoding stage while predicting correctly at an\nintermediate stage. We review the training process and attribute the overlooked\nphenomenon to two limitations: lack of training emphasis and cascading errors\nfrom decoding sequence. We design and present Selective Query Recollection\n(SQR), a simple and effective training strategy for query-based object\ndetectors. It cumulatively collects intermediate queries as decoding stages go\ndeeper and selectively forwards the queries to the downstream stages aside from\nthe sequential structure. Such-wise, SQR places training emphasis on later\nstages and allows later stages to work with intermediate queries from earlier\nstages directly. SQR can be easily plugged into various query-based object\ndetectors and significantly enhances their performance while leaving the\ninference pipeline unchanged. As a result, we apply SQR on Adamixer, DAB-DETR,\nand Deformable-DETR across various settings (backbone, number of queries,\nschedule) and consistently brings 1.4-2.8 AP improvement.\n","authors":["Fangyi Chen","Han Zhang","Kai Hu","Yu-kai Huang","Chenchen Zhu","Marios Savvides"],"pdf_url":"https://arxiv.org/pdf/2212.07593v3.pdf","comment":"CVPR2023"},{"id":"http://arxiv.org/abs/2303.12241v1","updated":"2023-03-22T00:21:50Z","published":"2023-03-22T00:21:50Z","title":"Preventing Dimensional Collapse of Incomplete Multi-View Clustering via\n  Direct Contrastive Learning","summary":"  Incomplete multi-view clustering (IMVC) is an unsupervised approach, among\nwhich IMVC via contrastive learning has received attention due to its excellent\nperformance. The previous methods have the following problems: 1) Over-reliance\non additional projection heads when solving the dimensional collapse problem in\nwhich latent features are only valid in lower-dimensional subspaces during\nclustering. However, many parameters in the projection heads are unnecessary.\n2) The recovered view contain inconsistent private information and useless\nprivate information will mislead the learning of common semantics due to\nconsistent learning and reconstruction learning on the same feature. To address\nthe above issues, we propose a novel incomplete multi-view contrastive\nclustering framework. This framework directly optimizes the latent feature\nsubspace, utilizes the learned feature vectors and their sub-vectors for\nreconstruction learning and consistency learning, thereby effectively avoiding\ndimensional collapse without relying on projection heads. Since reconstruction\nloss and contrastive loss are performed on different features, the adverse\neffect of useless private information is reduced. For the incomplete data, the\nmissing information is recovered by the cross-view prediction mechanism and the\ninconsistent information from different views is discarded by the minimum\nconditional entropy to further avoid the influence of private information.\nExtensive experimental results of the method on 5 public datasets show that the\nmethod achieves state-of-the-art clustering results.\n","authors":["Kaiwu Zhang","Shiqiang Du","Baokai Liu","Shengxia Gao"],"pdf_url":"https://arxiv.org/pdf/2303.12241v1.pdf","comment":null}],"Information Retrieval":[{"id":"http://arxiv.org/abs/2303.12507v1","updated":"2023-03-22T12:29:18Z","published":"2023-03-22T12:29:18Z","title":"End-to-End Personalized Next Location Recommendation via Contrastive\n  User Preference Modeling","summary":"  Predicting the next location is a highly valuable and common need in many\nlocation-based services such as destination prediction and route planning. The\ngoal of next location recommendation is to predict the next point-of-interest a\nuser might go to based on the user's historical trajectory. Most existing\nmodels learn mobility patterns merely from users' historical check-in sequences\nwhile overlooking the significance of user preference modeling. In this work, a\nnovel Point-of-Interest Transformer (POIFormer) with contrastive user\npreference modeling is developed for end-to-end next location recommendation.\nThis model consists of three major modules: history encoder, query generator,\nand preference decoder. History encoder is designed to model mobility patterns\nfrom historical check-in sequences, while query generator explicitly learns\nuser preferences to generate user-specific intention queries. Finally,\npreference decoder combines the intention queries and historical information to\npredict the user's next location. Extensive comparisons with representative\nschemes and ablation studies on four real-world datasets demonstrate the\neffectiveness and superiority of the proposed scheme under various settings.\n","authors":["Yan Luo","Ye Liu","Fu-lai Chung","Yu Liu","Chang Wen Chen"],"pdf_url":"https://arxiv.org/pdf/2303.12507v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2208.01918v4","updated":"2023-03-22T11:15:58Z","published":"2022-08-03T08:54:13Z","title":"DeepProphet2 -- A Deep Learning Gene Recommendation Engine","summary":"  New powerful tools for tackling life science problems have been created by\nrecent advances in machine learning. The purpose of the paper is to discuss the\npotential advantages of gene recommendation performed by artificial\nintelligence (AI). Indeed, gene recommendation engines try to solve this\nproblem: if the user is interested in a set of genes, which other genes are\nlikely to be related to the starting set and should be investigated? This task\nwas solved with a custom deep learning recommendation engine, DeepProphet2\n(DP2), which is freely available to researchers worldwide via\nhttps://www.generecommender.com?utm_source=DeepProphet2_paper&utm_medium=pdf.\nHereafter, insights behind the algorithm and its practical applications are\nillustrated.\n  The gene recommendation problem can be addressed by mapping the genes to a\nmetric space where a distance can be defined to represent the real semantic\ndistance between them. To achieve this objective a transformer-based model has\nbeen trained on a well-curated freely available paper corpus, PubMed. The paper\ndescribes multiple optimization procedures that were employed to obtain the\nbest bias-variance trade-off, focusing on embedding size and network depth. In\nthis context, the model's ability to discover sets of genes implicated in\ndiseases and pathways was assessed through cross-validation. A simple\nassumption guided the procedure: the network had no direct knowledge of\npathways and diseases but learned genes' similarities and the interactions\namong them. Moreover, to further investigate the space where the neural network\nrepresents genes, the dimensionality of the embedding was reduced, and the\nresults were projected onto a human-comprehensible space. In conclusion, a set\nof use cases illustrates the algorithm's potential applications in a real word\nsetting.\n","authors":["Daniele Brambilla","Davide Maria Giacomini","Luca Muscarnera","Andrea Mazzoleni"],"pdf_url":"https://arxiv.org/pdf/2208.01918v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2111.03837v2","updated":"2023-03-22T22:18:01Z","published":"2021-11-06T09:04:16Z","title":"Focusing on Potential Named Entities During Active Label Acquisition","summary":"  Named entity recognition (NER) aims to identify mentions of named entities in\nan unstructured text and classify them into predefined named entity classes.\nWhile deep learning-based pre-trained language models help to achieve good\npredictive performances in NER, many domain-specific NER applications still\ncall for a substantial amount of labeled data. Active learning (AL), a general\nframework for the label acquisition problem, has been used for NER tasks to\nminimize the annotation cost without sacrificing model performance. However,\nthe heavily imbalanced class distribution of tokens introduces challenges in\ndesigning effective AL querying methods for NER. We propose several AL sentence\nquery evaluation functions that pay more attention to potential positive\ntokens, and evaluate these proposed functions with both sentence-based and\ntoken-based cost evaluation strategies. We also propose a better data-driven\nnormalization approach to penalize sentences that are too long or too short.\nOur experiments on three datasets from different domains reveal that the\nproposed approach reduces the number of annotated tokens while achieving better\nor comparable prediction performance with conventional methods.\n","authors":["Ali Osman Berk Sapci","Oznur Tastan","Reyyan Yeniterzi"],"pdf_url":"https://arxiv.org/pdf/2111.03837v2.pdf","comment":"20 pages, 8 figures"}],"Machine Learning":[{"id":"http://arxiv.org/abs/2211.09703v2","updated":"2023-03-22T17:58:09Z","published":"2022-11-17T17:38:55Z","title":"EfficientTrain: Exploring Generalized Curriculum Learning for Training\n  Visual Backbones","summary":"  The superior performance of modern deep networks usually comes with a costly\ntraining procedure. This paper presents a new curriculum learning approach for\nthe efficient training of visual backbones (e.g., vision Transformers). Our\nwork is inspired by the inherent learning dynamics of deep networks: we\nexperimentally show that at an earlier training stage, the model mainly learns\nto recognize some 'easier-to-learn' discriminative patterns within each\nexample, e.g., the lower-frequency components of images and the original\ninformation before data augmentation. Driven by this phenomenon, we propose a\ncurriculum where the model always leverages all the training data at each\nepoch, while the curriculum starts with only exposing the 'easier-to-learn'\npatterns of each example, and introduces gradually more difficult patterns. To\nimplement this idea, we 1) introduce a cropping operation in the Fourier\nspectrum of the inputs, which enables the model to learn from only the\nlower-frequency components efficiently, 2) demonstrate that exposing the\nfeatures of original images amounts to adopting weaker data augmentation, and\n3) integrate 1) and 2) and design a curriculum learning schedule with a\ngreedy-search algorithm. The resulting approach, EfficientTrain, is simple,\ngeneral, yet surprisingly effective. In the absence of hyper-parameter tuning,\nit reduces the training wall-time of a wide variety of popular models (e.g.,\nResNet, ConvNeXt, DeiT, PVT, Swin, and CSWin) by >1.5x on ImageNet-1K/22K\nwithout sacrificing the accuracy. It is also effective for self-supervised\nlearning (e.g., MAE). Code is available at\nhttps://github.com/LeapLabTHU/EfficientTrain.\n","authors":["Yulin Wang","Yang Yue","Rui Lu","Tianjiao Liu","Zhao Zhong","Shiji Song","Gao Huang"],"pdf_url":"https://arxiv.org/pdf/2211.09703v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.12788v1","updated":"2023-03-22T17:57:47Z","published":"2023-03-22T17:57:47Z","title":"Open-source Frame Semantic Parsing","summary":"  While the state-of-the-art for frame semantic parsing has progressed\ndramatically in recent years, it is still difficult for end-users to apply\nstate-of-the-art models in practice. To address this, we present Frame Semantic\nTransformer, an open-source Python library which achieves near state-of-the-art\nperformance on FrameNet 1.7, while focusing on ease-of-use. We use a T5 model\nfine-tuned on Propbank and FrameNet exemplars as a base, and improve\nperformance by using FrameNet lexical units to provide hints to T5 at inference\ntime. We enhance robustness to real-world data by using textual data\naugmentations during training.\n","authors":["David Chanin"],"pdf_url":"https://arxiv.org/pdf/2303.12788v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.12785v1","updated":"2023-03-22T17:56:18Z","published":"2023-03-22T17:56:18Z","title":"Matryoshka Policy Gradient for Entropy-Regularized RL: Convergence and\n  Global Optimality","summary":"  A novel Policy Gradient (PG) algorithm, called Matryoshka Policy Gradient\n(MPG), is introduced and studied, in the context of max-entropy reinforcement\nlearning, where an agent aims at maximising entropy bonuses additional to its\ncumulative rewards. MPG differs from standard PG in that it trains a sequence\nof policies to learn finite horizon tasks simultaneously, instead of a single\npolicy for the single standard objective. For softmax policies, we prove\nconvergence of MPG and global optimality of the limit by showing that the only\ncritical point of the MPG objective is the optimal policy; these results hold\ntrue even in the case of continuous compact state space. MPG is intuitive,\ntheoretically sound and we furthermore show that the optimal policy of the\nstandard max-entropy objective can be approximated arbitrarily well by the\noptimal policy of the MPG framework. Finally, we justify that MPG is well\nsuited when the policies are parametrized with neural networks and we provide\nan simple criterion to verify the global optimality of the policy at\nconvergence. As a proof of concept, we evaluate numerically MPG on standard\ntest benchmarks.\n","authors":["François Ged","Maria Han Veiga"],"pdf_url":"https://arxiv.org/pdf/2303.12785v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.12783v1","updated":"2023-03-22T17:52:54Z","published":"2023-03-22T17:52:54Z","title":"Conformal Prediction for Time Series with Modern Hopfield Networks","summary":"  To quantify uncertainty, conformal prediction methods are gaining\ncontinuously more interest and have already been successfully applied to\nvarious domains. However, they are difficult to apply to time series as the\nautocorrelative structure of time series violates basic assumptions required by\nconformal prediction. We propose HopCPT, a novel conformal prediction approach\nfor time series that not only copes with temporal structures but leverages\nthem. We show that our approach is theoretically well justified for time series\nwhere temporal dependencies are present. In experiments, we demonstrate that\nour new approach outperforms state-of-the-art conformal prediction methods on\nmultiple real-world time series datasets from four different domains.\n","authors":["Andreas Auer","Martin Gauch","Daniel Klotz","Sepp Hochreiter"],"pdf_url":"https://arxiv.org/pdf/2303.12783v1.pdf","comment":"under Review"},{"id":"http://arxiv.org/abs/2303.12767v1","updated":"2023-03-22T17:32:56Z","published":"2023-03-22T17:32:56Z","title":"Can we trust the evaluation on ChatGPT?","summary":"  ChatGPT, the first large language model (LLM) with mass adoption, has\ndemonstrated remarkable performance in numerous natural language tasks. Despite\nits evident usefulness, evaluating ChatGPT's performance in diverse problem\ndomains remains challenging due to the closed nature of the model and its\ncontinuous updates via Reinforcement Learning from Human Feedback (RLHF). We\nhighlight the issue of data contamination in ChatGPT evaluations, with a case\nstudy of the task of stance detection. We discuss the challenge of preventing\ndata contamination and ensuring fair model evaluation in the age of closed and\ncontinuously trained models.\n","authors":["Rachith Aiyappa","Jisun An","Haewoon Kwak","Yong-Yeol Ahn"],"pdf_url":"https://arxiv.org/pdf/2303.12767v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.09540v3","updated":"2023-03-22T17:22:35Z","published":"2023-03-16T17:53:24Z","title":"SemDeDup: Data-efficient learning at web-scale through semantic\n  deduplication","summary":"  Progress in machine learning has been driven in large part by massive\nincreases in data. However, large web-scale datasets such as LAION are largely\nuncurated beyond searches for exact duplicates, potentially leaving much\nredundancy. Here, we introduce SemDeDup, a method which leverages embeddings\nfrom pre-trained models to identify and remove semantic duplicates: data pairs\nwhich are semantically similar, but not exactly identical. Removing semantic\nduplicates preserves performance and speeds up learning. Analyzing a subset of\nLAION, we show that SemDeDup can remove 50% of the data with minimal\nperformance loss, effectively halving training time. Moreover, performance\nincreases out of distribution. Also, analyzing language models trained on C4, a\npartially curated dataset, we show that SemDeDup improves over prior approaches\nwhile providing efficiency gains. SemDeDup provides an example of how simple\nways of leveraging quality embeddings can be used to make models learn faster\nwith less data.\n","authors":["Amro Abbas","Kushal Tirumala","Dániel Simig","Surya Ganguli","Ari S. Morcos"],"pdf_url":"https://arxiv.org/pdf/2303.09540v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.12761v1","updated":"2023-03-22T17:14:38Z","published":"2023-03-22T17:14:38Z","title":"LSTM-based Video Quality Prediction Accounting for Temporal Distortions\n  in Videoconferencing Calls","summary":"  Current state-of-the-art video quality models, such as VMAF, give excellent\nprediction results by comparing the degraded video with its reference video.\nHowever, they do not consider temporal distortions (e.g., frame freezes or\nskips) that occur during videoconferencing calls. In this paper, we present a\ndata-driven approach for modeling such distortions automatically by training an\nLSTM with subjective quality ratings labeled via crowdsourcing. The videos were\ncollected from live videoconferencing calls in 83 different network conditions.\nWe applied QR codes as markers on the source videos to create aligned\nreferences and compute temporal features based on the alignment vectors. Using\nthese features together with VMAF core features, our proposed model achieves a\nPCC of 0.99 on the validation set. Furthermore, our model outputs per-frame\nquality that gives detailed insight into the cause of video quality\nimpairments. The VCM model and dataset are open-sourced at\nhttps://github.com/microsoft/Video_Call_MOS.\n","authors":["Gabriel Mittag","Babak Naderi","Vishak Gopal","Ross Cutler"],"pdf_url":"https://arxiv.org/pdf/2303.12761v1.pdf","comment":"Accepted at ICASSP 2023"},{"id":"http://arxiv.org/abs/2205.09900v3","updated":"2023-03-22T17:13:37Z","published":"2022-05-19T23:43:15Z","title":"Estimating the randomness of quantum circuit ensembles up to 50 qubits","summary":"  Random quantum circuits have been utilized in the contexts of quantum\nsupremacy demonstrations, variational quantum algorithms for chemistry and\nmachine learning, and blackhole information. The ability of random circuits to\napproximate any random unitaries has consequences on their complexity,\nexpressibility, and trainability. To study this property of random circuits, we\ndevelop numerical protocols for estimating the frame potential, the distance\nbetween a given ensemble and the exact randomness. Our tensor-network-based\nalgorithm has polynomial complexity for shallow circuits and is high-performing\nusing CPU and GPU parallelism. We study 1. local and parallel random circuits\nto verify the linear growth in complexity as stated by the Brown-Susskind\nconjecture, and; 2. hardware-efficient ans\\\"atze to shed light on its\nexpressibility and the barren plateau problem in the context of variational\nalgorithms. Our work shows that large-scale tensor network simulations could\nprovide important hints toward open problems in quantum information science.\n","authors":["Minzhao Liu","Junyu Liu","Yuri Alexeev","Liang Jiang"],"pdf_url":"https://arxiv.org/pdf/2205.09900v3.pdf","comment":"11 pages, many figures"},{"id":"http://arxiv.org/abs/2303.12718v1","updated":"2023-03-22T16:58:44Z","published":"2023-03-22T16:58:44Z","title":"Strategy Synthesis in Markov Decision Processes Under Limited Sampling\n  Access","summary":"  A central task in control theory, artificial intelligence, and formal methods\nis to synthesize reward-maximizing strategies for agents that operate in\npartially unknown environments. In environments modeled by gray-box Markov\ndecision processes (MDPs), the impact of the agents' actions are known in terms\nof successor states but not the stochastics involved. In this paper, we devise\na strategy synthesis algorithm for gray-box MDPs via reinforcement learning\nthat utilizes interval MDPs as internal model. To compete with limited sampling\naccess in reinforcement learning, we incorporate two novel concepts into our\nalgorithm, focusing on rapid and successful learning rather than on stochastic\nguarantees and optimality: lower confidence bound exploration reinforces\nvariants of already learned practical strategies and action scoping reduces the\nlearning action space to promising actions. We illustrate benefits of our\nalgorithms by means of a prototypical implementation applied on examples from\nthe AI and formal methods communities.\n","authors":["Christel Baier","Clemens Dubslaff","Patrick Wienhöft","Stefan J. Kiebel"],"pdf_url":"https://arxiv.org/pdf/2303.12718v1.pdf","comment":"Accepted for publication at NASA Formal Methods (NFM) 2023. This is\n  an extended version with the full appendix containing proofs, further\n  pseudocode with explanations and additional experiment figures"},{"id":"http://arxiv.org/abs/2303.12703v1","updated":"2023-03-22T16:45:54Z","published":"2023-03-22T16:45:54Z","title":"Causal Reasoning in the Presence of Latent Confounders via Neural ADMG\n  Learning","summary":"  Latent confounding has been a long-standing obstacle for causal reasoning\nfrom observational data. One popular approach is to model the data using\nacyclic directed mixed graphs (ADMGs), which describe ancestral relations\nbetween variables using directed and bidirected edges. However, existing\nmethods using ADMGs are based on either linear functional assumptions or a\ndiscrete search that is complicated to use and lacks computational tractability\nfor large datasets. In this work, we further extend the existing body of work\nand develop a novel gradient-based approach to learning an ADMG with non-linear\nfunctional relations from observational data. We first show that the presence\nof latent confounding is identifiable under the assumptions of bow-free ADMGs\nwith non-linear additive noise models. With this insight, we propose a novel\nneural causal model based on autoregressive flows for ADMG learning. This not\nonly enables us to determine complex causal structural relationships behind the\ndata in the presence of latent confounding, but also estimate their functional\nrelationships (hence treatment effects) simultaneously. We further validate our\napproach via experiments on both synthetic and real-world datasets, and\ndemonstrate the competitive performance against relevant baselines.\n","authors":["Matthew Ashman","Chao Ma","Agrin Hilmkil","Joel Jennings","Cheng Zhang"],"pdf_url":"https://arxiv.org/pdf/2303.12703v1.pdf","comment":"Camera ready version for ICLR 2023"},{"id":"http://arxiv.org/abs/2303.12695v1","updated":"2023-03-22T16:42:19Z","published":"2023-03-22T16:42:19Z","title":"Adaptive Conformal Prediction by Reweighting Nonconformity Score","summary":"  Despite attractive theoretical guarantees and practical successes, Predictive\nInterval (PI) given by Conformal Prediction (CP) may not reflect the\nuncertainty of a given model. This limitation arises from CP methods using a\nconstant correction for all test points, disregarding their individual\nuncertainties, to ensure coverage properties. To address this issue, we propose\nusing a Quantile Regression Forest (QRF) to learn the distribution of\nnonconformity scores and utilizing the QRF's weights to assign more importance\nto samples with residuals similar to the test point. This approach results in\nPI lengths that are more aligned with the model's uncertainty. In addition, the\nweights learnt by the QRF provide a partition of the features space, allowing\nfor more efficient computations and improved adaptiveness of the PI through\ngroupwise conformalization. Our approach enjoys an assumption-free finite\nsample marginal and training-conditional coverage, and under suitable\nassumptions, it also ensures conditional coverage. Our methods work for any\nnonconformity score and are available as a Python package. We conduct\nexperiments on simulated and real-world data that demonstrate significant\nimprovements compared to existing methods.\n","authors":["Salim I. Amoukou","Nicolas J. B Brunel"],"pdf_url":"https://arxiv.org/pdf/2303.12695v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.12669v1","updated":"2023-03-22T15:47:16Z","published":"2023-03-22T15:47:16Z","title":"An Extended Study of Human-like Behavior under Adversarial Training","summary":"  Neural networks have a number of shortcomings. Amongst the severest ones is\nthe sensitivity to distribution shifts which allows models to be easily fooled\ninto wrong predictions by small perturbations to inputs that are often\nimperceivable to humans and do not have to carry semantic meaning. Adversarial\ntraining poses a partial solution to address this issue by training models on\nworst-case perturbations. Yet, recent work has also pointed out that the\nreasoning in neural networks is different from humans. Humans identify objects\nby shape, while neural nets mainly employ texture cues. Exemplarily, a model\ntrained on photographs will likely fail to generalize to datasets containing\nsketches. Interestingly, it was also shown that adversarial training seems to\nfavorably increase the shift toward shape bias. In this work, we revisit this\nobservation and provide an extensive analysis of this effect on various\narchitectures, the common $\\ell_2$- and $\\ell_\\infty$-training, and\nTransformer-based models. Further, we provide a possible explanation for this\nphenomenon from a frequency perspective.\n","authors":["Paul Gavrikov","Janis Keuper","Margret Keuper"],"pdf_url":"https://arxiv.org/pdf/2303.12669v1.pdf","comment":"6 pages, accepted at the CVPR 2023 Workshop \"The 3rd Workshop of\n  Adversarial Machine Learning on Computer Vision: Art of Robustness\""},{"id":"http://arxiv.org/abs/2303.12659v1","updated":"2023-03-22T15:37:43Z","published":"2023-03-22T15:37:43Z","title":"Posthoc Interpretation via Quantization","summary":"  In this paper, we introduce a new approach, called \"Posthoc Interpretation\nvia Quantization (PIQ)\", for interpreting decisions made by trained\nclassifiers. Our method utilizes vector quantization to transform the\nrepresentations of a classifier into a discrete, class-specific latent space.\nThe class-specific codebooks act as a bottleneck that forces the interpreter to\nfocus on the parts of the input data deemed relevant by the classifier for\nmaking a prediction. We evaluated our method through quantitative and\nqualitative studies and found that PIQ generates interpretations that are more\neasily understood by participants to our user studies when compared to several\nother interpretation methods in the literature.\n","authors":["Cem Subakan","Francesco Paissan","Mirco Ravanelli"],"pdf_url":"https://arxiv.org/pdf/2303.12659v1.pdf","comment":"* Equal contribution"},{"id":"http://arxiv.org/abs/2303.12658v1","updated":"2023-03-22T15:36:19Z","published":"2023-03-22T15:36:19Z","title":"Reliable and Efficient Evaluation of Adversarial Robustness for Deep\n  Hashing-Based Retrieval","summary":"  Deep hashing has been extensively applied to massive image retrieval due to\nits efficiency and effectiveness. Recently, several adversarial attacks have\nbeen presented to reveal the vulnerability of deep hashing models against\nadversarial examples. However, existing attack methods suffer from degraded\nperformance or inefficiency because they underutilize the semantic relations\nbetween original samples or spend a lot of time learning these relations with a\ndeep neural network. In this paper, we propose a novel Pharos-guided Attack,\ndubbed PgA, to evaluate the adversarial robustness of deep hashing networks\nreliably and efficiently. Specifically, we design pharos code to represent the\nsemantics of the benign image, which preserves the similarity to semantically\nrelevant samples and dissimilarity to irrelevant ones. It is proven that we can\nquickly calculate the pharos code via a simple math formula. Accordingly, PgA\ncan directly conduct a reliable and efficient attack on deep hashing-based\nretrieval by maximizing the similarity between the hash code of the adversarial\nexample and the pharos code. Extensive experiments on the benchmark datasets\nverify that the proposed algorithm outperforms the prior state-of-the-arts in\nboth attack strength and speed.\n","authors":["Xunguang Wang","Jiawang Bai","Xinyue Xu","Xiaomeng Li"],"pdf_url":"https://arxiv.org/pdf/2303.12658v1.pdf","comment":"arXiv admin note: text overlap with arXiv:2204.10779"},{"id":"http://arxiv.org/abs/2303.12643v1","updated":"2023-03-22T15:25:07Z","published":"2023-03-22T15:25:07Z","title":"Traffic Volume Prediction using Memory-Based Recurrent Neural Networks:\n  A comparative analysis of LSTM and GRU","summary":"  Predicting traffic volume in real-time can improve both traffic flow and road\nsafety. A precise traffic volume forecast helps alert drivers to the flow of\ntraffic along their preferred routes, preventing potential deadlock situations.\nExisting parametric models cannot reliably forecast traffic volume in dynamic\nand complex traffic conditions. Therefore, in order to evaluate and forecast\nthe traffic volume for every given time step in a real-time manner, we develop\nnon-linear memory-based deep neural network models. Our extensive experiments\nrun on the Metro Interstate Traffic Volume dataset demonstrate the\neffectiveness of the proposed models in predicting traffic volume in highly\ndynamic and heterogeneous traffic environments.\n","authors":["Lokesh Chandra Das"],"pdf_url":"https://arxiv.org/pdf/2303.12643v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.12642v1","updated":"2023-03-22T15:23:22Z","published":"2023-03-22T15:23:22Z","title":"Democratising AI: Multiple Meanings, Goals, and Methods","summary":"  Numerous parties are calling for the democratisation of AI, but the phrase is\nused to refer to a variety of goals, the pursuit of which sometimes conflict.\nThis paper identifies four kinds of AI democratisation that are commonly\ndiscussed: (1) the democratisation of AI use, (2) the democratisation of AI\ndevelopment, (3) the democratisation of AI profits, and (4) the democratisation\nof AI governance. Numerous goals and methods of achieving each form of\ndemocratisation are discussed. The main takeaway from this paper is that AI\ndemocratisation is a multifarious and sometimes conflicting concept that should\nnot be conflated with improving AI accessibility. If we want to move beyond\nambiguous commitments to democratising AI, to productive discussions of\nconcrete policies and trade-offs, then we need to recognise the principal role\nof the democratisation of AI governance in navigating tradeoffs and risks\nacross decisions around use, development, and profits.\n","authors":["Elizabeth Seger","Aviv Ovadya","Ben Garfinkel","Divya Siddarth","Allan Dafoe"],"pdf_url":"https://arxiv.org/pdf/2303.12642v1.pdf","comment":"Submitted to conference"},{"id":"http://arxiv.org/abs/2102.05368v2","updated":"2023-03-22T15:21:10Z","published":"2021-02-10T10:13:39Z","title":"RoBIC: A benchmark suite for assessing classifiers robustness","summary":"  Many defenses have emerged with the development of adversarial attacks.\nModels must be objectively evaluated accordingly. This paper systematically\ntackles this concern by proposing a new parameter-free benchmark we coin RoBIC.\nRoBIC fairly evaluates the robustness of image classifiers using a new\nhalf-distortion measure. It gauges the robustness of the network against white\nand black box attacks, independently of its accuracy. RoBIC is faster than the\nother available benchmarks. We present the significant differences in the\nrobustness of 16 recent models as assessed by RoBIC.\n","authors":["Thibault Maho","Benoît Bonnet","Teddy Furon","Erwan Le Merrer"],"pdf_url":"https://arxiv.org/pdf/2102.05368v2.pdf","comment":"4 pages, accepted to ICIP 2021"},{"id":"http://arxiv.org/abs/2209.04187v2","updated":"2023-03-22T15:19:17Z","published":"2022-09-09T08:51:01Z","title":"Efficient Multi-view Clustering via Unified and Discrete Bipartite Graph\n  Learning","summary":"  Although previous graph-based multi-view clustering algorithms have gained\nsignificant progress, most of them are still faced with three limitations.\nFirst, they often suffer from high computational complexity, which restricts\ntheir applications in large-scale scenarios. Second, they usually perform graph\nlearning either at the single-view level or at the view-consensus level, but\noften neglect the possibility of the joint learning of single-view and\nconsensus graphs. Third, many of them rely on the k-means for discretization of\nthe spectral embeddings, which lack the ability to directly learn the graph\nwith discrete cluster structure. In light of this, this paper presents an\nefficient multi-view clustering approach via unified and discrete bipartite\ngraph learning (UDBGL). Specifically, the anchor-based subspace learning is\nincorporated to learn the view-specific bipartite graphs from multiple views,\nupon which the bipartite graph fusion is leveraged to learn a view-consensus\nbipartite graph with adaptive weight learning. Further, the Laplacian rank\nconstraint is imposed to ensure that the fused bipartite graph has discrete\ncluster structures (with a specific number of connected components). By\nsimultaneously formulating the view-specific bipartite graph learning, the\nview-consensus bipartite graph learning, and the discrete cluster structure\nlearning into a unified objective function, an efficient minimization algorithm\nis then designed to tackle this optimization problem and directly achieve a\ndiscrete clustering solution without requiring additional partitioning, which\nnotably has linear time complexity in data size. Experiments on a variety of\nmulti-view datasets demonstrate the robustness and efficiency of our UDBGL\napproach. The code is available at https://github.com/huangdonghere/UDBGL.\n","authors":["Si-Guo Fang","Dong Huang","Xiao-Sha Cai","Chang-Dong Wang","Chaobo He","Yong Tang"],"pdf_url":"https://arxiv.org/pdf/2209.04187v2.pdf","comment":"Accepted by IEEE Transactions on Neural Networks and Learning Systems"},{"id":"http://arxiv.org/abs/2303.12634v1","updated":"2023-03-22T15:17:16Z","published":"2023-03-22T15:17:16Z","title":"Semi-supervised counterfactual explanations","summary":"  Counterfactual explanations for machine learning models are used to find\nminimal interventions to the feature values such that the model changes the\nprediction to a different output or a target output. A valid counterfactual\nexplanation should have likely feature values. Here, we address the challenge\nof generating counterfactual explanations that lie in the same data\ndistribution as that of the training data and more importantly, they belong to\nthe target class distribution. This requirement has been addressed through the\nincorporation of auto-encoder reconstruction loss in the counterfactual search\nprocess. Connecting the output behavior of the classifier to the latent space\nof the auto-encoder has further improved the speed of the counterfactual search\nprocess and the interpretability of the resulting counterfactual explanations.\nContinuing this line of research, we show further improvement in the\ninterpretability of counterfactual explanations when the auto-encoder is\ntrained in a semi-supervised fashion with class tagged input data. We\nempirically evaluate our approach on several datasets and show considerable\nimprovement in-terms of several metrics.\n","authors":["Shravan Kumar Sajja","Sumanta Mukherjee","Satyam Dwivedi"],"pdf_url":"https://arxiv.org/pdf/2303.12634v1.pdf","comment":"12 pages, 3 figures, 4 tables"},{"id":"http://arxiv.org/abs/2204.10779v5","updated":"2023-03-22T14:57:41Z","published":"2022-04-18T04:51:08Z","title":"CgAT: Center-Guided Adversarial Training for Deep Hashing-Based\n  Retrieval","summary":"  Deep hashing has been extensively utilized in massive image retrieval because\nof its efficiency and effectiveness. However, deep hashing models are\nvulnerable to adversarial examples, making it essential to develop adversarial\ndefense methods for image retrieval. Existing solutions achieved limited\ndefense performance because of using weak adversarial samples for training and\nlacking discriminative optimization objectives to learn robust features. In\nthis paper, we present a min-max based Center-guided Adversarial Training,\nnamely CgAT, to improve the robustness of deep hashing networks through worst\nadversarial examples. Specifically, we first formulate the center code as a\nsemantically-discriminative representative of the input image content, which\npreserves the semantic similarity with positive samples and dissimilarity with\nnegative examples. We prove that a mathematical formula can calculate the\ncenter code immediately. After obtaining the center codes in each optimization\niteration of the deep hashing network, they are adopted to guide the\nadversarial training process. On the one hand, CgAT generates the worst\nadversarial examples as augmented data by maximizing the Hamming distance\nbetween the hash codes of the adversarial examples and the center codes. On the\nother hand, CgAT learns to mitigate the effects of adversarial samples by\nminimizing the Hamming distance to the center codes. Extensive experiments on\nthe benchmark datasets demonstrate the effectiveness of our adversarial\ntraining algorithm in defending against adversarial attacks for deep\nhashing-based retrieval. Compared with the current state-of-the-art defense\nmethod, we significantly improve the defense performance by an average of\n18.61\\%, 12.35\\%, and 11.56\\% on FLICKR-25K, NUS-WIDE, and MS-COCO,\nrespectively. The code is available at https://github.com/xunguangwang/CgAT.\n","authors":["Xunguang Wang","Yiqun Lin","Xiaomeng Li"],"pdf_url":"https://arxiv.org/pdf/2204.10779v5.pdf","comment":null},{"id":"http://arxiv.org/abs/2302.03262v2","updated":"2023-03-22T14:31:42Z","published":"2023-02-07T05:20:20Z","title":"Membership Inference Attacks against Diffusion Models","summary":"  Diffusion models have attracted attention in recent years as innovative\ngenerative models. In this paper, we investigate whether a diffusion model is\nresistant to a membership inference attack, which evaluates the privacy leakage\nof a machine learning model. We primarily discuss the diffusion model from the\nstandpoints of comparison with a generative adversarial network (GAN) as\nconventional models and hyperparameters unique to the diffusion model, i.e.,\ntime steps, sampling steps, and sampling variances. We conduct extensive\nexperiments with DDIM as a diffusion model and DCGAN as a GAN on the CelebA and\nCIFAR-10 datasets in both white-box and black-box settings and then confirm if\nthe diffusion model is comparably resistant to a membership inference attack as\nGAN. Next, we demonstrate that the impact of time steps is significant and\nintermediate steps in a noise schedule are the most vulnerable to the attack.\nWe also found two key insights through further analysis. First, we identify\nthat DDIM is vulnerable to the attack for small sample sizes instead of\nachieving a lower FID. Second, sampling steps in hyperparameters are important\nfor resistance to the attack, whereas the impact of sampling variances is quite\nlimited.\n","authors":["Tomoya Matsumoto","Takayuki Miura","Naoto Yanai"],"pdf_url":"https://arxiv.org/pdf/2302.03262v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.10880v3","updated":"2023-03-22T14:25:54Z","published":"2023-03-20T05:38:30Z","title":"Rotating without Seeing: Towards In-hand Dexterity through Touch","summary":"  Tactile information plays a critical role in human dexterity. It reveals\nuseful contact information that may not be inferred directly from vision. In\nfact, humans can even perform in-hand dexterous manipulation without using\nvision. Can we enable the same ability for the multi-finger robot hand? In this\npaper, we present Touch Dexterity, a new system that can perform in-hand object\nrotation using only touching without seeing the object. Instead of relying on\nprecise tactile sensing in a small region, we introduce a new system design\nusing dense binary force sensors (touch or no touch) overlaying one side of the\nwhole robot hand (palm, finger links, fingertips). Such a design is low-cost,\ngiving a larger coverage of the object, and minimizing the Sim2Real gap at the\nsame time. We train an in-hand rotation policy using Reinforcement Learning on\ndiverse objects in simulation. Relying on touch-only sensing, we can directly\ndeploy the policy in a real robot hand and rotate novel objects that are not\npresented in training. Extensive ablations are performed on how tactile\ninformation help in-hand manipulation.Our project is available at\nhttps://touchdexterity.github.io.\n","authors":["Zhao-Heng Yin","Binghao Huang","Yuzhe Qin","Qifeng Chen","Xiaolong Wang"],"pdf_url":"https://arxiv.org/pdf/2303.10880v3.pdf","comment":"Project page: https://touchdexterity.github.io"},{"id":"http://arxiv.org/abs/2303.10382v2","updated":"2023-03-22T14:19:24Z","published":"2023-03-18T10:13:32Z","title":"Interpretable Reinforcement Learning via Neural Additive Models for\n  Inventory Management","summary":"  The COVID-19 pandemic has highlighted the importance of supply chains and the\nrole of digital management to react to dynamic changes in the environment. In\nthis work, we focus on developing dynamic inventory ordering policies for a\nmulti-echelon, i.e. multi-stage, supply chain. Traditional inventory\noptimization methods aim to determine a static reordering policy. Thus, these\npolicies are not able to adjust to dynamic changes such as those observed\nduring the COVID-19 crisis. On the other hand, conventional strategies offer\nthe advantage of being interpretable, which is a crucial feature for supply\nchain managers in order to communicate decisions to their stakeholders. To\naddress this limitation, we propose an interpretable reinforcement learning\napproach that aims to be as interpretable as the traditional static policies\nwhile being as flexible and environment-agnostic as other deep learning-based\nreinforcement learning solutions. We propose to use Neural Additive Models as\nan interpretable dynamic policy of a reinforcement learning agent, showing that\nthis approach is competitive with a standard full connected policy. Finally, we\nuse the interpretability property to gain insights into a complex ordering\nstrategy for a simple, linear three-echelon inventory supply chain.\n","authors":["Julien Siems","Maximilian Schambach","Sebastian Schulze","Johannes S. Otterbach"],"pdf_url":"https://arxiv.org/pdf/2303.10382v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.12589v1","updated":"2023-03-22T14:19:06Z","published":"2023-03-22T14:19:06Z","title":"Do Backdoors Assist Membership Inference Attacks?","summary":"  When an adversary provides poison samples to a machine learning model,\nprivacy leakage, such as membership inference attacks that infer whether a\nsample was included in the training of the model, becomes effective by moving\nthe sample to an outlier. However, the attacks can be detected because\ninference accuracy deteriorates due to poison samples. In this paper, we\ndiscuss a \\textit{backdoor-assisted membership inference attack}, a novel\nmembership inference attack based on backdoors that return the adversary's\nexpected output for a triggered sample. We found three crucial insights through\nexperiments with an academic benchmark dataset. We first demonstrate that the\nbackdoor-assisted membership inference attack is unsuccessful. Second, when we\nanalyzed loss distributions to understand the reason for the unsuccessful\nresults, we found that backdoors cannot separate loss distributions of training\nand non-training samples. In other words, backdoors cannot affect the\ndistribution of clean samples. Third, we also show that poison and triggered\nsamples activate neurons of different distributions. Specifically, backdoors\nmake any clean sample an inlier, contrary to poisoning samples. As a result, we\nconfirm that backdoors cannot assist membership inference.\n","authors":["Yumeki Goto","Nami Ashizawa","Toshiki Shibahara","Naoto Yanai"],"pdf_url":"https://arxiv.org/pdf/2303.12589v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2211.05568v3","updated":"2023-03-22T14:08:17Z","published":"2022-11-10T13:44:57Z","title":"Unbiased Supervised Contrastive Learning","summary":"  Many datasets are biased, namely they contain easy-to-learn features that are\nhighly correlated with the target class only in the dataset but not in the true\nunderlying distribution of the data. For this reason, learning unbiased models\nfrom biased data has become a very relevant research topic in the last years.\nIn this work, we tackle the problem of learning representations that are robust\nto biases. We first present a margin-based theoretical framework that allows us\nto clarify why recent contrastive losses (InfoNCE, SupCon, etc.) can fail when\ndealing with biased data. Based on that, we derive a novel formulation of the\nsupervised contrastive loss (epsilon-SupInfoNCE), providing more accurate\ncontrol of the minimal distance between positive and negative samples.\nFurthermore, thanks to our theoretical framework, we also propose FairKL, a new\ndebiasing regularization loss, that works well even with extremely biased data.\nWe validate the proposed losses on standard vision datasets including CIFAR10,\nCIFAR100, and ImageNet, and we assess the debiasing capability of FairKL with\nepsilon-SupInfoNCE, reaching state-of-the-art performance on a number of biased\ndatasets, including real instances of biases in the wild.\n","authors":["Carlo Alberto Barbano","Benoit Dufumier","Enzo Tartaglione","Marco Grangetto","Pietro Gori"],"pdf_url":"https://arxiv.org/pdf/2211.05568v3.pdf","comment":"Accepted at ICLR 2023"},{"id":"http://arxiv.org/abs/2303.12578v1","updated":"2023-03-22T14:03:23Z","published":"2023-03-22T14:03:23Z","title":"Neuro-Symbolic Reasoning Shortcuts: Mitigation Strategies and their\n  Limitations","summary":"  Neuro-symbolic predictors learn a mapping from sub-symbolic inputs to\nhigher-level concepts and then carry out (probabilistic) logical inference on\nthis intermediate representation. This setup offers clear advantages in terms\nof consistency to symbolic prior knowledge, and is often believed to provide\ninterpretability benefits in that - by virtue of complying with the knowledge -\nthe learned concepts can be better understood by human stakeholders. However,\nit was recently shown that this setup is affected by reasoning shortcuts\nwhereby predictions attain high accuracy by leveraging concepts with unintended\nsemantics, yielding poor out-of-distribution performance and compromising\ninterpretability. In this short paper, we establish a formal link between\nreasoning shortcuts and the optima of the loss function, and identify\nsituations in which reasoning shortcuts can arise. Based on this, we discuss\nlimitations of natural mitigation strategies such as reconstruction and concept\nsupervision.\n","authors":["Emanuele Marconato","Stefano Teso","Andrea Passerini"],"pdf_url":"https://arxiv.org/pdf/2303.12578v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.12558v1","updated":"2023-03-22T13:41:42Z","published":"2023-03-22T13:41:42Z","title":"Wasserstein Auto-encoded MDPs: Formal Verification of Efficiently\n  Distilled RL Policies with Many-sided Guarantees","summary":"  Although deep reinforcement learning (DRL) has many success stories, the\nlarge-scale deployment of policies learned through these advanced techniques in\nsafety-critical scenarios is hindered by their lack of formal guarantees.\nVariational Markov Decision Processes (VAE-MDPs) are discrete latent space\nmodels that provide a reliable framework for distilling formally verifiable\ncontrollers from any RL policy. While the related guarantees address relevant\npractical aspects such as the satisfaction of performance and safety\nproperties, the VAE approach suffers from several learning flaws (posterior\ncollapse, slow learning speed, poor dynamics estimates), primarily due to the\nabsence of abstraction and representation guarantees to support latent\noptimization. We introduce the Wasserstein auto-encoded MDP (WAE-MDP), a latent\nspace model that fixes those issues by minimizing a penalized form of the\noptimal transport between the behaviors of the agent executing the original\npolicy and the distilled policy, for which the formal guarantees apply. Our\napproach yields bisimulation guarantees while learning the distilled policy,\nallowing concrete optimization of the abstraction and representation model\nquality. Our experiments show that, besides distilling policies up to 10 times\nfaster, the latent model quality is indeed better in general. Moreover, we\npresent experiments from a simple time-to-failure verification algorithm on the\nlatent space. The fact that our approach enables such simple verification\ntechniques highlights its applicability.\n","authors":["Florent Delgrange","Ann Nowé","Guillermo A. Pérez"],"pdf_url":"https://arxiv.org/pdf/2303.12558v1.pdf","comment":"ICLR 2023, 9 pages main text, 14 pages appendix (excluding\n  references)"},{"id":"http://arxiv.org/abs/2108.02235v3","updated":"2023-03-22T13:26:59Z","published":"2021-08-04T18:29:42Z","title":"Dynamic Relevance Learning for Few-Shot Object Detection","summary":"  Expensive bounding-box annotations have limited the development of object\ndetection task. Thus, it is necessary to focus on more challenging task of\nfew-shot object detection. It requires the detector to recognize objects of\nnovel classes with only a few training samples. Nowadays, many existing popular\nmethods adopting training way similar to meta-learning have achieved promising\nperformance, such as Meta R-CNN series. However, support data is only used as\nthe class attention to guide the detecting of query images each time. Their\nrelevance to each other remains unexploited. Moreover, a lot of recent works\ntreat the support data and query images as independent branch without\nconsidering the relationship between them. To address this issue, we propose a\ndynamic relevance learning model, which utilizes the relationship between all\nsupport images and Region of Interest (RoI) on the query images to construct a\ndynamic graph convolutional network (GCN). By adjusting the prediction\ndistribution of the base detector using the output of this GCN, the proposed\nmodel serves as a hard auxiliary classification task, which guides the detector\nto improve the class representation implicitly. Comprehensive experiments have\nbeen conducted on Pascal VOC and MS-COCO dataset. The proposed model achieves\nthe best overall performance, which shows its effectiveness of learning more\ngeneralized features. Our code is available at\nhttps://github.com/liuweijie19980216/DRL-for-FSOD.\n","authors":["Weijie Liu","Chong Wang","Haohe Li","Shenghao Yu","Jiafei Wu"],"pdf_url":"https://arxiv.org/pdf/2108.02235v3.pdf","comment":"12 pages, 8 figures, 7 tables"},{"id":"http://arxiv.org/abs/2303.09986v2","updated":"2023-03-22T13:24:45Z","published":"2023-03-17T14:02:35Z","title":"Towards AI-controlled FES-restoration of movements: Learning cycling\n  stimulation pattern with reinforcement learning","summary":"  Functional electrical stimulation (FES) has been increasingly integrated with\nother rehabilitation devices, including robots. FES cycling is one of the\ncommon FES applications in rehabilitation, which is performed by stimulating\nleg muscles in a certain pattern. The appropriate pattern varies across\nindividuals and requires manual tuning which can be time-consuming and\nchallenging for the individual user. Here, we present an AI-based method for\nfinding the patterns, which requires no extra hardware or sensors. Our method\nhas two phases, starting with finding model-based patterns using reinforcement\nlearning and detailed musculoskeletal models. The models, built using\nopen-source software, can be customised through our automated script and can be\ntherefore used by non-technical individuals without extra cost. Next, our\nmethod fine-tunes the pattern using real cycling data. We test our both in\nsimulation and experimentally on a stationary tricycle. In the simulation test,\nour method can robustly deliver model-based patterns for different cycling\nconfigurations. The experimental evaluation shows that our method can find a\nmodel-based pattern that induces higher cycling speed than an EMG-based\npattern. By using just 100 seconds of cycling data, our method can deliver a\nfine-tuned pattern that gives better cycling performance. Beyond FES cycling,\nthis work is a showcase, displaying the feasibility and potential of\nhuman-in-the-loop AI in real-world rehabilitation.\n","authors":["Nat Wannawas","A. Aldo Faisal"],"pdf_url":"https://arxiv.org/pdf/2303.09986v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.12540v1","updated":"2023-03-22T13:16:37Z","published":"2023-03-22T13:16:37Z","title":"Deployment of Image Analysis Algorithms under Prevalence Shifts","summary":"  Domain gaps are among the most relevant roadblocks in the clinical\ntranslation of machine learning (ML)-based solutions for medical image\nanalysis. While current research focuses on new training paradigms and network\narchitectures, little attention is given to the specific effect of prevalence\nshifts on an algorithm deployed in practice. Such discrepancies between class\nfrequencies in the data used for a method's development/validation and that in\nits deployment environment(s) are of great importance, for example in the\ncontext of artificial intelligence (AI) democratization, as disease prevalences\nmay vary widely across time and location. Our contribution is twofold. First,\nwe empirically demonstrate the potentially severe consequences of missing\nprevalence handling by analyzing (i) the extent of miscalibration, (ii) the\ndeviation of the decision threshold from the optimum, and (iii) the ability of\nvalidation metrics to reflect neural network performance on the deployment\npopulation as a function of the discrepancy between development and deployment\nprevalence. Second, we propose a workflow for prevalence-aware image\nclassification that uses estimated deployment prevalences to adjust a trained\nclassifier to a new environment, without requiring additional annotated\ndeployment data. Comprehensive experiments based on a diverse set of 30 medical\nclassification tasks showcase the benefit of the proposed workflow in\ngenerating better classifier decisions and more reliable performance estimates\ncompared to current practice.\n","authors":["Patrick Godau","Piotr Kalinowski","Evangelia Christodoulou","Annika Reinke","Minu Tizabi","Luciana Ferrer","Paul Jäger","Lena Maier-Hein"],"pdf_url":"https://arxiv.org/pdf/2303.12540v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2210.17323v2","updated":"2023-03-22T13:10:47Z","published":"2022-10-31T13:42:40Z","title":"GPTQ: Accurate Post-Training Quantization for Generative Pre-trained\n  Transformers","summary":"  Generative Pre-trained Transformer models, known as GPT or OPT, set\nthemselves apart through breakthrough performance across complex language\nmodelling tasks, but also by their extremely high computational and storage\ncosts. Specifically, due to their massive size, even inference for large,\nhighly-accurate GPT models may require multiple performant GPUs, which limits\nthe usability of such models. While there is emerging work on relieving this\npressure via model compression, the applicability and performance of existing\ncompression techniques is limited by the scale and complexity of GPT models. In\nthis paper, we address this challenge, and propose GPTQ, a new one-shot weight\nquantization method based on approximate second-order information, that is both\nhighly-accurate and highly-efficient. Specifically, GPTQ can quantize GPT\nmodels with 175 billion parameters in approximately four GPU hours, reducing\nthe bitwidth down to 3 or 4 bits per weight, with negligible accuracy\ndegradation relative to the uncompressed baseline. Our method more than doubles\nthe compression gains relative to previously-proposed one-shot quantization\nmethods, preserving accuracy, allowing us for the first time to execute an 175\nbillion-parameter model inside a single GPU for generative inference. Moreover,\nwe also show that our method can still provide reasonable accuracy in the\nextreme quantization regime, in which weights are quantized to 2-bit or even\nternary quantization levels. We show experimentally that these improvements can\nbe leveraged for end-to-end inference speedups over FP16, of around 3.25x when\nusing high-end GPUs (NVIDIA A100) and 4.5x when using more cost-effective ones\n(NVIDIA A6000). The implementation is available at\nhttps://github.com/IST-DASLab/gptq.\n","authors":["Elias Frantar","Saleh Ashkboos","Torsten Hoefler","Dan Alistarh"],"pdf_url":"https://arxiv.org/pdf/2210.17323v2.pdf","comment":"ICLR 2023"},{"id":"http://arxiv.org/abs/2303.12524v1","updated":"2023-03-22T13:00:00Z","published":"2023-03-22T13:00:00Z","title":"Split-Et-Impera: A Framework for the Design of Distributed Deep Learning\n  Applications","summary":"  Many recent pattern recognition applications rely on complex distributed\narchitectures in which sensing and computational nodes interact together\nthrough a communication network. Deep neural networks (DNNs) play an important\nrole in this scenario, furnishing powerful decision mechanisms, at the price of\na high computational effort. Consequently, powerful state-of-the-art DNNs are\nfrequently split over various computational nodes, e.g., a first part stays on\nan embedded device and the rest on a server. Deciding where to split a DNN is a\nchallenge in itself, making the design of deep learning applications even more\ncomplicated. Therefore, we propose Split-Et-Impera, a novel and practical\nframework that i) determines the set of the best-split points of a neural\nnetwork based on deep network interpretability principles without performing a\ntedious try-and-test approach, ii) performs a communication-aware simulation\nfor the rapid evaluation of different neural network rearrangements, and iii)\nsuggests the best match between the quality of service requirements of the\napplication and the performance in terms of accuracy and latency time.\n","authors":["Luigi Capogrosso","Federico Cunico","Michele Lora","Marco Cristani","Franco Fummi","Davide Quaglia"],"pdf_url":"https://arxiv.org/pdf/2303.12524v1.pdf","comment":"26th International Symposium on Design and Diagnostics of Electronic\n  Circuits and Systems (DDECS)"},{"id":"http://arxiv.org/abs/2210.01081v2","updated":"2023-03-22T12:53:18Z","published":"2022-10-03T16:51:12Z","title":"On The Effects Of Data Normalisation For Domain Adaptation On EEG Data","summary":"  In the Machine Learning (ML) literature, a well-known problem is the Dataset\nShift problem where, differently from the ML standard hypothesis, the data in\nthe training and test sets can follow different probability distributions,\nleading ML systems toward poor generalisation performances. This problem is\nintensely felt in the Brain-Computer Interface (BCI) context, where bio-signals\nas Electroencephalographic (EEG) are often used. In fact, EEG signals are\nhighly non-stationary both over time and between different subjects. To\novercome this problem, several proposed solutions are based on recent transfer\nlearning approaches such as Domain Adaption (DA). In several cases, however,\nthe actual causes of the improvements remain ambiguous. This paper focuses on\nthe impact of data normalisation, or standardisation strategies applied\ntogether with DA methods. In particular, using \\textit{SEED}, \\textit{DEAP},\nand \\textit{BCI Competition IV 2a} EEG datasets, we experimentally evaluated\nthe impact of different normalization strategies applied with and without\nseveral well-known DA methods, comparing the obtained performances. It results\nthat the choice of the normalisation strategy plays a key role on the\nclassifier performances in DA scenarios, and interestingly, in several cases,\nthe use of only an appropriate normalisation schema outperforms the DA\ntechnique.\n","authors":["Andrea Apicella","Francesco Isgrò","Andrea Pollastro","Roberto Prevete"],"pdf_url":"https://arxiv.org/pdf/2210.01081v2.pdf","comment":"Accepted to be published in its final version on Engineering\n  Applications of Artificial Intelligence (EAAI)"},{"id":"http://arxiv.org/abs/2206.10555v2","updated":"2023-03-22T12:43:10Z","published":"2022-06-21T17:35:57Z","title":"LargeKernel3D: Scaling up Kernels in 3D Sparse CNNs","summary":"  Recent advance in 2D CNNs has revealed that large kernels are important.\nHowever, when directly applying large convolutional kernels in 3D CNNs, severe\ndifficulties are met, where those successful module designs in 2D become\nsurprisingly ineffective on 3D networks, including the popular depth-wise\nconvolution. To address this vital challenge, we instead propose the\nspatial-wise partition convolution and its large-kernel module. As a result, it\navoids the optimization and efficiency issues of naive 3D large kernels. Our\nlarge-kernel 3D CNN network, LargeKernel3D, yields notable improvement in 3D\ntasks of semantic segmentation and object detection. It achieves 73.9% mIoU on\nthe ScanNetv2 semantic segmentation and 72.8% NDS nuScenes object detection\nbenchmarks, ranking 1st on the nuScenes LIDAR leaderboard. The performance\nfurther boosts to 74.2% NDS with a simple multi-modal fusion. In addition,\nLargeKernel3D can be scaled to 17x17x17 kernel size on Waymo 3D object\ndetection. For the first time, we show that large kernels are feasible and\nessential for 3D visual tasks.\n","authors":["Yukang Chen","Jianhui Liu","Xiangyu Zhang","Xiaojuan Qi","Jiaya Jia"],"pdf_url":"https://arxiv.org/pdf/2206.10555v2.pdf","comment":"In CVPR 2023. Code is at\n  https://github.com/dvlab-research/LargeKernel3D"},{"id":"http://arxiv.org/abs/2301.00774v3","updated":"2023-03-22T12:33:46Z","published":"2023-01-02T17:48:56Z","title":"SparseGPT: Massive Language Models Can Be Accurately Pruned in One-Shot","summary":"  We show for the first time that large-scale generative pretrained transformer\n(GPT) family models can be pruned to at least 50% sparsity in one-shot, without\nany retraining, at minimal loss of accuracy. This is achieved via a new pruning\nmethod called SparseGPT, specifically designed to work efficiently and\naccurately on massive GPT-family models. We can execute SparseGPT on the\nlargest available open-source models, OPT-175B and BLOOM-176B, in under 4.5\nhours, and can reach 60% unstructured sparsity with negligible increase in\nperplexity: remarkably, more than 100 billion weights from these models can be\nignored at inference time. SparseGPT generalizes to semi-structured (2:4 and\n4:8) patterns, and is compatible with weight quantization approaches. The code\nis available at: https://github.com/IST-DASLab/sparsegpt.\n","authors":["Elias Frantar","Dan Alistarh"],"pdf_url":"https://arxiv.org/pdf/2301.00774v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.12499v1","updated":"2023-03-22T12:10:44Z","published":"2023-03-22T12:10:44Z","title":"On Domain-Specific Pre-Training for Effective Semantic Perception in\n  Agricultural Robotics","summary":"  Agricultural robots have the prospect to enable more efficient and\nsustainable agricultural production of food, feed, and fiber. Perception of\ncrops and weeds is a central component of agricultural robots that aim to\nmonitor fields and assess the plants as well as their growth stage in an\nautomatic manner. Semantic perception mostly relies on deep learning using\nsupervised approaches, which require time and qualified workers to label fairly\nlarge amounts of data. In this paper, we look into the problem of reducing the\namount of labels without compromising the final segmentation performance. For\nrobots operating in the field, pre-training networks in a supervised way is\nalready a popular method to reduce the number of required labeled images. We\ninvestigate the possibility of pre-training in a self-supervised fashion using\ndata from the target domain. To better exploit this data, we propose a set of\ndomain-specific augmentation strategies. We evaluate our pre-training on\nsemantic segmentation and leaf instance segmentation, two important tasks in\nour domain. The experimental results suggest that pre-training with\ndomain-specific data paired with our data augmentation strategy leads to\nsuperior performance compared to commonly used pre-trainings. Furthermore, the\npre-trained networks obtain similar performance to the fully supervised with\nless labeled data.\n","authors":["Gianmarco Roggiolani","Federico Magistri","Tiziano Guadagnino","Jan Weyler","Giorgio Grisetti","Cyrill Stachniss","Jens Behley"],"pdf_url":"https://arxiv.org/pdf/2303.12499v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.12497v1","updated":"2023-03-22T12:09:12Z","published":"2023-03-22T12:09:12Z","title":"Lower Bound on the Bayesian Risk via Information Measure","summary":"  This paper focuses on parameter estimation and introduces a new method for\nlower bounding the Bayesian risk. The method allows for the use of virtually\n\\emph{any} information measure, including R\\'enyi's $\\alpha$,\n$\\varphi$-Divergences, and Sibson's $\\alpha$-Mutual Information. The approach\nconsiders divergences as functionals of measures and exploits the duality\nbetween spaces of measures and spaces of functions. In particular, we show that\none can lower bound the risk with any information measure by upper bounding its\ndual via Markov's inequality. We are thus able to provide estimator-independent\nimpossibility results thanks to the Data-Processing Inequalities that\ndivergences satisfy. The results are then applied to settings of interest\ninvolving both discrete and continuous parameters, including the\n``Hide-and-Seek'' problem, and compared to the state-of-the-art techniques. An\nimportant observation is that the behaviour of the lower bound in the number of\nsamples is influenced by the choice of the information measure. We leverage\nthis by introducing a new divergence inspired by the ``Hockey-Stick''\nDivergence, which is demonstrated empirically to provide the largest\nlower-bound across all considered settings. If the observations are subject to\nprivatisation, stronger impossibility results can be obtained via Strong\nData-Processing Inequalities. The paper also discusses some generalisations and\nalternative directions.\n","authors":["Amedeo Roberto Esposito","Adrien Vandenbroucque","Michael Gastpar"],"pdf_url":"https://arxiv.org/pdf/2303.12497v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.12481v1","updated":"2023-03-22T11:49:35Z","published":"2023-03-22T11:49:35Z","title":"Revisiting DeepFool: generalization and improvement","summary":"  Deep neural networks have been known to be vulnerable to adversarial\nexamples, which are inputs that are modified slightly to fool the network into\nmaking incorrect predictions. This has led to a significant amount of research\non evaluating the robustness of these networks against such perturbations. One\nparticularly important robustness metric is the robustness to minimal l2\nadversarial perturbations. However, existing methods for evaluating this\nrobustness metric are either computationally expensive or not very accurate. In\nthis paper, we introduce a new family of adversarial attacks that strike a\nbalance between effectiveness and computational efficiency. Our proposed\nattacks are generalizations of the well-known DeepFool (DF) attack, while they\nremain simple to understand and implement. We demonstrate that our attacks\noutperform existing methods in terms of both effectiveness and computational\nefficiency. Our proposed attacks are also suitable for evaluating the\nrobustness of large models and can be used to perform adversarial training (AT)\nto achieve state-of-the-art robustness to minimal l2 adversarial perturbations.\n","authors":["Alireza Abdollahpourrostam","Mahed Abroshan","Seyed-Mohsen Moosavi-Dezfooli"],"pdf_url":"https://arxiv.org/pdf/2303.12481v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2206.04192v2","updated":"2023-03-22T11:20:01Z","published":"2022-06-08T23:34:39Z","title":"ExpressivE: A Spatio-Functional Embedding For Knowledge Graph Completion","summary":"  Knowledge graphs are inherently incomplete. Therefore substantial research\nhas been directed toward knowledge graph completion (KGC), i.e., predicting\nmissing triples from the information represented in the knowledge graph (KG).\nKG embedding models (KGEs) have yielded promising results for KGC, yet any\ncurrent KGE is incapable of: (1) fully capturing vital inference patterns\n(e.g., composition), (2) capturing prominent patterns jointly (e.g., hierarchy\nand composition), and (3) providing an intuitive interpretation of captured\npatterns. In this work, we propose ExpressivE, a fully expressive\nspatio-functional KGE that solves all these challenges simultaneously.\nExpressivE embeds pairs of entities as points and relations as\nhyper-parallelograms in the virtual triple space $\\mathbb{R}^{2d}$. This model\ndesign allows ExpressivE not only to capture a rich set of inference patterns\njointly but additionally to display any supported inference pattern through the\nspatial relation of hyper-parallelograms, offering an intuitive and consistent\ngeometric interpretation of ExpressivE embeddings and their captured patterns.\nExperimental results on standard KGC benchmarks reveal that ExpressivE is\ncompetitive with state-of-the-art KGEs and even significantly outperforms them\non WN18RR.\n","authors":["Aleksandar Pavlović","Emanuel Sallinger"],"pdf_url":"https://arxiv.org/pdf/2206.04192v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2208.01918v4","updated":"2023-03-22T11:15:58Z","published":"2022-08-03T08:54:13Z","title":"DeepProphet2 -- A Deep Learning Gene Recommendation Engine","summary":"  New powerful tools for tackling life science problems have been created by\nrecent advances in machine learning. The purpose of the paper is to discuss the\npotential advantages of gene recommendation performed by artificial\nintelligence (AI). Indeed, gene recommendation engines try to solve this\nproblem: if the user is interested in a set of genes, which other genes are\nlikely to be related to the starting set and should be investigated? This task\nwas solved with a custom deep learning recommendation engine, DeepProphet2\n(DP2), which is freely available to researchers worldwide via\nhttps://www.generecommender.com?utm_source=DeepProphet2_paper&utm_medium=pdf.\nHereafter, insights behind the algorithm and its practical applications are\nillustrated.\n  The gene recommendation problem can be addressed by mapping the genes to a\nmetric space where a distance can be defined to represent the real semantic\ndistance between them. To achieve this objective a transformer-based model has\nbeen trained on a well-curated freely available paper corpus, PubMed. The paper\ndescribes multiple optimization procedures that were employed to obtain the\nbest bias-variance trade-off, focusing on embedding size and network depth. In\nthis context, the model's ability to discover sets of genes implicated in\ndiseases and pathways was assessed through cross-validation. A simple\nassumption guided the procedure: the network had no direct knowledge of\npathways and diseases but learned genes' similarities and the interactions\namong them. Moreover, to further investigate the space where the neural network\nrepresents genes, the dimensionality of the embedding was reduced, and the\nresults were projected onto a human-comprehensible space. In conclusion, a set\nof use cases illustrates the algorithm's potential applications in a real word\nsetting.\n","authors":["Daniele Brambilla","Davide Maria Giacomini","Luca Muscarnera","Andrea Mazzoleni"],"pdf_url":"https://arxiv.org/pdf/2208.01918v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.09975v2","updated":"2023-03-22T11:14:08Z","published":"2023-03-17T13:48:17Z","title":"MedNeXt: Transformer-driven Scaling of ConvNets for Medical Image\n  Segmentation","summary":"  There has been exploding interest in embracing Transformer-based\narchitectures for medical image segmentation. However, the lack of large-scale\nannotated medical datasets make achieving performances equivalent to those in\nnatural images challenging. Convolutional networks, in contrast, have higher\ninductive biases and consequently, are easily trainable to high performance.\nRecently, the ConvNeXt architecture attempted to modernize the standard ConvNet\nby mirroring Transformer blocks. In this work, we improve upon this to design a\nmodernized and scalable convolutional architecture customized to challenges of\ndata-scarce medical settings. We introduce MedNeXt, a Transformer-inspired\nlarge kernel segmentation network which introduces - 1) A fully ConvNeXt 3D\nEncoder-Decoder Network for medical image segmentation, 2) Residual ConvNeXt up\nand downsampling blocks to preserve semantic richness across scales, 3) A novel\ntechnique to iteratively increase kernel sizes by upsampling small kernel\nnetworks, to prevent performance saturation on limited medical data, 4)\nCompound scaling at multiple levels (depth, width, kernel size) of MedNeXt.\nThis leads to state-of-the-art performance on 4 tasks on CT and MRI modalities\nand varying dataset sizes, representing a modernized deep architecture for\nmedical image segmentation.\n","authors":["Saikat Roy","Gregor Koehler","Constantin Ulrich","Michael Baumgartner","Jens Petersen","Fabian Isensee","Paul F. Jaeger","Klaus Maier-Hein"],"pdf_url":"https://arxiv.org/pdf/2303.09975v2.pdf","comment":"12 pages"},{"id":"http://arxiv.org/abs/2303.12454v1","updated":"2023-03-22T10:52:21Z","published":"2023-03-22T10:52:21Z","title":"$\\mathcal{C}^k$-continuous Spline Approximation with TensorFlow Gradient\n  Descent Optimizers","summary":"  In this work we present an \"out-of-the-box\" application of Machine Learning\n(ML) optimizers for an industrial optimization problem. We introduce a\npiecewise polynomial model (spline) for fitting of $\\mathcal{C}^k$-continuos\nfunctions, which can be deployed in a cam approximation setting. We then use\nthe gradient descent optimization context provided by the machine learning\nframework TensorFlow to optimize the model parameters with respect to\napproximation quality and $\\mathcal{C}^k$-continuity and evaluate available\noptimizers. Our experiments show that the problem solution is feasible using\nTensorFlow gradient tapes and that AMSGrad and SGD show the best results among\navailable TensorFlow optimizers. Furthermore, we introduce a novel\nregularization approach to improve SGD convergence. Although experiments show\nthat remaining discontinuities after optimization are small, we can eliminate\nthese errors using a presented algorithm which has impact only on affected\nderivatives in the local spline segment.\n","authors":["Stefan Huber","Hannes Waclawek"],"pdf_url":"https://arxiv.org/pdf/2303.12454v1.pdf","comment":"This preprint has not undergone peer review or any post-submission\n  improvements or corrections. The Version of Record of this contribution is\n  published in Computer Aided Systems Theory - EUROCAST 2022 and is available\n  online at https://doi.org/10.1007/978-3-031-25312-6_68"},{"id":"http://arxiv.org/abs/2303.12440v1","updated":"2023-03-22T10:20:56Z","published":"2023-03-22T10:20:56Z","title":"Learning Human-Inspired Force Strategies for Robotic Assembly","summary":"  The programming of robotic assembly tasks is a key component in manufacturing\nand automation. Force-sensitive assembly, however, often requires reactive\nstrategies to handle slight changes in positioning and unforeseen part jamming.\nLearning such strategies from human performance is a promising approach, but\nfaces two common challenges: the handling of low part clearances which is\ndifficult to capture from demonstrations and learning intuitive strategies\noffline without access to the real hardware. We address these two challenges by\nlearning probabilistic force strategies from data that are easily acquired\noffline in a robot-less simulation from human demonstrations with a joystick.\nWe combine a Long Short Term Memory (LSTM) and a Mixture Density Network (MDN)\nto model human-inspired behavior in such a way that the learned strategies\ntransfer easily onto real hardware. The experiments show a UR10e robot that\ncompletes a plastic assembly with clearances of less than 100 micrometers whose\nstrategies were solely demonstrated in simulation.\n","authors":["Stefan Scherzinger","Arne Roennau","Rüdiger Dillmann"],"pdf_url":"https://arxiv.org/pdf/2303.12440v1.pdf","comment":"8 pages, 8 figures. Submitted to the IEEE International Conference on\n  Automation Science and Engineering (CASE) 2023"},{"id":"http://arxiv.org/abs/2211.12872v3","updated":"2023-03-22T10:05:15Z","published":"2022-11-23T11:26:24Z","title":"μSplit: efficient image decomposition for microscopy data","summary":"  We present uSplit, a dedicated approach for trained image decomposition in\nthe context of fluorescence microscopy images. We find that best results using\nregular deep architectures are achieved when large image patches are used\nduring training, making memory consumption the limiting factor to further\nimproving performance. We therefore introduce lateral contextualization (LC), a\nmemory efficient way to train powerful networks and show that LC leads to\nconsistent and significant improvements on the task at hand. We integrate LC\nwith U-Nets, Hierarchical AEs, and Hierarchical VAEs, for which we formulate a\nmodified ELBO loss. Additionally, LC enables training deeper hierarchical\nmodels than otherwise possible and, interestingly, helps to reduce tiling\nartefacts that are inherently impossible to avoid when using tiled VAE\npredictions. We apply uSplit to five decomposition tasks, one on a synthetic\ndataset, four others derived from real microscopy data. LC achieves SOTA\nresults (average improvements to the best baseline of 2.36 dB PSNR), while\nsimultaneously requiring considerably less GPU memory.\n","authors":[" Ashesh","Alexander Krull","Moises Di Sante","Francesco Silvio Pasqualini","Florian Jug"],"pdf_url":"https://arxiv.org/pdf/2211.12872v3.pdf","comment":"10 pages, 7 figures, 9 pages supplement, 8 supplementary figures"},{"id":"http://arxiv.org/abs/2302.08757v2","updated":"2023-03-22T09:54:23Z","published":"2023-02-17T08:41:31Z","title":"Enhanced Sampling of Configuration and Path Space in a Generalized\n  Ensemble by Shooting Point Exchange","summary":"  The computer simulation of many molecular processes is complicated by long\ntime scales caused by rare transitions between long-lived states. Here, we\npropose a new approach to simulate such rare events, which combines transition\npath sampling with enhanced exploration of configuration space. The method\nrelies on exchange moves between configuration and trajectory space, carried\nout based on a generalized ensemble. This scheme substantially enhances the\nefficiency of the transition path sampling simulations, particularly for\nsystems with multiple transition channels, and yields information on\nthermodynamics, kinetics and reaction coordinates of molecular processes\nwithout distorting their dynamics. The method is illustrated using the\nisomerization of proline in the KPTP tetrapeptide.\n","authors":["Sebastian Falkner","Alessandro Coretti","Christoph Dellago"],"pdf_url":"https://arxiv.org/pdf/2302.08757v2.pdf","comment":"Added Supplementary Information for simulation details and network\n  parameters"},{"id":"http://arxiv.org/abs/2301.12876v2","updated":"2023-03-22T09:52:25Z","published":"2023-01-30T13:30:56Z","title":"Guiding Online Reinforcement Learning with Action-Free Offline\n  Pretraining","summary":"  Offline RL methods have been shown to reduce the need for environment\ninteraction by training agents using offline collected episodes. However, these\nmethods typically require action information to be logged during data\ncollection, which can be difficult or even impossible in some practical cases.\nIn this paper, we investigate the potential of using action-free offline\ndatasets to improve online reinforcement learning, name this problem\nReinforcement Learning with Action-Free Offline Pretraining (AFP-RL). We\nintroduce Action-Free Guide (AF-Guide), a method that guides online training by\nextracting knowledge from action-free offline datasets. AF-Guide consists of an\nAction-Free Decision Transformer (AFDT) implementing a variant of Upside-Down\nReinforcement Learning. It learns to plan the next states from the offline\ndataset, and a Guided Soft Actor-Critic (Guided SAC) that learns online with\nguidance from AFDT. Experimental results show that AF-Guide can improve sample\nefficiency and performance in online training thanks to the knowledge from the\naction-free offline dataset. Code is available at\nhttps://github.com/Vision-CAIR/AF-Guide.\n","authors":["Deyao Zhu","Yuhui Wang","Jürgen Schmidhuber","Mohamed Elhoseiny"],"pdf_url":"https://arxiv.org/pdf/2301.12876v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.12424v1","updated":"2023-03-22T09:51:08Z","published":"2023-03-22T09:51:08Z","title":"Unsupervised Domain Adaptation for Training Event-Based Networks Using\n  Contrastive Learning and Uncorrelated Conditioning","summary":"  Event-based cameras offer reliable measurements for preforming computer\nvision tasks in high-dynamic range environments and during fast motion\nmaneuvers. However, adopting deep learning in event-based vision faces the\nchallenge of annotated data scarcity due to recency of event cameras.\nTransferring the knowledge that can be obtained from conventional camera\nannotated data offers a practical solution to this challenge. We develop an\nunsupervised domain adaptation algorithm for training a deep network for\nevent-based data image classification using contrastive learning and\nuncorrelated conditioning of data. Our solution outperforms the existing\nalgorithms for this purpose.\n","authors":["Dayuan Jian","Mohammad Rostami"],"pdf_url":"https://arxiv.org/pdf/2303.12424v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.12414v1","updated":"2023-03-22T09:23:29Z","published":"2023-03-22T09:23:29Z","title":"Delay-Aware Hierarchical Federated Learning","summary":"  Federated learning has gained popularity as a means of training models\ndistributed across the wireless edge. The paper introduces delay-aware\nfederated learning (DFL) to improve the efficiency of distributed machine\nlearning (ML) model training by addressing communication delays between edge\nand cloud. DFL employs multiple stochastic gradient descent iterations on\ndevice datasets during each global aggregation interval and intermittently\naggregates model parameters through edge servers in local subnetworks. The\ncloud server synchronizes the local models with the global deployed model\ncomputed via a local-global combiner at global synchronization. The convergence\nbehavior of DFL is theoretically investigated under a generalized data\nheterogeneity metric. A set of conditions is obtained to achieve the sub-linear\nconvergence rate of O(1/k). Based on these findings, an adaptive control\nalgorithm is developed for DFL, implementing policies to mitigate energy\nconsumption and edge-to-cloud communication latency while aiming for a\nsublinear convergence rate. Numerical evaluations show DFL's superior\nperformance in terms of faster global model convergence, reduced resource\nconsumption, and robustness against communication delays compared to existing\nFL algorithms. In summary, this proposed method offers improved efficiency and\nsatisfactory results when dealing with both convex and non-convex loss\nfunctions.\n","authors":["Frank Po-Chen Lin","Seyyedali Hosseinalipour","Christopher Brinton","Nicolò Michelusi"],"pdf_url":"https://arxiv.org/pdf/2303.12414v1.pdf","comment":"A condensed version of this paper was presented at IEEE Globecom 2020"},{"id":"http://arxiv.org/abs/2303.12410v1","updated":"2023-03-22T09:19:39Z","published":"2023-03-22T09:19:39Z","title":"EDGI: Equivariant Diffusion for Planning with Embodied Agents","summary":"  Embodied agents operate in a structured world, often solving tasks with\nspatial, temporal, and permutation symmetries. Most algorithms for planning and\nmodel-based reinforcement learning (MBRL) do not take this rich geometric\nstructure into account, leading to sample inefficiency and poor generalization.\nWe introduce the Equivariant Diffuser for Generating Interactions (EDGI), an\nalgorithm for MBRL and planning that is equivariant with respect to the product\nof the spatial symmetry group $\\mathrm{SE(3)}$, the discrete-time translation\ngroup $\\mathbb{Z}$, and the object permutation group $\\mathrm{S}_n$. EDGI\nfollows the Diffuser framework (Janner et al. 2022) in treating both learning a\nworld model and planning in it as a conditional generative modeling problem,\ntraining a diffusion model on an offline trajectory dataset. We introduce a new\n$\\mathrm{SE(3)} \\times \\mathbb{Z} \\times \\mathrm{S}_n$-equivariant diffusion\nmodel that supports multiple representations. We integrate this model in a\nplanning loop, where conditioning and classifier-based guidance allow us to\nsoftly break the symmetry for specific tasks as needed. On navigation and\nobject manipulation tasks, EDGI improves sample efficiency and generalization.\n","authors":["Johann Brehmer","Joey Bose","Pim de Haan","Taco Cohen"],"pdf_url":"https://arxiv.org/pdf/2303.12410v1.pdf","comment":"Reincarnating RL workshop at ICLR 2023"},{"id":"http://arxiv.org/abs/2102.01977v5","updated":"2023-03-22T09:17:12Z","published":"2021-02-03T09:51:03Z","title":"Instance-Dependent Bounds for Zeroth-order Lipschitz Optimization with\n  Error Certificates","summary":"  We study the problem of zeroth-order (black-box) optimization of a Lipschitz\nfunction $f$ defined on a compact subset $\\mathcal X$ of $\\mathbb R^d$, with\nthe additional constraint that algorithms must certify the accuracy of their\nrecommendations. We characterize the optimal number of evaluations of any\nLipschitz function $f$ to find and certify an approximate maximizer of $f$ at\naccuracy $\\varepsilon$. Under a weak assumption on $\\mathcal X$, this optimal\nsample complexity is shown to be nearly proportional to the integral\n$\\int_{\\mathcal X} \\mathrm{d}\\boldsymbol x/( \\max(f) - f(\\boldsymbol x) +\n\\varepsilon )^d$. This result, which was only (and partially) known in\ndimension $d=1$, solves an open problem dating back to 1991. In terms of\ntechniques, our upper bound relies on a packing bound by Bouttier al. (2020)\nfor the Piyavskii-Shubert algorithm that we link to the above integral. We also\nshow that a certified version of the computationally tractable DOO algorithm\nmatches these packing and integral bounds. Our instance-dependent lower bound\ndiffers from traditional worst-case lower bounds in the Lipschitz setting and\nrelies on a local worst-case analysis that could likely prove useful for other\nlearning tasks.\n","authors":["François Bachoc","Tommaso R Cesari","Sébastien Gerchinovitz"],"pdf_url":"https://arxiv.org/pdf/2102.01977v5.pdf","comment":null},{"id":"http://arxiv.org/abs/2106.10836v2","updated":"2023-03-22T09:12:09Z","published":"2021-06-21T03:55:33Z","title":"Active Learning for Deep Neural Networks on Edge Devices","summary":"  When dealing with deep neural network (DNN) applications on edge devices,\ncontinuously updating the model is important. Although updating a model with\nreal incoming data is ideal, using all of them is not always feasible due to\nlimits, such as labeling and communication costs. Thus, it is necessary to\nfilter and select the data to use for training (i.e., active learning) on the\ndevice. In this paper, we formalize a practical active learning problem for\nDNNs on edge devices and propose a general task-agnostic framework to tackle\nthis problem, which reduces it to a stream submodular maximization. This\nframework is light enough to be run with low computational resources, yet\nprovides solutions whose quality is theoretically guaranteed thanks to the\nsubmodular property. Through this framework, we can configure data selection\ncriteria flexibly, including using methods proposed in previous active learning\nstudies. We evaluate our approach on both classification and object detection\ntasks in a practical setting to simulate a real-life scenario. The results of\nour study show that the proposed framework outperforms all other methods in\nboth tasks, while running at a practical speed on real devices.\n","authors":["Yuya Senzaki","Christian Hamelain"],"pdf_url":"https://arxiv.org/pdf/2106.10836v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.12398v1","updated":"2023-03-22T09:06:07Z","published":"2023-03-22T09:06:07Z","title":"Multiscale Attention via Wavelet Neural Operators for Vision\n  Transformers","summary":"  Transformers have achieved widespread success in computer vision. At their\nheart, there is a Self-Attention (SA) mechanism, an inductive bias that\nassociates each token in the input with every other token through a weighted\nbasis. The standard SA mechanism has quadratic complexity with the sequence\nlength, which impedes its utility to long sequences appearing in high\nresolution vision. Recently, inspired by operator learning for PDEs, Adaptive\nFourier Neural Operators (AFNO) were introduced for high resolution attention\nbased on global convolution that is efficiently implemented via FFT. However,\nthe AFNO global filtering cannot well represent small and moderate scale\nstructures that commonly appear in natural images. To leverage the\ncoarse-to-fine scale structures we introduce a Multiscale Wavelet Attention\n(MWA) by leveraging wavelet neural operators which incurs linear complexity in\nthe sequence size. We replace the attention in ViT with MWA and our experiments\nwith CIFAR and ImageNet classification demonstrate significant improvement over\nalternative Fourier-based attentions such as AFNO and Global Filter Network\n(GFN).\n","authors":["Anahita Nekoozadeh","Mohammad Reza Ahmadzadeh","Zahra Mardani","Morteza Mardani"],"pdf_url":"https://arxiv.org/pdf/2303.12398v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2212.00210v2","updated":"2023-03-22T08:58:15Z","published":"2022-12-01T01:39:28Z","title":"Shape-Guided Diffusion with Inside-Outside Attention","summary":"  When manipulating an object, existing text-to-image diffusion models often\nignore the shape of the object and generate content that is incorrectly scaled,\ncut off, or replaced with background content. We propose a training-free\nmethod, Shape-Guided Diffusion, that modifies pretrained diffusion models to be\nsensitive to shape input specified by a user or automatically inferred from\ntext. We use a novel Inside-Outside Attention mechanism during the inversion\nand generation process to apply this shape constraint to the cross- and\nself-attention maps. Our mechanism designates which spatial region is the\nobject (inside) vs. background (outside) then associates edits specified by\ntext prompts to the correct region. We demonstrate the efficacy of our method\non the shape-guided editing task, where the model must replace an object\naccording to a text prompt and object mask. We curate a new ShapePrompts\nbenchmark derived from MS-COCO and achieve SOTA results in shape faithfulness\nwithout a degradation in text alignment or image realism according to both\nautomatic metrics and annotator ratings. Our data and code will be made\navailable at https://shape-guided-diffusion.github.io.\n","authors":["Dong Huk Park","Grace Luo","Clayton Toste","Samaneh Azadi","Xihui Liu","Maka Karalashvili","Anna Rohrbach","Trevor Darrell"],"pdf_url":"https://arxiv.org/pdf/2212.00210v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.11330v2","updated":"2023-03-22T08:48:15Z","published":"2023-03-20T17:59:58Z","title":"Legs as Manipulator: Pushing Quadrupedal Agility Beyond Locomotion","summary":"  Locomotion has seen dramatic progress for walking or running across\nchallenging terrains. However, robotic quadrupeds are still far behind their\nbiological counterparts, such as dogs, which display a variety of agile skills\nand can use the legs beyond locomotion to perform several basic manipulation\ntasks like interacting with objects and climbing. In this paper, we take a step\ntowards bridging this gap by training quadruped robots not only to walk but\nalso to use the front legs to climb walls, press buttons, and perform object\ninteraction in the real world. To handle this challenging optimization, we\ndecouple the skill learning broadly into locomotion, which involves anything\nthat involves movement whether via walking or climbing a wall, and\nmanipulation, which involves using one leg to interact while balancing on the\nother three legs. These skills are trained in simulation using curriculum and\ntransferred to the real world using our proposed sim2real variant that builds\nupon recent locomotion success. Finally, we combine these skills into a robust\nlong-term plan by learning a behavior tree that encodes a high-level task\nhierarchy from one clean expert demonstration. We evaluate our method in both\nsimulation and real-world showing successful executions of both short as well\nas long-range tasks and how robustness helps confront external perturbations.\nVideos at https://robot-skills.github.io\n","authors":["Xuxin Cheng","Ashish Kumar","Deepak Pathak"],"pdf_url":"https://arxiv.org/pdf/2303.11330v2.pdf","comment":"Accepted at ICRA 2023. Videos at https://robot-skills.github.io"},{"id":"http://arxiv.org/abs/2302.08175v5","updated":"2023-03-22T08:32:12Z","published":"2023-02-16T09:44:55Z","title":"A numerical approximation method for the Fisher-Rao distance between\n  multivariate normal distributions","summary":"  We present a simple method to approximate Rao's distance between multivariate\nnormal distributions based on discretizing curves joining normal distributions\nand approximating Rao's distances between successive nearby normal\ndistributions on the curves by the square root of Jeffreys divergence, the\nsymmetrized Kullback-Leibler divergence. We consider experimentally the linear\ninterpolation curves in the ordinary, natural and expectation parameterizations\nof the normal distributions, and compare these curves with a curve derived from\nthe Calvo and Oller's isometric embedding of the Fisher-Rao $d$-variate normal\nmanifold into the cone of $(d+1)\\times (d+1)$ symmetric positive-definite\nmatrices [Journal of multivariate analysis 35.2 (1990): 223-242]. We report on\nour experiments and assess the quality of our approximation technique by\ncomparing the numerical approximations with both lower and upper bounds.\nFinally, we present several information-geometric properties of the Calvo and\nOller's isometric embedding.\n","authors":["Frank Nielsen"],"pdf_url":"https://arxiv.org/pdf/2302.08175v5.pdf","comment":"42 pages, 17 figures, 3 tables"},{"id":"http://arxiv.org/abs/2303.12375v1","updated":"2023-03-22T08:22:12Z","published":"2023-03-22T08:22:12Z","title":"Disturbance Injection under Partial Automation: Robust Imitation\n  Learning for Long-horizon Tasks","summary":"  Partial Automation (PA) with intelligent support systems has been introduced\nin industrial machinery and advanced automobiles to reduce the burden of long\nhours of human operation. Under PA, operators perform manual operations\n(providing actions) and operations that switch to automatic/manual mode\n(mode-switching). Since PA reduces the total duration of manual operation,\nthese two action and mode-switching operations can be replicated by imitation\nlearning with high sample efficiency. To this end, this paper proposes\nDisturbance Injection under Partial Automation (DIPA) as a novel imitation\nlearning framework. In DIPA, mode and actions (in the manual mode) are assumed\nto be observables in each state and are used to learn both action and\nmode-switching policies. The above learning is robustified by injecting\ndisturbances into the operator's actions to optimize the disturbance's level\nfor minimizing the covariate shift under PA. We experimentally validated the\neffectiveness of our method for long-horizon tasks in two simulations and a\nreal robot environment and confirmed that our method outperformed the previous\nmethods and reduced the demonstration burden.\n","authors":["Hirotaka Tahara","Hikaru Sasaki","Hanbit Oh","Edgar Anarossi","Takamitsu Matsubara"],"pdf_url":"https://arxiv.org/pdf/2303.12375v1.pdf","comment":"8 pages, Accepted by Robotics and Automation Letters (RA-L) 2023"},{"id":"http://arxiv.org/abs/2303.12367v1","updated":"2023-03-22T08:06:41Z","published":"2023-03-22T08:06:41Z","title":"AIIPot: Adaptive Intelligent-Interaction Honeypot for IoT Devices","summary":"  The proliferation of the Internet of Things (IoT) has raised concerns about\nthe security of connected devices. There is a need to develop suitable and\ncost-efficient methods to identify vulnerabilities in IoT devices in order to\naddress them before attackers seize opportunities to compromise them. The\ndeception technique is a prominent approach to improving the security posture\nof IoT systems. Honeypot is a popular deception technique that mimics\ninteraction in real fashion and encourages unauthorised users (attackers) to\nlaunch attacks. Due to the large number and the heterogeneity of IoT devices,\nmanually crafting the low and high-interaction honeypots is not affordable.\nThis has forced researchers to seek innovative ways to build honeypots for IoT\ndevices. In this paper, we propose a honeypot for IoT devices that uses machine\nlearning techniques to learn and interact with attackers automatically. The\nevaluation of the proposed model indicates that our system can improve the\nsession length with attackers and capture more attacks on the IoT network.\n","authors":["Volviane Saphir Mfogo","Alain Zemkoho","Laurent Njilla","Marcellin Nkenlifack","Charles Kamhoua"],"pdf_url":"https://arxiv.org/pdf/2303.12367v1.pdf","comment":"7 pages, 7 figures"},{"id":"http://arxiv.org/abs/2303.12364v1","updated":"2023-03-22T08:03:27Z","published":"2023-03-22T08:03:27Z","title":"ExBEHRT: Extended Transformer for Electronic Health Records to Predict\n  Disease Subtypes & Progressions","summary":"  In this study, we introduce ExBEHRT, an extended version of BEHRT (BERT\napplied to electronic health records), and apply different algorithms to\ninterpret its results. While BEHRT considers only diagnoses and patient age, we\nextend the feature space to several multimodal records, namely demographics,\nclinical characteristics, vital signs, smoking status, diagnoses, procedures,\nmedications, and laboratory tests, by applying a novel method to unify the\nfrequencies and temporal dimensions of the different features. We show that\nadditional features significantly improve model performance for various\ndownstream tasks in different diseases. To ensure robustness, we interpret\nmodel predictions using an adaptation of expected gradients, which has not been\npreviously applied to transformers with EHR data and provides more granular\ninterpretations than previous approaches such as feature and token importances.\nFurthermore, by clustering the model representations of oncology patients, we\nshow that the model has an implicit understanding of the disease and is able to\nclassify patients with the same cancer type into different risk groups. Given\nthe additional features and interpretability, ExBEHRT can help make informed\ndecisions about disease trajectories, diagnoses, and risk factors of various\ndiseases.\n","authors":["Maurice Rupp","Oriane Peter","Thirupathi Pattipaka"],"pdf_url":"https://arxiv.org/pdf/2303.12364v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.12363v1","updated":"2023-03-22T07:56:59Z","published":"2023-03-22T07:56:59Z","title":"Distribution-restrained Softmax Loss for the Model Robustness","summary":"  Recently, the robustness of deep learning models has received widespread\nattention, and various methods for improving model robustness have been\nproposed, including adversarial training, model architecture modification,\ndesign of loss functions, certified defenses, and so on. However, the principle\nof the robustness to attacks is still not fully understood, also the related\nresearch is still not sufficient. Here, we have identified a significant factor\nthat affects the robustness of models: the distribution characteristics of\nsoftmax values for non-real label samples. We found that the results after an\nattack are highly correlated with the distribution characteristics, and thus we\nproposed a loss function to suppress the distribution diversity of softmax. A\nlarge number of experiments have shown that our method can improve robustness\nwithout significant time consumption.\n","authors":["Hao Wang","Chen Li","Jinzhe Jiang","Xin Zhang","Yaqian Zhao","Weifeng Gong"],"pdf_url":"https://arxiv.org/pdf/2303.12363v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.12357v1","updated":"2023-03-22T07:50:15Z","published":"2023-03-22T07:50:15Z","title":"Wasserstein Adversarial Examples on Univariant Time Series Data","summary":"  Adversarial examples are crafted by adding indistinguishable perturbations to\nnormal examples in order to fool a well-trained deep learning model to\nmisclassify. In the context of computer vision, this notion of\nindistinguishability is typically bounded by $L_{\\infty}$ or other norms.\nHowever, these norms are not appropriate for measuring indistinguishiability\nfor time series data. In this work, we propose adversarial examples in the\nWasserstein space for time series data for the first time and utilize\nWasserstein distance to bound the perturbation between normal examples and\nadversarial examples. We introduce Wasserstein projected gradient descent\n(WPGD), an adversarial attack method for perturbing univariant time series\ndata. We leverage the closed-form solution of Wasserstein distance in the 1D\nspace to calculate the projection step of WPGD efficiently with the gradient\ndescent method. We further propose a two-step projection so that the search of\nadversarial examples in the Wasserstein space is guided and constrained by\nEuclidean norms to yield more effective and imperceptible perturbations. We\nempirically evaluate the proposed attack on several time series datasets in the\nhealthcare domain. Extensive results demonstrate that the Wasserstein attack is\npowerful and can successfully attack most of the target classifiers with a high\nattack success rate. To better study the nature of Wasserstein adversarial\nexample, we evaluate a strong defense mechanism named Wasserstein smoothing for\npotential certified robustness defense. Although the defense can achieve some\naccuracy gain, it still has limitations in many cases and leaves space for\ndeveloping a stronger certified robustness method to Wasserstein adversarial\nexamples on univariant time series data.\n","authors":["Wenjie Wang","Li Xiong","Jian Lou"],"pdf_url":"https://arxiv.org/pdf/2303.12357v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.12354v1","updated":"2023-03-22T07:44:35Z","published":"2023-03-22T07:44:35Z","title":"Deep Reinforcement Learning for Localizability-Enhanced Navigation in\n  Dynamic Human Environments","summary":"  Reliable localization is crucial for autonomous robots to navigate\nefficiently and safely. Some navigation methods can plan paths with high\nlocalizability (which describes the capability of acquiring reliable\nlocalization). By following these paths, the robot can access the sensor\nstreams that facilitate more accurate location estimation results by the\nlocalization algorithms. However, most of these methods require prior knowledge\nand struggle to adapt to unseen scenarios or dynamic changes. To overcome these\nlimitations, we propose a novel approach for localizability-enhanced navigation\nvia deep reinforcement learning in dynamic human environments. Our proposed\nplanner automatically extracts geometric features from 2D laser data that are\nhelpful for localization. The planner learns to assign different importance to\nthe geometric features and encourages the robot to navigate through areas that\nare helpful for laser localization. To facilitate the learning of the planner,\nwe suggest two techniques: (1) an augmented state representation that considers\nthe dynamic changes and the confidence of the localization results, which\nprovides more information and allows the robot to make better decisions, (2) a\nreward metric that is capable to offer both sparse and dense feedback on\nbehaviors that affect localization accuracy. Our method exhibits significant\nimprovements in lost rate and arrival rate when tested in previously unseen\nenvironments.\n","authors":["Yuan Chen","Quecheng Qiu","Xiangyu Liu","Guangda Chen","Shunyi Yao","Jie Peng","Jianmin Ji","Yanyong Zhang"],"pdf_url":"https://arxiv.org/pdf/2303.12354v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.12352v1","updated":"2023-03-22T07:40:01Z","published":"2023-03-22T07:40:01Z","title":"Training Multilayer Perceptrons by Sampling with Quantum Annealers","summary":"  A successful application of quantum annealing to machine learning is training\nrestricted Boltzmann machines (RBM). However, many neural networks for vision\napplications are feedforward structures, such as multilayer perceptrons (MLP).\nBackpropagation is currently the most effective technique to train MLPs for\nsupervised learning. This paper aims to be forward-looking by exploring the\ntraining of MLPs using quantum annealers. We exploit an equivalence between\nMLPs and energy-based models (EBM), which are a variation of RBMs with a\nmaximum conditional likelihood objective. This leads to a strategy to train\nMLPs with quantum annealers as a sampling engine. We prove our setup for MLPs\nwith sigmoid activation functions and one hidden layer, and demonstrated\ntraining of binary image classifiers on small subsets of the MNIST and\nFashion-MNIST datasets using the D-Wave quantum annealer. Although problem\nsizes that are feasible on current annealers are limited, we obtained\ncomprehensive results on feasible instances that validate our ideas. Our work\nestablishes the potential of quantum computing for training MLPs.\n","authors":["Frances Fengyi Yang","Michele Sasdelli","Tat-Jun Chin"],"pdf_url":"https://arxiv.org/pdf/2303.12352v1.pdf","comment":"22 pages, 15 figures"},{"id":"http://arxiv.org/abs/2201.10981v3","updated":"2023-03-22T07:35:10Z","published":"2022-01-26T14:52:23Z","title":"Joint Liver and Hepatic Lesion Segmentation in MRI using a Hybrid CNN\n  with Transformer Layers","summary":"  Deep learning-based segmentation of the liver and hepatic lesions therein\nsteadily gains relevance in clinical practice due to the increasing incidence\nof liver cancer each year. Whereas various network variants with overall\npromising results in the field of medical image segmentation have been\nsuccessfully developed over the last years, almost all of them struggle with\nthe challenge of accurately segmenting hepatic lesions in magnetic resonance\nimaging (MRI). This led to the idea of combining elements of convolutional and\ntransformer-based architectures to overcome the existing limitations. This work\npresents a hybrid network called SWTR-Unet, consisting of a pretrained ResNet,\ntransformer blocks as well as a common Unet-style decoder path. This network\nwas primarily applied to single-modality non-contrast-enhanced liver MRI and\nadditionally to the publicly available computed tomography (CT) data of the\nliver tumor segmentation (LiTS) challenge to verify the applicability on other\nmodalities. For a broader evaluation, multiple state-of-the-art networks were\nimplemented and applied, ensuring a direct comparability. Furthermore,\ncorrelation analysis and an ablation study were carried out, to investigate\nvarious influencing factors on the segmentation accuracy of the presented\nmethod. With Dice scores of averaged 98+-2% for liver and 81+-28% lesion\nsegmentation on the MRI dataset and 97+-2% and 79+-25%, respectively on the CT\ndataset, the proposed SWTR-Unet proved to be a precise approach for liver and\nhepatic lesion segmentation with state-of-the-art results for MRI and competing\naccuracy in CT imaging. The achieved segmentation accuracy was found to be on\npar with manually performed expert segmentations as indicated by inter-observer\nvariabilities for liver lesion segmentation. In conclusion, the presented\nmethod could save valuable time and resources in clinical practice.\n","authors":["Georg Hille","Shubham Agrawal","Pavan Tummala","Christian Wybranski","Maciej Pech","Alexey Surov","Sylvia Saalfeld"],"pdf_url":"https://arxiv.org/pdf/2201.10981v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2208.04589v2","updated":"2023-03-22T07:34:13Z","published":"2022-08-09T08:07:21Z","title":"Long-term Causal Effects Estimation via Latent Surrogates Representation\n  Learning","summary":"  Estimating long-term causal effects based on short-term surrogates is a\nsignificant but challenging problem in many real-world applications, e.g.,\nmarketing and medicine. Despite its success in certain domains, most existing\nmethods estimate causal effects in an idealistic and simplistic way - ignoring\nthe causal structure among short-term outcomes and treating all of them as\nsurrogates. However, such methods cannot be well applied to real-world\nscenarios, in which the partially observed surrogates are mixed with their\nproxies among short-term outcomes. To this end, we develop our flexible method,\nLaser, to estimate long-term causal effects in the more realistic situation\nthat the surrogates are observed or have observed proxies.Given the\nindistinguishability between the surrogates and proxies, we utilize\nidentifiable variational auto-encoder (iVAE) to recover the whole valid\nsurrogates on all the surrogates candidates without the need of distinguishing\nthe observed surrogates or the proxies of latent surrogates. With the help of\nthe recovered surrogates, we further devise an unbiased estimation of long-term\ncausal effects. Extensive experimental results on the real-world and\nsemi-synthetic datasets demonstrate the effectiveness of our proposed method.\n","authors":["Ruichu Cai","Weilin Chen","Zeqin Yang","Shu Wan","Chen Zheng","Xiaoqing Yang","Jiecheng Guo"],"pdf_url":"https://arxiv.org/pdf/2208.04589v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2208.00690v3","updated":"2023-03-22T07:20:37Z","published":"2022-08-01T08:58:02Z","title":"Generative Bias for Robust Visual Question Answering","summary":"  The task of Visual Question Answering (VQA) is known to be plagued by the\nissue of VQA models exploiting biases within the dataset to make its final\nprediction. Various previous ensemble based debiasing methods have been\nproposed where an additional model is purposefully trained to be biased in\norder to train a robust target model. However, these methods compute the bias\nfor a model simply from the label statistics of the training data or from\nsingle modal branches. In this work, in order to better learn the bias a target\nVQA model suffers from, we propose a generative method to train the bias model\ndirectly from the target model, called GenB. In particular, GenB employs a\ngenerative network to learn the bias in the target model through a combination\nof the adversarial objective and knowledge distillation. We then debias our\ntarget model with GenB as a bias model, and show through extensive experiments\nthe effects of our method on various VQA bias datasets including VQA-CP2,\nVQA-CP1, GQA-OOD, and VQA-CE, and show state-of-the-art results with the LXMERT\narchitecture on VQA-CP2.\n","authors":["Jae Won Cho","Dong-jin Kim","Hyeonggon Ryu","In So Kweon"],"pdf_url":"https://arxiv.org/pdf/2208.00690v3.pdf","comment":"CVPR 2023"},{"id":"http://arxiv.org/abs/2303.12341v1","updated":"2023-03-22T06:35:08Z","published":"2023-03-22T06:35:08Z","title":"EasyDGL: Encode, Train and Interpret for Continuous-time Dynamic Graph\n  Learning","summary":"  Dynamic graphs arise in various real-world applications, and it is often\nwelcomed to model the dynamics directly in continuous time domain for its\nflexibility. This paper aims to design an easy-to-use pipeline (termed as\nEasyDGL which is also due to its implementation by DGL toolkit) composed of\nthree key modules with both strong fitting ability and interpretability.\nSpecifically the proposed pipeline which involves encoding, training and\ninterpreting: i) a temporal point process (TPP) modulated attention\narchitecture to endow the continuous-time resolution with the coupled\nspatiotemporal dynamics of the observed graph with edge-addition events; ii) a\nprincipled loss composed of task-agnostic TPP posterior maximization based on\nobserved events on the graph, and a task-aware loss with a masking strategy\nover dynamic graph, where the covered tasks include dynamic link prediction,\ndynamic node classification and node traffic forecasting; iii) interpretation\nof the model outputs (e.g., representations and predictions) with scalable\nperturbation-based quantitative analysis in the graph Fourier domain, which\ncould more comprehensively reflect the behavior of the learned model. Extensive\nexperimental results on public benchmarks show the superior performance of our\nEasyDGL for time-conditioned predictive tasks, and in particular demonstrate\nthat EasyDGL can effectively quantify the predictive power of frequency content\nthat a model learn from the evolving graph data.\n","authors":["Chao Chen","Haoyu Geng","Nianzu Yang","Xiaokang Yang","Junchi Yan"],"pdf_url":"https://arxiv.org/pdf/2303.12341v1.pdf","comment":"9 figures, 7 tables"},{"id":"http://arxiv.org/abs/2303.11577v2","updated":"2023-03-22T06:07:28Z","published":"2023-03-21T03:51:15Z","title":"Feature-adjacent multi-fidelity physics-informed machine learning for\n  partial differential equations","summary":"  Physics-informed neural networks have emerged as an alternative method for\nsolving partial differential equations. However, for complex problems, the\ntraining of such networks can still require high-fidelity data which can be\nexpensive to generate. To reduce or even eliminate the dependency on\nhigh-fidelity data, we propose a novel multi-fidelity architecture which is\nbased on a feature space shared by the low- and high-fidelity solutions. In the\nfeature space, the projections of the low-fidelity and high-fidelity solutions\nare adjacent by constraining their relative distance. The feature space is\nrepresented with an encoder and its mapping to the original solution space is\neffected through a decoder. The proposed multi-fidelity approach is validated\non forward and inverse problems for steady and unsteady problems described by\npartial differential equations.\n","authors":["Wenqian Chen","Panos Stinis"],"pdf_url":"https://arxiv.org/pdf/2303.11577v2.pdf","comment":"12 figures"},{"id":"http://arxiv.org/abs/2202.02892v4","updated":"2023-03-22T05:23:53Z","published":"2022-02-07T00:15:03Z","title":"Lossy Compression of Noisy Data for Private and Data-Efficient Learning","summary":"  Storage-efficient privacy-preserving learning is crucial due to increasing\namounts of sensitive user data required for modern learning tasks. We propose a\nframework for reducing the storage cost of user data while at the same time\nproviding privacy guarantees, without essential loss in the utility of the data\nfor learning. Our method comprises noise injection followed by lossy\ncompression. We show that, when appropriately matching the lossy compression to\nthe distribution of the added noise, the compressed examples converge, in\ndistribution, to that of the noise-free training data as the sample size of the\ntraining data (or the dimension of the training data) increases. In this sense,\nthe utility of the data for learning is essentially maintained, while reducing\nstorage and privacy leakage by quantifiable amounts. We present experimental\nresults on the CelebA dataset for gender classification and find that our\nsuggested pipeline delivers in practice on the promise of the theory: the\nindividuals in the images are unrecognizable (or less recognizable, depending\non the noise level), overall storage of the data is substantially reduced, with\nno essential loss (and in some cases a slight boost) to the classification\naccuracy. As an added bonus, our experiments suggest that our method yields a\nsubstantial boost to robustness in the face of adversarial test data.\n","authors":["Berivan Isik","Tsachy Weissman"],"pdf_url":"https://arxiv.org/pdf/2202.02892v4.pdf","comment":"Published at the IEEE Journal on Selected Areas in Information Theory\n  (JSAIT). Preliminary version was presented at the IEEE International\n  Symposium on Information Theory (ISIT), 2022, with a slightly different\n  title, \"Learning under Storage and Privacy Constraints.\""},{"id":"http://arxiv.org/abs/2303.12317v1","updated":"2023-03-22T05:21:21Z","published":"2023-03-22T05:21:21Z","title":"Re-thinking Federated Active Learning based on Inter-class Diversity","summary":"  Although federated learning has made awe-inspiring advances, most studies\nhave assumed that the client's data are fully labeled. However, in a real-world\nscenario, every client may have a significant amount of unlabeled instances.\nAmong the various approaches to utilizing unlabeled data, a federated active\nlearning framework has emerged as a promising solution. In the decentralized\nsetting, there are two types of available query selector models, namely\n'global' and 'local-only' models, but little literature discusses their\nperformance dominance and its causes. In this work, we first demonstrate that\nthe superiority of two selector models depends on the global and local\ninter-class diversity. Furthermore, we observe that the global and local-only\nmodels are the keys to resolving the imbalance of each side. Based on our\nfindings, we propose LoGo, a FAL sampling strategy robust to varying local\nheterogeneity levels and global imbalance ratio, that integrates both models by\ntwo steps of active selection scheme. LoGo consistently outperforms six active\nlearning strategies in the total number of 38 experimental settings.\n","authors":["SangMook Kim","Sangmin Bae","Hwanjun Song","Se-Young Yun"],"pdf_url":"https://arxiv.org/pdf/2303.12317v1.pdf","comment":"CVPR 2023"},{"id":"http://arxiv.org/abs/2303.12316v1","updated":"2023-03-22T05:14:36Z","published":"2023-03-22T05:14:36Z","title":"TsSHAP: Robust model agnostic feature-based explainability for time\n  series forecasting","summary":"  A trustworthy machine learning model should be accurate as well as\nexplainable. Understanding why a model makes a certain decision defines the\nnotion of explainability. While various flavors of explainability have been\nwell-studied in supervised learning paradigms like classification and\nregression, literature on explainability for time series forecasting is\nrelatively scarce.\n  In this paper, we propose a feature-based explainability algorithm, TsSHAP,\nthat can explain the forecast of any black-box forecasting model. The method is\nagnostic of the forecasting model and can provide explanations for a forecast\nin terms of interpretable features defined by the user a prior.\n  The explanations are in terms of the SHAP values obtained by applying the\nTreeSHAP algorithm on a surrogate model that learns a mapping between the\ninterpretable feature space and the forecast of the black-box model.\n  Moreover, we formalize the notion of local, semi-local, and global\nexplanations in the context of time series forecasting, which can be useful in\nseveral scenarios. We validate the efficacy and robustness of TsSHAP through\nextensive experiments on multiple datasets.\n","authors":["Vikas C. Raykar","Arindam Jati","Sumanta Mukherjee","Nupur Aggarwal","Kanthi Sarpatwar","Giridhar Ganapavarapu","Roman Vaculin"],"pdf_url":"https://arxiv.org/pdf/2303.12316v1.pdf","comment":"11 pages, 8 figures"},{"id":"http://arxiv.org/abs/2303.12314v1","updated":"2023-03-22T05:04:21Z","published":"2023-03-22T05:04:21Z","title":"Self-supervised Meta-Prompt Learning with Meta-Gradient Regularization\n  for Few-shot Generalization","summary":"  Prompt tuning is a parameter-efficient method, which learns soft prompts and\nconditions frozen language models to perform specific downstream tasks. Though\neffective, prompt tuning under few-shot settings on the one hand heavily relies\non a good initialization of soft prompts. On the other hand, it can easily\nresult in overfitting. Existing works leverage pre-training or supervised\nmeta-learning to initialize soft prompts but they cannot data-efficiently\ngeneralize to unseen downstream tasks. To address the above problems, this\npaper proposes a novel Self-sUpervised meta-Prompt learning framework with\nmeta-gradient Regularization for few-shot generalization (SUPMER). We first\ndesign a set of self-supervised anchor meta-training tasks with different task\nformats and further enrich the task distribution with curriculum-based task\naugmentation. Then a novel meta-gradient regularization method is integrated\ninto meta-prompt learning. It meta-learns to transform the raw gradients during\nfew-shot learning into a domain-generalizable direction, thus alleviating the\nproblem of overfitting. Extensive experiments show that SUPMER achieves better\nperformance for different few-shot downstream tasks, and also exhibits a\nstronger domain generalization ability.\n","authors":["Kaihang Pan","Juncheng Li","Hongye Song","Jun Lin","Xiaozhong Liu","Siliang Tang"],"pdf_url":"https://arxiv.org/pdf/2303.12314v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.12311v1","updated":"2023-03-22T05:01:14Z","published":"2023-03-22T05:01:14Z","title":"Frozen Language Model Helps ECG Zero-Shot Learning","summary":"  The electrocardiogram (ECG) is one of the most commonly used non-invasive,\nconvenient medical monitoring tools that assist in the clinical diagnosis of\nheart diseases. Recently, deep learning (DL) techniques, particularly\nself-supervised learning (SSL), have demonstrated great potential in the\nclassification of ECG. SSL pre-training has achieved competitive performance\nwith only a small amount of annotated data after fine-tuning. However, current\nSSL methods rely on the availability of annotated data and are unable to\npredict labels not existing in fine-tuning datasets. To address this challenge,\nwe propose Multimodal ECG-Text Self-supervised pre-training (METS), the first\nwork to utilize the auto-generated clinical reports to guide ECG SSL\npre-training. We use a trainable ECG encoder and a frozen language model to\nembed paired ECG and automatically machine-generated clinical reports\nseparately. The SSL aims to maximize the similarity between paired ECG and\nauto-generated report while minimize the similarity between ECG and other\nreports. In downstream classification tasks, METS achieves around 10%\nimprovement in performance without using any annotated data via zero-shot\nclassification, compared to other supervised and SSL baselines that rely on\nannotated data. Furthermore, METS achieves the highest recall and F1 scores on\nthe MIT-BIH dataset, despite MIT-BIH containing different classes of ECG\ncompared to the pre-trained dataset. The extensive experiments have\ndemonstrated the advantages of using ECG-Text multimodal self-supervised\nlearning in terms of generalizability, effectiveness, and efficiency.\n","authors":["Jun Li","Che Liu","Sibo Cheng","Rossella Arcucci","Shenda Hong"],"pdf_url":"https://arxiv.org/pdf/2303.12311v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2210.02672v2","updated":"2023-03-22T04:59:05Z","published":"2022-10-06T04:30:59Z","title":"Orthogonal Non-negative Matrix Factorization: a\n  Maximum-Entropy-Principle Approach","summary":"  In this paper, we introduce a new methodology to solve the orthogonal\nnonnegative matrix factorization (ONMF) problem, where the objective is to\napproximate an input data matrix by a product of two nonnegative matrices, the\nfeatures matrix and the mixing matrix, where one of them is orthogonal. We show\nhow the ONMF can be interpreted as a specific facility-location problem (FLP),\nand adapt a maximum-entropy-principle based solution for FLP to the ONMF\nproblem. The proposed approach guarantees orthogonality and sparsity of the\nfeatures or the mixing matrix, while ensuring nonnegativity of both.\nAdditionally, our methodology develops a quantitative characterization of\n``true\" number of underlying features - a hyperparameter required for the ONMF.\nAn evaluation of the proposed method conducted on synthetic datasets, as well\nas a standard genetic microarray dataset indicates significantly better\nsparsity, orthogonality, and performance speed compared to similar methods in\nthe literature, with comparable or improved reconstruction errors.\n","authors":["Salar Basiri","Mustafa Kapadia","Srinivasa Salapaka"],"pdf_url":"https://arxiv.org/pdf/2210.02672v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2205.02900v2","updated":"2023-03-22T04:57:56Z","published":"2022-05-05T19:39:32Z","title":"New-Onset Diabetes Assessment Using Artificial Intelligence-Enhanced\n  Electrocardiography","summary":"  Undiagnosed diabetes is present in 21.4% of adults with diabetes. Diabetes\ncan remain asymptomatic and undetected due to limitations in screening rates.\nTo address this issue, questionnaires, such as the American Diabetes\nAssociation (ADA) Risk test, have been recommended for use by physicians and\nthe public. Based on evidence that blood glucose concentration can affect\ncardiac electrophysiology, we hypothesized that an artificial intelligence\n(AI)-enhanced electrocardiogram (ECG) could identify adults with new-onset\ndiabetes. We trained a neural network to estimate HbA1c using a 12-lead ECG and\nreadily available demographics. We retrospectively assembled a dataset\ncomprised of patients with paired ECG and HbA1c data. The population of\npatients who receive both an ECG and HbA1c may a biased sample of the complete\noutpatient population, so we adjusted the importance placed on each patient to\ngenerate a more representative pseudo-population. We found ECG-based assessment\noutperforms the ADA Risk test, achieving a higher area under the curve (0.80\nvs. 0.68) and positive predictive value (13% vs. 9%) -- 2.6 times the\nprevalence of diabetes in the cohort. The AI-enhanced ECG significantly\noutperforms electrophysiologist interpretation of the ECG, suggesting that the\ntask is beyond current clinical capabilities. Given the prevalence of ECGs in\nclinics and via wearable devices, such a tool would make precise, automated\ndiabetes assessment widely accessible.\n","authors":["Neil Jethani","Aahlad Puli","Hao Zhang","Leonid Garber","Lior Jankelson","Yindalon Aphinyanaphongs","Rajesh Ranganath"],"pdf_url":"https://arxiv.org/pdf/2205.02900v2.pdf","comment":"21 pages, 8 figures"},{"id":"http://arxiv.org/abs/2303.12306v1","updated":"2023-03-22T04:49:00Z","published":"2023-03-22T04:49:00Z","title":"Logical Expressiveness of Graph Neural Network for Knowledge Graph\n  Reasoning","summary":"  Graph Neural Networks (GNNs) have been recently introduced to learn from\nknowledge graph (KG) and achieved state-of-the-art performance in KG reasoning.\nHowever, a theoretical certification for their good empirical performance is\nstill absent. Besides, while logic in KG is important for inductive and\ninterpretable inference, existing GNN-based methods are just designed to fit\ndata distributions with limited knowledge of their logical expressiveness. We\npropose to fill the above gap in this paper. Specifically, we theoretically\nanalyze GNN from logical expressiveness and find out what kind of logical rules\ncan be captured from KG. Our results first show that GNN can capture logical\nrules from graded modal logic, providing a new theoretical tool for analyzing\nthe expressiveness of GNN for KG reasoning; and a query labeling trick makes it\neasier for GNN to capture logical rules, explaining why SOTA methods are mainly\nbased on labeling trick. Finally, insights from our theory motivate the\ndevelopment of an entity labeling method for capturing difficult logical rules.\nExperimental results are consistent with our theoretical results and verify the\neffectiveness of our proposed method.\n","authors":["Haiquan Qiu","Yongqi Zhang","Yong Li","Quanming Yao"],"pdf_url":"https://arxiv.org/pdf/2303.12306v1.pdf","comment":"15 pages, 4 figures"},{"id":"http://arxiv.org/abs/2303.12302v1","updated":"2023-03-22T04:19:18Z","published":"2023-03-22T04:19:18Z","title":"Anomaly Detection in Aeronautics Data with Quantum-compatible Discrete\n  Deep Generative Model","summary":"  Deep generative learning cannot only be used for generating new data with\nstatistical characteristics derived from input data but also for anomaly\ndetection, by separating nominal and anomalous instances based on their\nreconstruction quality. In this paper, we explore the performance of three\nunsupervised deep generative models -- variational autoencoders (VAEs) with\nGaussian, Bernoulli, and Boltzmann priors -- in detecting anomalies in\nflight-operations data of commercial flights consisting of multivariate time\nseries. We devised two VAE models with discrete latent variables (DVAEs), one\nwith a factorized Bernoulli prior and one with a restricted Boltzmann machine\n(RBM) as prior, because of the demand for discrete-variable models in\nmachine-learning applications and because the integration of quantum devices\nbased on two-level quantum systems requires such models. The DVAE with RBM\nprior, using a relatively simple -- and classically or quantum-mechanically\nenhanceable -- sampling technique for the evolution of the RBM's negative\nphase, performed better than the Bernoulli DVAE and on par with the Gaussian\nmodel, which has a continuous latent space. Our studies demonstrate the\ncompetitiveness of a discrete deep generative model with its Gaussian\ncounterpart on anomaly-detection tasks. Moreover, the DVAE model with RBM prior\ncan be easily integrated with quantum sampling by outsourcing its generative\nprocess to measurements of quantum states obtained from a quantum annealer or\ngate-model device.\n","authors":["Thomas Templin","Milad Memarzadeh","Walter Vinci","P. Aaron Lott","Ata Akbari Asanjan","Anthony Alexiades Armenakas","Eleanor Rieffel"],"pdf_url":"https://arxiv.org/pdf/2303.12302v1.pdf","comment":"25 pages, 7 figures, 3 tables, appendix, supplementary material"},{"id":"http://arxiv.org/abs/2303.12298v1","updated":"2023-03-22T04:07:26Z","published":"2023-03-22T04:07:26Z","title":"A General Algorithm for Solving Rank-one Matrix Sensing","summary":"  Matrix sensing has many real-world applications in science and engineering,\nsuch as system control, distance embedding, and computer vision. The goal of\nmatrix sensing is to recover a matrix $A_\\star \\in \\mathbb{R}^{n \\times n}$,\nbased on a sequence of measurements $(u_i,b_i) \\in \\mathbb{R}^{n} \\times\n\\mathbb{R}$ such that $u_i^\\top A_\\star u_i = b_i$. Previous work [ZJD15]\nfocused on the scenario where matrix $A_{\\star}$ has a small rank, e.g.\nrank-$k$. Their analysis heavily relies on the RIP assumption, making it\nunclear how to generalize to high-rank matrices. In this paper, we relax that\nrank-$k$ assumption and solve a much more general matrix sensing problem. Given\nan accuracy parameter $\\delta \\in (0,1)$, we can compute $A \\in \\mathbb{R}^{n\n\\times n}$ in $\\widetilde{O}(m^{3/2} n^2 \\delta^{-1} )$, such that $ |u_i^\\top\nA u_i - b_i| \\leq \\delta$ for all $i \\in [m]$. We design an efficient algorithm\nwith provable convergence guarantees using stochastic gradient descent for this\nproblem.\n","authors":["Lianke Qin","Zhao Song","Ruizhe Zhang"],"pdf_url":"https://arxiv.org/pdf/2303.12298v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.12296v1","updated":"2023-03-22T04:06:29Z","published":"2023-03-22T04:06:29Z","title":"Prototype Helps Federated Learning: Towards Faster Convergence","summary":"  Federated learning (FL) is a distributed machine learning technique in which\nmultiple clients cooperate to train a shared model without exchanging their raw\ndata. However, heterogeneity of data distribution among clients usually leads\nto poor model inference. In this paper, a prototype-based federated learning\nframework is proposed, which can achieve better inference performance with only\na few changes to the last global iteration of the typical federated learning\nprocess. In the last iteration, the server aggregates the prototypes\ntransmitted from distributed clients and then sends them back to local clients\nfor their respective model inferences. Experiments on two baseline datasets\nshow that our proposal can achieve higher accuracy (at least 1%) and relatively\nefficient communication than two popular baselines under different\nheterogeneous settings.\n","authors":["Yu Qiao","Seong-Bae Park","Sun Moo Kang","Choong Seon Hong"],"pdf_url":"https://arxiv.org/pdf/2303.12296v1.pdf","comment":"3 pages, 3 figures"},{"id":"http://arxiv.org/abs/2211.13436v2","updated":"2023-03-22T04:05:32Z","published":"2022-11-24T06:36:45Z","title":"Solving Bilevel Knapsack Problem using Graph Neural Networks","summary":"  The Bilevel Optimization Problem is a hierarchical optimization problem with\ntwo agents, a leader and a follower. The leader make their own decisions first,\nand the followers make the best choices accordingly. The leader knows the\ninformation of the followers, and the goal of the problem is to find the\noptimal solution by considering the reactions of the followers from the\nleader's point of view. For the Bilevel Optimization Problem, there are no\ngeneral and efficient algorithms or commercial solvers to get an optimal\nsolution, and it is very difficult to get a good solution even for a simple\nproblem. In this paper, we propose a deep learning approach using Graph Neural\nNetworks to solve the bilevel knapsack problem. We train the model to predict\nthe leader's solution and use it to transform the hierarchical optimization\nproblem into a single-level optimization problem to get the solution. Our model\nfound the feasible solution that was about 500 times faster than the exact\nalgorithm with $1.7\\%$ optimal gap. Also, our model performed well on problems\nof different size from the size it was trained on.\n","authors":["Sunhyeon Kwon","Hwayong Choi","Sungsoo Park"],"pdf_url":"https://arxiv.org/pdf/2211.13436v2.pdf","comment":"27 pages, 2 figures"},{"id":"http://arxiv.org/abs/2303.12291v1","updated":"2023-03-22T03:46:51Z","published":"2023-03-22T03:46:51Z","title":"Fairness Improves Learning from Noisily Labeled Long-Tailed Data","summary":"  Both long-tailed and noisily labeled data frequently appear in real-world\napplications and impose significant challenges for learning. Most prior works\ntreat either problem in an isolated way and do not explicitly consider the\ncoupling effects of the two. Our empirical observation reveals that such\nsolutions fail to consistently improve the learning when the dataset is\nlong-tailed with label noise. Moreover, with the presence of label noise,\nexisting methods do not observe universal improvements across different\nsub-populations; in other words, some sub-populations enjoyed the benefits of\nimproved accuracy at the cost of hurting others. Based on these observations,\nwe introduce the Fairness Regularizer (FR), inspired by regularizing the\nperformance gap between any two sub-populations. We show that the introduced\nfairness regularizer improves the performances of sub-populations on the tail\nand the overall learning performance. Extensive experiments demonstrate the\neffectiveness of the proposed solution when complemented with certain existing\npopular robust or class-balanced methods.\n","authors":["Jiaheng Wei","Zhaowei Zhu","Gang Niu","Tongliang Liu","Sijia Liu","Masashi Sugiyama","Yang Liu"],"pdf_url":"https://arxiv.org/pdf/2303.12291v1.pdf","comment":"Paper under review"},{"id":"http://arxiv.org/abs/2303.12289v1","updated":"2023-03-22T03:42:39Z","published":"2023-03-22T03:42:39Z","title":"Adaptive Road Configurations for Improved Autonomous Vehicle-Pedestrian\n  Interactions using Reinforcement Learning","summary":"  The deployment of Autonomous Vehicles (AVs) poses considerable challenges and\nunique opportunities for the design and management of future urban road\ninfrastructure. In light of this disruptive transformation, the Right-Of-Way\n(ROW) composition of road space has the potential to be renewed. Design\napproaches and intelligent control models have been proposed to address this\nproblem, but we lack an operational framework that can dynamically generate ROW\nplans for AVs and pedestrians in response to real-time demand. Based on\nmicroscopic traffic simulation, this study explores Reinforcement Learning (RL)\nmethods for evolving ROW compositions. We implement a centralised paradigm and\na distributive learning paradigm to separately perform the dynamic control on\nseveral road network configurations. Experimental results indicate that the\nalgorithms have the potential to improve traffic flow efficiency and allocate\nmore space for pedestrians. Furthermore, the distributive learning algorithm\noutperforms its centralised counterpart regarding computational cost (49.55\\%),\nbenchmark rewards (25.35\\%), best cumulative rewards (24.58\\%), optimal actions\n(13.49\\%) and rate of convergence. This novel road management technique could\npotentially contribute to the flow-adaptive and active mobility-friendly\nstreets in the AVs era.\n","authors":["Qiming Ye","Yuxiang Feng","Jose Javier Escribano Macias","Marc Stettler","Panagiotis Angeloudis"],"pdf_url":"https://arxiv.org/pdf/2303.12289v1.pdf","comment":"11 pages, 7 figures, Copyright \\c{opyright} 2023, IEEE"},{"id":"http://arxiv.org/abs/2301.08403v2","updated":"2023-03-22T03:41:35Z","published":"2023-01-20T02:35:43Z","title":"Sequence Generation via Subsequence Similarity: Theory and Application\n  to UAV Identification","summary":"  The ability to generate synthetic sequences is crucial for a wide range of\napplications, and recent advances in deep learning architectures and generative\nframeworks have greatly facilitated this process. Particularly, unconditional\none-shot generative models constitute an attractive line of research that\nfocuses on capturing the internal information of a single image or video to\ngenerate samples with similar contents. Since many of those one-shot models are\nshifting toward efficient non-deep and non-adversarial approaches, we examine\nthe versatility of a one-shot generative model for augmenting whole datasets.\nIn this work, we focus on how similarity at the subsequence level affects\nsimilarity at the sequence level, and derive bounds on the optimal transport of\nreal and generated sequences based on that of corresponding subsequences. We\nuse a one-shot generative model to sample from the vicinity of individual\nsequences and generate subsequence-similar ones and demonstrate the improvement\nof this approach by applying it to the problem of Unmanned Aerial Vehicle (UAV)\nidentification using limited radio-frequency (RF) signals. In the context of\nUAV identification, RF fingerprinting is an effective method for distinguishing\nlegitimate devices from malicious ones, but heterogenous environments and\nchannel impairments can impose data scarcity and affect the performance of\nclassification models. By using subsequence similarity to augment sequences of\nRF data with a low ratio (5%-20%) of training dataset, we achieve significant\nimprovements in performance metrics such as accuracy, precision, recall, and F1\nscore.\n","authors":["Amir Kazemi","Salar Basiri","Volodymyr Kindratenko","Srinivasa Salapaka"],"pdf_url":"https://arxiv.org/pdf/2301.08403v2.pdf","comment":"9 pages, 8 figures, 2 tables"},{"id":"http://arxiv.org/abs/2210.13230v3","updated":"2023-03-22T03:33:32Z","published":"2022-10-19T22:07:13Z","title":"An Experimental Study of Dimension Reduction Methods on Machine Learning\n  Algorithms with Applications to Psychometrics","summary":"  Developing interpretable machine learning models has become an increasingly\nimportant issue. One way in which data scientists have been able to develop\ninterpretable models has been to use dimension reduction techniques. In this\npaper, we examine several dimension reduction techniques including two recent\napproaches developed in the network psychometrics literature called exploratory\ngraph analysis (EGA) and unique variable analysis (UVA). We compared EGA and\nUVA with two other dimension reduction techniques common in the machine\nlearning literature (principal component analysis and independent component\nanalysis) as well as no reduction to the variables real data. We show that EGA\nand UVA perform as well as the other reduction techniques or no reduction.\nConsistent with previous literature, we show that dimension reduction can\ndecrease, increase, or provide the same accuracy as no reduction of variables.\nOur tentative results find that dimension reduction tends to lead to better\nperformance when used for classification tasks.\n","authors":["Sean H. Merritt","Alexander P. Christensen"],"pdf_url":"https://arxiv.org/pdf/2210.13230v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.12287v1","updated":"2023-03-22T03:28:12Z","published":"2023-03-22T03:28:12Z","title":"Hardness of Independent Learning and Sparse Equilibrium Computation in\n  Markov Games","summary":"  We consider the problem of decentralized multi-agent reinforcement learning\nin Markov games. A fundamental question is whether there exist algorithms that,\nwhen adopted by all agents and run independently in a decentralized fashion,\nlead to no-regret for each player, analogous to celebrated convergence results\nin normal-form games. While recent work has shown that such algorithms exist\nfor restricted settings (notably, when regret is defined with respect to\ndeviations to Markovian policies), the question of whether independent\nno-regret learning can be achieved in the standard Markov game framework was\nopen. We provide a decisive negative resolution this problem, both from a\ncomputational and statistical perspective. We show that:\n  - Under the widely-believed assumption that PPAD-hard problems cannot be\nsolved in polynomial time, there is no polynomial-time algorithm that attains\nno-regret in general-sum Markov games when executed independently by all\nplayers, even when the game is known to the algorithm designer and the number\nof players is a small constant.\n  - When the game is unknown, no algorithm, regardless of computational\nefficiency, can achieve no-regret without observing a number of episodes that\nis exponential in the number of players.\n  Perhaps surprisingly, our lower bounds hold even for seemingly easier setting\nin which all agents are controlled by a a centralized algorithm. They are\nproven via lower bounds for a simpler problem we refer to as SparseCCE, in\nwhich the goal is to compute a coarse correlated equilibrium that is sparse in\nthe sense that it can be represented as a mixture of a small number of product\npolicies. The crux of our approach is a novel application of aggregation\ntechniques from online learning, whereby we show that any algorithm for the\nSparseCCE problem can be used to compute approximate Nash equilibria for\nnon-zero sum normal-form games.\n","authors":["Dylan J. Foster","Noah Golowich","Sham M. Kakade"],"pdf_url":"https://arxiv.org/pdf/2303.12287v1.pdf","comment":"51 pages"},{"id":"http://arxiv.org/abs/2303.12285v1","updated":"2023-03-22T03:24:52Z","published":"2023-03-22T03:24:52Z","title":"Reducing Air Pollution through Machine Learning","summary":"  This paper presents a data-driven approach to mitigate the effects of air\npollution from industrial plants on nearby cities by linking operational\ndecisions with weather conditions. Our method combines predictive and\nprescriptive machine learning models to forecast short-term wind speed and\ndirection and recommend operational decisions to reduce or pause the industrial\nplant's production. We exhibit several trade-offs between reducing\nenvironmental impact and maintaining production activities. The predictive\ncomponent of our framework employs various machine learning models, such as\ngradient-boosted tree-based models and ensemble methods, for time series\nforecasting. The prescriptive component utilizes interpretable optimal policy\ntrees to propose multiple trade-offs, such as reducing dangerous emissions by\n33-47% and unnecessary costs by 40-63%. Our deployed models significantly\nreduced forecasting errors, with a range of 38-52% for less than 12-hour lead\ntime and 14-46% for 12 to 48-hour lead time compared to official weather\nforecasts. We have successfully implemented the predictive component at the OCP\nSafi site, which is Morocco's largest chemical industrial plant, and are\ncurrently in the process of deploying the prescriptive component. Our framework\nenables sustainable industrial development by eliminating the\npollution-industrial activity trade-off through data-driven weather-based\noperational decisions, significantly enhancing factory optimization and\nsustainability. This modernizes factory planning and resource allocation while\nmaintaining environmental compliance. The predictive component has boosted\nproduction efficiency, leading to cost savings and reduced environmental impact\nby minimizing air pollution.\n","authors":["Dimitris Bertsimas","Leonard Boussioux","Cynthia Zeng"],"pdf_url":"https://arxiv.org/pdf/2303.12285v1.pdf","comment":"Submitted to Manufacturing and Service Operations Management"},{"id":"http://arxiv.org/abs/2302.07260v2","updated":"2023-03-22T03:17:48Z","published":"2023-02-14T18:55:21Z","title":"Scalable Bayesian optimization with high-dimensional outputs using\n  randomized prior networks","summary":"  Several fundamental problems in science and engineering consist of global\noptimization tasks involving unknown high-dimensional (black-box) functions\nthat map a set of controllable variables to the outcomes of an expensive\nexperiment. Bayesian Optimization (BO) techniques are known to be effective in\ntackling global optimization problems using a relatively small number objective\nfunction evaluations, but their performance suffers when dealing with\nhigh-dimensional outputs. To overcome the major challenge of dimensionality,\nhere we propose a deep learning framework for BO and sequential decision making\nbased on bootstrapped ensembles of neural architectures with randomized priors.\nUsing appropriate architecture choices, we show that the proposed framework can\napproximate functional relationships between design variables and quantities of\ninterest, even in cases where the latter take values in high-dimensional vector\nspaces or even infinite-dimensional function spaces. In the context of BO, we\naugmented the proposed probabilistic surrogates with re-parameterized Monte\nCarlo approximations of multiple-point (parallel) acquisition functions, as\nwell as methodological extensions for accommodating black-box constraints and\nmulti-fidelity information sources. We test the proposed framework against\nstate-of-the-art methods for BO and demonstrate superior performance across\nseveral challenging tasks with high-dimensional outputs, including a\nconstrained optimization task involving shape optimization of rotor blades in\nturbo-machinery.\n","authors":["Mohamed Aziz Bhouri","Michael Joly","Robert Yu","Soumalya Sarkar","Paris Perdikaris"],"pdf_url":"https://arxiv.org/pdf/2302.07260v2.pdf","comment":"18 pages, 8 figures"},{"id":"http://arxiv.org/abs/2303.12281v1","updated":"2023-03-22T03:15:33Z","published":"2023-03-22T03:15:33Z","title":"Synthetic Health-related Longitudinal Data with Mixed-type Variables\n  Generated using Diffusion Models","summary":"  This paper presents a novel approach to simulating electronic health records\n(EHRs) using diffusion probabilistic models (DPMs). Specifically, we\ndemonstrate the effectiveness of DPMs in synthesising longitudinal EHRs that\ncapture mixed-type variables, including numeric, binary, and categorical\nvariables. To our knowledge, this represents the first use of DPMs for this\npurpose. We compared our DPM-simulated datasets to previous state-of-the-art\nresults based on generative adversarial networks (GANs) for two clinical\napplications: acute hypotension and human immunodeficiency virus (ART for HIV).\nGiven the lack of similar previous studies in DPMs, a core component of our\nwork involves exploring the advantages and caveats of employing DPMs across a\nwide range of aspects. In addition to assessing the realism of the synthetic\ndatasets, we also trained reinforcement learning (RL) agents on the synthetic\ndata to evaluate their utility for supporting the development of downstream\nmachine learning models. Finally, we estimated that our DPM-simulated datasets\nare secure and posed a low patient exposure risk for public access.\n","authors":["Nicholas I-Hsien Kuo","Louisa Jorm","Sebastiano Barbieri"],"pdf_url":"https://arxiv.org/pdf/2303.12281v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.07543v3","updated":"2023-03-22T03:11:28Z","published":"2023-03-14T00:13:57Z","title":"WDiscOOD: Out-of-Distribution Detection via Whitened Linear Discriminant\n  Analysis","summary":"  Deep neural networks are susceptible to generating overconfident yet\nerroneous predictions when presented with data beyond known concepts. This\nchallenge underscores the importance of detecting out-of-distribution (OOD)\nsamples in the open world. In this work, we propose a novel feature-space OOD\ndetection score that jointly reasons with both class-specific and\nclass-agnostic information. Specifically, our approach utilizes Whitened Linear\nDiscriminant Analysis to project features into two subspaces - the\ndiscriminative and residual subspaces - in which the ID classes are maximally\nseparated and closely clustered, respectively. The OOD score is then determined\nby combining the deviation from the input data to the ID distribution in both\nsubspaces. The efficacy of our method, named WDiscOOD, is verified on the\nlarge-scale ImageNet-1k benchmark, with six OOD datasets that covers a variety\nof distribution shifts. WDiscOOD demonstrates superior performance on deep\nclassifiers with diverse backbone architectures, including CNN and vision\ntransformer. Furthermore, we also show that our method can more effectively\ndetect novel concepts in representation space trained with contrastive\nobjectives, including supervised contrastive loss and multi-modality\ncontrastive loss.\n","authors":["Yiye Chen","Yunzhi Lin","Ruinian Xu","Patricio A. Vela"],"pdf_url":"https://arxiv.org/pdf/2303.07543v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.12277v1","updated":"2023-03-22T03:05:28Z","published":"2023-03-22T03:05:28Z","title":"Stochastic Nonsmooth Convex Optimization with Heavy-Tailed Noises","summary":"  Recently, several studies consider the stochastic optimization problem but in\na heavy-tailed noise regime, i.e., the difference between the stochastic\ngradient and the true gradient is assumed to have a finite $p$-th moment (say\nbeing upper bounded by $\\sigma^{p}$ for some $\\sigma\\geq0$) where $p\\in(1,2]$,\nwhich not only generalizes the traditional finite variance assumption ($p=2$)\nbut also has been observed in practice for several different tasks. Under this\nchallenging assumption, lots of new progress has been made for either convex or\nnonconvex problems, however, most of which only consider smooth objectives. In\ncontrast, people have not fully explored and well understood this problem when\nfunctions are nonsmooth. This paper aims to fill this crucial gap by providing\na comprehensive analysis of stochastic nonsmooth convex optimization with\nheavy-tailed noises. We revisit a simple clipping-based algorithm, whereas,\nwhich is only proved to converge in expectation but under the additional strong\nconvexity assumption. Under appropriate choices of parameters, for both convex\nand strongly convex functions, we not only establish the first high-probability\nrates but also give refined in-expectation bounds compared with existing works.\nRemarkably, all of our results are optimal (or nearly optimal up to logarithmic\nfactors) with respect to the time horizon $T$ even when $T$ is unknown in\nadvance. Additionally, we show how to make the algorithm parameter-free with\nrespect to $\\sigma$, in other words, the algorithm can still guarantee\nconvergence without any prior knowledge of $\\sigma$.\n","authors":["Zijian Liu","Zhengyuan Zhou"],"pdf_url":"https://arxiv.org/pdf/2303.12277v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.08996v2","updated":"2023-03-22T02:42:51Z","published":"2023-03-16T00:00:52Z","title":"Learning Spatio-Temporal Aggregations for Large-Scale Capacity Expansion\n  Problems","summary":"  Effective investment planning decisions are crucial to ensure cyber-physical\ninfrastructures satisfy performance requirements over an extended time horizon.\nComputing these decisions often requires solving Capacity Expansion Problems\n(CEPs). In the context of regional-scale energy systems, these problems are\nprohibitively expensive to solve due to large network sizes, heterogeneous node\ncharacteristics, and a large number of operational periods. To maintain\ntractability, traditional approaches aggregate network nodes and/or select a\nset of representative time periods. Often, these reductions do not capture\nsupply-demand variations that crucially impact CEP costs and constraints,\nleading to suboptimal decisions. Here, we propose a novel graph convolutional\nautoencoder approach for spatio-temporal aggregation of a generic CEP with\nheterogeneous nodes (CEPHN). Our architecture leverages graph pooling to\nidentify nodes with similar characteristics and minimizes a multi-objective\nloss function. This loss function is tailored to induce desirable spatial and\ntemporal aggregations with regard to tractability and optimality. In\nparticular, the output of the graph pooling provides a spatial aggregation\nwhile clustering the low-dimensional encoded representations yields a temporal\naggregation. We apply our approach to generation expansion planning of a\ncoupled 88-node power and natural gas system in New England. The resulting\naggregation leads to a simpler CEPHN with 6 nodes and a small set of\nrepresentative days selected from one year. We evaluate aggregation outcomes\nover a range of hyperparameters governing the loss function and compare\nresulting upper bounds on the original problem with those obtained using\nbenchmark methods. We show that our approach provides upper bounds that are 33%\n(resp. 10%) lower those than obtained from benchmark spatial (resp. temporal)\naggregation approaches.\n","authors":["Aron Brenner","Rahman Khorramfar","Saurabh Amin"],"pdf_url":"https://arxiv.org/pdf/2303.08996v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2204.12037v8","updated":"2023-03-22T02:41:58Z","published":"2022-04-26T02:22:28Z","title":"Causal Reasoning Meets Visual Representation Learning: A Prospective\n  Study","summary":"  Visual representation learning is ubiquitous in various real-world\napplications, including visual comprehension, video understanding, multi-modal\nanalysis, human-computer interaction, and urban computing. Due to the emergence\nof huge amounts of multi-modal heterogeneous spatial/temporal/spatial-temporal\ndata in big data era, the lack of interpretability, robustness, and\nout-of-distribution generalization are becoming the challenges of the existing\nvisual models. The majority of the existing methods tend to fit the original\ndata/variable distributions and ignore the essential causal relations behind\nthe multi-modal knowledge, which lacks unified guidance and analysis about why\nmodern visual representation learning methods easily collapse into data bias\nand have limited generalization and cognitive abilities. Inspired by the strong\ninference ability of human-level agents, recent years have therefore witnessed\ngreat effort in developing causal reasoning paradigms to realize robust\nrepresentation and model learning with good cognitive ability. In this paper,\nwe conduct a comprehensive review of existing causal reasoning methods for\nvisual representation learning, covering fundamental theories, models, and\ndatasets. The limitations of current methods and datasets are also discussed.\nMoreover, we propose some prospective challenges, opportunities, and future\nresearch directions for benchmarking causal reasoning algorithms in visual\nrepresentation learning. This paper aims to provide a comprehensive overview of\nthis emerging field, attract attention, encourage discussions, bring to the\nforefront the urgency of developing novel causal reasoning methods, publicly\navailable benchmarks, and consensus-building standards for reliable visual\nrepresentation learning and related real-world applications more efficiently.\n","authors":["Yang Liu","Yushen Wei","Hong Yan","Guanbin Li","Liang Lin"],"pdf_url":"https://arxiv.org/pdf/2204.12037v8.pdf","comment":"35 pages, 14 figures. This work has been accepted by Machine\n  Intelligence Research. The arxiv version is kept updating by adding more\n  novel methods, datasets and insights. The official video interpretation of\n  this paper can be referred at https://youtu.be/2lfNaTkcTHI"},{"id":"http://arxiv.org/abs/2201.12224v4","updated":"2023-03-22T02:33:47Z","published":"2022-01-28T16:27:21Z","title":"Learning Stationary Nash Equilibrium Policies in $n$-Player Stochastic\n  Games with Independent Chains","summary":"  We consider a subclass of $n$-player stochastic games, in which players have\ntheir own internal state/action spaces while they are coupled through their\npayoff functions. It is assumed that players' internal chains are driven by\nindependent transition probabilities. Moreover, players can receive only\nrealizations of their payoffs, not the actual functions, and cannot observe\neach other's states/actions. For this class of games, we first show that\nfinding a stationary Nash equilibrium (NE) policy without any assumption on the\nreward functions is interactable. However, for general reward functions, we\ndevelop polynomial-time learning algorithms based on dual averaging and dual\nmirror descent, which converge in terms of the averaged Nikaido-Isoda distance\nto the set of $\\epsilon$-NE policies almost surely or in expectation. In\nparticular, under extra assumptions on the reward functions such as social\nconcavity, we derive polynomial upper bounds on the number of iterates to\nachieve an $\\epsilon$-NE policy with high probability. Finally, we evaluate the\neffectiveness of the proposed algorithms in learning $\\epsilon$-NE policies\nusing numerical experiments for energy management in smart grids.\n","authors":["S. Rasoul Etesami"],"pdf_url":"https://arxiv.org/pdf/2201.12224v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.12267v1","updated":"2023-03-22T02:28:54Z","published":"2023-03-22T02:28:54Z","title":"AUTO: Adaptive Outlier Optimization for Online Test-Time OOD Detection","summary":"  Out-of-distribution (OOD) detection is a crucial aspect of deploying machine\nlearning models in open-world applications. Empirical evidence suggests that\ntraining with auxiliary outliers substantially improves OOD detection. However,\nsuch outliers typically exhibit a distribution gap compared to the test OOD\ndata and do not cover all possible test OOD scenarios. Additionally,\nincorporating these outliers introduces additional training burdens. In this\npaper, we introduce a novel paradigm called test-time OOD detection, which\nutilizes unlabeled online data directly at test time to improve OOD detection\nperformance. While this paradigm is efficient, it also presents challenges such\nas catastrophic forgetting. To address these challenges, we propose adaptive\noutlier optimization (AUTO), which consists of an in-out-aware filter, an ID\nmemory bank, and a semantically-consistent objective. AUTO adaptively mines\npseudo-ID and pseudo-OOD samples from test data, utilizing them to optimize\nnetworks in real time during inference. Extensive results on CIFAR-10,\nCIFAR-100, and ImageNet benchmarks demonstrate that AUTO significantly enhances\nOOD detection performance.\n","authors":["Puning Yang","Jian Liang","Jie Cao","Ran He"],"pdf_url":"https://arxiv.org/pdf/2303.12267v1.pdf","comment":"15 pages"},{"id":"http://arxiv.org/abs/2303.12261v1","updated":"2023-03-22T02:04:39Z","published":"2023-03-22T02:04:39Z","title":"Challenges and opportunities for machine learning in multiscale\n  computational modeling","summary":"  Many mechanical engineering applications call for multiscale computational\nmodeling and simulation. However, solving for complex multiscale systems\nremains computationally onerous due to the high dimensionality of the solution\nspace. Recently, machine learning (ML) has emerged as a promising solution that\ncan either serve as a surrogate for, accelerate or augment traditional\nnumerical methods. Pioneering work has demonstrated that ML provides solutions\nto governing systems of equations with comparable accuracy to those obtained\nusing direct numerical methods, but with significantly faster computational\nspeed. These high-speed, high-fidelity estimations can facilitate the solving\nof complex multiscale systems by providing a better initial solution to\ntraditional solvers. This paper provides a perspective on the opportunities and\nchallenges of using ML for complex multiscale modeling and simulation. We first\noutline the current state-of-the-art ML approaches for simulating multiscale\nsystems and highlight some of the landmark developments. Next, we discuss\ncurrent challenges for ML in multiscale computational modeling, such as the\ndata and discretization dependence, interpretability, and data sharing and\ncollaborative platform development. Finally, we suggest several potential\nresearch directions for the future.\n","authors":["Phong C. H. Nguyen","Joseph B. Choi","H. S. Udaykumar","Stephen Baek"],"pdf_url":"https://arxiv.org/pdf/2303.12261v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.12260v1","updated":"2023-03-22T02:00:51Z","published":"2023-03-22T02:00:51Z","title":"Information-Based Sensor Placement for Data-Driven Estimation of\n  Unsteady Flows","summary":"  Estimation of unsteady flow fields around flight vehicles may improve flow\ninteractions and lead to enhanced vehicle performance. Although flow-field\nrepresentations can be very high-dimensional, their dynamics can have low-order\nrepresentations and may be estimated using a few, appropriately placed\nmeasurements. This paper presents a sensor-selection framework for the intended\napplication of data-driven, flow-field estimation. This framework combines\ndata-driven modeling, steady-state Kalman Filter design, and a sparsification\ntechnique for sequential selection of sensors. This paper also uses the sensor\nselection framework to design sensor arrays that can perform well across a\nvariety of operating conditions. Flow estimation results on numerical data show\nthat the proposed framework produces arrays that are highly effective at\nflow-field estimation for the flow behind and an airfoil at a high angle of\nattack using embedded pressure sensors. Analysis of the flow fields reveals\nthat paths of impinging stagnation points along the airfoil's surface during a\nshedding period of the flow are highly informative locations for placement of\npressure sensors.\n","authors":["John Graff","Albert Medina","Francis Lagor"],"pdf_url":"https://arxiv.org/pdf/2303.12260v1.pdf","comment":"23 pages, 9 figures, submitted to AIAA Journal"},{"id":"http://arxiv.org/abs/2211.04561v2","updated":"2023-03-22T01:53:41Z","published":"2022-11-08T21:16:00Z","title":"A physics-aware deep learning model for energy localization in\n  multiscale shock-to-detonation simulations of heterogeneous energetic\n  materials","summary":"  Predictive simulations of the shock-to-detonation transition (SDT) in\nheterogeneous energetic materials (EM) are vital to the design and control of\ntheir energy release and sensitivity. Due to the complexity of the\nthermo-mechanics of EM during the SDT, both macro-scale response and sub-grid\nmesoscale energy localization must be captured accurately. This work proposes\nan efficient and accurate multiscale framework for SDT simulations of EM. We\nintroduce a new approach for SDT simulation by using deep learning to model the\nmesoscale energy localization of shock-initiated EM microstructures. The\nproposed multiscale modeling framework is divided into two stages. First, a\nphysics-aware recurrent convolutional neural network (PARC) is used to model\nthe mesoscale energy localization of shock-initiated heterogeneous EM\nmicrostructures. PARC is trained using direct numerical simulations (DNS) of\nhotspot ignition and growth within microstructures of pressed HMX material\nsubjected to different input shock strengths. After training, PARC is employed\nto supply hotspot ignition and growth rates for macroscale SDT simulations. We\nshow that PARC can play the role of a surrogate model in a multiscale\nsimulation framework, while drastically reducing the computation cost and\nproviding improved representations of the sub-grid physics. The proposed\nmultiscale modeling approach will provide a new tool for material scientists in\ndesigning high-performance and safer energetic materials.\n","authors":["Phong C. H. Nguyen","Yen-Thi Nguyen","Pradeep K. Seshadri","Joseph B. Choi","H. S. Udaykumar","Stephen Baek"],"pdf_url":"https://arxiv.org/pdf/2211.04561v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.12255v1","updated":"2023-03-22T01:45:35Z","published":"2023-03-22T01:45:35Z","title":"Encoding Binary Concepts in the Latent Space of Generative Models for\n  Enhancing Data Representation","summary":"  Binary concepts are empirically used by humans to generalize efficiently. And\nthey are based on Bernoulli distribution which is the building block of\ninformation. These concepts span both low-level and high-level features such as\n\"large vs small\" and \"a neuron is active or inactive\". Binary concepts are\nubiquitous features and can be used to transfer knowledge to improve model\ngeneralization. We propose a novel binarized regularization to facilitate\nlearning of binary concepts to improve the quality of data generation in\nautoencoders. We introduce a binarizing hyperparameter $r$ in data generation\nprocess to disentangle the latent space symmetrically. We demonstrate that this\nmethod can be applied easily to existing variational autoencoder (VAE) variants\nto encourage symmetric disentanglement, improve reconstruction quality, and\nprevent posterior collapse without computation overhead. We also demonstrate\nthat this method can boost existing models to learn more transferable\nrepresentations and generate more representative samples for the input\ndistribution which can alleviate catastrophic forgetting using generative\nreplay under continual learning settings.\n","authors":["Zizhao Hu","Mohammad Rostami"],"pdf_url":"https://arxiv.org/pdf/2303.12255v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.11553v2","updated":"2023-03-22T01:13:23Z","published":"2023-03-21T02:44:15Z","title":"Dynamic Vertex Replacement Grammars","summary":"  Context-free graph grammars have shown a remarkable ability to model\nstructures in real-world relational data. However, graph grammars lack the\nability to capture time-changing phenomena since the left-to-right transitions\nof a production rule do not represent temporal change. In the present work, we\ndescribe dynamic vertex-replacement grammars (DyVeRG), which generalize vertex\nreplacement grammars in the time domain by providing a formal framework for\nupdating a learned graph grammar in accordance with modifications to its\nunderlying data. We show that DyVeRG grammars can be learned from, and used to\ngenerate, real-world dynamic graphs faithfully while remaining\nhuman-interpretable. We also demonstrate their ability to forecast by computing\ndyvergence scores, a novel graph similarity measurement exposed by this\nframework.\n","authors":["Daniel Gonzalez Cedre","Justus Isaiah Hibshman","Timothy La Fond","Grant Boquet","Tim Weninger"],"pdf_url":"https://arxiv.org/pdf/2303.11553v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.12247v1","updated":"2023-03-22T01:01:14Z","published":"2023-03-22T01:01:14Z","title":"Exploring the Benefits of Visual Prompting in Differential Privacy","summary":"  Visual Prompting (VP) is an emerging and powerful technique that allows\nsample-efficient adaptation to downstream tasks by engineering a well-trained\nfrozen source model. In this work, we explore the benefits of VP in\nconstructing compelling neural network classifiers with differential privacy\n(DP). We explore and integrate VP into canonical DP training methods and\ndemonstrate its simplicity and efficiency. In particular, we discover that VP\nin tandem with PATE, a state-of-the-art DP training method that leverages the\nknowledge transfer from an ensemble of teachers, achieves the state-of-the-art\nprivacy-utility trade-off with minimum expenditure of privacy budget. Moreover,\nwe conduct additional experiments on cross-domain image classification with a\nsufficient domain gap to further unveil the advantage of VP in DP. Lastly, we\nalso conduct extensive ablation studies to validate the effectiveness and\ncontribution of VP under DP consideration.\n","authors":["Yizhe Li","Yu-Lin Tsai","Xuebin Ren","Chia-Mu Yu","Pin-Yu Chen"],"pdf_url":"https://arxiv.org/pdf/2303.12247v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.12245v1","updated":"2023-03-22T00:51:11Z","published":"2023-03-22T00:51:11Z","title":"Error Analysis of Physics-Informed Neural Networks for Approximating\n  Dynamic PDEs of Second Order in Time","summary":"  We consider the approximation of a class of dynamic partial differential\nequations (PDE) of second order in time by the physics-informed neural network\n(PINN) approach, and provide an error analysis of PINN for the wave equation,\nthe Sine-Gordon equation and the linear elastodynamic equation. Our analyses\nshow that, with feed-forward neural networks having two hidden layers and the\n$\\tanh$ activation function, the PINN approximation errors for the solution\nfield, its time derivative and its gradient field can be effectively bounded by\nthe training loss and the number of training data points (quadrature points).\nOur analyses further suggest new forms for the training loss function, which\ncontain certain residuals that are crucial to the error estimate but would be\nabsent from the canonical PINN loss formulation. Adopting these new forms for\nthe loss function leads to a variant PINN algorithm. We present ample numerical\nexperiments with the new PINN algorithm for the wave equation, the Sine-Gordon\nequation and the linear elastodynamic equation, which show that the method can\ncapture the solution well.\n","authors":["Yanxia Qian","Yongchao Zhang","Yunqing Huang","Suchuan Dong"],"pdf_url":"https://arxiv.org/pdf/2303.12245v1.pdf","comment":"46 pages, 14 figures, 3 tables"},{"id":"http://arxiv.org/abs/2204.14079v3","updated":"2023-03-22T00:49:31Z","published":"2022-04-29T13:21:14Z","title":"Fix the Noise: Disentangling Source Feature for Transfer Learning of\n  StyleGAN","summary":"  Transfer learning of StyleGAN has recently shown great potential to solve\ndiverse tasks, especially in domain translation. Previous methods utilized a\nsource model by swapping or freezing weights during transfer learning, however,\nthey have limitations on visual quality and controlling source features. In\nother words, they require additional models that are computationally demanding\nand have restricted control steps that prevent a smooth transition. In this\npaper, we propose a new approach to overcome these limitations. Instead of\nswapping or freezing, we introduce a simple feature matching loss to improve\ngeneration quality. In addition, to control the degree of source features, we\ntrain a target model with the proposed strategy, FixNoise, to preserve the\nsource features only in a disentangled subspace of a target feature space.\nOwing to the disentangled feature space, our method can smoothly control the\ndegree of the source features in a single model. Extensive experiments\ndemonstrate that the proposed method can generate more consistent and realistic\nimages than previous works.\n","authors":["Dongyeun Lee","Jae Young Lee","Doyeon Kim","Jaehyun Choi","Junmo Kim"],"pdf_url":"https://arxiv.org/pdf/2204.14079v3.pdf","comment":"Full CVPR 2023 paper is available at arXiv:2303.11545. Best paper of\n  CVPRW AICC 2022 (CVPR 2022 Workshop on AI for Content Creation). The code is\n  available at https://github.com/LeeDongYeun/FixNoise"},{"id":"http://arxiv.org/abs/2212.04493v2","updated":"2023-03-22T00:30:56Z","published":"2022-12-08T18:59:05Z","title":"SDFusion: Multimodal 3D Shape Completion, Reconstruction, and Generation","summary":"  In this work, we present a novel framework built to simplify 3D asset\ngeneration for amateur users. To enable interactive generation, our method\nsupports a variety of input modalities that can be easily provided by a human,\nincluding images, text, partially observed shapes and combinations of these,\nfurther allowing to adjust the strength of each input. At the core of our\napproach is an encoder-decoder, compressing 3D shapes into a compact latent\nrepresentation, upon which a diffusion model is learned. To enable a variety of\nmulti-modal inputs, we employ task-specific encoders with dropout followed by a\ncross-attention mechanism. Due to its flexibility, our model naturally supports\na variety of tasks, outperforming prior works on shape completion, image-based\n3D reconstruction, and text-to-3D. Most interestingly, our model can combine\nall these tasks into one swiss-army-knife tool, enabling the user to perform\nshape generation using incomplete shapes, images, and textual descriptions at\nthe same time, providing the relative weights for each input and facilitating\ninteractivity. Despite our approach being shape-only, we further show an\nefficient method to texture the generated shape using large-scale text-to-image\nmodels.\n","authors":["Yen-Chi Cheng","Hsin-Ying Lee","Sergey Tulyakov","Alexander Schwing","Liangyan Gui"],"pdf_url":"https://arxiv.org/pdf/2212.04493v2.pdf","comment":"In CVPR 2023. Project page and code is available at:\n  https://yccyenchicheng.github.io/SDFusion/. Fix some typos"},{"id":"http://arxiv.org/abs/2208.09723v2","updated":"2023-03-22T00:20:32Z","published":"2022-08-20T17:13:43Z","title":"Matrix Completion with Cross-Concentrated Sampling: Bridging Uniform\n  Sampling and CUR Sampling","summary":"  While uniform sampling has been widely studied in the matrix completion\nliterature, CUR sampling approximates a low-rank matrix via row and column\nsamples. Unfortunately, both sampling models lack flexibility for various\ncircumstances in real-world applications. In this work, we propose a novel and\neasy-to-implement sampling strategy, coined Cross-Concentrated Sampling (CCS).\nBy bridging uniform sampling and CUR sampling, CCS provides extra flexibility\nthat can potentially save sampling costs in applications. In addition, we also\nprovide a sufficient condition for CCS-based matrix completion. Moreover, we\npropose a highly efficient non-convex algorithm, termed Iterative CUR\nCompletion (ICURC), for the proposed CCS model. Numerical experiments verify\nthe empirical advantages of CCS and ICURC against uniform sampling and its\nbaseline algorithms, on both synthetic and real-world datasets.\n","authors":["HanQin Cai","Longxiu Huang","Pengyu Li","Deanna Needell"],"pdf_url":"https://arxiv.org/pdf/2208.09723v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.12961v1","updated":"2023-03-22T23:54:14Z","published":"2023-03-22T23:54:14Z","title":"The Shaky Foundations of Clinical Foundation Models: A Survey of Large\n  Language Models and Foundation Models for EMRs","summary":"  The successes of foundation models such as ChatGPT and AlphaFold have spurred\nsignificant interest in building similar models for electronic medical records\n(EMRs) to improve patient care and hospital operations. However, recent hype\nhas obscured critical gaps in our understanding of these models' capabilities.\nWe review over 80 foundation models trained on non-imaging EMR data (i.e.\nclinical text and/or structured data) and create a taxonomy delineating their\narchitectures, training data, and potential use cases. We find that most models\nare trained on small, narrowly-scoped clinical datasets (e.g. MIMIC-III) or\nbroad, public biomedical corpora (e.g. PubMed) and are evaluated on tasks that\ndo not provide meaningful insights on their usefulness to health systems. In\nlight of these findings, we propose an improved evaluation framework for\nmeasuring the benefits of clinical foundation models that is more closely\ngrounded to metrics that matter in healthcare.\n","authors":["Michael Wornow","Yizhe Xu","Rahul Thapa","Birju Patel","Ethan Steinberg","Scott Fleming","Michael A. Pfeffer","Jason Fries","Nigam H. Shah"],"pdf_url":"https://arxiv.org/pdf/2303.12961v1.pdf","comment":"16 pages, 4 figures, submitted to NPJ Digital Medicine"},{"id":"http://arxiv.org/abs/2303.12959v1","updated":"2023-03-22T23:38:10Z","published":"2023-03-22T23:38:10Z","title":"Variantional autoencoder with decremental information bottleneck for\n  disentanglement","summary":"  One major challenge of disentanglement learning with variational autoencoders\nis the trade-off between disentanglement and reconstruction fidelity. Previous\nincremental methods with only on latent space cannot optimize these two targets\nsimultaneously, so they expand the Information Bottleneck while training to\n{optimize from disentanglement to reconstruction. However, a large bottleneck\nwill lose the constraint of disentanglement, causing the information diffusion\nproblem. To tackle this issue, we present a novel decremental variational\nautoencoder with disentanglement-invariant transformations to optimize multiple\nobjectives in different layers, termed DeVAE, for balancing disentanglement and\nreconstruction fidelity by decreasing the information bottleneck of diverse\nlatent spaces gradually. Benefiting from the multiple latent spaces, DeVAE\nallows simultaneous optimization of multiple objectives to optimize\nreconstruction while keeping the constraint of disentanglement, avoiding\ninformation diffusion. DeVAE is also compatible with large models with\nhigh-dimension latent space. Experimental results on dSprites and Shapes3D that\nDeVAE achieves \\fix{R2q6}{a good balance between disentanglement and\nreconstruction.DeVAE shows high tolerant of hyperparameters and on\nhigh-dimensional latent spaces.\n","authors":["Jiantao Wu","Shentong Mo","Muhammad Awais","Sara Atito","Xingshen Zhang","Lin Wang","Xiang Yang"],"pdf_url":"https://arxiv.org/pdf/2303.12959v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.12957v1","updated":"2023-03-22T23:37:28Z","published":"2023-03-22T23:37:28Z","title":"Reinforcement Learning with Exogenous States and Rewards","summary":"  Exogenous state variables and rewards can slow reinforcement learning by\ninjecting uncontrolled variation into the reward signal. This paper formalizes\nexogenous state variables and rewards and shows that if the reward function\ndecomposes additively into endogenous and exogenous components, the MDP can be\ndecomposed into an exogenous Markov Reward Process (based on the exogenous\nreward) and an endogenous Markov Decision Process (optimizing the endogenous\nreward). Any optimal policy for the endogenous MDP is also an optimal policy\nfor the original MDP, but because the endogenous reward typically has reduced\nvariance, the endogenous MDP is easier to solve. We study settings where the\ndecomposition of the state space into exogenous and endogenous state spaces is\nnot given but must be discovered. The paper introduces and proves correctness\nof algorithms for discovering the exogenous and endogenous subspaces of the\nstate space when they are mixed through linear combination. These algorithms\ncan be applied during reinforcement learning to discover the exogenous space,\nremove the exogenous reward, and focus reinforcement learning on the endogenous\nMDP. Experiments on a variety of challenging synthetic MDPs show that these\nmethods, applied online, discover large exogenous state spaces and produce\nsubstantial speedups in reinforcement learning.\n","authors":["George Trimponias","Thomas G. Dietterich"],"pdf_url":"https://arxiv.org/pdf/2303.12957v1.pdf","comment":"Greatly extends the initial work reported in 1806.01584"},{"id":"http://arxiv.org/abs/2303.12952v1","updated":"2023-03-22T23:24:47Z","published":"2023-03-22T23:24:47Z","title":"TSI-GAN: Unsupervised Time Series Anomaly Detection using Convolutional\n  Cycle-Consistent Generative Adversarial Networks","summary":"  Anomaly detection is widely used in network intrusion detection, autonomous\ndriving, medical diagnosis, credit card frauds, etc. However, several key\nchallenges remain open, such as lack of ground truth labels, presence of\ncomplex temporal patterns, and generalizing over different datasets. This paper\nproposes TSI-GAN, an unsupervised anomaly detection model for time-series that\ncan learn complex temporal patterns automatically and generalize well, i.e., no\nneed for choosing dataset-specific parameters, making statistical assumptions\nabout underlying data, or changing model architectures. To achieve these goals,\nwe convert each input time-series into a sequence of 2D images using two\nencoding techniques with the intent of capturing temporal patterns and various\ntypes of deviance. Moreover, we design a reconstructive GAN that uses\nconvolutional layers in an encoder-decoder network and employs\ncycle-consistency loss during training to ensure that inverse mappings are\naccurate as well. In addition, we also instrument a Hodrick-Prescott filter in\npost-processing to mitigate false positives. We evaluate TSI-GAN using 250\nwell-curated and harder-than-usual datasets and compare with 8 state-of-the-art\nbaseline methods. The results demonstrate the superiority of TSI-GAN to all the\nbaselines, offering an overall performance improvement of 13% and 31% over the\nsecond-best performer MERLIN and the third-best performer LSTM-AE,\nrespectively.\n","authors":["Shyam Sundar Saravanan","Tie Luo","Mao Van Ngo"],"pdf_url":"https://arxiv.org/pdf/2303.12952v1.pdf","comment":"To appear in the Proceedings of PAKDD 2023 (27th Pacific-Asia\n  Conference on Knowledge Discovery and Data Mining)"},{"id":"http://arxiv.org/abs/2303.02829v2","updated":"2023-03-22T22:51:36Z","published":"2023-03-06T01:46:51Z","title":"Attribution-Scores and Causal Counterfactuals as Explanations in\n  Artificial Intelligence","summary":"  In this expository article we highlight the relevance of explanations for\nartificial intelligence, in general, and for the newer developments in {\\em\nexplainable AI}, referring to origins and connections of and among different\napproaches. We describe in simple terms, explanations in data management and\nmachine learning that are based on attribution-scores, and counterfactuals as\nfound in the area of causality. We elaborate on the importance of logical\nreasoning when dealing with counterfactuals, and their use for score\ncomputation.\n","authors":["Leopoldo Bertossi"],"pdf_url":"https://arxiv.org/pdf/2303.02829v2.pdf","comment":"Submitted as chapter contribution. In this version some additional\n  comments were added, and some wrong equation references corrected"},{"id":"http://arxiv.org/abs/2110.15332v2","updated":"2023-03-22T22:24:18Z","published":"2021-10-28T17:46:14Z","title":"Proximal Reinforcement Learning: Efficient Off-Policy Evaluation in\n  Partially Observed Markov Decision Processes","summary":"  In applications of offline reinforcement learning to observational data, such\nas in healthcare or education, a general concern is that observed actions might\nbe affected by unobserved factors, inducing confounding and biasing estimates\nderived under the assumption of a perfect Markov decision process (MDP) model.\nHere we tackle this by considering off-policy evaluation in a partially\nobserved MDP (POMDP). Specifically, we consider estimating the value of a given\ntarget policy in a POMDP given trajectories with only partial state\nobservations generated by a different and unknown policy that may depend on the\nunobserved state. We tackle two questions: what conditions allow us to identify\nthe target policy value from the observed data and, given identification, how\nto best estimate it. To answer these, we extend the framework of proximal\ncausal inference to our POMDP setting, providing a variety of settings where\nidentification is made possible by the existence of so-called bridge functions.\nWe then show how to construct semiparametrically efficient estimators in these\nsettings. We term the resulting framework proximal reinforcement learning\n(PRL). We demonstrate the benefits of PRL in an extensive simulation study and\non the problem of sepsis management.\n","authors":["Andrew Bennett","Nathan Kallus"],"pdf_url":"https://arxiv.org/pdf/2110.15332v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2111.03837v2","updated":"2023-03-22T22:18:01Z","published":"2021-11-06T09:04:16Z","title":"Focusing on Potential Named Entities During Active Label Acquisition","summary":"  Named entity recognition (NER) aims to identify mentions of named entities in\nan unstructured text and classify them into predefined named entity classes.\nWhile deep learning-based pre-trained language models help to achieve good\npredictive performances in NER, many domain-specific NER applications still\ncall for a substantial amount of labeled data. Active learning (AL), a general\nframework for the label acquisition problem, has been used for NER tasks to\nminimize the annotation cost without sacrificing model performance. However,\nthe heavily imbalanced class distribution of tokens introduces challenges in\ndesigning effective AL querying methods for NER. We propose several AL sentence\nquery evaluation functions that pay more attention to potential positive\ntokens, and evaluate these proposed functions with both sentence-based and\ntoken-based cost evaluation strategies. We also propose a better data-driven\nnormalization approach to penalize sentences that are too long or too short.\nOur experiments on three datasets from different domains reveal that the\nproposed approach reduces the number of annotated tokens while achieving better\nor comparable prediction performance with conventional methods.\n","authors":["Ali Osman Berk Sapci","Oznur Tastan","Reyyan Yeniterzi"],"pdf_url":"https://arxiv.org/pdf/2111.03837v2.pdf","comment":"20 pages, 8 figures"},{"id":"http://arxiv.org/abs/2303.12928v1","updated":"2023-03-22T21:55:30Z","published":"2023-03-22T21:55:30Z","title":"Leveraging Multi-time Hamilton-Jacobi PDEs for Certain Scientific\n  Machine Learning Problems","summary":"  Hamilton-Jacobi partial differential equations (HJ PDEs) have deep\nconnections with a wide range of fields, including optimal control,\ndifferential games, and imaging sciences. By considering the time variable to\nbe a higher dimensional quantity, HJ PDEs can be extended to the multi-time\ncase. In this paper, we establish a novel theoretical connection between\nspecific optimization problems arising in machine learning and the multi-time\nHopf formula, which corresponds to a representation of the solution to certain\nmulti-time HJ PDEs. Through this connection, we increase the interpretability\nof the training process of certain machine learning applications by showing\nthat when we solve these learning problems, we also solve a multi-time HJ PDE\nand, by extension, its corresponding optimal control problem. As a first\nexploration of this connection, we develop the relation between the regularized\nlinear regression problem and the Linear Quadratic Regulator (LQR). We then\nleverage our theoretical connection to adapt standard LQR solvers (namely,\nthose based on the Riccati ordinary differential equations) to design new\ntraining approaches for machine learning. Finally, we provide some numerical\nexamples that demonstrate the versatility and possible computational advantages\nof our Riccati-based approach in the context of continual learning,\npost-training calibration, transfer learning, and sparse dynamics\nidentification.\n","authors":["Paula Chen","Tingwei Meng","Zongren Zou","Jérôme Darbon","George Em Karniadakis"],"pdf_url":"https://arxiv.org/pdf/2303.12928v1.pdf","comment":"26 pages in total, 23 pages for the main text, 9 figures"},{"id":"http://arxiv.org/abs/2301.02232v2","updated":"2023-03-22T21:53:02Z","published":"2023-01-05T18:57:12Z","title":"CA$^2$T-Net: Category-Agnostic 3D Articulation Transfer from Single\n  Image","summary":"  We present a neural network approach to transfer the motion from a single\nimage of an articulated object to a rest-state (i.e., unarticulated) 3D model.\nOur network learns to predict the object's pose, part segmentation, and\ncorresponding motion parameters to reproduce the articulation shown in the\ninput image. The network is composed of three distinct branches that take a\nshared joint image-shape embedding and is trained end-to-end. Unlike previous\nmethods, our approach is independent of the topology of the object and can work\nwith objects from arbitrary categories. Our method, trained with only synthetic\ndata, can be used to automatically animate a mesh, infer motion from real\nimages, and transfer articulation to functionally similar but geometrically\ndistinct 3D models at test time.\n","authors":["Jasmine Collins","Anqi Liang","Jitendra Malik","Hao Zhang","Frédéric Devernay"],"pdf_url":"https://arxiv.org/pdf/2301.02232v2.pdf","comment":"8 pages"},{"id":"http://arxiv.org/abs/2012.09422v4","updated":"2023-03-22T21:49:23Z","published":"2020-12-17T07:21:06Z","title":"The Variational Method of Moments","summary":"  The conditional moment problem is a powerful formulation for describing\nstructural causal parameters in terms of observables, a prominent example being\ninstrumental variable regression. A standard approach reduces the problem to a\nfinite set of marginal moment conditions and applies the optimally weighted\ngeneralized method of moments (OWGMM), but this requires we know a finite set\nof identifying moments, can still be inefficient even if identifying, or can be\ntheoretically efficient but practically unwieldy if we use a growing sieve of\nmoment conditions. Motivated by a variational minimax reformulation of OWGMM,\nwe define a very general class of estimators for the conditional moment\nproblem, which we term the variational method of moments (VMM) and which\nnaturally enables controlling infinitely-many moments. We provide a detailed\ntheoretical analysis of multiple VMM estimators, including ones based on kernel\nmethods and neural nets, and provide conditions under which these are\nconsistent, asymptotically normal, and semiparametrically efficient in the full\nconditional moment model. We additionally provide algorithms for valid\nstatistical inference based on the same kind of variational reformulations,\nboth for kernel- and neural-net-based varieties. Finally, we demonstrate the\nstrong performance of our proposed estimation and inference algorithms in a\ndetailed series of synthetic experiments.\n","authors":["Andrew Bennett","Nathan Kallus"],"pdf_url":"https://arxiv.org/pdf/2012.09422v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.12922v1","updated":"2023-03-22T21:36:56Z","published":"2023-03-22T21:36:56Z","title":"Revisiting the Fragility of Influence Functions","summary":"  In the last few years, many works have tried to explain the predictions of\ndeep learning models. Few methods, however, have been proposed to verify the\naccuracy or faithfulness of these explanations. Recently, influence functions,\nwhich is a method that approximates the effect that leave-one-out training has\non the loss function, has been shown to be fragile. The proposed reason for\ntheir fragility remains unclear. Although previous work suggests the use of\nregularization to increase robustness, this does not hold in all cases. In this\nwork, we seek to investigate the experiments performed in the prior work in an\neffort to understand the underlying mechanisms of influence function fragility.\nFirst, we verify influence functions using procedures from the literature under\nconditions where the convexity assumptions of influence functions are met.\nThen, we relax these assumptions and study the effects of non-convexity by\nusing deeper models and more complex datasets. Here, we analyze the key metrics\nand procedures that are used to validate influence functions. Our results\nindicate that the validation procedures may cause the observed fragility.\n","authors":["Jacob R. Epifano","Ravi P. Ramachandran","Aaron J. Masino","Ghulam Rasool"],"pdf_url":"https://arxiv.org/pdf/2303.12922v1.pdf","comment":"11 pages, 5 figures, accepted to Neural Networks"},{"id":"http://arxiv.org/abs/2303.12921v1","updated":"2023-03-22T21:35:50Z","published":"2023-03-22T21:35:50Z","title":"Stability is Stable: Connections between Replicability, Privacy, and\n  Adaptive Generalization","summary":"  The notion of replicable algorithms was introduced in Impagliazzo et al.\n[STOC '22] to describe randomized algorithms that are stable under the\nresampling of their inputs. More precisely, a replicable algorithm gives the\nsame output with high probability when its randomness is fixed and it is run on\na new i.i.d. sample drawn from the same distribution. Using replicable\nalgorithms for data analysis can facilitate the verification of published\nresults by ensuring that the results of an analysis will be the same with high\nprobability, even when that analysis is performed on a new data set.\n  In this work, we establish new connections and separations between\nreplicability and standard notions of algorithmic stability. In particular, we\ngive sample-efficient algorithmic reductions between perfect generalization,\napproximate differential privacy, and replicability for a broad class of\nstatistical problems. Conversely, we show any such equivalence must break down\ncomputationally: there exist statistical problems that are easy under\ndifferential privacy, but that cannot be solved replicably without breaking\npublic-key cryptography. Furthermore, these results are tight: our reductions\nare statistically optimal, and we show that any computational separation\nbetween DP and replicability must imply the existence of one-way functions.\n  Our statistical reductions give a new algorithmic framework for translating\nbetween notions of stability, which we instantiate to answer several open\nquestions in replicability and privacy. This includes giving sample-efficient\nreplicable algorithms for various PAC learning, distribution estimation, and\ndistribution testing problems, algorithmic amplification of $\\delta$ in\napproximate DP, conversions from item-level to user-level privacy, and the\nexistence of private agnostic-to-realizable learning reductions under\nstructured distributions.\n","authors":["Mark Bun","Marco Gaboardi","Max Hopkins","Russell Impagliazzo","Rex Lei","Toniann Pitassi","Jessica Sorrell","Satchit Sivakumar"],"pdf_url":"https://arxiv.org/pdf/2303.12921v1.pdf","comment":"STOC 2023"},{"id":"http://arxiv.org/abs/2303.12915v1","updated":"2023-03-22T21:09:54Z","published":"2023-03-22T21:09:54Z","title":"Self-distillation for surgical action recognition","summary":"  Surgical scene understanding is a key prerequisite for contextaware decision\nsupport in the operating room. While deep learning-based approaches have\nalready reached or even surpassed human performance in various fields, the task\nof surgical action recognition remains a major challenge. With this\ncontribution, we are the first to investigate the concept of self-distillation\nas a means of addressing class imbalance and potential label ambiguity in\nsurgical video analysis. Our proposed method is a heterogeneous ensemble of\nthree models that use Swin Transfomers as backbone and the concepts of\nself-distillation and multi-task learning as core design choices. According to\nablation studies performed with the CholecT45 challenge data via\ncross-validation, the biggest performance boost is achieved by the usage of\nsoft labels obtained by self-distillation. External validation of our method on\nan independent test set was achieved by providing a Docker container of our\ninference model to the challenge organizers. According to their analysis, our\nmethod outperforms all other solutions submitted to the latest challenge in the\nfield. Our approach thus shows the potential of self-distillation for becoming\nan important tool in medical image analysis applications.\n","authors":["Amine Yamlahi","Thuy Nuong Tran","Patrick Godau","Melanie Schellenberg","Dominik Michael","Finn-Henri Smidt","Jan-Hinrich Noelke","Tim Adler","Minu Dietlinde Tizabi","Chinedu Nwoye","Nicolas Padoy","Lena Maier-Hein"],"pdf_url":"https://arxiv.org/pdf/2303.12915v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.12914v1","updated":"2023-03-22T21:09:49Z","published":"2023-03-22T21:09:49Z","title":"TRON: Transformer Neural Network Acceleration with Non-Coherent Silicon\n  Photonics","summary":"  Transformer neural networks are rapidly being integrated into\nstate-of-the-art solutions for natural language processing (NLP) and computer\nvision. However, the complex structure of these models creates challenges for\naccelerating their execution on conventional electronic platforms. We propose\nthe first silicon photonic hardware neural network accelerator called TRON for\ntransformer-based models such as BERT, and Vision Transformers. Our analysis\ndemonstrates that TRON exhibits at least 14x better throughput and 8x better\nenergy efficiency, in comparison to state-of-the-art transformer accelerators.\n","authors":["Salma Afifi","Febin Sunny","Mahdi Nikdast","Sudeep Pasricha"],"pdf_url":"https://arxiv.org/pdf/2303.12914v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.12910v1","updated":"2023-03-22T21:03:40Z","published":"2023-03-22T21:03:40Z","title":"Cross-Layer Design for AI Acceleration with Non-Coherent Optical\n  Computing","summary":"  Emerging AI applications such as ChatGPT, graph convolutional networks, and\nother deep neural networks require massive computational resources for training\nand inference. Contemporary computing platforms such as CPUs, GPUs, and TPUs\nare struggling to keep up with the demands of these AI applications.\nNon-coherent optical computing represents a promising approach for light-speed\nacceleration of AI workloads. In this paper, we show how cross-layer design can\novercome challenges in non-coherent optical computing platforms. We describe\napproaches for optical device engineering, tuning circuit enhancements, and\narchitectural innovations to adapt optical computing to a variety of AI\nworkloads. We also discuss techniques for hardware/software co-design that can\nintelligently map and adapt AI software to improve its performance on\nnon-coherent optical computing platforms.\n","authors":["Febin Sunny","Mahdi Nikdast","Sudeep Pasricha"],"pdf_url":"https://arxiv.org/pdf/2303.12910v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2203.00870v3","updated":"2023-03-22T20:56:42Z","published":"2022-03-02T04:44:52Z","title":"Faith-Shap: The Faithful Shapley Interaction Index","summary":"  Shapley values, which were originally designed to assign attributions to\nindividual players in coalition games, have become a commonly used approach in\nexplainable machine learning to provide attributions to input features for\nblack-box machine learning models. A key attraction of Shapley values is that\nthey uniquely satisfy a very natural set of axiomatic properties. However,\nextending the Shapley value to assigning attributions to interactions rather\nthan individual players, an interaction index, is non-trivial: as the natural\nset of axioms for the original Shapley values, extended to the context of\ninteractions, no longer specify a unique interaction index. Many proposals thus\nintroduce additional less ''natural'' axioms, while sacrificing the key axiom\nof efficiency, in order to obtain unique interaction indices. In this work,\nrather than introduce additional conflicting axioms, we adopt the viewpoint of\nShapley values as coefficients of the most faithful linear approximation to the\npseudo-Boolean coalition game value function. By extending linear to\n$\\ell$-order polynomial approximations, we can then define the general family\nof faithful interaction indices. We show that by additionally requiring the\nfaithful interaction indices to satisfy interaction-extensions of the standard\nindividual Shapley axioms (dummy, symmetry, linearity, and efficiency), we\nobtain a unique Faithful Shapley Interaction index, which we denote Faith-Shap,\nas a natural generalization of the Shapley value to interactions. We then\nprovide some illustrative contrasts of Faith-Shap with previously proposed\ninteraction indices, and further investigate some of its interesting algebraic\nproperties. We further show the computational efficiency of computing\nFaith-Shap, together with some additional qualitative insights, via some\nillustrative experiments.\n","authors":["Che-Ping Tsai","Chih-Kuan Yeh","Pradeep Ravikumar"],"pdf_url":"https://arxiv.org/pdf/2203.00870v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.12891v1","updated":"2023-03-22T20:09:31Z","published":"2023-03-22T20:09:31Z","title":"Feature Reduction Method Comparison Towards Explainability and\n  Efficiency in Cybersecurity Intrusion Detection Systems","summary":"  In the realm of cybersecurity, intrusion detection systems (IDS) detect and\nprevent attacks based on collected computer and network data. In recent\nresearch, IDS models have been constructed using machine learning (ML) and deep\nlearning (DL) methods such as Random Forest (RF) and deep neural networks\n(DNN). Feature selection (FS) can be used to construct faster, more\ninterpretable, and more accurate models. We look at three different FS\ntechniques; RF information gain (RF-IG), correlation feature selection using\nthe Bat Algorithm (CFS-BA), and CFS using the Aquila Optimizer (CFS-AO). Our\nresults show CFS-BA to be the most efficient of the FS methods, building in 55%\nof the time of the best RF-IG model while achieving 99.99% of its accuracy.\nThis reinforces prior contributions attesting to CFS-BA's accuracy while\nbuilding upon the relationship between subset size, CFS score, and RF-IG score\nin final results.\n","authors":["Adam M. Lehavi","Seongtae Kim"],"pdf_url":"https://arxiv.org/pdf/2303.12891v1.pdf","comment":"Published in 2022 21st IEEE International Conference on Machine\n  Learning and Applications. 8 pages. 5 figures"},{"id":"http://arxiv.org/abs/2303.12888v1","updated":"2023-03-22T20:05:22Z","published":"2023-03-22T20:05:22Z","title":"A dynamic risk score for early prediction of cardiogenic shock using\n  machine learning","summary":"  Myocardial infarction and heart failure are major cardiovascular diseases\nthat affect millions of people in the US. The morbidity and mortality are\nhighest among patients who develop cardiogenic shock. Early recognition of\ncardiogenic shock is critical. Prompt implementation of treatment measures can\nprevent the deleterious spiral of ischemia, low blood pressure, and reduced\ncardiac output due to cardiogenic shock. However, early identification of\ncardiogenic shock has been challenging due to human providers' inability to\nprocess the enormous amount of data in the cardiac intensive care unit (ICU)\nand lack of an effective risk stratification tool. We developed a deep\nlearning-based risk stratification tool, called CShock, for patients admitted\ninto the cardiac ICU with acute decompensated heart failure and/or myocardial\ninfarction to predict onset of cardiogenic shock. To develop and validate\nCShock, we annotated cardiac ICU datasets with physician adjudicated outcomes.\nCShock achieved an area under the receiver operator characteristic curve\n(AUROC) of 0.820, which substantially outperformed CardShock (AUROC 0.519), a\nwell-established risk score for cardiogenic shock prognosis. CShock was\nexternally validated in an independent patient cohort and achieved an AUROC of\n0.800, demonstrating its generalizability in other cardiac ICUs.\n","authors":["Yuxuan Hu","Albert Lui","Mark Goldstein","Mukund Sudarshan","Andrea Tinsay","Cindy Tsui","Samuel Maidman","John Medamana","Neil Jethani","Aaalad Puli","Vuthy Nguy","Yindalon Aphinyanaphongs","Nicholas Kiefer","Nathaniel Smilowitz","James Horowitz","Tania Ahuja","Glenn Fishman","Judith Hochman","Stuart Katz","Samuel Bernard","Rajesh Ranganath"],"pdf_url":"https://arxiv.org/pdf/2303.12888v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.12878v1","updated":"2023-03-22T19:36:56Z","published":"2023-03-22T19:36:56Z","title":"Robust Consensus in Ranking Data Analysis: Definitions, Properties and\n  Computational Issues","summary":"  As the issue of robustness in AI systems becomes vital, statistical learning\ntechniques that are reliable even in presence of partly contaminated data have\nto be developed. Preference data, in the form of (complete) rankings in the\nsimplest situations, are no exception and the demand for appropriate concepts\nand tools is all the more pressing given that technologies fed by or producing\nthis type of data (e.g. search engines, recommending systems) are now massively\ndeployed. However, the lack of vector space structure for the set of rankings\n(i.e. the symmetric group $\\mathfrak{S}_n$) and the complex nature of\nstatistics considered in ranking data analysis make the formulation of\nrobustness objectives in this domain challenging. In this paper, we introduce\nnotions of robustness, together with dedicated statistical methods, for\nConsensus Ranking the flagship problem in ranking data analysis, aiming at\nsummarizing a probability distribution on $\\mathfrak{S}_n$ by a median ranking.\nPrecisely, we propose specific extensions of the popular concept of breakdown\npoint, tailored to consensus ranking, and address the related computational\nissues. Beyond the theoretical contributions, the relevance of the approach\nproposed is supported by an experimental study.\n","authors":["Morgane Goibert","Clément Calauzènes","Ekhine Irurozki","Stéphan Clémençon"],"pdf_url":"https://arxiv.org/pdf/2303.12878v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.12872v1","updated":"2023-03-22T19:17:57Z","published":"2023-03-22T19:17:57Z","title":"Human Uncertainty in Concept-Based AI Systems","summary":"  Placing a human in the loop may abate the risks of deploying AI systems in\nsafety-critical settings (e.g., a clinician working with a medical AI system).\nHowever, mitigating risks arising from human error and uncertainty within such\nhuman-AI interactions is an important and understudied issue. In this work, we\nstudy human uncertainty in the context of concept-based models, a family of AI\nsystems that enable human feedback via concept interventions where an expert\nintervenes on human-interpretable concepts relevant to the task. Prior work in\nthis space often assumes that humans are oracles who are always certain and\ncorrect. Yet, real-world decision-making by humans is prone to occasional\nmistakes and uncertainty. We study how existing concept-based models deal with\nuncertain interventions from humans using two novel datasets: UMNIST, a visual\ndataset with controlled simulated uncertainty based on the MNIST dataset, and\nCUB-S, a relabeling of the popular CUB concept dataset with rich,\ndensely-annotated soft labels from humans. We show that training with uncertain\nconcept labels may help mitigate weaknesses of concept-based systems when\nhandling uncertain interventions. These results allow us to identify several\nopen challenges, which we argue can be tackled through future multidisciplinary\nresearch on building interactive uncertainty-aware systems. To facilitate\nfurther research, we release a new elicitation platform, UElic, to collect\nuncertain feedback from humans in collaborative prediction tasks.\n","authors":["Katherine M. Collins","Matthew Barker","Mateo Espinosa Zarlenga","Naveen Raman","Umang Bhatt","Mateja Jamnik","Ilia Sucholutsky","Adrian Weller","Krishnamurthy Dvijotham"],"pdf_url":"https://arxiv.org/pdf/2303.12872v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.12865v1","updated":"2023-03-22T18:59:48Z","published":"2023-03-22T18:59:48Z","title":"NeRF-GAN Distillation for Efficient 3D-Aware Generation with\n  Convolutions","summary":"  Pose-conditioned convolutional generative models struggle with high-quality\n3D-consistent image generation from single-view datasets, due to their lack of\nsufficient 3D priors. Recently, the integration of Neural Radiance Fields\n(NeRFs) and generative models, such as Generative Adversarial Networks (GANs),\nhas transformed 3D-aware generation from single-view images. NeRF-GANs exploit\nthe strong inductive bias of 3D neural representations and volumetric rendering\nat the cost of higher computational complexity. This study aims at revisiting\npose-conditioned 2D GANs for efficient 3D-aware generation at inference time by\ndistilling 3D knowledge from pretrained NeRF-GANS. We propose a simple and\neffective method, based on re-using the well-disentangled latent space of a\npre-trained NeRF-GAN in a pose-conditioned convolutional network to directly\ngenerate 3D-consistent images corresponding to the underlying 3D\nrepresentations. Experiments on several datasets demonstrate that the proposed\nmethod obtains results comparable with volumetric rendering in terms of quality\nand 3D consistency while benefiting from the superior computational advantage\nof convolutional networks. The code will be available at:\nhttps://github.com/mshahbazi72/NeRF-GAN-Distillation\n","authors":["Mohamad Shahbazi","Evangelos Ntavelis","Alessio Tonioni","Edo Collins","Danda Pani Paudel","Martin Danelljan","Luc Van Gool"],"pdf_url":"https://arxiv.org/pdf/2303.12865v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.12861v1","updated":"2023-03-22T18:55:43Z","published":"2023-03-22T18:55:43Z","title":"Cube-Based 3D Denoising Diffusion Probabilistic Model for Cone Beam\n  Computed Tomography Reconstruction with Incomplete Data","summary":"  Deep learning (DL) has been extensively researched in the field of computed\ntomography (CT) reconstruction with incomplete data, particularly in\nsparse-view CT reconstruction. However, applying DL to sparse-view cone beam CT\n(CBCT) remains challenging. Many models learn the mapping from sparse-view CT\nimages to ground truth but struggle to achieve satisfactory performance in\nterms of global artifact removal. Incorporating sinogram data and utilizing\ndual-domain information can enhance anti-artifact performance, but this\nrequires storing the entire sinogram in memory. This presents a memory issue\nfor high-resolution CBCT sinograms, limiting further research and application.\nIn this paper, we propose a cube-based 3D denoising diffusion probabilistic\nmodel (DDPM) for CBCT reconstruction using down-sampled data. A DDPM network,\ntrained on cubes extracted from paired fully sampled sinograms and down-sampled\nsinograms, is employed to inpaint down-sampled sinograms. Our method divides\nthe entire sinogram into overlapping cubes and processes these cubes in\nparallel using multiple GPUs, overcoming memory limitations. Experimental\nresults demonstrate that our approach effectively suppresses few-view artifacts\nwhile preserving textural details faithfully.\n","authors":["Wenjun Xia","Chuang Niu","Wenxiang Cong","Ge Wang"],"pdf_url":"https://arxiv.org/pdf/2303.12861v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2010.01400v2","updated":"2023-03-22T18:49:54Z","published":"2020-10-03T17:48:57Z","title":"Joint Inference of Diffusion and Structure in Partially Observed Social\n  Networks Using Coupled Matrix Factorization","summary":"  Access to complete data in large-scale networks is often infeasible.\nTherefore, the problem of missing data is a crucial and unavoidable issue in\nthe analysis and modeling of real-world social networks. However, most of the\nresearch on different aspects of social networks does not consider this\nlimitation. One effective way to solve this problem is to recover the missing\ndata as a pre-processing step. In this paper, a model is learned from partially\nobserved data to infer unobserved diffusion and structure networks. To jointly\ndiscover omitted diffusion activities and hidden network structures, we develop\na probabilistic generative model called \"DiffStru.\" The interrelations among\nlinks of nodes and cascade processes are utilized in the proposed method via\nlearning coupled with low-dimensional latent factors. Besides inferring unseen\ndata, latent factors such as community detection may also aid in network\nclassification problems. We tested different missing data scenarios on\nsimulated independent cascades over LFR networks and real datasets, including\nTwitter and Memtracker. Experiments on these synthetic and real-world datasets\nshow that the proposed method successfully detects invisible social behaviors,\npredicts links, and identifies latent features.\n","authors":["Maryam Ramezani","Aryan Ahadinia","Amirmohammad Ziaei","Hamid R. Rabiee"],"pdf_url":"https://arxiv.org/pdf/2010.01400v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2210.14868v2","updated":"2023-03-22T18:37:20Z","published":"2022-10-26T17:17:06Z","title":"Multi-lingual Evaluation of Code Generation Models","summary":"  We present new benchmarks on evaluation code generation models: MBXP and\nMultilingual HumanEval, and MathQA-X. These datasets cover over 10 programming\nlanguages and are generated using a scalable conversion framework that\ntranspiles prompts and test cases from the original Python datasets into the\ncorresponding data in the target language. Using these benchmarks, we are able\nto assess the performance of code generation models in a multi-lingual fashion,\nand discovered generalization ability of language models on out-of-domain\nlanguages, advantages of multi-lingual models over mono-lingual, the ability of\nfew-shot prompting to teach the model new languages, and zero-shot translation\nabilities even on mono-lingual settings. Furthermore, we use our code\ngeneration model to perform large-scale bootstrapping to obtain synthetic\ncanonical solutions in several languages, which can be used for other\ncode-related evaluations such as code insertion, robustness, or summarization\ntasks. Overall, our benchmarks represents a significant step towards a deeper\nunderstanding of language models' code generation abilities. We publicly\nrelease our code and datasets at https://github.com/amazon-research/mxeval.\n","authors":["Ben Athiwaratkun","Sanjay Krishna Gouda","Zijian Wang","Xiaopeng Li","Yuchen Tian","Ming Tan","Wasi Uddin Ahmad","Shiqi Wang","Qing Sun","Mingyue Shang","Sujan Kumar Gonugondla","Hantian Ding","Varun Kumar","Nathan Fulton","Arash Farahani","Siddhartha Jain","Robert Giaquinto","Haifeng Qian","Murali Krishna Ramanathan","Ramesh Nallapati","Baishakhi Ray","Parminder Bhatia","Sudipta Sengupta","Dan Roth","Bing Xiang"],"pdf_url":"https://arxiv.org/pdf/2210.14868v2.pdf","comment":"Code and data release: https://github.com/amazon-research/mxeval"},{"id":"http://arxiv.org/abs/2303.12856v1","updated":"2023-03-22T18:31:15Z","published":"2023-03-22T18:31:15Z","title":"Anti-symmetric Barron functions and their approximation with sums of\n  determinants","summary":"  A fundamental problem in quantum physics is to encode functions that are\ncompletely anti-symmetric under permutations of identical particles. The Barron\nspace consists of high-dimensional functions that can be parameterized by\ninfinite neural networks with one hidden layer. By explicitly encoding the\nanti-symmetric structure, we prove that the anti-symmetric functions which\nbelong to the Barron space can be efficiently approximated with sums of\ndeterminants. This yields a factorial improvement in complexity compared to the\nstandard representation in the Barron space and provides a theoretical\nexplanation for the effectiveness of determinant-based architectures in\nab-initio quantum chemistry.\n","authors":["Nilin Abrahamsen","Lin Lin"],"pdf_url":"https://arxiv.org/pdf/2303.12856v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.12853v1","updated":"2023-03-22T18:23:24Z","published":"2023-03-22T18:23:24Z","title":"Three iterations of $(1-d)$-WL test distinguish non isometric clouds of\n  $d$-dimensional points","summary":"  The Weisfeiler--Lehman (WL) test is a fundamental iterative algorithm for\nchecking isomorphism of graphs. It has also been observed that it underlies the\ndesign of several graph neural network architectures, whose capabilities and\nperformance can be understood in terms of the expressive power of this test.\nMotivated by recent developments in machine learning applications to datasets\ninvolving three-dimensional objects, we study when the WL test is {\\em\ncomplete} for clouds of euclidean points represented by complete distance\ngraphs, i.e., when it can distinguish, up to isometry, any arbitrary such\ncloud.\n  Our main result states that the $(d-1)$-dimensional WL test is complete for\npoint clouds in $d$-dimensional Euclidean space, for any $d\\ge 2$, and that\nonly three iterations of the test suffice. Our result is tight for $d = 2, 3$.\nWe also observe that the $d$-dimensional WL test only requires one iteration to\nachieve completeness.\n","authors":["Valentino Delle Rose","Alexander Kozachinskiy","Cristóbal Rojas","Mircea Petrache","Pablo Barceló"],"pdf_url":"https://arxiv.org/pdf/2303.12853v1.pdf","comment":"14 pages"},{"id":"http://arxiv.org/abs/2303.12848v1","updated":"2023-03-22T18:14:02Z","published":"2023-03-22T18:14:02Z","title":"Test-time Defense against Adversarial Attacks: Detection and\n  Reconstruction of Adversarial Examples via Masked Autoencoder","summary":"  Existing defense methods against adversarial attacks can be categorized into\ntraining time and test time defenses. Training time defense, i.e., adversarial\ntraining, requires a significant amount of extra time for training and is often\nnot able to be generalized to unseen attacks. On the other hand, test time\ndefense by test time weight adaptation requires access to perform gradient\ndescent on (part of) the model weights, which could be infeasible for models\nwith frozen weights. To address these challenges, we propose DRAM, a novel\ndefense method to Detect and Reconstruct multiple types of Adversarial attacks\nvia Masked autoencoder (MAE). We demonstrate how to use MAE losses to build a\nKS-test to detect adversarial attacks. Moreover, the MAE losses can be used to\nrepair adversarial samples from unseen attack types. In this sense, DRAM\nneither requires model weight updates in test time nor augments the training\nset with more adversarial samples. Evaluating DRAM on the large-scale ImageNet\ndata, we achieve the best detection rate of 82% on average on eight types of\nadversarial attacks compared with other detection baselines. For\nreconstruction, DRAM improves the robust accuracy by 6% ~ 41% for Standard\nResNet50 and 3% ~ 8% for Robust ResNet50 compared with other self-supervision\ntasks, such as rotation prediction and contrastive learning.\n","authors":["Yun-Yun Tsai","Ju-Chin Chao","Albert Wen","Zhaoyuan Yang","Chengzhi Mao","Tapan Shah","Junfeng Yang"],"pdf_url":"https://arxiv.org/pdf/2303.12848v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2301.00366v2","updated":"2023-03-22T18:11:14Z","published":"2023-01-01T07:42:50Z","title":"Self-Supervised Object Segmentation with a Cut-and-Pasting GAN","summary":"  This paper proposes a novel self-supervised based Cut-and-Paste GAN to\nperform foreground object segmentation and generate realistic composite images\nwithout manual annotations. We accomplish this goal by a simple yet effective\nself-supervised approach coupled with the U-Net based discriminator. The\nproposed method extends the ability of the standard discriminators to learn not\nonly the global data representations via classification (real/fake) but also\nlearn semantic and structural information through pseudo labels created using\nthe self-supervised task. The proposed method empowers the generator to create\nmeaningful masks by forcing it to learn informative per-pixel as well as global\nimage feedback from the discriminator. Our experiments demonstrate that our\nproposed method significantly outperforms the state-of-the-art methods on the\nstandard benchmark datasets.\n","authors":["Kunal Chaturvedi","Ali Braytee","Jun Li","Mukesh Prasad"],"pdf_url":"https://arxiv.org/pdf/2301.00366v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2208.11010v5","updated":"2023-03-22T18:03:42Z","published":"2022-08-23T14:46:54Z","title":"Convex mixed-integer optimization with Frank-Wolfe methods","summary":"  Mixed-integer nonlinear optimization encompasses a broad class of problems\nthat present both theoretical and computational challenges. We propose a new\ntype of method to solve these problems based on a branch-and-bound algorithm\nwith convex node relaxations. These relaxations are solved with a Frank-Wolfe\nalgorithm over the convex hull of mixed-integer feasible points instead of the\ncontinuous relaxation via calls to a mixed-integer linear solver as the linear\noracle. The proposed method computes feasible solutions while working on a\nsingle representation of the polyhedral constraints, leveraging the full extent\nof mixed-integer linear solvers without an outer approximation scheme and can\nexploit inexact solutions of node subproblems.\n","authors":["Deborah Hendrych","Hannah Troppens","Mathieu Besançon","Sebastian Pokutta"],"pdf_url":"https://arxiv.org/pdf/2208.11010v5.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.12834v1","updated":"2023-03-22T18:00:02Z","published":"2023-03-22T18:00:02Z","title":"The power and limitations of learning quantum dynamics incoherently","summary":"  Quantum process learning is emerging as an important tool to study quantum\nsystems. While studied extensively in coherent frameworks, where the target and\nmodel system can share quantum information, less attention has been paid to\nwhether the dynamics of quantum systems can be learned without the system and\ntarget directly interacting. Such incoherent frameworks are practically\nappealing since they open up methods of transpiling quantum processes between\nthe different physical platforms without the need for technically challenging\nhybrid entanglement schemes. Here we provide bounds on the sample complexity of\nlearning unitary processes incoherently by analyzing the number of measurements\nthat are required to emulate well-established coherent learning strategies. We\nprove that if arbitrary measurements are allowed, then any efficiently\nrepresentable unitary can be efficiently learned within the incoherent\nframework; however, when restricted to shallow-depth measurements only\nlow-entangling unitaries can be learned. We demonstrate our incoherent learning\nalgorithm for low entangling unitaries by successfully learning a 16-qubit\nunitary on \\texttt{ibmq\\_kolkata}, and further demonstrate the scalabilty of\nour proposed algorithm through extensive numerical experiments.\n","authors":["Sofiene Jerbi","Joe Gibbs","Manuel S. Rudolph","Matthias C. Caro","Patrick J. Coles","Hsin-Yuan Huang","Zoë Holmes"],"pdf_url":"https://arxiv.org/pdf/2303.12834v1.pdf","comment":"6+9 pages, 7 figures"},{"id":"http://arxiv.org/abs/2303.12821v1","updated":"2023-03-22T16:47:48Z","published":"2023-03-22T16:47:48Z","title":"Towards A Visual Programming Tool to Create Deep Learning Models","summary":"  Deep Learning (DL) developers come from different backgrounds, e.g.,\nmedicine, genomics, finance, and computer science. To create a DL model, they\nmust learn and use high-level programming languages (e.g., Python), thus\nneeding to handle related setups and solve programming errors. This paper\npresents DeepBlocks, a visual programming tool that allows DL developers to\ndesign, train, and evaluate models without relying on specific programming\nlanguages. DeepBlocks works by building on the typical model structure: a\nsequence of learnable functions whose arrangement defines the specific\ncharacteristics of the model. We derived DeepBlocks' design goals from a\n5-participants formative interview, and we validated the first implementation\nof the tool through a typical use case. Results are promising and show that\ndevelopers could visually design complex DL architectures.\n","authors":["Tommaso Calò","Luigi De Russis"],"pdf_url":"https://arxiv.org/pdf/2303.12821v1.pdf","comment":null}],"Multimedia":[{"id":"http://arxiv.org/abs/2303.12658v1","updated":"2023-03-22T15:36:19Z","published":"2023-03-22T15:36:19Z","title":"Reliable and Efficient Evaluation of Adversarial Robustness for Deep\n  Hashing-Based Retrieval","summary":"  Deep hashing has been extensively applied to massive image retrieval due to\nits efficiency and effectiveness. Recently, several adversarial attacks have\nbeen presented to reveal the vulnerability of deep hashing models against\nadversarial examples. However, existing attack methods suffer from degraded\nperformance or inefficiency because they underutilize the semantic relations\nbetween original samples or spend a lot of time learning these relations with a\ndeep neural network. In this paper, we propose a novel Pharos-guided Attack,\ndubbed PgA, to evaluate the adversarial robustness of deep hashing networks\nreliably and efficiently. Specifically, we design pharos code to represent the\nsemantics of the benign image, which preserves the similarity to semantically\nrelevant samples and dissimilarity to irrelevant ones. It is proven that we can\nquickly calculate the pharos code via a simple math formula. Accordingly, PgA\ncan directly conduct a reliable and efficient attack on deep hashing-based\nretrieval by maximizing the similarity between the hash code of the adversarial\nexample and the pharos code. Extensive experiments on the benchmark datasets\nverify that the proposed algorithm outperforms the prior state-of-the-arts in\nboth attack strength and speed.\n","authors":["Xunguang Wang","Jiawang Bai","Xinyue Xu","Xiaomeng Li"],"pdf_url":"https://arxiv.org/pdf/2303.12658v1.pdf","comment":"arXiv admin note: text overlap with arXiv:2204.10779"},{"id":"http://arxiv.org/abs/2303.12337v1","updated":"2023-03-22T06:26:56Z","published":"2023-03-22T06:26:56Z","title":"Music-Driven Group Choreography","summary":"  Music-driven choreography is a challenging problem with a wide variety of\nindustrial applications. Recently, many methods have been proposed to\nsynthesize dance motions from music for a single dancer. However, generating\ndance motion for a group remains an open problem. In this paper, we present\n$\\rm AIOZ-GDANCE$, a new large-scale dataset for music-driven group dance\ngeneration. Unlike existing datasets that only support single dance, our new\ndataset contains group dance videos, hence supporting the study of group\nchoreography. We propose a semi-autonomous labeling method with humans in the\nloop to obtain the 3D ground truth for our dataset. The proposed dataset\nconsists of $16.7$ hours of paired music and 3D motion from in-the-wild videos,\ncovering $7$ dance styles and $16$ music genres. We show that naively applying\nsingle dance generation technique to creating group dance motion may lead to\nunsatisfactory results, such as inconsistent movements and collisions between\ndancers. Based on our new dataset, we propose a new method that takes an input\nmusic sequence and a set of 3D positions of dancers to efficiently produce\nmultiple group-coherent choreographies. We propose new evaluation metrics for\nmeasuring group dance quality and perform intensive experiments to demonstrate\nthe effectiveness of our method.\n","authors":["Nhat Le","Thang Pham","Tuong Do","Erman Tjiputra","Quang D. Tran","Anh Nguyen"],"pdf_url":"https://arxiv.org/pdf/2303.12337v1.pdf","comment":"accepted in cvpr 2023"},{"id":"http://arxiv.org/abs/2303.12296v1","updated":"2023-03-22T04:06:29Z","published":"2023-03-22T04:06:29Z","title":"Prototype Helps Federated Learning: Towards Faster Convergence","summary":"  Federated learning (FL) is a distributed machine learning technique in which\nmultiple clients cooperate to train a shared model without exchanging their raw\ndata. However, heterogeneity of data distribution among clients usually leads\nto poor model inference. In this paper, a prototype-based federated learning\nframework is proposed, which can achieve better inference performance with only\na few changes to the last global iteration of the typical federated learning\nprocess. In the last iteration, the server aggregates the prototypes\ntransmitted from distributed clients and then sends them back to local clients\nfor their respective model inferences. Experiments on two baseline datasets\nshow that our proposal can achieve higher accuracy (at least 1%) and relatively\nefficient communication than two popular baselines under different\nheterogeneous settings.\n","authors":["Yu Qiao","Seong-Bae Park","Sun Moo Kang","Choong Seon Hong"],"pdf_url":"https://arxiv.org/pdf/2303.12296v1.pdf","comment":"3 pages, 3 figures"},{"id":"http://arxiv.org/abs/2207.11900v4","updated":"2023-03-22T02:51:19Z","published":"2022-07-25T04:22:41Z","title":"GA2MIF: Graph and Attention based Two-stage Multi-source Information\n  Fusion for Conversational Emotion Detection","summary":"  Multimodal Emotion Recognition in Conversation (ERC) plays an influential\nrole in the field of human-computer interaction and conversational robotics\nsince it can motivate machines to provide empathetic services. Multimodal data\nmodeling is an up-and-coming research area in recent years, which is inspired\nby human capability to integrate multiple senses. Several graph-based\napproaches claim to capture interactive information between modalities, but the\nheterogeneity of multimodal data makes these methods prohibit optimal\nsolutions. In this work, we introduce a multimodal fusion approach named Graph\nand Attention based Two-stage Multi-source Information Fusion (GA2MIF) for\nemotion detection in conversation. Our proposed method circumvents the problem\nof taking heterogeneous graph as input to the model while eliminating complex\nredundant connections in the construction of graph. GA2MIF focuses on\ncontextual modeling and cross-modal modeling through leveraging Multi-head\nDirected Graph ATtention networks (MDGATs) and Multi-head Pairwise Cross-modal\nATtention networks (MPCATs), respectively. Extensive experiments on two public\ndatasets (i.e., IEMOCAP and MELD) demonstrate that the proposed GA2MIF has the\ncapacity to validly capture intra-modal long-range contextual information and\ninter-modal complementary information, as well as outperforms the prevalent\nState-Of-The-Art (SOTA) models by a remarkable margin.\n","authors":["Jiang Li","Xiaoping Wang","Guoqing Lv","Zhigang Zeng"],"pdf_url":"https://arxiv.org/pdf/2207.11900v4.pdf","comment":"14 pages"},{"id":"http://arxiv.org/abs/2204.12037v8","updated":"2023-03-22T02:41:58Z","published":"2022-04-26T02:22:28Z","title":"Causal Reasoning Meets Visual Representation Learning: A Prospective\n  Study","summary":"  Visual representation learning is ubiquitous in various real-world\napplications, including visual comprehension, video understanding, multi-modal\nanalysis, human-computer interaction, and urban computing. Due to the emergence\nof huge amounts of multi-modal heterogeneous spatial/temporal/spatial-temporal\ndata in big data era, the lack of interpretability, robustness, and\nout-of-distribution generalization are becoming the challenges of the existing\nvisual models. The majority of the existing methods tend to fit the original\ndata/variable distributions and ignore the essential causal relations behind\nthe multi-modal knowledge, which lacks unified guidance and analysis about why\nmodern visual representation learning methods easily collapse into data bias\nand have limited generalization and cognitive abilities. Inspired by the strong\ninference ability of human-level agents, recent years have therefore witnessed\ngreat effort in developing causal reasoning paradigms to realize robust\nrepresentation and model learning with good cognitive ability. In this paper,\nwe conduct a comprehensive review of existing causal reasoning methods for\nvisual representation learning, covering fundamental theories, models, and\ndatasets. The limitations of current methods and datasets are also discussed.\nMoreover, we propose some prospective challenges, opportunities, and future\nresearch directions for benchmarking causal reasoning algorithms in visual\nrepresentation learning. This paper aims to provide a comprehensive overview of\nthis emerging field, attract attention, encourage discussions, bring to the\nforefront the urgency of developing novel causal reasoning methods, publicly\navailable benchmarks, and consensus-building standards for reliable visual\nrepresentation learning and related real-world applications more efficiently.\n","authors":["Yang Liu","Yushen Wei","Hong Yan","Guanbin Li","Liang Lin"],"pdf_url":"https://arxiv.org/pdf/2204.12037v8.pdf","comment":"35 pages, 14 figures. This work has been accepted by Machine\n  Intelligence Research. The arxiv version is kept updating by adding more\n  novel methods, datasets and insights. The official video interpretation of\n  this paper can be referred at https://youtu.be/2lfNaTkcTHI"},{"id":"http://arxiv.org/abs/2303.12930v1","updated":"2023-03-22T22:00:17Z","published":"2023-03-22T22:00:17Z","title":"Dense-Localizing Audio-Visual Events in Untrimmed Videos: A Large-Scale\n  Benchmark and Baseline","summary":"  Existing audio-visual event localization (AVE) handles manually trimmed\nvideos with only a single instance in each of them. However, this setting is\nunrealistic as natural videos often contain numerous audio-visual events with\ndifferent categories. To better adapt to real-life applications, in this paper\nwe focus on the task of dense-localizing audio-visual events, which aims to\njointly localize and recognize all audio-visual events occurring in an\nuntrimmed video. The problem is challenging as it requires fine-grained\naudio-visual scene and context understanding. To tackle this problem, we\nintroduce the first Untrimmed Audio-Visual (UnAV-100) dataset, which contains\n10K untrimmed videos with over 30K audio-visual events. Each video has 2.8\naudio-visual events on average, and the events are usually related to each\nother and might co-occur as in real-life scenes. Next, we formulate the task\nusing a new learning-based framework, which is capable of fully integrating\naudio and visual modalities to localize audio-visual events with various\nlengths and capture dependencies between them in a single pass. Extensive\nexperiments demonstrate the effectiveness of our method as well as the\nsignificance of multi-scale cross-modal perception and dependency modeling for\nthis task.\n","authors":["Tiantian Geng","Teng Wang","Jinming Duan","Runmin Cong","Feng Zheng"],"pdf_url":"https://arxiv.org/pdf/2303.12930v1.pdf","comment":"Accepted by CVPR2023"}]},"2023-03-23T00:00:00Z":{"Computation and Language":[{"id":"http://arxiv.org/abs/2303.13519v1","updated":"2023-03-23T17:59:54Z","published":"2023-03-23T17:59:54Z","title":"Learning and Verification of Task Structure in Instructional Videos","summary":"  Given the enormous number of instructional videos available online, learning\na diverse array of multi-step task models from videos is an appealing goal. We\nintroduce a new pre-trained video model, VideoTaskformer, focused on\nrepresenting the semantics and structure of instructional videos. We pre-train\nVideoTaskformer using a simple and effective objective: predicting weakly\nsupervised textual labels for steps that are randomly masked out from an\ninstructional video (masked step modeling). Compared to prior work which learns\nstep representations locally, our approach involves learning them globally,\nleveraging video of the entire surrounding task as context. From these learned\nrepresentations, we can verify if an unseen video correctly executes a given\ntask, as well as forecast which steps are likely to be taken after a given\nstep. We introduce two new benchmarks for detecting mistakes in instructional\nvideos, to verify if there is an anomalous step and if steps are executed in\nthe right order. We also introduce a long-term forecasting benchmark, where the\ngoal is to predict long-range future steps from a given step. Our method\noutperforms previous baselines on these tasks, and we believe the tasks will be\na valuable way for the community to measure the quality of step\nrepresentations. Additionally, we evaluate VideoTaskformer on 3 existing\nbenchmarks -- procedural activity recognition, step classification, and step\nforecasting -- and demonstrate on each that our method outperforms existing\nbaselines and achieves new state-of-the-art performance.\n","authors":["Medhini Narasimhan","Licheng Yu","Sean Bell","Ning Zhang","Trevor Darrell"],"pdf_url":"https://arxiv.org/pdf/2303.13519v1.pdf","comment":"Wesbite at https://medhini.github.io/task_structure"},{"id":"http://arxiv.org/abs/2303.13455v1","updated":"2023-03-23T17:24:31Z","published":"2023-03-23T17:24:31Z","title":"CoBIT: A Contrastive Bi-directional Image-Text Generation Model","summary":"  The field of vision and language has witnessed a proliferation of pre-trained\nfoundation models. Most existing methods are independently pre-trained with\ncontrastive objective like CLIP, image-to-text generative objective like PaLI,\nor text-to-image generative objective like Parti. However, the three objectives\ncan be pre-trained on the same data, image-text pairs, and intuitively they\ncomplement each other as contrasting provides global alignment capacity and\ngeneration grants fine-grained understanding. In this work, we present a\nContrastive Bi-directional Image-Text generation model (CoBIT), which attempts\nto unify the three pre-training objectives in one framework. Specifically,\nCoBIT employs a novel unicoder-decoder structure, consisting of an image\nunicoder, a text unicoder and a cross-modal decoder. The image/text unicoders\ncan switch between encoding and decoding in different tasks, enabling\nflexibility and shared knowledge that benefits both image-to-text and\ntext-to-image generations. CoBIT achieves superior performance in image\nunderstanding, image-text understanding (Retrieval, Captioning, VQA, SNLI-VE)\nand text-based content creation, particularly in zero-shot scenarios. For\ninstance, 82.7% in zero-shot ImageNet classification, 9.37 FID score in\nzero-shot text-to-image generation and 44.8 CIDEr in zero-shot captioning.\n","authors":["Haoxuan You","Mandy Guo","Zhecan Wang","Kai-Wei Chang","Jason Baldridge","Jiahui Yu"],"pdf_url":"https://arxiv.org/pdf/2303.13455v1.pdf","comment":"14 pages, 5 figures"},{"id":"http://arxiv.org/abs/2303.13451v1","updated":"2023-03-23T17:17:46Z","published":"2023-03-23T17:17:46Z","title":"Development and validation of a natural language processing algorithm to\n  pseudonymize documents in the context of a clinical data warehouse","summary":"  The objective of this study is to address the critical issue of\nde-identification of clinical reports in order to allow access to data for\nresearch purposes, while ensuring patient privacy. The study highlights the\ndifficulties faced in sharing tools and resources in this domain and presents\nthe experience of the Greater Paris University Hospitals (AP-HP) in\nimplementing a systematic pseudonymization of text documents from its Clinical\nData Warehouse. We annotated a corpus of clinical documents according to 12\ntypes of identifying entities, and built a hybrid system, merging the results\nof a deep learning model as well as manual rules. Our results show an overall\nperformance of 0.99 of F1-score. We discuss implementation choices and present\nexperiments to better understand the effort involved in such a task, including\ndataset size, document types, language models, or rule addition. We share\nguidelines and code under a 3-Clause BSD license.\n","authors":["Xavier Tannier","Perceval Wajsbürt","Alice Calliger","Basile Dura","Alexandre Mouchet","Martin Hilka","Romain Bey"],"pdf_url":"https://arxiv.org/pdf/2303.13451v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.13408v1","updated":"2023-03-23T16:29:27Z","published":"2023-03-23T16:29:27Z","title":"Paraphrasing evades detectors of AI-generated text, but retrieval is an\n  effective defense","summary":"  To detect the deployment of large language models for malicious use cases\n(e.g., fake content creation or academic plagiarism), several approaches have\nrecently been proposed for identifying AI-generated text via watermarks or\nstatistical irregularities. How robust are these detection algorithms to\nparaphrases of AI-generated text? To stress test these detectors, we first\ntrain an 11B parameter paraphrase generation model (DIPPER) that can paraphrase\nparagraphs, optionally leveraging surrounding text (e.g., user-written prompts)\nas context. DIPPER also uses scalar knobs to control the amount of lexical\ndiversity and reordering in the paraphrases. Paraphrasing text generated by\nthree large language models (including GPT3.5-davinci-003) with DIPPER\nsuccessfully evades several detectors, including watermarking, GPTZero,\nDetectGPT, and OpenAI's text classifier. For example, DIPPER drops the\ndetection accuracy of DetectGPT from 70.3% to 4.6% (at a constant false\npositive rate of 1%), without appreciably modifying the input semantics. To\nincrease the robustness of AI-generated text detection to paraphrase attacks,\nwe introduce a simple defense that relies on retrieving semantically-similar\ngenerations and must be maintained by a language model API provider. Given a\ncandidate text, our algorithm searches a database of sequences previously\ngenerated by the API, looking for sequences that match the candidate text\nwithin a certain threshold. We empirically verify our defense using a database\nof 15M generations from a fine-tuned T5-XXL model and find that it can detect\n80% to 97% of paraphrased generations across different settings, while only\nclassifying 1% of human-written sequences as AI-generated. We will open source\nour code, model and data for future research.\n","authors":["Kalpesh Krishna","Yixiao Song","Marzena Karpinska","John Wieting","Mohit Iyyer"],"pdf_url":"https://arxiv.org/pdf/2303.13408v1.pdf","comment":"Preprint (27 pages). Code, models, data will be added to\n  https://github.com/martiansideofthemoon/ai-detection-paraphrases"},{"id":"http://arxiv.org/abs/2303.13386v1","updated":"2023-03-23T15:58:41Z","published":"2023-03-23T15:58:41Z","title":"Compositional Zero-Shot Domain Transfer with Text-to-Text Models","summary":"  Label scarcity is a bottleneck for improving task performance in specialised\ndomains. We propose a novel compositional transfer learning framework (DoT5 -\ndomain compositional zero-shot T5) for zero-shot domain transfer. Without\naccess to in-domain labels, DoT5 jointly learns domain knowledge (from MLM of\nunlabelled in-domain free text) and task knowledge (from task training on more\nreadily available general-domain data) in a multi-task manner. To improve the\ntransferability of task training, we design a strategy named NLGU: we\nsimultaneously train NLG for in-domain label-to-data generation which enables\ndata augmentation for self-finetuning and NLU for label prediction. We evaluate\nDoT5 on the biomedical domain and the resource-lean subdomain of radiology,\nfocusing on NLI, text summarisation and embedding learning. DoT5 demonstrates\nthe effectiveness of compositional transfer learning through multi-task\nlearning. In particular, DoT5 outperforms the current SOTA in zero-shot\ntransfer by over 7 absolute points in accuracy on RadNLI. We validate DoT5 with\nablations and a case study demonstrating its ability to solve challenging NLI\nexamples requiring in-domain expertise.\n","authors":["Fangyu Liu","Qianchu Liu","Shruthi Bannur","Fernando Pérez-García","Naoto Usuyama","Sheng Zhang","Tristan Naumann","Aditya Nori","Hoifung Poon","Javier Alvarez-Valle","Ozan Oktay","Stephanie L. Hyland"],"pdf_url":"https://arxiv.org/pdf/2303.13386v1.pdf","comment":"Accepted at TACL, pre-MIT Press publication version. 16 pages, 4\n  figures"},{"id":"http://arxiv.org/abs/2303.13351v1","updated":"2023-03-23T15:29:21Z","published":"2023-03-23T15:29:21Z","title":"DBLP-QuAD: A Question Answering Dataset over the DBLP Scholarly\n  Knowledge Graph","summary":"  In this work we create a question answering dataset over the DBLP scholarly\nknowledge graph (KG). DBLP is an on-line reference for bibliographic\ninformation on major computer science publications that indexes over 4.4\nmillion publications published by more than 2.2 million authors. Our dataset\nconsists of 10,000 question answer pairs with the corresponding SPARQL queries\nwhich can be executed over the DBLP KG to fetch the correct answer. DBLP-QuAD\nis the largest scholarly question answering dataset.\n","authors":["Debayan Banerjee","Sushil Awale","Ricardo Usbeck","Chris Biemann"],"pdf_url":"https://arxiv.org/pdf/2303.13351v1.pdf","comment":"12 pages ceur-ws 1 column accepted at International Bibliometric\n  Information Retrieval Workshp @ ECIR 2023"},{"id":"http://arxiv.org/abs/2303.13310v1","updated":"2023-03-23T14:44:47Z","published":"2023-03-23T14:44:47Z","title":"SwissBERT: The Multilingual Language Model for Switzerland","summary":"  We present SwissBERT, a masked language model created specifically for\nprocessing Switzerland-related text. SwissBERT is a pre-trained model that we\nadapted to news articles written in the national languages of Switzerland --\nGerman, French, Italian, and Romansh. We evaluate SwissBERT on natural language\nunderstanding tasks related to Switzerland and find that it tends to outperform\nprevious models on these tasks, especially when processing contemporary news\nand/or Romansh Grischun. Since SwissBERT uses language adapters, it may be\nextended to Swiss German dialects in future work. The model and our open-source\ncode are publicly released at https://github.com/ZurichNLP/swissbert.\n","authors":["Jannis Vamvas","Johannes Graën","Rico Sennrich"],"pdf_url":"https://arxiv.org/pdf/2303.13310v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2210.16848v2","updated":"2023-03-23T14:35:30Z","published":"2022-10-30T14:15:43Z","title":"Using Context-to-Vector with Graph Retrofitting to Improve Word\n  Embeddings","summary":"  Although contextualized embeddings generated from large-scale pre-trained\nmodels perform well in many tasks, traditional static embeddings (e.g.,\nSkip-gram, Word2Vec) still play an important role in low-resource and\nlightweight settings due to their low computational cost, ease of deployment,\nand stability. In this paper, we aim to improve word embeddings by 1)\nincorporating more contextual information from existing pre-trained models into\nthe Skip-gram framework, which we call Context-to-Vec; 2) proposing a\npost-processing retrofitting method for static embeddings independent of\ntraining by employing priori synonym knowledge and weighted vector\ndistribution. Through extrinsic and intrinsic tasks, our methods are well\nproven to outperform the baselines by a large margin.\n","authors":["Jiangbin Zheng","Yile Wang","Ge Wang","Jun Xia","Yufei Huang","Guojiang Zhao","Yue Zhang","Stan Z. Li"],"pdf_url":"https://arxiv.org/pdf/2210.16848v2.pdf","comment":"Accepted to ACL 2022"},{"id":"http://arxiv.org/abs/2303.13284v1","updated":"2023-03-23T14:06:26Z","published":"2023-03-23T14:06:26Z","title":"GETT-QA: Graph Embedding based T2T Transformer for Knowledge Graph\n  Question Answering","summary":"  In this work, we present an end-to-end Knowledge Graph Question Answering\n(KGQA) system named GETT-QA. GETT-QA uses T5, a popular text-to-text\npre-trained language model. The model takes a question in natural language as\ninput and produces a simpler form of the intended SPARQL query. In the simpler\nform, the model does not directly produce entity and relation IDs. Instead, it\nproduces corresponding entity and relation labels. The labels are grounded to\nKG entity and relation IDs in a subsequent step. To further improve the\nresults, we instruct the model to produce a truncated version of the KG\nembedding for each entity. The truncated KG embedding enables a finer search\nfor disambiguation purposes. We find that T5 is able to learn the truncated KG\nembeddings without any change of loss function, improving KGQA performance. As\na result, we report strong results for LC-QuAD 2.0 and SimpleQuestions-Wikidata\ndatasets on end-to-end KGQA over Wikidata.\n","authors":["Debayan Banerjee","Pranav Ajit Nair","Ricardo Usbeck","Chris Biemann"],"pdf_url":"https://arxiv.org/pdf/2303.13284v1.pdf","comment":"16 pages single column format accepted at ESWC 2023 research track"},{"id":"http://arxiv.org/abs/2303.13283v1","updated":"2023-03-23T14:04:23Z","published":"2023-03-23T14:04:23Z","title":"Visual-Language Prompt Tuning with Knowledge-guided Context Optimization","summary":"  Prompt tuning is an effective way to adapt the pre-trained visual-language\nmodel (VLM) to the downstream task using task-related textual tokens.\nRepresentative CoOp-based work combines the learnable textual tokens with the\nclass tokens to obtain specific textual knowledge. However, the specific\ntextual knowledge is the worse generalization to the unseen classes because it\nforgets the essential general textual knowledge having a strong generalization\nability. To tackle this issue, we introduce a novel Knowledge-guided Context\nOptimization (KgCoOp) to enhance the generalization ability of the learnable\nprompt for unseen classes. The key insight of KgCoOp is that forgetting about\nessential knowledge can be alleviated by reducing the discrepancy between the\nlearnable prompt and the hand-crafted prompt. Especially, KgCoOp minimizes the\ndiscrepancy between the textual embeddings generated by learned prompts and the\nhand-crafted prompts. Finally, adding the KgCoOp upon the contrastive loss can\nmake a discriminative prompt for both seen and unseen tasks. Extensive\nevaluation of several benchmarks demonstrates that the proposed\nKnowledge-guided Context Optimization is an efficient method for prompt tuning,\n\\emph{i.e.,} achieves better performance with less training time.\n","authors":["Hantao Yao","Rui Zhang","Changsheng Xu"],"pdf_url":"https://arxiv.org/pdf/2303.13283v1.pdf","comment":"accepted by CVPR23"},{"id":"http://arxiv.org/abs/2303.13220v1","updated":"2023-03-23T12:34:30Z","published":"2023-03-23T12:34:30Z","title":"Parameter-Efficient Sparse Retrievers and Rerankers using Adapters","summary":"  Parameter-Efficient transfer learning with Adapters have been studied in\nNatural Language Processing (NLP) as an alternative to full fine-tuning.\nAdapters are memory-efficient and scale well with downstream tasks by training\nsmall bottle-neck layers added between transformer layers while keeping the\nlarge pretrained language model (PLMs) frozen. In spite of showing promising\nresults in NLP, these methods are under-explored in Information Retrieval.\nWhile previous studies have only experimented with dense retriever or in a\ncross lingual retrieval scenario, in this paper we aim to complete the picture\non the use of adapters in IR. First, we study adapters for SPLADE, a sparse\nretriever, for which adapters not only retain the efficiency and effectiveness\notherwise achieved by finetuning, but are memory-efficient and orders of\nmagnitude lighter to train. We observe that Adapters-SPLADE not only optimizes\njust 2\\% of training parameters, but outperforms fully fine-tuned counterpart\nand existing parameter-efficient dense IR models on IR benchmark datasets.\nSecondly, we address domain adaptation of neural retrieval thanks to adapters\non cross-domain BEIR datasets and TripClick. Finally, we also consider\nknowledge sharing between rerankers and first stage rankers. Overall, our study\ncomplete the examination of adapters for neural IR\n","authors":["Vaishali Pal","Carlos Lassance","Hervé Déjean","Stéphane Clinchant"],"pdf_url":"https://arxiv.org/pdf/2303.13220v1.pdf","comment":"accepted at ECIR'23"},{"id":"http://arxiv.org/abs/2303.13217v1","updated":"2023-03-23T12:28:25Z","published":"2023-03-23T12:28:25Z","title":"Fairness-guided Few-shot Prompting for Large Language Models","summary":"  Large language models have demonstrated surprising ability to perform\nin-context learning, i.e., these models can be directly applied to solve\nnumerous downstream tasks by conditioning on a prompt constructed by a few\ninput-output examples. However, prior research has shown that in-context\nlearning can suffer from high instability due to variations in training\nexamples, example order, and prompt formats. Therefore, the construction of an\nappropriate prompt is essential for improving the performance of in-context\nlearning. In this paper, we revisit this problem from the view of predictive\nbias. Specifically, we introduce a metric to evaluate the predictive bias of a\nfixed prompt against labels or a given attributes. Then we empirically show\nthat prompts with higher bias always lead to unsatisfactory predictive quality.\nBased on this observation, we propose a novel search strategy based on the\ngreedy search to identify the near-optimal prompt for improving the performance\nof in-context learning. We perform comprehensive experiments with\nstate-of-the-art mainstream models such as GPT-3 on various downstream tasks.\nOur results indicate that our method can enhance the model's in-context\nlearning performance in an effective and interpretable manner.\n","authors":["Huan Ma","Changqing Zhang","Yatao Bian","Lemao Liu","Zhirui Zhang","Peilin Zhao","Shu Zhang","Huazhu Fu","Qinghua Hu","Bingzhe Wu"],"pdf_url":"https://arxiv.org/pdf/2303.13217v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.13112v1","updated":"2023-03-23T09:00:07Z","published":"2023-03-23T09:00:07Z","title":"A Simple Explanation for the Phase Transition in Large Language Models\n  with List Decoding","summary":"  Various recent experimental results show that large language models (LLM)\nexhibit emergent abilities that are not present in small models. System\nperformance is greatly improved after passing a certain critical threshold of\nscale. In this letter, we provide a simple explanation for such a phase\ntransition phenomenon. For this, we model an LLM as a sequence-to-sequence\nrandom function. Instead of using instant generation at each step, we use a\nlist decoder that keeps a list of candidate sequences at each step and defers\nthe generation of the output sequence at the end. We show that there is a\ncritical threshold such that the expected number of erroneous candidate\nsequences remains bounded when an LLM is below the threshold, and it grows\nexponentially when an LLM is above the threshold. Such a threshold is related\nto the basic reproduction number in a contagious disease.\n","authors":["Cheng-Shang Chang"],"pdf_url":"https://arxiv.org/pdf/2303.13112v1.pdf","comment":"5 pages, 1 figure"},{"id":"http://arxiv.org/abs/2303.13099v1","updated":"2023-03-23T08:30:35Z","published":"2023-03-23T08:30:35Z","title":"Multi-View Zero-Shot Open Intent Induction from Dialogues: Multi Domain\n  Batch and Proxy Gradient Transfer","summary":"  In Task Oriented Dialogue (TOD) system, detecting and inducing new intents\nare two main challenges to apply the system in the real world. In this paper,\nwe suggest the semantic multi-view model to resolve these two challenges: (1)\nSBERT for General Embedding (GE), (2) Multi Domain Batch (MDB) for dialogue\ndomain knowledge, and (3) Proxy Gradient Transfer (PGT) for cluster-specialized\nsemantic. MDB feeds diverse dialogue datasets to the model at once to tackle\nthe multi-domain problem by learning the multiple domain knowledge. We\nintroduce a novel method PGT, which employs the Siamese network to fine-tune\nthe model with a clustering method directly.Our model can learn how to cluster\ndialogue utterances by using PGT. Experimental results demonstrate that our\nmulti-view model with MDB and PGT significantly improves the Open Intent\nInduction performance compared to baseline systems.\n","authors":["Hyukhun Koh","Haesung Pyun","Nakyeong Yang","Kyomin Jung"],"pdf_url":"https://arxiv.org/pdf/2303.13099v1.pdf","comment":"8 pages, 3 figures, ACL 2023 workshop (DSTC)"},{"id":"http://arxiv.org/abs/2303.13072v1","updated":"2023-03-23T06:54:37Z","published":"2023-03-23T06:54:37Z","title":"Beyond Universal Transformer: block reusing with adaptor in Transformer\n  for automatic speech recognit","summary":"  Transformer-based models have recently made significant achievements in the\napplication of end-to-end (E2E) automatic speech recognition (ASR). It is\npossible to deploy the E2E ASR system on smart devices with the help of\nTransformer-based models. While these models still have the disadvantage of\nrequiring a large number of model parameters. To overcome the drawback of\nuniversal Transformer models for the application of ASR on edge devices, we\npropose a solution that can reuse the block in Transformer models for the\noccasion of the small footprint ASR system, which meets the objective of\naccommodating resource limitations without compromising recognition accuracy.\nSpecifically, we design a novel block-reusing strategy for speech Transformer\n(BRST) to enhance the effectiveness of parameters and propose an adapter module\n(ADM) that can produce a compact and adaptable model with only a few additional\ntrainable parameters accompanying each reusing block. We conducted an\nexperiment with the proposed method on the public AISHELL-1 corpus, and the\nresults show that the proposed approach achieves the character error rate (CER)\nof 9.3%/6.63% with only 7.6M/8.3M parameters without and with the ADM,\nrespectively. In addition, we also make a deeper analysis to show the effect of\nADM in the general block-reusing method.\n","authors":["Haoyu Tang","Zhaoyi Liu","Chang Zeng","Xinfeng Li"],"pdf_url":"https://arxiv.org/pdf/2303.13072v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.13065v1","updated":"2023-03-23T06:33:06Z","published":"2023-03-23T06:33:06Z","title":"Retrieval-Augmented Classification with Decoupled Representation","summary":"  Pretrained language models (PLMs) have shown marvelous improvements across\nvarious NLP tasks. Most Chinese PLMs simply treat an input text as a sequence\nof characters, and completely ignore word information. Although Whole Word\nMasking can alleviate this, the semantics in words is still not well\nrepresented. In this paper, we revisit the segmentation granularity of Chinese\nPLMs. We propose a mixed-granularity Chinese BERT (MigBERT) by considering both\ncharacters and words. To achieve this, we design objective functions for\nlearning both character and word-level representations. We conduct extensive\nexperiments on various Chinese NLP tasks to evaluate existing PLMs as well as\nthe proposed MigBERT. Experimental results show that MigBERT achieves new SOTA\nperformance on all these tasks. Further analysis demonstrates that words are\nsemantically richer than characters. More interestingly, we show that MigBERT\nalso works with Japanese. Our code has been released\nhere~\\footnote{\\url{https://github.com/xnliang98/MigBERT}} and you can download\nour model here~\\footnote{\\url{https://huggingface.co/xnliang/MigBERT-large/}}.\n","authors":["Xinnian Liang","Shuangzhi Wu","Hui Huang","Jiaqi Bai","Chao Bian","Zhoujun Li"],"pdf_url":"https://arxiv.org/pdf/2303.13065v1.pdf","comment":"preprint"},{"id":"http://arxiv.org/abs/2303.13035v1","updated":"2023-03-23T04:47:46Z","published":"2023-03-23T04:47:46Z","title":"SPeC: A Soft Prompt-Based Calibration on Mitigating Performance\n  Variability in Clinical Notes Summarization","summary":"  Electronic health records (EHRs) store an extensive array of patient\ninformation, encompassing medical histories, diagnoses, treatments, and test\noutcomes. These records are crucial for enabling healthcare providers to make\nwell-informed decisions regarding patient care. Summarizing clinical notes\nfurther assists healthcare professionals in pinpointing potential health risks\nand making better-informed decisions. This process contributes to reducing\nerrors and enhancing patient outcomes by ensuring providers have access to the\nmost pertinent and current patient data. Recent research has shown that\nincorporating prompts with large language models (LLMs) substantially boosts\nthe efficacy of summarization tasks. However, we show that this approach also\nleads to increased output variance, resulting in notably divergent outputs even\nwhen prompts share similar meanings. To tackle this challenge, we introduce a\nmodel-agnostic Soft Prompt-Based Calibration (SPeC) pipeline that employs soft\nprompts to diminish variance while preserving the advantages of prompt-based\nsummarization. Experimental findings on multiple clinical note tasks and LLMs\nindicate that our method not only bolsters performance but also effectively\ncurbs variance for various LLMs, providing a more uniform and dependable\nsolution for summarizing vital medical information.\n","authors":["Yu-Neng Chuang","Ruixiang Tang","Xiaoqian Jiang","Xia Hu"],"pdf_url":"https://arxiv.org/pdf/2303.13035v1.pdf","comment":null},{"id":"http://arxiv.org/abs/1908.04531v2","updated":"2023-03-23T04:24:09Z","published":"2019-08-13T08:29:48Z","title":"Offensive Language and Hate Speech Detection for Danish","summary":"  The presence of offensive language on social media platforms and the\nimplications this poses is becoming a major concern in modern society. Given\nthe enormous amount of content created every day, automatic methods are\nrequired to detect and deal with this type of content. Until now, most of the\nresearch has focused on solving the problem for the English language, while the\nproblem is multilingual.\n  We construct a Danish dataset containing user-generated comments from\n\\textit{Reddit} and \\textit{Facebook}. It contains user generated comments from\nvarious social media platforms, and to our knowledge, it is the first of its\nkind. Our dataset is annotated to capture various types and target of offensive\nlanguage. We develop four automatic classification systems, each designed to\nwork for both the English and the Danish language. In the detection of\noffensive language in English, the best performing system achieves a macro\naveraged F1-score of $0.74$, and the best performing system for Danish achieves\na macro averaged F1-score of $0.70$. In the detection of whether or not an\noffensive post is targeted, the best performing system for English achieves a\nmacro averaged F1-score of $0.62$, while the best performing system for Danish\nachieves a macro averaged F1-score of $0.73$. Finally, in the detection of the\ntarget type in a targeted offensive post, the best performing system for\nEnglish achieves a macro averaged F1-score of $0.56$, and the best performing\nsystem for Danish achieves a macro averaged F1-score of $0.63$.\n  Our work for both the English and the Danish language captures the type and\ntargets of offensive language, and present automatic methods for detecting\ndifferent kinds of offensive language such as hate speech and cyberbullying.\n","authors":["Gudbjartur Ingi Sigurbergsson","Leon Derczynski"],"pdf_url":"https://arxiv.org/pdf/1908.04531v2.pdf","comment":"Proceedings of the Twelfth Language Resources and Evaluation\n  Conference"},{"id":"http://arxiv.org/abs/2012.06431v2","updated":"2023-03-23T04:22:08Z","published":"2020-12-11T15:46:15Z","title":"Discriminating Between Similar Nordic Languages","summary":"  Automatic language identification is a challenging problem. Discriminating\nbetween closely related languages is especially difficult. This paper presents\na machine learning approach for automatic language identification for the\nNordic languages, which often suffer miscategorisation by existing\nstate-of-the-art tools. Concretely we will focus on discrimination between six\nNordic languages: Danish, Swedish, Norwegian (Nynorsk), Norwegian (Bokm{\\aa}l),\nFaroese and Icelandic.\n","authors":["René Haas","Leon Derczynski"],"pdf_url":"https://arxiv.org/pdf/2012.06431v2.pdf","comment":"Proceedings of the Eighth Workshop on NLP for Similar Languages,\n  Varieties and Dialects"},{"id":"http://arxiv.org/abs/2303.09892v2","updated":"2023-03-23T03:52:05Z","published":"2023-03-17T11:13:30Z","title":"Memotion 3: Dataset on Sentiment and Emotion Analysis of Codemixed\n  Hindi-English Memes","summary":"  Memes are the new-age conveyance mechanism for humor on social media sites.\nMemes often include an image and some text. Memes can be used to promote\ndisinformation or hatred, thus it is crucial to investigate in details. We\nintroduce Memotion 3, a new dataset with 10,000 annotated memes. Unlike other\nprevalent datasets in the domain, including prior iterations of Memotion,\nMemotion 3 introduces Hindi-English Codemixed memes while prior works in the\narea were limited to only the English memes. We describe the Memotion task, the\ndata collection and the dataset creation methodologies. We also provide a\nbaseline for the task. The baseline code and dataset will be made available at\nhttps://github.com/Shreyashm16/Memotion-3.0\n","authors":["Shreyash Mishra","S Suryavardan","Parth Patwa","Megha Chakraborty","Anku Rani","Aishwarya Reganti","Aman Chadha","Amitava Das","Amit Sheth","Manoj Chinnakotla","Asif Ekbal","Srijan Kumar"],"pdf_url":"https://arxiv.org/pdf/2303.09892v2.pdf","comment":"Defactify2 @AAAI"},{"id":"http://arxiv.org/abs/2303.13013v1","updated":"2023-03-23T03:30:30Z","published":"2023-03-23T03:30:30Z","title":"GesGPT: Speech Gesture Synthesis With Text Parsing from GPT","summary":"  Gesture synthesis has gained significant attention as a critical research\narea, focusing on producing contextually appropriate and natural gestures\ncorresponding to speech or textual input. Although deep learning-based\napproaches have achieved remarkable progress, they often overlook the rich\nsemantic information present in the text, leading to less expressive and\nmeaningful gestures. We propose GesGPT, a novel approach to gesture generation\nthat leverages the semantic analysis capabilities of Large Language Models\n(LLMs), such as GPT. By capitalizing on the strengths of LLMs for text\nanalysis, we design prompts to extract gesture-related information from textual\ninput. Our method entails developing prompt principles that transform gesture\ngeneration into an intention classification problem based on GPT, and utilizing\na curated gesture library and integration module to produce semantically rich\nco-speech gestures. Experimental results demonstrate that GesGPT effectively\ngenerates contextually appropriate and expressive gestures, offering a new\nperspective on semantic co-speech gesture generation.\n","authors":["Nan Gao","Zeyu Zhao","Zhi Zeng","Shuwu Zhang","Dongdong Weng"],"pdf_url":"https://arxiv.org/pdf/2303.13013v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.13001v1","updated":"2023-03-23T02:50:38Z","published":"2023-03-23T02:50:38Z","title":"Is ChatGPT A Good Keyphrase Generator? A Preliminary Study","summary":"  The emergence of ChatGPT has recently garnered significant attention from the\ncomputational linguistics community. To demonstrate its capabilities as a\nkeyphrase generator, we conduct a preliminary evaluation of ChatGPT for the\nkeyphrase generation task. We evaluate its performance in various aspects,\nincluding keyphrase generation prompts, keyphrase generation diversity,\nmulti-domain keyphrase generation, and long document understanding. Our\nevaluation is based on six benchmark datasets, and we adopt the prompt\nsuggested by OpenAI while extending it to six candidate prompts. We find that\nChatGPT performs exceptionally well on all six candidate prompts, with minor\nperformance differences observed across the datasets. Based on our findings, we\nconclude that ChatGPT has great potential for keyphrase generation. Moreover,\nwe discover that ChatGPT still faces challenges when it comes to generating\nabsent keyphrases. Meanwhile, in the final section, we also present some\nlimitations and future expansions of this report.\n","authors":["Mingyang Song","Haiyun Jiang","Shuming Shi","Songfang Yao","Shilong Lu","Yi Feng","Huafeng Liu","Liping Jing"],"pdf_url":"https://arxiv.org/pdf/2303.13001v1.pdf","comment":"Technical Report, 7 pages"},{"id":"http://arxiv.org/abs/2210.01936v3","updated":"2023-03-23T23:21:38Z","published":"2022-10-04T22:13:25Z","title":"When and why vision-language models behave like bags-of-words, and what\n  to do about it?","summary":"  Despite the success of large vision and language models (VLMs) in many\ndownstream applications, it is unclear how well they encode compositional\ninformation. Here, we create the Attribution, Relation, and Order (ARO)\nbenchmark to systematically evaluate the ability of VLMs to understand\ndifferent types of relationships, attributes, and order. ARO consists of Visual\nGenome Attribution, to test the understanding of objects' properties; Visual\nGenome Relation, to test for relational understanding; and COCO &\nFlickr30k-Order, to test for order sensitivity. ARO is orders of magnitude\nlarger than previous benchmarks of compositionality, with more than 50,000 test\ncases. We show where state-of-the-art VLMs have poor relational understanding,\ncan blunder when linking objects to their attributes, and demonstrate a severe\nlack of order sensitivity. VLMs are predominantly trained and evaluated on\nlarge datasets with rich compositional structure in the images and captions.\nYet, training on these datasets has not been enough to address the lack of\ncompositional understanding, and evaluating on these datasets has failed to\nsurface this deficiency. To understand why these limitations emerge and are not\nrepresented in the standard tests, we zoom into the evaluation and training\nprocedures. We demonstrate that it is possible to perform well on retrieval\nover existing datasets without using the composition and order information.\nGiven that contrastive pretraining optimizes for retrieval on datasets with\nsimilar shortcuts, we hypothesize that this can explain why the models do not\nneed to learn to represent compositional information. This finding suggests a\nnatural solution: composition-aware hard negative mining. We show that a\nsimple-to-implement modification of contrastive learning significantly improves\nthe performance on tasks requiring understanding of order and compositionality.\n","authors":["Mert Yuksekgonul","Federico Bianchi","Pratyusha Kalluri","Dan Jurafsky","James Zou"],"pdf_url":"https://arxiv.org/pdf/2210.01936v3.pdf","comment":"ICLR 2023 Oral (notable-top-5%)"},{"id":"http://arxiv.org/abs/2303.13675v1","updated":"2023-03-23T21:10:04Z","published":"2023-03-23T21:10:04Z","title":"Mordecai 3: A Neural Geoparser and Event Geocoder","summary":"  Mordecai3 is a new end-to-end text geoparser and event geolocation system.\nThe system performs toponym resolution using a new neural ranking model to\nresolve a place name extracted from a document to its entry in the Geonames\ngazetteer. It also performs event geocoding, the process of linking events\nreported in text with the place names where they are reported to occur, using\nan off-the-shelf question-answering model. The toponym resolution model is\ntrained on a diverse set of existing training data, along with several thousand\nnewly annotated examples. The paper describes the model, its training process,\nand performance comparisons with existing geoparsers. The system is available\nas an open source Python library, Mordecai 3, and replaces an earlier\ngeoparser, Mordecai v2, one of the most widely used text geoparsers (Halterman\n2017).\n","authors":["Andrew Halterman"],"pdf_url":"https://arxiv.org/pdf/2303.13675v1.pdf","comment":"6 pages, 1 figure, 4 tables"},{"id":"http://arxiv.org/abs/2212.08620v2","updated":"2023-03-23T18:45:37Z","published":"2022-12-16T17:57:41Z","title":"POTATO: The Portable Text Annotation Tool","summary":"  We present POTATO, the Portable text annotation tool, a free, fully\nopen-sourced annotation system that 1) supports labeling many types of text and\nmultimodal data; 2) offers easy-to-configure features to maximize the\nproductivity of both deployers and annotators (convenient templates for common\nML/NLP tasks, active learning, keypress shortcuts, keyword highlights,\ntooltips); and 3) supports a high degree of customization (editable UI,\ninserting pre-screening questions, attention and qualification tests).\nExperiments over two annotation tasks suggest that POTATO improves labeling\nspeed through its specially-designed productivity features, especially for long\ndocuments and complex tasks. POTATO is available at\nhttps://github.com/davidjurgens/potato and will continue to be updated.\n","authors":["Jiaxin Pei","Aparna Ananthasubramaniam","Xingyao Wang","Naitian Zhou","Jackson Sargent","Apostolos Dedeloudis","David Jurgens"],"pdf_url":"https://arxiv.org/pdf/2212.08620v2.pdf","comment":"EMNLP 2022 DEMO"},{"id":"http://arxiv.org/abs/2303.13592v1","updated":"2023-03-23T18:16:30Z","published":"2023-03-23T18:16:30Z","title":"Prompting Large Language Models to Generate Code-Mixed Texts: The Case\n  of South East Asian Languages","summary":"  While code-mixing is a common linguistic practice in many parts of the world,\ncollecting high-quality and low-cost code-mixed data remains a challenge for\nnatural language processing (NLP) research. The proliferation of Large Language\nModels (LLMs) in recent times compels one to ask: can these systems be used for\ndata generation? In this article, we explore prompting LLMs in a zero-shot\nmanner to create code-mixed data for five languages in South East Asia (SEA) --\nIndonesian, Malay, Chinese, Tagalog, Vietnamese, as well as the creole language\nSinglish. We find that ChatGPT shows the most potential, capable of producing\ncode-mixed text 68% of the time when the term \"code-mixing\" is explicitly\ndefined. Moreover, both ChatGPT and InstructGPT's (davinci-003) performances in\ngenerating Singlish texts are noteworthy, averaging a 96% success rate across a\nvariety of prompts. The code-mixing proficiency of ChatGPT and InstructGPT,\nhowever, is dampened by word choice errors that lead to semantic inaccuracies.\nOther multilingual models such as BLOOMZ and Flan-T5-XXL are unable to produce\ncode-mixed texts altogether. By highlighting the limited promises of LLMs in a\nspecific form of low-resource data generation, we call for a measured approach\nwhen applying similar techniques to other data-scarce NLP contexts.\n","authors":["Zheng-Xin Yong","Ruochen Zhang","Jessica Zosa Forde","Skyler Wang","Samuel Cahyawijaya","Holy Lovenia","Lintang Sutawika","Jan Christian Blaise Cruz","Long Phan","Yin Lin Tan","Alham Fikri Aji"],"pdf_url":"https://arxiv.org/pdf/2303.13592v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.13570v1","updated":"2023-03-23T15:59:06Z","published":"2023-03-23T15:59:06Z","title":"Return of the RNN: Residual Recurrent Networks for Invertible Sentence\n  Embeddings","summary":"  This study presents a novel model for invertible sentence embeddings using a\nresidual recurrent network trained on an unsupervised encoding task. Rather\nthan the probabilistic outputs common to neural machine translation models, our\napproach employs a regression-based output layer to reconstruct the input\nsequence's word vectors. The model achieves high accuracy and fast training\nwith the ADAM optimizer, a significant finding given that RNNs typically\nrequire memory units, such as LSTMs, or second-order optimization methods. We\nincorporate residual connections and introduce a \"match drop\" technique, where\ngradients are calculated only for incorrect words. Our approach demonstrates\npotential for various natural language processing applications, particularly in\nneural network-based systems that require high-quality sentence embeddings.\n","authors":["Jeremy Wilkerson"],"pdf_url":"https://arxiv.org/pdf/2303.13570v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.13559v1","updated":"2023-03-23T02:54:00Z","published":"2023-03-23T02:54:00Z","title":"Enhancing Unsupervised Speech Recognition with Diffusion GANs","summary":"  We enhance the vanilla adversarial training method for unsupervised Automatic\nSpeech Recognition (ASR) by a diffusion-GAN. Our model (1) injects instance\nnoises of various intensities to the generator's output and unlabeled reference\ntext which are sampled from pretrained phoneme language models with a length\nconstraint, (2) asks diffusion timestep-dependent discriminators to separate\nthem, and (3) back-propagates the gradients to update the generator.\nWord/phoneme error rate comparisons with wav2vec-U under Librispeech (3.1% for\ntest-clean and 5.6% for test-other), TIMIT and MLS datasets, show that our\nenhancement strategies work effectively.\n","authors":["Xianchao Wu"],"pdf_url":"https://arxiv.org/pdf/2303.13559v1.pdf","comment":"5 pages, 1 figure, accepted by ICASSP 2023"},{"id":"http://arxiv.org/abs/2303.15293v1","updated":"2023-03-23T18:02:23Z","published":"2023-03-23T18:02:23Z","title":"A Deliberation-based Joint Acoustic and Text Decoder","summary":"  We propose a new two-pass E2E speech recognition model that improves ASR\nperformance by training on a combination of paired data and unpaired text data.\nPreviously, the joint acoustic and text decoder (JATD) has shown promising\nresults through the use of text data during model training and the recently\nintroduced deliberation architecture has reduced recognition errors by\nleveraging first-pass decoding results. Our method, dubbed Deliberation-JATD,\ncombines the spelling correcting abilities of deliberation with JATD's use of\nunpaired text data to further improve performance. The proposed model produces\nsubstantial gains across multiple test sets, especially those focused on rare\nwords, where it reduces word error rate (WER) by between 12% and 22.5%\nrelative. This is done without increasing model size or requiring multi-stage\ntraining, making Deliberation-JATD an efficient candidate for on-device\napplications.\n","authors":["Sepand Mavandadi","Tara N. Sainath","Ke Hu","Zelin Wu"],"pdf_url":"https://arxiv.org/pdf/2303.15293v1.pdf","comment":"Interspeech 2021"}],"Computer Vision and Pattern Recognition":[{"id":"http://arxiv.org/abs/2303.13519v1","updated":"2023-03-23T17:59:54Z","published":"2023-03-23T17:59:54Z","title":"Learning and Verification of Task Structure in Instructional Videos","summary":"  Given the enormous number of instructional videos available online, learning\na diverse array of multi-step task models from videos is an appealing goal. We\nintroduce a new pre-trained video model, VideoTaskformer, focused on\nrepresenting the semantics and structure of instructional videos. We pre-train\nVideoTaskformer using a simple and effective objective: predicting weakly\nsupervised textual labels for steps that are randomly masked out from an\ninstructional video (masked step modeling). Compared to prior work which learns\nstep representations locally, our approach involves learning them globally,\nleveraging video of the entire surrounding task as context. From these learned\nrepresentations, we can verify if an unseen video correctly executes a given\ntask, as well as forecast which steps are likely to be taken after a given\nstep. We introduce two new benchmarks for detecting mistakes in instructional\nvideos, to verify if there is an anomalous step and if steps are executed in\nthe right order. We also introduce a long-term forecasting benchmark, where the\ngoal is to predict long-range future steps from a given step. Our method\noutperforms previous baselines on these tasks, and we believe the tasks will be\na valuable way for the community to measure the quality of step\nrepresentations. Additionally, we evaluate VideoTaskformer on 3 existing\nbenchmarks -- procedural activity recognition, step classification, and step\nforecasting -- and demonstrate on each that our method outperforms existing\nbaselines and achieves new state-of-the-art performance.\n","authors":["Medhini Narasimhan","Licheng Yu","Sean Bell","Ning Zhang","Trevor Darrell"],"pdf_url":"https://arxiv.org/pdf/2303.13519v1.pdf","comment":"Wesbite at https://medhini.github.io/task_structure"},{"id":"http://arxiv.org/abs/2303.13518v1","updated":"2023-03-23T17:59:53Z","published":"2023-03-23T17:59:53Z","title":"Three ways to improve feature alignment for open vocabulary detection","summary":"  The core problem in zero-shot open vocabulary detection is how to align\nvisual and text features, so that the detector performs well on unseen classes.\nPrevious approaches train the feature pyramid and detection head from scratch,\nwhich breaks the vision-text feature alignment established during pretraining,\nand struggles to prevent the language model from forgetting unseen classes.\n  We propose three methods to alleviate these issues. Firstly, a simple scheme\nis used to augment the text embeddings which prevents overfitting to a small\nnumber of classes seen during training, while simultaneously saving memory and\ncomputation. Secondly, the feature pyramid network and the detection head are\nmodified to include trainable gated shortcuts, which encourages vision-text\nfeature alignment and guarantees it at the start of detection training.\nFinally, a self-training approach is used to leverage a larger corpus of\nimage-text pairs thus improving detection performance on classes with no human\nannotated bounding boxes.\n  Our three methods are evaluated on the zero-shot version of the LVIS\nbenchmark, each of them showing clear and significant benefits. Our final\nnetwork achieves the new stateof-the-art on the mAP-all metric and demonstrates\ncompetitive performance for mAP-rare, as well as superior transfer to COCO and\nObjects365.\n","authors":["Relja Arandjelović","Alex Andonian","Arthur Mensch","Olivier J. Hénaff","Jean-Baptiste Alayrac","Andrew Zisserman"],"pdf_url":"https://arxiv.org/pdf/2303.13518v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.13516v1","updated":"2023-03-23T17:59:42Z","published":"2023-03-23T17:59:42Z","title":"Ablating Concepts in Text-to-Image Diffusion Models","summary":"  Large-scale text-to-image diffusion models can generate high-fidelity images\nwith powerful compositional ability. However, these models are typically\ntrained on an enormous amount of Internet data, often containing copyrighted\nmaterial, licensed images, and personal photos. Furthermore, they have been\nfound to replicate the style of various living artists or memorize exact\ntraining samples. How can we remove such copyrighted concepts or images without\nretraining the model from scratch? To achieve this goal, we propose an\nefficient method of ablating concepts in the pretrained model, i.e., preventing\nthe generation of a target concept. Our algorithm learns to match the image\ndistribution for a target style, instance, or text prompt we wish to ablate to\nthe distribution corresponding to an anchor concept. This prevents the model\nfrom generating target concepts given its text condition. Extensive experiments\nshow that our method can successfully prevent the generation of the ablated\nconcept while preserving closely related concepts in the model.\n","authors":["Nupur Kumari","Bingliang Zhang","Sheng-Yu Wang","Eli Shechtman","Richard Zhang","Jun-Yan Zhu"],"pdf_url":"https://arxiv.org/pdf/2303.13516v1.pdf","comment":"project website: https://www.cs.cmu.edu/~concept-ablation/"},{"id":"http://arxiv.org/abs/2303.13515v1","updated":"2023-03-23T17:59:40Z","published":"2023-03-23T17:59:40Z","title":"Persistent Nature: A Generative Model of Unbounded 3D Worlds","summary":"  Despite increasingly realistic image quality, recent 3D image generative\nmodels often operate on 3D volumes of fixed extent with limited camera motions.\nWe investigate the task of unconditionally synthesizing unbounded nature\nscenes, enabling arbitrarily large camera motion while maintaining a persistent\n3D world model. Our scene representation consists of an extendable, planar\nscene layout grid, which can be rendered from arbitrary camera poses via a 3D\ndecoder and volume rendering, and a panoramic skydome. Based on this\nrepresentation, we learn a generative world model solely from single-view\ninternet photos. Our method enables simulating long flights through 3D\nlandscapes, while maintaining global scene consistency--for instance, returning\nto the starting point yields the same view of the scene. Our approach enables\nscene extrapolation beyond the fixed bounds of current 3D generative models,\nwhile also supporting a persistent, camera-independent world representation\nthat stands in contrast to auto-regressive 3D prediction models. Our project\npage: https://chail.github.io/persistent-nature/.\n","authors":["Lucy Chai","Richard Tucker","Zhengqi Li","Phillip Isola","Noah Snavely"],"pdf_url":"https://arxiv.org/pdf/2303.13515v1.pdf","comment":"CVPR camera ready version, project page:\n  https://chail.github.io/persistent-nature/"},{"id":"http://arxiv.org/abs/2303.13514v1","updated":"2023-03-23T17:59:35Z","published":"2023-03-23T17:59:35Z","title":"SAOR: Single-View Articulated Object Reconstruction","summary":"  We introduce SAOR, a novel approach for estimating the 3D shape, texture, and\nviewpoint of an articulated object from a single image captured in the wild.\nUnlike prior approaches that rely on pre-defined category-specific 3D templates\nor tailored 3D skeletons, SAOR learns to articulate shapes from single-view\nimage collections with a skeleton-free part-based model without requiring any\n3D object shape priors. To prevent ill-posed solutions, we propose a\ncross-instance consistency loss that exploits disentangled object shape\ndeformation and articulation. This is helped by a new silhouette-based sampling\nmechanism to enhance viewpoint diversity during training. Our method only\nrequires estimated object silhouettes and relative depth maps from\noff-the-shelf pre-trained networks during training. At inference time, given a\nsingle-view image, it efficiently outputs an explicit mesh representation. We\nobtain improved qualitative and quantitative results on challenging quadruped\nanimals compared to relevant existing work.\n","authors":["Mehmet Aygün","Oisin Mac Aodha"],"pdf_url":"https://arxiv.org/pdf/2303.13514v1.pdf","comment":"https://mehmetaygun.github.io/saor"},{"id":"http://arxiv.org/abs/2303.13511v1","updated":"2023-03-23T17:59:10Z","published":"2023-03-23T17:59:10Z","title":"Neural Preset for Color Style Transfer","summary":"  In this paper, we present a Neural Preset technique to address the\nlimitations of existing color style transfer methods, including visual\nartifacts, vast memory requirement, and slow style switching speed. Our method\nis based on two core designs. First, we propose Deterministic Neural Color\nMapping (DNCM) to consistently operate on each pixel via an image-adaptive\ncolor mapping matrix, avoiding artifacts and supporting high-resolution inputs\nwith a small memory footprint. Second, we develop a two-stage pipeline by\ndividing the task into color normalization and stylization, which allows\nefficient style switching by extracting color styles as presets and reusing\nthem on normalized input images. Due to the unavailability of pairwise\ndatasets, we describe how to train Neural Preset via a self-supervised\nstrategy. Various advantages of Neural Preset over existing methods are\ndemonstrated through comprehensive evaluations. Besides, we show that our\ntrained model can naturally support multiple applications without fine-tuning,\nincluding low-light image enhancement, underwater image correction, image\ndehazing, and image harmonization.\n","authors":["Zhanghan Ke","Yuhao Liu","Lei Zhu","Nanxuan Zhao","Rynson W. H. Lau"],"pdf_url":"https://arxiv.org/pdf/2303.13511v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.13509v1","updated":"2023-03-23T17:59:02Z","published":"2023-03-23T17:59:02Z","title":"Position-Guided Point Cloud Panoptic Segmentation Transformer","summary":"  DEtection TRansformer (DETR) started a trend that uses a group of learnable\nqueries for unified visual perception. This work begins by applying this\nappealing paradigm to LiDAR-based point cloud segmentation and obtains a simple\nyet effective baseline. Although the naive adaptation obtains fair results, the\ninstance segmentation performance is noticeably inferior to previous works. By\ndiving into the details, we observe that instances in the sparse point clouds\nare relatively small to the whole scene and often have similar geometry but\nlack distinctive appearance for segmentation, which are rare in the image\ndomain. Considering instances in 3D are more featured by their positional\ninformation, we emphasize their roles during the modeling and design a robust\nMixed-parameterized Positional Embedding (MPE) to guide the segmentation\nprocess. It is embedded into backbone features and later guides the mask\nprediction and query update processes iteratively, leading to Position-Aware\nSegmentation (PA-Seg) and Masked Focal Attention (MFA). All these designs impel\nthe queries to attend to specific regions and identify various instances. The\nmethod, named Position-guided Point cloud Panoptic segmentation transFormer\n(P3Former), outperforms previous state-of-the-art methods by 3.4% and 1.2% PQ\non SemanticKITTI and nuScenes benchmark, respectively. The source code and\nmodels are available at https://github.com/SmartBot-PJLab/P3Former .\n","authors":["Zeqi Xiao","Wenwei Zhang","Tai Wang","Chen Change Loy","Dahua Lin","Jiangmiao Pang"],"pdf_url":"https://arxiv.org/pdf/2303.13509v1.pdf","comment":"Project page: https://github.com/SmartBot-PJLab/P3Former"},{"id":"http://arxiv.org/abs/2303.13510v1","updated":"2023-03-23T17:59:02Z","published":"2023-03-23T17:59:02Z","title":"MV-JAR: Masked Voxel Jigsaw and Reconstruction for LiDAR-Based\n  Self-Supervised Pre-Training","summary":"  This paper introduces the Masked Voxel Jigsaw and Reconstruction (MV-JAR)\nmethod for LiDAR-based self-supervised pre-training and a carefully designed\ndata-efficient 3D object detection benchmark on the Waymo dataset. Inspired by\nthe scene-voxel-point hierarchy in downstream 3D object detectors, we design\nmasking and reconstruction strategies accounting for voxel distributions in the\nscene and local point distributions within the voxel. We employ a\nReversed-Furthest-Voxel-Sampling strategy to address the uneven distribution of\nLiDAR points and propose MV-JAR, which combines two techniques for modeling the\naforementioned distributions, resulting in superior performance. Our\nexperiments reveal limitations in previous data-efficient experiments, which\nuniformly sample fine-tuning splits with varying data proportions from each\nLiDAR sequence, leading to similar data diversity across splits. To address\nthis, we propose a new benchmark that samples scene sequences for diverse\nfine-tuning splits, ensuring adequate model convergence and providing a more\naccurate evaluation of pre-training methods. Experiments on our Waymo benchmark\nand the KITTI dataset demonstrate that MV-JAR consistently and significantly\nimproves 3D detection performance across various data scales, achieving up to a\n6.3% increase in mAPH compared to training from scratch. Codes and the\nbenchmark will be available at https://github.com/SmartBot-PJLab/MV-JAR .\n","authors":["Runsen Xu","Tai Wang","Wenwei Zhang","Runjian Chen","Jinkun Cao","Jiangmiao Pang","Dahua Lin"],"pdf_url":"https://arxiv.org/pdf/2303.13510v1.pdf","comment":"Accepted by CVPR 2023 with a carefully designed benchmark on Waymo.\n  Codes and the benchmark will be available at\n  https://github.com/SmartBot-PJLab/MV-JAR"},{"id":"http://arxiv.org/abs/2303.13508v1","updated":"2023-03-23T17:59:00Z","published":"2023-03-23T17:59:00Z","title":"DreamBooth3D: Subject-Driven Text-to-3D Generation","summary":"  We present DreamBooth3D, an approach to personalize text-to-3D generative\nmodels from as few as 3-6 casually captured images of a subject. Our approach\ncombines recent advances in personalizing text-to-image models (DreamBooth)\nwith text-to-3D generation (DreamFusion). We find that naively combining these\nmethods fails to yield satisfactory subject-specific 3D assets due to\npersonalized text-to-image models overfitting to the input viewpoints of the\nsubject. We overcome this through a 3-stage optimization strategy where we\njointly leverage the 3D consistency of neural radiance fields together with the\npersonalization capability of text-to-image models. Our method can produce\nhigh-quality, subject-specific 3D assets with text-driven modifications such as\nnovel poses, colors and attributes that are not seen in any of the input images\nof the subject.\n","authors":["Amit Raj","Srinivas Kaza","Ben Poole","Michael Niemeyer","Nataniel Ruiz","Ben Mildenhall","Shiran Zada","Kfir Aberman","Michael Rubinstein","Jonathan Barron","Yuanzhen Li","Varun Jampani"],"pdf_url":"https://arxiv.org/pdf/2303.13508v1.pdf","comment":"Project page at https://dreambooth3d.github.io/ Video Summary at\n  https://youtu.be/TFtaoAqSkEA"},{"id":"http://arxiv.org/abs/2303.13504v1","updated":"2023-03-23T17:58:05Z","published":"2023-03-23T17:58:05Z","title":"ReBotNet: Fast Real-time Video Enhancement","summary":"  Most video restoration networks are slow, have high computational load, and\ncan't be used for real-time video enhancement. In this work, we design an\nefficient and fast framework to perform real-time video enhancement for\npractical use-cases like live video calls and video streams. Our proposed\nmethod, called Recurrent Bottleneck Mixer Network (ReBotNet), employs a\ndual-branch framework. The first branch learns spatio-temporal features by\ntokenizing the input frames along the spatial and temporal dimensions using a\nConvNext-based encoder and processing these abstract tokens using a bottleneck\nmixer. To further improve temporal consistency, the second branch employs a\nmixer directly on tokens extracted from individual frames. A common decoder\nthen merges the features form the two branches to predict the enhanced frame.\nIn addition, we propose a recurrent training approach where the last frame's\nprediction is leveraged to efficiently enhance the current frame while\nimproving temporal consistency. To evaluate our method, we curate two new\ndatasets that emulate real-world video call and streaming scenarios, and show\nextensive results on multiple datasets where ReBotNet outperforms existing\napproaches with lower computations, reduced memory requirements, and faster\ninference time.\n","authors":["Jeya Maria Jose Valanarasu","Rahul Garg","Andeep Toor","Xin Tong","Weijuan Xi","Andreas Lugmayr","Vishal M. Patel","Anne Menini"],"pdf_url":"https://arxiv.org/pdf/2303.13504v1.pdf","comment":"Project Website: https://jeya-maria-jose.github.io/rebotnet-web/"},{"id":"http://arxiv.org/abs/2303.13505v1","updated":"2023-03-23T17:58:05Z","published":"2023-03-23T17:58:05Z","title":"A Large-scale Study of Spatiotemporal Representation Learning with a New\n  Benchmark on Action Recognition","summary":"  The goal of building a benchmark (suite of datasets) is to provide a unified\nprotocol for fair evaluation and thus facilitate the evolution of a specific\narea. Nonetheless, we point out that existing protocols of action recognition\ncould yield partial evaluations due to several limitations. To comprehensively\nprobe the effectiveness of spatiotemporal representation learning, we introduce\nBEAR, a new BEnchmark on video Action Recognition. BEAR is a collection of 18\nvideo datasets grouped into 5 categories (anomaly, gesture, daily, sports, and\ninstructional), which covers a diverse set of real-world applications. With\nBEAR, we thoroughly evaluate 6 common spatiotemporal models pre-trained by both\nsupervised and self-supervised learning. We also report transfer performance\nvia standard finetuning, few-shot finetuning, and unsupervised domain\nadaptation. Our observation suggests that current state-of-the-art cannot\nsolidly guarantee high performance on datasets close to real-world\napplications, and we hope BEAR can serve as a fair and challenging evaluation\nbenchmark to gain insights on building next-generation spatiotemporal learners.\nOur dataset, code, and models are released at:\nhttps://github.com/AndongDeng/BEAR\n","authors":["Andong Deng","Taojiannan Yang","Chen Chen"],"pdf_url":"https://arxiv.org/pdf/2303.13505v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.13501v1","updated":"2023-03-23T17:57:28Z","published":"2023-03-23T17:57:28Z","title":"Chordal Averaging on Flag Manifolds and Its Applications","summary":"  This paper presents a new, provably-convergent algorithm for computing the\nflag-mean and flag-median of a set of points on a flag manifold under the\nchordal metric. The flag manifold is a mathematical space consisting of flags,\nwhich are sequences of nested subspaces of a vector space that increase in\ndimension. The flag manifold is a superset of a wide range of known matrix\ngroups, including Stiefel and Grassmanians, making it a general object that is\nuseful in a wide variety computer vision problems.\n  To tackle the challenge of computing first order flag statistics, we first\ntransform the problem into one that involves auxiliary variables constrained to\nthe Stiefel manifold. The Stiefel manifold is a space of orthogonal frames, and\nleveraging the numerical stability and efficiency of Stiefel-manifold\noptimization enables us to compute the flag-mean effectively. Through a series\nof experiments, we show the competence of our method in Grassmann and rotation\naveraging, as well as principal component analysis.\n","authors":["Nathan Mankovich","Tolga Birdal"],"pdf_url":"https://arxiv.org/pdf/2303.13501v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2208.05845v2","updated":"2023-03-23T17:56:52Z","published":"2022-08-11T14:28:21Z","title":"A Comprehensive Analysis of AI Biases in DeepFake Detection With\n  Massively Annotated Databases","summary":"  In recent years, image and video manipulations with Deepfake have become a\nsevere concern for security and society. Many detection models and datasets\nhave been proposed to detect Deepfake data reliably. However, there is an\nincreased concern that these models and training databases might be biased and,\nthus, cause Deepfake detectors to fail. In this work, we investigate the bias\nissue caused by public Deepfake datasets by (a) providing large-scale\ndemographic and non-demographic attribute annotations of 47 different\nattributes for five popular Deepfake datasets and (b) comprehensively analysing\nAI-bias of three state-of-the-art Deepfake detection backbone models on these\ndatasets. The investigation analyses the influence of a large variety of\ndistinctive attributes (from over 65M labels) on the detection performance,\nincluding demographic (age, gender, ethnicity) and non-demographic (hair, skin,\naccessories, etc.) information. The results indicate that investigated\ndatabases lack diversity and, more importantly, show that the utilised Deepfake\ndetection backbone models are strongly biased towards many investigated\nattributes. The Deepfake detection backbone methods, which are trained with\nbiased datasets, might output incorrect detection results, thereby leading to\ngeneralisability, fairness, and security issues. We hope that the findings of\nthis study and the annotation databases will help to evaluate and mitigate bias\nin future Deepfake detection techniques. The annotation datasets are publicly\navailable.\n","authors":["Ying Xu","Philipp Terhörst","Kiran Raja","Marius Pedersen"],"pdf_url":"https://arxiv.org/pdf/2208.05845v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.13497v1","updated":"2023-03-23T17:56:20Z","published":"2023-03-23T17:56:20Z","title":"TriPlaneNet: An Encoder for EG3D Inversion","summary":"  Recent progress in NeRF-based GANs has introduced a number of approaches for\nhigh-resolution and high-fidelity generative modeling of human heads with a\npossibility for novel view rendering. At the same time, one must solve an\ninverse problem to be able to re-render or modify an existing image or video.\nDespite the success of universal optimization-based methods for 2D GAN\ninversion, those, applied to 3D GANs, may fail to produce 3D-consistent\nrenderings. Fast encoder-based techniques, such as those developed for\nStyleGAN, may also be less appealing due to the lack of identity preservation.\nIn our work, we introduce a real-time method that bridges the gap between the\ntwo approaches by directly utilizing the tri-plane representation introduced\nfor EG3D generative model. In particular, we build upon a feed-forward\nconvolutional encoder for the latent code and extend it with a\nfully-convolutional predictor of tri-plane numerical offsets. As shown in our\nwork, the renderings are similar in quality to optimization-based techniques\nand significantly outperform the baselines for novel view. As we empirically\nprove, this is a consequence of directly operating in the tri-plane space, not\nin the GAN parameter space, while making use of an encoder-based trainable\napproach.\n","authors":["Ananta R. Bhattarai","Matthias Nießner","Artem Sevastopolsky"],"pdf_url":"https://arxiv.org/pdf/2303.13497v1.pdf","comment":"Video: https://youtu.be/GpmSswHMeWU Project page:\n  https://anantarb.github.io/triplanenet"},{"id":"http://arxiv.org/abs/2303.13496v1","updated":"2023-03-23T17:56:12Z","published":"2023-03-23T17:56:12Z","title":"The effectiveness of MAE pre-pretraining for billion-scale pretraining","summary":"  This paper revisits the standard pretrain-then-finetune paradigm used in\ncomputer vision for visual recognition tasks. Typically, state-of-the-art\nfoundation models are pretrained using large scale (weakly) supervised datasets\nwith billions of images. We introduce an additional pre-pretraining stage that\nis simple and uses the self-supervised MAE technique to initialize the model.\nWhile MAE has only been shown to scale with the size of models, we find that it\nscales with the size of the training dataset as well. Thus, our MAE-based\npre-pretraining scales with both model and data size making it applicable for\ntraining foundation models. Pre-pretraining consistently improves both the\nmodel convergence and the downstream transfer performance across a range of\nmodel scales (millions to billions of parameters), and dataset sizes (millions\nto billions of images). We measure the effectiveness of pre-pretraining on 10\ndifferent visual recognition tasks spanning image classification, video\nrecognition, object detection, low-shot classification and zero-shot\nrecognition. Our largest model achieves new state-of-the-art results on\niNaturalist-18 (91.3%), 1-shot ImageNet-1k (62.1%), and zero-shot transfer on\nFood-101 (96.0%). Our study reveals that model initialization plays a\nsignificant role, even for web-scale pretraining with billions of images.\n","authors":["Mannat Singh","Quentin Duval","Kalyan Vasudev Alwala","Haoqi Fan","Vaibhav Aggarwal","Aaron Adcock","Armand Joulin","Piotr Dollár","Christoph Feichtenhofer","Ross Girshick","Rohit Girdhar","Ishan Misra"],"pdf_url":"https://arxiv.org/pdf/2303.13496v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.13495v1","updated":"2023-03-23T17:56:10Z","published":"2023-03-23T17:56:10Z","title":"ReVersion: Diffusion-Based Relation Inversion from Images","summary":"  Diffusion models gain increasing popularity for their generative\ncapabilities. Recently, there have been surging needs to generate customized\nimages by inverting diffusion models from exemplar images. However, existing\ninversion methods mainly focus on capturing object appearances. How to invert\nobject relations, another important pillar in the visual world, remains\nunexplored. In this work, we propose ReVersion for the Relation Inversion task,\nwhich aims to learn a specific relation (represented as \"relation prompt\") from\nexemplar images. Specifically, we learn a relation prompt from a frozen\npre-trained text-to-image diffusion model. The learned relation prompt can then\nbe applied to generate relation-specific images with new objects, backgrounds,\nand styles. Our key insight is the \"preposition prior\" - real-world relation\nprompts can be sparsely activated upon a set of basis prepositional words.\nSpecifically, we propose a novel relation-steering contrastive learning scheme\nto impose two critical properties of the relation prompt: 1) The relation\nprompt should capture the interaction between objects, enforced by the\npreposition prior. 2) The relation prompt should be disentangled away from\nobject appearances. We further devise relation-focal importance sampling to\nemphasize high-level interactions over low-level appearances (e.g., texture,\ncolor). To comprehensively evaluate this new task, we contribute ReVersion\nBenchmark, which provides various exemplar images with diverse relations.\nExtensive experiments validate the superiority of our approach over existing\nmethods across a wide range of visual relations.\n","authors":["Ziqi Huang","Tianxing Wu","Yuming Jiang","Kelvin C. K. Chan","Ziwei Liu"],"pdf_url":"https://arxiv.org/pdf/2303.13495v1.pdf","comment":"First two authors contributed equally. Project page:\n  https://ziqihuangg.github.io/projects/reversion.html Code:\n  https://github.com/ziqihuangg/ReVersion"},{"id":"http://arxiv.org/abs/2303.13483v1","updated":"2023-03-23T17:50:40Z","published":"2023-03-23T17:50:40Z","title":"NS3D: Neuro-Symbolic Grounding of 3D Objects and Relations","summary":"  Grounding object properties and relations in 3D scenes is a prerequisite for\na wide range of artificial intelligence tasks, such as visually grounded\ndialogues and embodied manipulation. However, the variability of the 3D domain\ninduces two fundamental challenges: 1) the expense of labeling and 2) the\ncomplexity of 3D grounded language. Hence, essential desiderata for models are\nto be data-efficient, generalize to different data distributions and tasks with\nunseen semantic forms, as well as ground complex language semantics (e.g.,\nview-point anchoring and multi-object reference). To address these challenges,\nwe propose NS3D, a neuro-symbolic framework for 3D grounding. NS3D translates\nlanguage into programs with hierarchical structures by leveraging large\nlanguage-to-code models. Different functional modules in the programs are\nimplemented as neural networks. Notably, NS3D extends prior neuro-symbolic\nvisual reasoning methods by introducing functional modules that effectively\nreason about high-arity relations (i.e., relations among more than two\nobjects), key in disambiguating objects in complex 3D scenes. Modular and\ncompositional architecture enables NS3D to achieve state-of-the-art results on\nthe ReferIt3D view-dependence task, a 3D referring expression comprehension\nbenchmark. Importantly, NS3D shows significantly improved performance on\nsettings of data-efficiency and generalization, and demonstrate zero-shot\ntransfer to an unseen 3D question-answering task.\n","authors":["Joy Hsu","Jiayuan Mao","Jiajun Wu"],"pdf_url":"https://arxiv.org/pdf/2303.13483v1.pdf","comment":"In CVPR 2023"},{"id":"http://arxiv.org/abs/2303.13482v1","updated":"2023-03-23T17:50:09Z","published":"2023-03-23T17:50:09Z","title":"TactoFind: A Tactile Only System for Object Retrieval","summary":"  We study the problem of object retrieval in scenarios where visual sensing is\nabsent, object shapes are unknown beforehand and objects can move freely, like\ngrabbing objects out of a drawer. Successful solutions require localizing free\nobjects, identifying specific object instances, and then grasping the\nidentified objects, only using touch feedback. Unlike vision, where cameras can\nobserve the entire scene, touch sensors are local and only observe parts of the\nscene that are in contact with the manipulator. Moreover, information gathering\nvia touch sensors necessitates applying forces on the touched surface which may\ndisturb the scene itself. Reasoning with touch, therefore, requires careful\nexploration and integration of information over time -- a challenge we tackle.\nWe present a system capable of using sparse tactile feedback from fingertip\ntouch sensors on a dexterous hand to localize, identify and grasp novel objects\nwithout any visual feedback. Videos are available at\nhttps://taochenshh.github.io/projects/tactofind.\n","authors":["Sameer Pai","Tao Chen","Megha Tippur","Edward Adelson","Abhishek Gupta","Pulkit Agrawal"],"pdf_url":"https://arxiv.org/pdf/2303.13482v1.pdf","comment":"Accepted in ICRA 2023"},{"id":"http://arxiv.org/abs/2303.13479v1","updated":"2023-03-23T17:48:12Z","published":"2023-03-23T17:48:12Z","title":"Prior-free Category-level Pose Estimation with Implicit Space\n  Transformation","summary":"  Category-level 6D pose estimation aims to predict the poses and sizes of\nunseen objects from a specific category. Thanks to prior deformation, which\nexplicitly adapts a category-specific 3D prior (i.e., a 3D template) to a given\nobject instance, prior-based methods attained great success and have become a\nmajor research stream. However, obtaining category-specific priors requires\ncollecting a large amount of 3D models, which is labor-consuming and often not\naccessible in practice. This motivates us to investigate whether priors are\nnecessary to make prior-based methods effective. Our empirical study shows that\nthe 3D prior itself is not the credit to the high performance. The keypoint\nactually is the explicit deformation process, which aligns camera and world\ncoordinates supervised by world-space 3D models (also called canonical space).\nInspired by these observation, we introduce a simple prior-free implicit space\ntransformation network, namely IST-Net, to transform camera-space features to\nworld-space counterparts and build correspondence between them in an implicit\nmanner without relying on 3D priors. Besides, we design camera- and world-space\nenhancers to enrich the features with pose-sensitive information and\ngeometrical constraints, respectively. Albeit simple, IST-Net becomes the first\nprior-free method that achieves state-of-the-art performance, with top\ninference speed on the REAL275 dataset. Our code and models will be publicly\navailable.\n","authors":["Jianhui Liu","Yukang Chen","Xiaoqing Ye","Xiaojuan Qi"],"pdf_url":"https://arxiv.org/pdf/2303.13479v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.13477v1","updated":"2023-03-23T17:46:54Z","published":"2023-03-23T17:46:54Z","title":"TransPoser: Transformer as an Optimizer for Joint Object Shape and Pose\n  Estimation","summary":"  We propose a novel method for joint estimation of shape and pose of rigid\nobjects from their sequentially observed RGB-D images. In sharp contrast to\npast approaches that rely on complex non-linear optimization, we propose to\nformulate it as a neural optimization that learns to efficiently estimate the\nshape and pose. We introduce Deep Directional Distance Function (DeepDDF), a\nneural network that directly outputs the depth image of an object given the\ncamera viewpoint and viewing direction, for efficient error computation in 2D\nimage space. We formulate the joint estimation itself as a Transformer which we\nrefer to as TransPoser. We fully leverage the tokenization and multi-head\nattention to sequentially process the growing set of observations and to\nefficiently update the shape and pose with a learned momentum, respectively.\nExperimental results on synthetic and real data show that DeepDDF achieves high\naccuracy as a category-level object shape representation and TransPoser\nachieves state-of-the-art accuracy efficiently for joint shape and pose\nestimation.\n","authors":["Yuta Yoshitake","Mai Nishimura","Shohei Nobuhara","Ko Nishino"],"pdf_url":"https://arxiv.org/pdf/2303.13477v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.13472v1","updated":"2023-03-23T17:43:17Z","published":"2023-03-23T17:43:17Z","title":"Plotting Behind the Scenes: Towards Learnable Game Engines","summary":"  Game engines are powerful tools in computer graphics. Their power comes at\nthe immense cost of their development. In this work, we present a framework to\ntrain game-engine-like neural models, solely from monocular annotated videos.\nThe result-a Learnable Game Engine (LGE)-maintains states of the scene, objects\nand agents in it, and enables rendering the environment from a controllable\nviewpoint. Similarly to a game engine, it models the logic of the game and the\nunderlying rules of physics, to make it possible for a user to play the game by\nspecifying both high- and low-level action sequences. Most captivatingly, our\nLGE unlocks the director's mode, where the game is played by plotting behind\nthe scenes, specifying high-level actions and goals for the agents in the form\nof language and desired states. This requires learning \"game AI\", encapsulated\nby our animation model, to navigate the scene using high-level constraints,\nplay against an adversary, devise the strategy to win a point. The key to\nlearning such game AI is the exploitation of a large and diverse text corpus,\ncollected in this work, describing detailed actions in a game and used to train\nour animation model. To render the resulting state of the environment and its\nagents, we use a compositional NeRF representation used in our synthesis model.\nTo foster future research, we present newly collected, annotated and calibrated\nlarge-scale Tennis and Minecraft datasets. Our method significantly outperforms\nexisting neural video game simulators in terms of rendering quality. Besides,\nour LGEs unlock applications beyond capabilities of the current state of the\nart. Our framework, data, and models are available at\nhttps://learnable-game-engines.github.io/lge-website.\n","authors":["Willi Menapace","Aliaksandr Siarohin","Stéphane Lathuilière","Panos Achlioptas","Vladislav Golyanik","Elisa Ricci","Sergey Tulyakov"],"pdf_url":"https://arxiv.org/pdf/2303.13472v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.13471v1","updated":"2023-03-23T17:43:11Z","published":"2023-03-23T17:43:11Z","title":"Egocentric Audio-Visual Object Localization","summary":"  Humans naturally perceive surrounding scenes by unifying sound and sight in a\nfirst-person view. Likewise, machines are advanced to approach human\nintelligence by learning with multisensory inputs from an egocentric\nperspective. In this paper, we explore the challenging egocentric audio-visual\nobject localization task and observe that 1) egomotion commonly exists in\nfirst-person recordings, even within a short duration; 2) The out-of-view sound\ncomponents can be created while wearers shift their attention. To address the\nfirst problem, we propose a geometry-aware temporal aggregation module to\nhandle the egomotion explicitly. The effect of egomotion is mitigated by\nestimating the temporal geometry transformation and exploiting it to update\nvisual representations. Moreover, we propose a cascaded feature enhancement\nmodule to tackle the second issue. It improves cross-modal localization\nrobustness by disentangling visually-indicated audio representation. During\ntraining, we take advantage of the naturally available audio-visual temporal\nsynchronization as the ``free'' self-supervision to avoid costly labeling. We\nalso annotate and create the Epic Sounding Object dataset for evaluation\npurposes. Extensive experiments show that our method achieves state-of-the-art\nlocalization performance in egocentric videos and can be generalized to diverse\naudio-visual scenes.\n","authors":["Chao Huang","Yapeng Tian","Anurag Kumar","Chenliang Xu"],"pdf_url":"https://arxiv.org/pdf/2303.13471v1.pdf","comment":"Accepted by CVPR 2023"},{"id":"http://arxiv.org/abs/2209.04634v2","updated":"2023-03-23T17:31:25Z","published":"2022-09-10T10:35:53Z","title":"Real-time event simulation with frame-based cameras","summary":"  Event cameras are becoming increasingly popular in robotics and computer\nvision due to their beneficial properties, e.g., high temporal resolution, high\nbandwidth, almost no motion blur, and low power consumption. However, these\ncameras remain expensive and scarce in the market, making them inaccessible to\nthe majority. Using event simulators minimizes the need for real event cameras\nto develop novel algorithms. However, due to the computational complexity of\nthe simulation, the event streams of existing simulators cannot be generated in\nreal-time but rather have to be pre-calculated from existing video sequences or\npre-rendered and then simulated from a virtual 3D scene. Although these offline\ngenerated event streams can be used as training data for learning tasks, all\nresponse time dependent applications cannot benefit from these simulators yet,\nas they still require an actual event camera. This work proposes simulation\nmethods that improve the performance of event simulation by two orders of\nmagnitude (making them real-time capable) while remaining competitive in the\nquality assessment.\n","authors":["Andreas Ziegler","Daniel Teigland","Jonas Tebbe","Thomas Gossard","Andreas Zell"],"pdf_url":"https://arxiv.org/pdf/2209.04634v2.pdf","comment":"Accepted for 2023 IEEE International Conference on Robotics and\n  Automation (ICRA 2023). Project web page:\n  https://cogsys-tuebingen.github.io/realtime_event_simulator/"},{"id":"http://arxiv.org/abs/2303.13455v1","updated":"2023-03-23T17:24:31Z","published":"2023-03-23T17:24:31Z","title":"CoBIT: A Contrastive Bi-directional Image-Text Generation Model","summary":"  The field of vision and language has witnessed a proliferation of pre-trained\nfoundation models. Most existing methods are independently pre-trained with\ncontrastive objective like CLIP, image-to-text generative objective like PaLI,\nor text-to-image generative objective like Parti. However, the three objectives\ncan be pre-trained on the same data, image-text pairs, and intuitively they\ncomplement each other as contrasting provides global alignment capacity and\ngeneration grants fine-grained understanding. In this work, we present a\nContrastive Bi-directional Image-Text generation model (CoBIT), which attempts\nto unify the three pre-training objectives in one framework. Specifically,\nCoBIT employs a novel unicoder-decoder structure, consisting of an image\nunicoder, a text unicoder and a cross-modal decoder. The image/text unicoders\ncan switch between encoding and decoding in different tasks, enabling\nflexibility and shared knowledge that benefits both image-to-text and\ntext-to-image generations. CoBIT achieves superior performance in image\nunderstanding, image-text understanding (Retrieval, Captioning, VQA, SNLI-VE)\nand text-based content creation, particularly in zero-shot scenarios. For\ninstance, 82.7% in zero-shot ImageNet classification, 9.37 FID score in\nzero-shot text-to-image generation and 44.8 CIDEr in zero-shot captioning.\n","authors":["Haoxuan You","Mandy Guo","Zhecan Wang","Kai-Wei Chang","Jason Baldridge","Jiahui Yu"],"pdf_url":"https://arxiv.org/pdf/2303.13455v1.pdf","comment":"14 pages, 5 figures"},{"id":"http://arxiv.org/abs/2303.13450v1","updated":"2023-03-23T17:17:29Z","published":"2023-03-23T17:17:29Z","title":"Set-the-Scene: Global-Local Training for Generating Controllable NeRF\n  Scenes","summary":"  Recent breakthroughs in text-guided image generation have led to remarkable\nprogress in the field of 3D synthesis from text. By optimizing neural radiance\nfields (NeRF) directly from text, recent methods are able to produce remarkable\nresults. Yet, these methods are limited in their control of each object's\nplacement or appearance, as they represent the scene as a whole. This can be a\nmajor issue in scenarios that require refining or manipulating objects in the\nscene. To remedy this deficit, we propose a novel GlobalLocal training\nframework for synthesizing a 3D scene using object proxies. A proxy represents\nthe object's placement in the generated scene and optionally defines its coarse\ngeometry. The key to our approach is to represent each object as an independent\nNeRF. We alternate between optimizing each NeRF on its own and as part of the\nfull scene. Thus, a complete representation of each object can be learned,\nwhile also creating a harmonious scene with style and lighting match. We show\nthat using proxies allows a wide variety of editing options, such as adjusting\nthe placement of each independent object, removing objects from a scene, or\nrefining an object. Our results show that Set-the-Scene offers a powerful\nsolution for scene synthesis and manipulation, filling a crucial gap in\ncontrollable text-to-3D synthesis.\n","authors":["Dana Cohen-Bar","Elad Richardson","Gal Metzer","Raja Giryes","Daniel Cohen-Or"],"pdf_url":"https://arxiv.org/pdf/2303.13450v1.pdf","comment":"project page at https://danacohen95.github.io/Set-the-Scene/"},{"id":"http://arxiv.org/abs/2303.13440v1","updated":"2023-03-23T17:02:00Z","published":"2023-03-23T17:02:00Z","title":"CLIP for All Things Zero-Shot Sketch-Based Image Retrieval, Fine-Grained\n  or Not","summary":"  In this paper, we leverage CLIP for zero-shot sketch based image retrieval\n(ZS-SBIR). We are largely inspired by recent advances on foundation models and\nthe unparalleled generalisation ability they seem to offer, but for the first\ntime tailor it to benefit the sketch community. We put forward novel designs on\nhow best to achieve this synergy, for both the category setting and the\nfine-grained setting (\"all\"). At the very core of our solution is a prompt\nlearning setup. First we show just via factoring in sketch-specific prompts, we\nalready have a category-level ZS-SBIR system that overshoots all prior arts, by\na large margin (24.8%) - a great testimony on studying the CLIP and ZS-SBIR\nsynergy. Moving onto the fine-grained setup is however trickier, and requires a\ndeeper dive into this synergy. For that, we come up with two specific designs\nto tackle the fine-grained matching nature of the problem: (i) an additional\nregularisation loss to ensure the relative separation between sketches and\nphotos is uniform across categories, which is not the case for the gold\nstandard standalone triplet loss, and (ii) a clever patch shuffling technique\nto help establishing instance-level structural correspondences between\nsketch-photo pairs. With these designs, we again observe significant\nperformance gains in the region of 26.9% over previous state-of-the-art. The\ntake-home message, if any, is the proposed CLIP and prompt learning paradigm\ncarries great promise in tackling other sketch-related tasks (not limited to\nZS-SBIR) where data scarcity remains a great challenge. Code and models will be\nmade available.\n","authors":["Aneeshan Sain","Ayan Kumar Bhunia","Pinaki Nath Chowdhury","Subhadeep Koley","Tao Xiang","Yi-Zhe Song"],"pdf_url":"https://arxiv.org/pdf/2303.13440v1.pdf","comment":"Accepted in Computer Vision and Pattern Recognition (CVPR), 2023"},{"id":"http://arxiv.org/abs/2303.13439v1","updated":"2023-03-23T17:01:59Z","published":"2023-03-23T17:01:59Z","title":"Text2Video-Zero: Text-to-Image Diffusion Models are Zero-Shot Video\n  Generators","summary":"  Recent text-to-video generation approaches rely on computationally heavy\ntraining and require large-scale video datasets. In this paper, we introduce a\nnew task of zero-shot text-to-video generation and propose a low-cost approach\n(without any training or optimization) by leveraging the power of existing\ntext-to-image synthesis methods (e.g., Stable Diffusion), making them suitable\nfor the video domain.\n  Our key modifications include (i) enriching the latent codes of the generated\nframes with motion dynamics to keep the global scene and the background time\nconsistent; and (ii) reprogramming frame-level self-attention using a new\ncross-frame attention of each frame on the first frame, to preserve the\ncontext, appearance, and identity of the foreground object.\n  Experiments show that this leads to low overhead, yet high-quality and\nremarkably consistent video generation. Moreover, our approach is not limited\nto text-to-video synthesis but is also applicable to other tasks such as\nconditional and content-specialized video generation, and Video\nInstruct-Pix2Pix, i.e., instruction-guided video editing.\n  As experiments show, our method performs comparably or sometimes better than\nrecent approaches, despite not being trained on additional video data. Our code\nwill be open sourced at: https://github.com/Picsart-AI-Research/Text2Video-Zero .\n","authors":["Levon Khachatryan","Andranik Movsisyan","Vahram Tadevosyan","Roberto Henschel","Zhangyang Wang","Shant Navasardyan","Humphrey Shi"],"pdf_url":"https://arxiv.org/pdf/2303.13439v1.pdf","comment":"The project is available at:\n  https://github.com/Picsart-AI-Research/Text2Video-Zero"},{"id":"http://arxiv.org/abs/2302.06514v4","updated":"2023-03-23T16:58:41Z","published":"2023-02-13T16:49:27Z","title":"Multiple Appropriate Facial Reaction Generation in Dyadic Interaction\n  Settings: What, Why and How?","summary":"  According to the Stimulus Organism Response (SOR) theory, all human\nbehavioral reactions are stimulated by context, where people will process the\nreceived stimulus and produce an appropriate reaction. This implies that in a\nspecific context for a given input stimulus, a person can react differently\naccording to their internal state and other contextual factors. Analogously, in\ndyadic interactions, humans communicate using verbal and nonverbal cues, where\na broad spectrum of listeners' non-verbal reactions might be appropriate for\nresponding to a specific speaker behaviour. There already exists a body of work\nthat investigated the problem of automatically generating an appropriate\nreaction for a given input. However, none attempted to automatically generate\nmultiple appropriate reactions in the context of dyadic interactions and\nevaluate the appropriateness of those reactions using objective measures. This\npaper starts by defining the facial Multiple Appropriate Reaction Generation\n(fMARG) task for the first time in the literature and proposes a new set of\nobjective evaluation metrics to evaluate the appropriateness of the generated\nreactions. The paper subsequently introduces a framework to predict, generate,\nand evaluate multiple appropriate facial reactions.\n","authors":["Siyang Song","Micol Spitale","Yiming Luo","Batuhan Bal","Hatice Gunes"],"pdf_url":"https://arxiv.org/pdf/2302.06514v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.13434v1","updated":"2023-03-23T16:56:01Z","published":"2023-03-23T16:56:01Z","title":"Patch-Mix Transformer for Unsupervised Domain Adaptation: A Game\n  Perspective","summary":"  Endeavors have been recently made to leverage the vision transformer (ViT)\nfor the challenging unsupervised domain adaptation (UDA) task. They typically\nadopt the cross-attention in ViT for direct domain alignment. However, as the\nperformance of cross-attention highly relies on the quality of pseudo labels\nfor targeted samples, it becomes less effective when the domain gap becomes\nlarge. We solve this problem from a game theory's perspective with the proposed\nmodel dubbed as PMTrans, which bridges source and target domains with an\nintermediate domain. Specifically, we propose a novel ViT-based module called\nPatchMix that effectively builds up the intermediate domain, i.e., probability\ndistribution, by learning to sample patches from both domains based on the\ngame-theoretical models. This way, it learns to mix the patches from the source\nand target domains to maximize the cross entropy (CE), while exploiting two\nsemi-supervised mixup losses in the feature and label spaces to minimize it. As\nsuch, we interpret the process of UDA as a min-max CE game with three players,\nincluding the feature extractor, classifier, and PatchMix, to find the Nash\nEquilibria. Moreover, we leverage attention maps from ViT to re-weight the\nlabel of each patch by its importance, making it possible to obtain more\ndomain-discriminative feature representations. We conduct extensive experiments\non four benchmark datasets, and the results show that PMTrans significantly\nsurpasses the ViT-based and CNN-based SoTA methods by +3.6% on Office-Home,\n+1.4% on Office-31, and +17.7% on DomainNet, respectively.\n","authors":["Jinjing Zhu","Haotian Bai","Lin Wang"],"pdf_url":"https://arxiv.org/pdf/2303.13434v1.pdf","comment":"Accepted by CVPR 2023 (Highlight)"},{"id":"http://arxiv.org/abs/2303.13430v1","updated":"2023-03-23T16:50:19Z","published":"2023-03-23T16:50:19Z","title":"Medical diffusion on a budget: textual inversion for medical image\n  generation","summary":"  Diffusion-based models for text-to-image generation have gained immense\npopularity due to recent advancements in efficiency, accessibility, and\nquality. Although it is becoming increasingly feasible to perform inference\nwith these systems using consumer-grade GPUs, training them from scratch still\nrequires access to large datasets and significant computational resources. In\nthe case of medical image generation, the availability of large, publicly\naccessible datasets that include text reports is limited due to legal and\nethical concerns. While training a diffusion model on a private dataset may\naddress this issue, it is not always feasible for institutions lacking the\nnecessary computational resources. This work demonstrates that pre-trained\nStable Diffusion models, originally trained on natural images, can be adapted\nto various medical imaging modalities by training text embeddings with textual\ninversion. In this study, we conducted experiments using medical datasets\ncomprising only 100 samples from three medical modalities. Embeddings were\ntrained in a matter of hours, while still retaining diagnostic relevance in\nimage generation. Experiments were designed to achieve several objectives.\nFirstly, we fine-tuned the training and inference processes of textual\ninversion, revealing that larger embeddings and more examples are required.\nSecondly, we validated our approach by demonstrating a 2\\% increase in the\ndiagnostic accuracy (AUC) for detecting prostate cancer on MRI, which is a\nchallenging multi-modal imaging modality, from 0.78 to 0.80. Thirdly, we\nperformed simulations by interpolating between healthy and diseased states,\ncombining multiple pathologies, and inpainting to show embedding flexibility\nand control of disease appearance. Finally, the embeddings trained in this\nstudy are small (less than 1 MB), which facilitates easy sharing of medical\ndata with reduced privacy concerns.\n","authors":["Bram de Wilde","Anindo Saha","Richard P. G. ten Broek","Henkjan Huisman"],"pdf_url":"https://arxiv.org/pdf/2303.13430v1.pdf","comment":null},{"id":"http://arxiv.org/abs/1909.12913v5","updated":"2023-03-23T16:43:29Z","published":"2019-09-18T15:46:48Z","title":"Student Engagement Detection Using Emotion Analysis, Eye Tracking and\n  Head Movement with Machine Learning","summary":"  With the increase of distance learning, in general, and e-learning, in\nparticular, having a system capable of determining the engagement of students\nis of primordial importance, and one of the biggest challenges, both for\nteachers, researchers and policy makers. Here, we present a system to detect\nthe engagement level of the students. It uses only information provided by the\ntypical built-in web-camera present in a laptop computer, and was designed to\nwork in real time. We combine information about the movements of the eyes and\nhead, and facial emotions to produce a concentration index with three classes\nof engagement: \"very engaged\", \"nominally engaged\" and \"not engaged at all\".\nThe system was tested in a typical e-learning scenario, and the results show\nthat it correctly identifies each period of time where students were \"very\nengaged\", \"nominally engaged\" and \"not engaged at all\". Additionally, the\nresults also show that the students with best scores also have higher\nconcentration indexes.\n","authors":["Prabin Sharma","Shubham Joshi","Subash Gautam","Sneha Maharjan","Salik Ram Khanal","Manuel Cabral Reis","João Barroso","Vítor Manuel de Jesus Filipe"],"pdf_url":"https://arxiv.org/pdf/1909.12913v5.pdf","comment":"9 pages, 9 Figures, 2 tables"},{"id":"http://arxiv.org/abs/2303.13412v1","updated":"2023-03-23T16:32:49Z","published":"2023-03-23T16:32:49Z","title":"Low-Light Image Enhancement by Learning Contrastive Representations in\n  Spatial and Frequency Domains","summary":"  Images taken under low-light conditions tend to suffer from poor visibility,\nwhich can decrease image quality and even reduce the performance of the\ndownstream tasks. It is hard for a CNN-based method to learn generalized\nfeatures that can recover normal images from the ones under various unknow\nlow-light conditions. In this paper, we propose to incorporate the contrastive\nlearning into an illumination correction network to learn abstract\nrepresentations to distinguish various low-light conditions in the\nrepresentation space, with the purpose of enhancing the generalizability of the\nnetwork. Considering that light conditions can change the frequency components\nof the images, the representations are learned and compared in both spatial and\nfrequency domains to make full advantage of the contrastive learning. The\nproposed method is evaluated on LOL and LOL-V2 datasets, the results show that\nthe proposed method achieves better qualitative and quantitative results\ncompared with other state-of-the-arts.\n","authors":["Yi Huang","Xiaoguang Tu","Gui Fu","Tingting Liu","Bokai Liu","Ming Yang","Ziliang Feng"],"pdf_url":"https://arxiv.org/pdf/2303.13412v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2211.12870v2","updated":"2023-03-23T16:29:04Z","published":"2022-11-23T11:21:00Z","title":"ActMAD: Activation Matching to Align Distributions for\n  Test-Time-Training","summary":"  Test-Time-Training (TTT) is an approach to cope with out-of-distribution\n(OOD) data by adapting a trained model to distribution shifts occurring at\ntest-time. We propose to perform this adaptation via Activation Matching\n(ActMAD): We analyze activations of the model and align activation statistics\nof the OOD test data to those of the training data. In contrast to existing\nmethods, which model the distribution of entire channels in the ultimate layer\nof the feature extractor, we model the distribution of each feature in multiple\nlayers across the network. This results in a more fine-grained supervision and\nmakes ActMAD attain state of the art performance on CIFAR-100C and Imagenet-C.\nActMAD is also architecture- and task-agnostic, which lets us go beyond image\nclassification, and score 15.4% improvement over previous approaches when\nevaluating a KITTI-trained object detector on KITTI-Fog. Our experiments\nhighlight that ActMAD can be applied to online adaptation in realistic\nscenarios, requiring little data to attain its full performance.\n","authors":["Muhammad Jehanzeb Mirza","Pol Jané Soneira","Wei Lin","Mateusz Kozinski","Horst Possegger","Horst Bischof"],"pdf_url":"https://arxiv.org/pdf/2211.12870v2.pdf","comment":"CVPR 2023 - Project Page: https://jmiemirza.github.io/ActMAD/"},{"id":"http://arxiv.org/abs/2303.13405v1","updated":"2023-03-23T16:28:15Z","published":"2023-03-23T16:28:15Z","title":"SC-MIL: Supervised Contrastive Multiple Instance Learning for Imbalanced\n  Classification in Pathology","summary":"  Multiple Instance learning (MIL) models have been extensively used in\npathology to predict biomarkers and risk-stratify patients from gigapixel-sized\nimages. Machine learning problems in medical imaging often deal with rare\ndiseases, making it important for these models to work in a label-imbalanced\nsetting. Furthermore, these imbalances can occur in out-of-distribution (OOD)\ndatasets when the models are deployed in the real-world. We leverage the idea\nthat decoupling feature and classifier learning can lead to improved decision\nboundaries for label imbalanced datasets. To this end, we investigate the\nintegration of supervised contrastive learning with multiple instance learning\n(SC-MIL). Specifically, we propose a joint-training MIL framework in the\npresence of label imbalance that progressively transitions from learning\nbag-level representations to optimal classifier learning. We perform\nexperiments with different imbalance settings for two well-studied problems in\ncancer pathology: subtyping of non-small cell lung cancer and subtyping of\nrenal cell carcinoma. SC-MIL provides large and consistent improvements over\nother techniques on both in-distribution (ID) and OOD held-out sets across\nmultiple imbalanced settings.\n","authors":["Dinkar Juyal","Siddhant Shingi","Syed Ashar Javed","Harshith Padigela","Chintan Shah","Anand Sampat","Archit Khosla","John Abel","Amaro Taylor-Weiner"],"pdf_url":"https://arxiv.org/pdf/2303.13405v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.13404v1","updated":"2023-03-23T16:27:30Z","published":"2023-03-23T16:27:30Z","title":"MSFA-Frequency-Aware Transformer for Hyperspectral Images Demosaicing","summary":"  Hyperspectral imaging systems that use multispectral filter arrays (MSFA)\ncapture only one spectral component in each pixel. Hyperspectral demosaicing is\nused to recover the non-measured components. While deep learning methods have\nshown promise in this area, they still suffer from several challenges,\nincluding limited modeling of non-local dependencies, lack of consideration of\nthe periodic MSFA pattern that could be linked to periodic artifacts, and\ndifficulty in recovering high-frequency details. To address these challenges,\nthis paper proposes a novel de-mosaicing framework, the MSFA-frequency-aware\nTransformer network (FDM-Net). FDM-Net integrates a novel MSFA-frequency-aware\nmulti-head self-attention mechanism (MaFormer) and a filter-based Fourier\nzero-padding method to reconstruct high pass components with greater difficulty\nand low pass components with relative ease, separately. The advantage of\nMaformer is that it can leverage the MSFA information and non-local\ndependencies present in the data. Additionally, we introduce a joint spatial\nand frequency loss to transfer MSFA information and enhance training on\nfrequency components that are hard to recover. Our experimental results\ndemonstrate that FDM-Net outperforms state-of-the-art methods with 6dB PSNR,\nand reconstructs high-fidelity details successfully.\n","authors":["Haijin Zeng","Kai Feng","Shaoguang Huang","Jiezhang Cao","Yongyong Chen","Hongyan Zhang","Hiep Luong","Wilfried Philips"],"pdf_url":"https://arxiv.org/pdf/2303.13404v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2212.10156v2","updated":"2023-03-23T16:26:08Z","published":"2022-12-20T10:47:53Z","title":"Planning-oriented Autonomous Driving","summary":"  Modern autonomous driving system is characterized as modular tasks in\nsequential order, i.e., perception, prediction, and planning. In order to\nperform a wide diversity of tasks and achieve advanced-level intelligence,\ncontemporary approaches either deploy standalone models for individual tasks,\nor design a multi-task paradigm with separate heads. However, they might suffer\nfrom accumulative errors or deficient task coordination. Instead, we argue that\na favorable framework should be devised and optimized in pursuit of the\nultimate goal, i.e., planning of the self-driving car. Oriented at this, we\nrevisit the key components within perception and prediction, and prioritize the\ntasks such that all these tasks contribute to planning. We introduce Unified\nAutonomous Driving (UniAD), a comprehensive framework up-to-date that\nincorporates full-stack driving tasks in one network. It is exquisitely devised\nto leverage advantages of each module, and provide complementary feature\nabstractions for agent interaction from a global perspective. Tasks are\ncommunicated with unified query interfaces to facilitate each other toward\nplanning. We instantiate UniAD on the challenging nuScenes benchmark. With\nextensive ablations, the effectiveness of using such a philosophy is proven by\nsubstantially outperforming previous state-of-the-arts in all aspects. Code and\nmodels are public.\n","authors":["Yihan Hu","Jiazhi Yang","Li Chen","Keyu Li","Chonghao Sima","Xizhou Zhu","Siqi Chai","Senyao Du","Tianwei Lin","Wenhai Wang","Lewei Lu","Xiaosong Jia","Qiang Liu","Jifeng Dai","Yu Qiao","Hongyang Li"],"pdf_url":"https://arxiv.org/pdf/2212.10156v2.pdf","comment":"CVPR 2023 award candidate. Project page:\n  https://opendrivelab.github.io/UniAD/"},{"id":"http://arxiv.org/abs/2303.13401v1","updated":"2023-03-23T16:22:59Z","published":"2023-03-23T16:22:59Z","title":"Optimization and Optimizers for Adversarial Robustness","summary":"  Empirical robustness evaluation (RE) of deep learning models against\nadversarial perturbations entails solving nontrivial constrained optimization\nproblems. Existing numerical algorithms that are commonly used to solve them in\npractice predominantly rely on projected gradient, and mostly handle\nperturbations modeled by the $\\ell_1$, $\\ell_2$ and $\\ell_\\infty$ distances. In\nthis paper, we introduce a novel algorithmic framework that blends a\ngeneral-purpose constrained-optimization solver PyGRANSO with Constraint\nFolding (PWCF), which can add more reliability and generality to the\nstate-of-the-art RE packages, e.g., AutoAttack. Regarding reliability, PWCF\nprovides solutions with stationarity measures and feasibility tests to assess\nthe solution quality. For generality, PWCF can handle perturbation models that\nare typically inaccessible to the existing projected gradient methods; the main\nrequirement is the distance metric to be almost everywhere differentiable.\nTaking advantage of PWCF and other existing numerical algorithms, we further\nexplore the distinct patterns in the solutions found for solving these\noptimization problems using various combinations of losses, perturbation\nmodels, and optimization algorithms. We then discuss the implications of these\npatterns on the current robustness evaluation and adversarial training.\n","authors":["Hengyue Liang","Buyun Liang","Le Peng","Ying Cui","Tim Mitchell","Ju Sun"],"pdf_url":"https://arxiv.org/pdf/2303.13401v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.13399v1","updated":"2023-03-23T16:19:43Z","published":"2023-03-23T16:19:43Z","title":"Multi-granularity Interaction Simulation for Unsupervised Interactive\n  Segmentation","summary":"  Interactive segmentation enables users to segment as needed by providing cues\nof objects, which introduces human-computer interaction for many fields, such\nas image editing and medical image analysis. Typically, massive and expansive\npixel-level annotations are spent to train deep models by object-oriented\ninteractions with manually labeled object masks. In this work, we reveal that\ninformative interactions can be made by simulation with semantic-consistent yet\ndiverse region exploration in an unsupervised paradigm. Concretely, we\nintroduce a Multi-granularity Interaction Simulation (MIS) approach to open up\na promising direction for unsupervised interactive segmentation. Drawing on the\nhigh-quality dense features produced by recent self-supervised models, we\npropose to gradually merge patches or regions with similar features to form\nmore extensive regions and thus, every merged region serves as a\nsemantic-meaningful multi-granularity proposal. By randomly sampling these\nproposals and simulating possible interactions based on them, we provide\nmeaningful interaction at multiple granularities to teach the model to\nunderstand interactions. Our MIS significantly outperforms non-deep learning\nunsupervised methods and is even comparable with some previous deep-supervised\nmethods without any annotation.\n","authors":["Kehan Li","Yian Zhao","Zhennan Wang","Zesen Cheng","Peng Jin","Xiangyang Ji","Li Yuan","Chang Liu","Jie Chen"],"pdf_url":"https://arxiv.org/pdf/2303.13399v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.13397v1","updated":"2023-03-23T16:15:18Z","published":"2023-03-23T16:15:18Z","title":"DDT: A Diffusion-Driven Transformer-based Framework for Human Mesh\n  Recovery from a Video","summary":"  Human mesh recovery (HMR) provides rich human body information for various\nreal-world applications such as gaming, human-computer interaction, and virtual\nreality. Compared to single image-based methods, video-based methods can\nutilize temporal information to further improve performance by incorporating\nhuman body motion priors. However, many-to-many approaches such as VIBE suffer\nfrom motion smoothness and temporal inconsistency. While many-to-one approaches\nsuch as TCMR and MPS-Net rely on the future frames, which is non-causal and\ntime inefficient during inference. To address these challenges, a novel\nDiffusion-Driven Transformer-based framework (DDT) for video-based HMR is\npresented. DDT is designed to decode specific motion patterns from the input\nsequence, enhancing motion smoothness and temporal consistency. As a\nmany-to-many approach, the decoder of our DDT outputs the human mesh of all the\nframes, making DDT more viable for real-world applications where time\nefficiency is crucial and a causal model is desired. Extensive experiments are\nconducted on the widely used datasets (Human3.6M, MPI-INF-3DHP, and 3DPW),\nwhich demonstrated the effectiveness and efficiency of our DDT.\n","authors":["Ce Zheng","Guo-Jun Qi","Chen Chen"],"pdf_url":"https://arxiv.org/pdf/2303.13397v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.13396v1","updated":"2023-03-23T16:15:07Z","published":"2023-03-23T16:15:07Z","title":"Zero-guidance Segmentation Using Zero Segment Labels","summary":"  CLIP has enabled new and exciting joint vision-language applications, one of\nwhich is open-vocabulary segmentation, which can locate any segment given an\narbitrary text query. In our research, we ask whether it is possible to\ndiscover semantic segments without any user guidance in the form of text\nqueries or predefined classes, and label them using natural language\nautomatically? We propose a novel problem zero-guidance segmentation and the\nfirst baseline that leverages two pre-trained generalist models, DINO and CLIP,\nto solve this problem without any fine-tuning or segmentation dataset. The\ngeneral idea is to first segment an image into small over-segments, encode them\ninto CLIP's visual-language space, translate them into text labels, and merge\nsemantically similar segments together. The key challenge, however, is how to\nencode a visual segment into a segment-specific embedding that balances global\nand local context information, both useful for recognition. Our main\ncontribution is a novel attention-masking technique that balances the two\ncontexts by analyzing the attention layers inside CLIP. We also introduce\nseveral metrics for the evaluation of this new task. With CLIP's innate\nknowledge, our method can precisely locate the Mona Lisa painting among a\nmuseum crowd. Project page: https://zero-guide-seg.github.io/.\n","authors":["Pitchaporn Rewatbowornwong","Nattanat Chatthee","Ekapol Chuangsuwanich","Supasorn Suwajanakorn"],"pdf_url":"https://arxiv.org/pdf/2303.13396v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.13391v1","updated":"2023-03-23T16:07:31Z","published":"2023-03-23T16:07:31Z","title":"Xplainer: From X-Ray Observations to Explainable Zero-Shot Diagnosis","summary":"  Automated diagnosis prediction from medical images is a valuable resource to\nsupport clinical decision-making. However, such systems usually need to be\ntrained on large amounts of annotated data, which often is scarce in the\nmedical domain. Zero-shot methods address this challenge by allowing a flexible\nadaption to new settings with different clinical findings without relying on\nlabeled data. Further, to integrate automated diagnosis in the clinical\nworkflow, methods should be transparent and explainable, increasing medical\nprofessionals' trust and facilitating correctness verification. In this work,\nwe introduce Xplainer, a novel framework for explainable zero-shot diagnosis in\nthe clinical setting. Xplainer adapts the classification-by-description\napproach of contrastive vision-language models to the multi-label medical\ndiagnosis task. Specifically, instead of directly predicting a diagnosis, we\nprompt the model to classify the existence of descriptive observations, which a\nradiologist would look for on an X-Ray scan, and use the descriptor\nprobabilities to estimate the likelihood of a diagnosis. Our model is\nexplainable by design, as the final diagnosis prediction is directly based on\nthe prediction of the underlying descriptors. We evaluate Xplainer on two chest\nX-ray datasets, CheXpert and ChestX-ray14, and demonstrate its effectiveness in\nimproving the performance and explainability of zero-shot diagnosis. Our\nresults suggest that Xplainer provides a more detailed understanding of the\ndecision-making process and can be a valuable tool for clinical diagnosis.\n","authors":["Chantal Pellegrini","Matthias Keicher","Ege Özsoy","Petra Jiraskova","Rickmer Braren","Nassir Navab"],"pdf_url":"https://arxiv.org/pdf/2303.13391v1.pdf","comment":"9 pages, 2 figures, 6 tables"},{"id":"http://arxiv.org/abs/2205.15448v3","updated":"2023-03-23T15:48:05Z","published":"2022-05-30T22:09:57Z","title":"FeatER: An Efficient Network for Human Reconstruction via Feature\n  Map-Based TransformER","summary":"  Recently, vision transformers have shown great success in a set of human\nreconstruction tasks such as 2D human pose estimation (2D HPE), 3D human pose\nestimation (3D HPE), and human mesh reconstruction (HMR) tasks. In these tasks,\nfeature map representations of the human structural information are often\nextracted first from the image by a CNN (such as HRNet), and then further\nprocessed by transformer to predict the heatmaps (encodes each joint's location\ninto a feature map with a Gaussian distribution) for HPE or HMR. However,\nexisting transformer architectures are not able to process these feature map\ninputs directly, forcing an unnatural flattening of the location-sensitive\nhuman structural information. Furthermore, much of the performance benefit in\nrecent HPE and HMR methods has come at the cost of ever-increasing computation\nand memory needs. Therefore, to simultaneously address these problems, we\npropose FeatER, a novel transformer design that preserves the inherent\nstructure of feature map representations when modeling attention while reducing\nmemory and computational costs. Taking advantage of FeatER, we build an\nefficient network for a set of human reconstruction tasks including 2D HPE, 3D\nHPE, and HMR. A feature map reconstruction module is applied to improve the\nperformance of the estimated human pose and mesh. Extensive experiments\ndemonstrate the effectiveness of FeatER on various human pose and mesh\ndatasets. For instance, FeatER outperforms the SOTA method MeshGraphormer by\nrequiring 5% of Params and 16% of MACs on Human3.6M and 3DPW datasets. The\nproject webpage is https://zczcwh.github.io/feater_page/.\n","authors":["Ce Zheng","Matias Mendieta","Taojiannan Yang","Guo-Jun Qi","Chen Chen"],"pdf_url":"https://arxiv.org/pdf/2205.15448v3.pdf","comment":"CVPR 2023"},{"id":"http://arxiv.org/abs/2303.13371v1","updated":"2023-03-23T15:42:05Z","published":"2023-03-23T15:42:05Z","title":"Plug-and-Play Regulators for Image-Text Matching","summary":"  Exploiting fine-grained correspondence and visual-semantic alignments has\nshown great potential in image-text matching. Generally, recent approaches\nfirst employ a cross-modal attention unit to capture latent region-word\ninteractions, and then integrate all the alignments to obtain the final\nsimilarity. However, most of them adopt one-time forward association or\naggregation strategies with complex architectures or additional information,\nwhile ignoring the regulation ability of network feedback. In this paper, we\ndevelop two simple but quite effective regulators which efficiently encode the\nmessage output to automatically contextualize and aggregate cross-modal\nrepresentations. Specifically, we propose (i) a Recurrent Correspondence\nRegulator (RCR) which facilitates the cross-modal attention unit progressively\nwith adaptive attention factors to capture more flexible correspondence, and\n(ii) a Recurrent Aggregation Regulator (RAR) which adjusts the aggregation\nweights repeatedly to increasingly emphasize important alignments and dilute\nunimportant ones. Besides, it is interesting that RCR and RAR are\nplug-and-play: both of them can be incorporated into many frameworks based on\ncross-modal interaction to obtain significant benefits, and their cooperation\nachieves further improvements. Extensive experiments on MSCOCO and Flickr30K\ndatasets validate that they can bring an impressive and consistent R@1 gain on\nmultiple models, confirming the general effectiveness and generalization\nability of the proposed methods. Code and pre-trained models are available at:\nhttps://github.com/Paranioar/RCAR.\n","authors":["Haiwen Diao","Ying Zhang","Wei Liu","Xiang Ruan","Huchuan Lu"],"pdf_url":"https://arxiv.org/pdf/2303.13371v1.pdf","comment":"13 pages, 9 figures, Accepted by TIP2023"},{"id":"http://arxiv.org/abs/2303.13357v1","updated":"2023-03-23T15:36:12Z","published":"2023-03-23T15:36:12Z","title":"POTTER: Pooling Attention Transformer for Efficient Human Mesh Recovery","summary":"  Transformer architectures have achieved SOTA performance on the human mesh\nrecovery (HMR) from monocular images. However, the performance gain has come at\nthe cost of substantial memory and computational overhead. A lightweight and\nefficient model to reconstruct accurate human mesh is needed for real-world\napplications. In this paper, we propose a pure transformer architecture named\nPOoling aTtention TransformER (POTTER) for the HMR task from single images.\nObserving that the conventional attention module is memory and computationally\nexpensive, we propose an efficient pooling attention module, which\nsignificantly reduces the memory and computational cost without sacrificing\nperformance. Furthermore, we design a new transformer architecture by\nintegrating a High-Resolution (HR) stream for the HMR task. The high-resolution\nlocal and global features from the HR stream can be utilized for recovering\nmore accurate human mesh. Our POTTER outperforms the SOTA method METRO by only\nrequiring 7% of total parameters and 14% of the Multiply-Accumulate Operations\non the Human3.6M (PA-MPJPE metric) and 3DPW (all three metrics) datasets. The\nproject webpage is https://zczcwh.github.io/potter_page.\n","authors":["Ce Zheng","Xianpeng Liu","Guo-Jun Qi","Chen Chen"],"pdf_url":"https://arxiv.org/pdf/2303.13357v1.pdf","comment":"CVPR 2023"},{"id":"http://arxiv.org/abs/2303.13340v1","updated":"2023-03-23T15:20:05Z","published":"2023-03-23T15:20:05Z","title":"Increasing Textual Context Size Boosts Medical Image-Text Matching","summary":"  This short technical report demonstrates a simple technique that yields state\nof the art results in medical image-text matching tasks. We analyze the use of\nOpenAI's CLIP, a general image-text matching model, and observe that CLIP's\nlimited textual input size has negative impact on downstream performance in the\nmedical domain where encoding longer textual contexts is often required. We\nthus train and release ClipMD, which is trained with a simple sliding window\ntechnique to encode textual captions. ClipMD was tested on two medical\nimage-text datasets and compared with other image-text matching models. The\nresults show that ClipMD outperforms other models on both datasets by a large\nmargin. We make our code and pretrained model publicly available.\n","authors":["Idan Glassberg","Tom Hope"],"pdf_url":"https://arxiv.org/pdf/2303.13340v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.13332v1","updated":"2023-03-23T15:13:04Z","published":"2023-03-23T15:13:04Z","title":"Clinically Relevant Latent Space Embedding of Cancer Histopathology\n  Slides through Variational Autoencoder Based Image Compression","summary":"  In this paper, we introduce a Variational Autoencoder (VAE) based training\napproach that can compress and decompress cancer pathology slides at a\ncompression ratio of 1:512, which is better than the previously reported state\nof the art (SOTA) in the literature, while still maintaining accuracy in\nclinical validation tasks. The compression approach was tested on more common\ncomputer vision datasets such as CIFAR10, and we explore which image\ncharacteristics enable this compression ratio on cancer imaging data but not\ngeneric images. We generate and visualize embeddings from the compressed latent\nspace and demonstrate how they are useful for clinical interpretation of data,\nand how in the future such latent embeddings can be used to accelerate search\nof clinical imaging data.\n","authors":["Mohammad Sadegh Nasr","Amir Hajighasemi","Paul Koomey","Parisa Boodaghi Malidarreh","Michael Robben","Jillur Rahman Saurav","Helen H. Shang","Manfred Huber","Jacob M. Luber"],"pdf_url":"https://arxiv.org/pdf/2303.13332v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2212.08356v2","updated":"2023-03-23T15:07:46Z","published":"2022-12-16T09:02:01Z","title":"Cyclical Compound Domain Test-time Adaptation via Continual\n  Domain-Matching Algorithm","summary":"  Test-time adaptation (TTA), a key component of lifelong learning in edge\ndevices, refers to the ability of a pre-trained model to adapt itself to new\nenvironments during test time. Due to its practical ability, TTA has attracted\nsignificant attention and experienced a rapid performance boost these days. In\nthis paper, we present an under-explored yet more realistic TTA scenario and\nprovide a strong baseline favorable to this scenario, named cyclical compound\ndomain (CCD). The CCD represents the real-world scenario in which the target\ndomain contains multiple sub-target domains (i.e., compound domain due to\nweather or time change) and the sub-target domains are likely to rise\ncyclically. Unfortunately, existing works do not faithfully account for this\nplausible scenario, only focusing on adapting to the current sub-target domain\nwhile discarding the past knowledge acquired from repeated sub-target domains.\nTherefore, we first propose a lightweight domain-matching algorithm that allows\nthe TTA model to manage knowledge from the compound domain. This algorithm\nidentifies the type of domain among sub-target domains by continuously matching\nthe current image's distribution with reference domain points. Moreover, our\nnewly proposed regularization method compares the present distribution with\nsource one in order to regularize the adaptation pace according to each data in\nsub-target domains. Qualitatively, we demonstrate that our simple-yet-effective\napproach improves the adaptation performance on various benchmarks, including\nimage classification on ImageNet-C and semantic segmentation on GTA5, C-driving\ndatasets, and Cityscapes with corruptions.\n","authors":["Junha Song","Kwanyong Park","InKyu Shin","Sanghyun Woo","Chaoning Zhang","In So Kweon"],"pdf_url":"https://arxiv.org/pdf/2212.08356v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.13325v1","updated":"2023-03-23T15:04:23Z","published":"2023-03-23T15:04:23Z","title":"DARE-GRAM : Unsupervised Domain Adaptation Regression by Aligning\n  Inverse Gram Matrices","summary":"  Unsupervised Domain Adaptation Regression (DAR) aims to bridge the domain gap\nbetween a labeled source dataset and an unlabelled target dataset for\nregression problems. Recent works mostly focus on learning a deep feature\nencoder by minimizing the discrepancy between source and target features. In\nthis work, we present a different perspective for the DAR problem by analyzing\nthe closed-form ordinary least square~(OLS) solution to the linear regressor in\nthe deep domain adaptation context. Rather than aligning the original feature\nembedding space, we propose to align the inverse Gram matrix of the features,\nwhich is motivated by its presence in the OLS solution and the Gram matrix's\nability to capture the feature correlations. Specifically, we propose a simple\nyet effective DAR method which leverages the pseudo-inverse low-rank property\nto align the scale and angle in a selected subspace generated by the\npseudo-inverse Gram matrix of the two domains. We evaluate our method on three\ndomain adaptation regression benchmarks. Experimental results demonstrate that\nour method achieves state-of-the-art performance. Our code is available at\nhttps://github.com/ismailnejjar/DARE-GRAM.\n","authors":["Ismail Nejjar","Qin Wang","Olga Fink"],"pdf_url":"https://arxiv.org/pdf/2303.13325v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.13297v1","updated":"2023-03-23T14:27:49Z","published":"2023-03-23T14:27:49Z","title":"Improving Generalization with Domain Convex Game","summary":"  Domain generalization (DG) tends to alleviate the poor generalization\ncapability of deep neural networks by learning model with multiple source\ndomains. A classical solution to DG is domain augmentation, the common belief\nof which is that diversifying source domains will be conducive to the\nout-of-distribution generalization. However, these claims are understood\nintuitively, rather than mathematically. Our explorations empirically reveal\nthat the correlation between model generalization and the diversity of domains\nmay be not strictly positive, which limits the effectiveness of domain\naugmentation. This work therefore aim to guarantee and further enhance the\nvalidity of this strand. To this end, we propose a new perspective on DG that\nrecasts it as a convex game between domains. We first encourage each\ndiversified domain to enhance model generalization by elaborately designing a\nregularization term based on supermodularity. Meanwhile, a sample filter is\nconstructed to eliminate low-quality samples, thereby avoiding the impact of\npotentially harmful information. Our framework presents a new avenue for the\nformal analysis of DG, heuristic analysis and extensive experiments demonstrate\nthe rationality and effectiveness.\n","authors":["Fangrui Lv","Jian Liang","Shuang Li","Jinming Zhang","Di Liu"],"pdf_url":"https://arxiv.org/pdf/2303.13297v1.pdf","comment":"accepted by CVPR 2023"},{"id":"http://arxiv.org/abs/2212.07422v2","updated":"2023-03-23T14:27:38Z","published":"2022-12-14T18:59:19Z","title":"ECON: Explicit Clothed humans Optimized via Normal integration","summary":"  The combination of deep learning, artist-curated scans, and Implicit\nFunctions (IF), is enabling the creation of detailed, clothed, 3D humans from\nimages. However, existing methods are far from perfect. IF-based methods\nrecover free-form geometry, but produce disembodied limbs or degenerate shapes\nfor novel poses or clothes. To increase robustness for these cases, existing\nwork uses an explicit parametric body model to constrain surface\nreconstruction, but this limits the recovery of free-form surfaces such as\nloose clothing that deviates from the body. What we want is a method that\ncombines the best properties of implicit representation and explicit body\nregularization. To this end, we make two key observations: (1) current networks\nare better at inferring detailed 2D maps than full-3D surfaces, and (2) a\nparametric model can be seen as a \"canvas\" for stitching together detailed\nsurface patches. Based on these, our method, ECON, has three main steps: (1) It\ninfers detailed 2D normal maps for the front and back side of a clothed person.\n(2) From these, it recovers 2.5D front and back surfaces, called d-BiNI, that\nare equally detailed, yet incomplete, and registers these w.r.t. each other\nwith the help of a SMPL-X body mesh recovered from the image. (3) It \"inpaints\"\nthe missing geometry between d-BiNI surfaces. If the face and hands are noisy,\nthey can optionally be replaced with the ones of SMPL-X. As a result, ECON\ninfers high-fidelity 3D humans even in loose clothes and challenging poses.\nThis goes beyond previous methods, according to the quantitative evaluation on\nthe CAPE and Renderpeople datasets. Perceptual studies also show that ECON's\nperceived realism is better by a large margin. Code and models are available\nfor research purposes at econ.is.tue.mpg.de\n","authors":["Yuliang Xiu","Jinlong Yang","Xu Cao","Dimitrios Tzionas","Michael J. Black"],"pdf_url":"https://arxiv.org/pdf/2212.07422v2.pdf","comment":"Homepage: https://xiuyuliang.cn/econ Code:\n  https://github.com/YuliangXiu/ECON"},{"id":"http://arxiv.org/abs/2303.13294v1","updated":"2023-03-23T14:26:21Z","published":"2023-03-23T14:26:21Z","title":"Considerations on the Evaluation of Biometric Quality Assessment\n  Algorithms","summary":"  Quality assessment algorithms can be used to estimate the utility of a\nbiometric sample for the purpose of biometric recognition. \"Error versus\nDiscard Characteristic\" (EDC) plots, and \"partial Area Under Curve\" (pAUC)\nvalues of curves therein, are generally used by researchers to evaluate the\npredictive performance of such quality assessment algorithms. An EDC curve\ndepends on an error type such as the \"False Non Match Rate\" (FNMR), a quality\nassessment algorithm, a biometric recognition system, a set of comparisons each\ncorresponding to a biometric sample pair, and a comparison score threshold\ncorresponding to a starting error. To compute an EDC curve, comparisons are\nprogressively discarded based on the associated samples' lowest quality scores,\nand the error is computed for the remaining comparisons. Additionally, a\ndiscard fraction limit or range must be selected to compute pAUC values, which\ncan then be used to quantitatively rank quality assessment algorithms.\n  This paper discusses and analyses various details for this kind of quality\nassessment algorithm evaluation, including general EDC properties,\ninterpretability improvements for pAUC values based on a hard lower error limit\nand a soft upper error limit, the use of relative instead of discrete rankings,\nstepwise vs. linear curve interpolation, and normalisation of quality scores to\na [0, 100] integer range. We also analyse the stability of quantitative quality\nassessment algorithm rankings based on pAUC values across varying pAUC discard\nfraction limits and starting errors, concluding that higher pAUC discard\nfraction limits should be preferred. The analyses are conducted both with\nsynthetic data and with real data for a face image quality assessment scenario,\nwith a focus on general modality-independent conclusions for EDC evaluations.\n","authors":["Torsten Schlett","Christian Rathgeb","Juan Tapia","Christoph Busch"],"pdf_url":"https://arxiv.org/pdf/2303.13294v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.13293v1","updated":"2023-03-23T14:26:16Z","published":"2023-03-23T14:26:16Z","title":"LABRAD-OR: Lightweight Memory Scene Graphs for Accurate Bimodal\n  Reasoning in Dynamic Operating Rooms","summary":"  Modern surgeries are performed in complex and dynamic settings, including\never-changing interactions between medical staff, patients, and equipment. The\nholistic modeling of the operating room (OR) is, therefore, a challenging but\nessential task, with the potential to optimize the performance of surgical\nteams and aid in developing new surgical technologies to improve patient\noutcomes. The holistic representation of surgical scenes as semantic scene\ngraphs (SGG), where entities are represented as nodes and relations between\nthem as edges, is a promising direction for fine-grained semantic OR\nunderstanding. We propose, for the first time, the use of temporal information\nfor more accurate and consistent holistic OR modeling. Specifically, we\nintroduce memory scene graphs, where the scene graphs of previous time steps\nact as the temporal representation guiding the current prediction. We design an\nend-to-end architecture that intelligently fuses the temporal information of\nour lightweight memory scene graphs with the visual information from point\nclouds and images. We evaluate our method on the 4D-OR dataset and demonstrate\nthat integrating temporality leads to more accurate and consistent results\nachieving an +5% increase and a new SOTA of 0.88 in macro F1. This work opens\nthe path for representing the entire surgery history with memory scene graphs\nand improves the holistic understanding in the OR. Introducing scene graphs as\nmemory representations can offer a valuable tool for many temporal\nunderstanding tasks.\n","authors":["Ege Özsoy","Tobias Czempiel","Felix Holm","Chantal Pellegrini","Nassir Navab"],"pdf_url":"https://arxiv.org/pdf/2303.13293v1.pdf","comment":"11 pages, 3 figures"},{"id":"http://arxiv.org/abs/2211.14086v2","updated":"2023-03-23T14:21:24Z","published":"2022-11-25T13:14:56Z","title":"ShadowNeuS: Neural SDF Reconstruction by Shadow Ray Supervision","summary":"  By supervising camera rays between a scene and multi-view image planes, NeRF\nreconstructs a neural scene representation for the task of novel view\nsynthesis. On the other hand, shadow rays between the light source and the\nscene have yet to be considered. Therefore, we propose a novel shadow ray\nsupervision scheme that optimizes both the samples along the ray and the ray\nlocation. By supervising shadow rays, we successfully reconstruct a neural SDF\nof the scene from single-view images under multiple lighting conditions. Given\nsingle-view binary shadows, we train a neural network to reconstruct a complete\nscene not limited by the camera's line of sight. By further modeling the\ncorrelation between the image colors and the shadow rays, our technique can\nalso be effectively extended to RGB inputs. We compare our method with previous\nworks on challenging tasks of shape reconstruction from single-view binary\nshadow or RGB images and observe significant improvements. The code and data\nare available at https://github.com/gerwang/ShadowNeuS.\n","authors":["Jingwang Ling","Zhibo Wang","Feng Xu"],"pdf_url":"https://arxiv.org/pdf/2211.14086v2.pdf","comment":"CVPR 2023. Project page: https://gerwang.github.io/shadowneus/"},{"id":"http://arxiv.org/abs/2103.10427v4","updated":"2023-03-23T14:21:02Z","published":"2021-03-18T17:58:02Z","title":"The Low-Rank Simplicity Bias in Deep Networks","summary":"  Modern deep neural networks are highly over-parameterized compared to the\ndata on which they are trained, yet they often generalize remarkably well. A\nflurry of recent work has asked: why do deep networks not overfit to their\ntraining data? In this work, we make a series of empirical observations that\ninvestigate and extend the hypothesis that deeper networks are inductively\nbiased to find solutions with lower effective rank embeddings. We conjecture\nthat this bias exists because the volume of functions that maps to low\neffective rank embedding increases with depth. We show empirically that our\nclaim holds true on finite width linear and non-linear models on practical\nlearning paradigms and show that on natural data, these are often the solutions\nthat generalize well. We then show that the simplicity bias exists at both\ninitialization and after training and is resilient to hyper-parameters and\nlearning methods. We further demonstrate how linear over-parameterization of\ndeep non-linear models can be used to induce low-rank bias, improving\ngeneralization performance on CIFAR and ImageNet without changing the modeling\ncapacity.\n","authors":["Minyoung Huh","Hossein Mobahi","Richard Zhang","Brian Cheung","Pulkit Agrawal","Phillip Isola"],"pdf_url":"https://arxiv.org/pdf/2103.10427v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.13290v1","updated":"2023-03-23T14:18:06Z","published":"2023-03-23T14:18:06Z","title":"Unsupervised Deep Probabilistic Approach for Partial Point Cloud\n  Registration","summary":"  Deep point cloud registration methods face challenges to partial overlaps and\nrely on labeled data. To address these issues, we propose UDPReg, an\nunsupervised deep probabilistic registration framework for point clouds with\npartial overlaps. Specifically, we first adopt a network to learn posterior\nprobability distributions of Gaussian mixture models (GMMs) from point clouds.\nTo handle partial point cloud registration, we apply the Sinkhorn algorithm to\npredict the distribution-level correspondences under the constraint of the\nmixing weights of GMMs. To enable unsupervised learning, we design three\ndistribution consistency-based losses: self-consistency, cross-consistency, and\nlocal contrastive. The self-consistency loss is formulated by encouraging GMMs\nin Euclidean and feature spaces to share identical posterior distributions. The\ncross-consistency loss derives from the fact that the points of two partially\noverlapping point clouds belonging to the same clusters share the cluster\ncentroids. The cross-consistency loss allows the network to flexibly learn a\ntransformation-invariant posterior distribution of two aligned point clouds.\nThe local contrastive loss facilitates the network to extract discriminative\nlocal features. Our UDPReg achieves competitive performance on the\n3DMatch/3DLoMatch and ModelNet/ModelLoNet benchmarks.\n","authors":["Guofeng Mei","Hao Tang","Xiaoshui Huang","Weijie Wang","Juan Liu","Jian Zhang","Luc Van Gool","Qiang Wu"],"pdf_url":"https://arxiv.org/pdf/2303.13290v1.pdf","comment":"CVPR 2023"},{"id":"http://arxiv.org/abs/2303.12710v2","updated":"2023-03-23T14:13:05Z","published":"2023-03-09T04:35:00Z","title":"A Unified Arbitrary Style Transfer Framework via Adaptive Contrastive\n  Learning","summary":"  We present Unified Contrastive Arbitrary Style Transfer (UCAST), a novel\nstyle representation learning and transfer framework, which can fit in most\nexisting arbitrary image style transfer models, e.g., CNN-based, ViT-based, and\nflow-based methods. As the key component in image style transfer tasks, a\nsuitable style representation is essential to achieve satisfactory results.\nExisting approaches based on deep neural network typically use second-order\nstatistics to generate the output. However, these hand-crafted features\ncomputed from a single image cannot leverage style information sufficiently,\nwhich leads to artifacts such as local distortions and style inconsistency. To\naddress these issues, we propose to learn style representation directly from a\nlarge amount of images based on contrastive learning, by taking the\nrelationships between specific styles and the holistic style distribution into\naccount. Specifically, we present an adaptive contrastive learning scheme for\nstyle transfer by introducing an input-dependent temperature. Our framework\nconsists of three key components, i.e., a parallel contrastive learning scheme\nfor style representation and style transfer, a domain enhancement module for\neffective learning of style distribution, and a generative network for style\ntransfer. We carry out qualitative and quantitative evaluations to show that\nour approach produces superior results than those obtained via state-of-the-art\nmethods.\n","authors":["Yuxin Zhang","Fan Tang","Weiming Dong","Haibin Huang","Chongyang Ma","Tong-Yee Lee","Changsheng Xu"],"pdf_url":"https://arxiv.org/pdf/2303.12710v2.pdf","comment":"arXiv admin note: substantial text overlap with arXiv:2205.09542"},{"id":"http://arxiv.org/abs/2303.13283v1","updated":"2023-03-23T14:04:23Z","published":"2023-03-23T14:04:23Z","title":"Visual-Language Prompt Tuning with Knowledge-guided Context Optimization","summary":"  Prompt tuning is an effective way to adapt the pre-trained visual-language\nmodel (VLM) to the downstream task using task-related textual tokens.\nRepresentative CoOp-based work combines the learnable textual tokens with the\nclass tokens to obtain specific textual knowledge. However, the specific\ntextual knowledge is the worse generalization to the unseen classes because it\nforgets the essential general textual knowledge having a strong generalization\nability. To tackle this issue, we introduce a novel Knowledge-guided Context\nOptimization (KgCoOp) to enhance the generalization ability of the learnable\nprompt for unseen classes. The key insight of KgCoOp is that forgetting about\nessential knowledge can be alleviated by reducing the discrepancy between the\nlearnable prompt and the hand-crafted prompt. Especially, KgCoOp minimizes the\ndiscrepancy between the textual embeddings generated by learned prompts and the\nhand-crafted prompts. Finally, adding the KgCoOp upon the contrastive loss can\nmake a discriminative prompt for both seen and unseen tasks. Extensive\nevaluation of several benchmarks demonstrates that the proposed\nKnowledge-guided Context Optimization is an efficient method for prompt tuning,\n\\emph{i.e.,} achieves better performance with less training time.\n","authors":["Hantao Yao","Rui Zhang","Changsheng Xu"],"pdf_url":"https://arxiv.org/pdf/2303.13283v1.pdf","comment":"accepted by CVPR23"},{"id":"http://arxiv.org/abs/2211.11635v4","updated":"2023-03-23T14:02:21Z","published":"2022-11-21T16:49:47Z","title":"Understanding and Improving Visual Prompting: A Label-Mapping\n  Perspective","summary":"  We revisit and advance visual prompting (VP), an input prompting technique\nfor vision tasks. VP can reprogram a fixed, pre-trained source model to\naccomplish downstream tasks in the target domain by simply incorporating\nuniversal prompts (in terms of input perturbation patterns) into downstream\ndata points. Yet, it remains elusive why VP stays effective even given a\nruleless label mapping (LM) between the source classes and the target classes.\nInspired by the above, we ask: How is LM interrelated with VP? And how to\nexploit such a relationship to improve its accuracy on target tasks? We peer\ninto the influence of LM on VP and provide an affirmative answer that a better\n'quality' of LM (assessed by mapping precision and explanation) can\nconsistently improve the effectiveness of VP. This is in contrast to the prior\nart where the factor of LM was missing. To optimize LM, we propose a new VP\nframework, termed ILM-VP (iterative label mapping-based visual prompting),\nwhich automatically re-maps the source labels to the target labels and\nprogressively improves the target task accuracy of VP. Further, when using a\ncontrastive language-image pretrained (CLIP) model, we propose to integrate an\nLM process to assist the text prompt selection of CLIP and to improve the\ntarget task accuracy. Extensive experiments demonstrate that our proposal\nsignificantly outperforms state-of-the-art VP methods. As highlighted below, we\nshow that when reprogramming an ImageNet-pretrained ResNet-18 to 13 target\ntasks, our method outperforms baselines by a substantial margin, e.g., 7.9% and\n6.7% accuracy improvements in transfer learning to the target Flowers102 and\nCIFAR100 datasets. Besides, our proposal on CLIP-based VP provides 13.7% and\n7.1% accuracy improvements on Flowers102 and DTD respectively. Our code is\navailable at https://github.com/OPTML-Group/ILM-VP.\n","authors":["Aochuan Chen","Yuguang Yao","Pin-Yu Chen","Yihua Zhang","Sijia Liu"],"pdf_url":"https://arxiv.org/pdf/2211.11635v4.pdf","comment":"CVPR 2023"},{"id":"http://arxiv.org/abs/2303.13278v1","updated":"2023-03-23T13:59:57Z","published":"2023-03-23T13:59:57Z","title":"Improved Anisotropic Gaussian Filters","summary":"  Elongated anisotropic Gaussian filters are used for the orientation\nestimation of fibers. In cases where computed tomography images are noisy,\nroughly resolved, and of low contrast, they are the method of choice even if\nbeing efficient only in virtual 2D slices. However, minor inaccuracies in the\nanisotropic Gaussian filters can carry over to the orientation estimation.\nTherefore, we propose a modified algorithm for 2D anisotropic Gaussian filters\nand show that this improves their precision. Applied to synthetic images of\nfiber bundles, it is more accurate and robust to noise. Finally, we demonstrate\nthe effectiveness of our approach by applying it to real-world images of sheet\nmolding compounds.\n","authors":["Alex Keilmann","Michael Godehardt","Ali Moghiseh","Claudia Redenbach","Katja Schladitz"],"pdf_url":"https://arxiv.org/pdf/2303.13278v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.13277v1","updated":"2023-03-23T13:58:11Z","published":"2023-03-23T13:58:11Z","title":"SINE: Semantic-driven Image-based NeRF Editing with Prior-guided Editing\n  Field","summary":"  Despite the great success in 2D editing using user-friendly tools, such as\nPhotoshop, semantic strokes, or even text prompts, similar capabilities in 3D\nareas are still limited, either relying on 3D modeling skills or allowing\nediting within only a few categories.In this paper, we present a novel\nsemantic-driven NeRF editing approach, which enables users to edit a neural\nradiance field with a single image, and faithfully delivers edited novel views\nwith high fidelity and multi-view consistency.To achieve this goal, we propose\na prior-guided editing field to encode fine-grained geometric and texture\nediting in 3D space, and develop a series of techniques to aid the editing\nprocess, including cyclic constraints with a proxy mesh to facilitate geometric\nsupervision, a color compositing mechanism to stabilize semantic-driven texture\nediting, and a feature-cluster-based regularization to preserve the irrelevant\ncontent unchanged.Extensive experiments and editing examples on both real-world\nand synthetic data demonstrate that our method achieves photo-realistic 3D\nediting using only a single edited image, pushing the bound of semantic-driven\nediting in 3D real-world scenes. Our project webpage:\nhttps://zju3dv.github.io/sine/.\n","authors":["Chong Bao","Yinda Zhang","Bangbang Yang","Tianxing Fan","Zesong Yang","Hujun Bao","Guofeng Zhang","Zhaopeng Cui"],"pdf_url":"https://arxiv.org/pdf/2303.13277v1.pdf","comment":"Accepted to CVPR 2023. Project Page: https://zju3dv.github.io/sine/"},{"id":"http://arxiv.org/abs/2303.13273v1","updated":"2023-03-23T13:53:16Z","published":"2023-03-23T13:53:16Z","title":"TAPS3D: Text-Guided 3D Textured Shape Generation from Pseudo Supervision","summary":"  In this paper, we investigate an open research task of generating\ncontrollable 3D textured shapes from the given textual descriptions. Previous\nworks either require ground truth caption labeling or extensive optimization\ntime. To resolve these issues, we present a novel framework, TAPS3D, to train a\ntext-guided 3D shape generator with pseudo captions. Specifically, based on\nrendered 2D images, we retrieve relevant words from the CLIP vocabulary and\nconstruct pseudo captions using templates. Our constructed captions provide\nhigh-level semantic supervision for generated 3D shapes. Further, in order to\nproduce fine-grained textures and increase geometry diversity, we propose to\nadopt low-level image regularization to enable fake-rendered images to align\nwith the real ones. During the inference phase, our proposed model can generate\n3D textured shapes from the given text without any additional optimization. We\nconduct extensive experiments to analyze each of our proposed components and\nshow the efficacy of our framework in generating high-fidelity 3D textured and\ntext-relevant shapes.\n","authors":["Jiacheng Wei","Hao Wang","Jiashi Feng","Guosheng Lin","Kim-Hui Yap"],"pdf_url":"https://arxiv.org/pdf/2303.13273v1.pdf","comment":"Accepted to CVPR2023"},{"id":"http://arxiv.org/abs/2303.13269v1","updated":"2023-03-23T13:50:46Z","published":"2023-03-23T13:50:46Z","title":"Disguise without Disruption: Utility-Preserving Face De-Identification","summary":"  With the increasing ubiquity of cameras and smart sensors, humanity is\ngenerating data at an exponential rate. Access to this trove of information,\noften covering yet-underrepresented use-cases (e.g., AI in medical settings)\ncould fuel a new generation of deep-learning tools. However, eager data\nscientists should first provide satisfying guarantees w.r.t. the privacy of\nindividuals present in these untapped datasets. This is especially important\nfor images or videos depicting faces, as their biometric information is the\ntarget of most identification methods. While a variety of solutions have been\nproposed to de-identify such images, they often corrupt other non-identifying\nfacial attributes that would be relevant for downstream tasks. In this paper,\nwe propose Disguise, a novel algorithm to seamlessly de-identify facial images\nwhile ensuring the usability of the altered data. Unlike prior arts, we ground\nour solution in both differential privacy and ensemble-learning research\ndomains. Our method extracts and swaps depicted identities with fake ones,\nsynthesized via variational mechanisms to maximize obfuscation and\nnon-invertibility; while leveraging the supervision from a mixture-of-experts\nto disentangle and preserve other utility attributes. We extensively evaluate\nour method on multiple datasets, demonstrating higher de-identification rate\nand superior consistency than prior art w.r.t. various downstream tasks.\n","authors":["Zikui Cai","Zhongpai Gao","Benjamin Planche","Meng Zheng","Terrence Chen","M. Salman Asif","Ziyan Wu"],"pdf_url":"https://arxiv.org/pdf/2303.13269v1.pdf","comment":"paper + supplementary material"},{"id":"http://arxiv.org/abs/2303.13251v1","updated":"2023-03-23T13:33:58Z","published":"2023-03-23T13:33:58Z","title":"A Bag-of-Prototypes Representation for Dataset-Level Applications","summary":"  This work investigates dataset vectorization for two dataset-level tasks:\nassessing training set suitability and test set difficulty. The former measures\nhow suitable a training set is for a target domain, while the latter studies\nhow challenging a test set is for a learned model. Central to the two tasks is\nmeasuring the underlying relationship between datasets. This needs a desirable\ndataset vectorization scheme, which should preserve as much discriminative\ndataset information as possible so that the distance between the resulting\ndataset vectors can reflect dataset-to-dataset similarity. To this end, we\npropose a bag-of-prototypes (BoP) dataset representation that extends the\nimage-level bag consisting of patch descriptors to dataset-level bag consisting\nof semantic prototypes. Specifically, we develop a codebook consisting of K\nprototypes clustered from a reference dataset. Given a dataset to be encoded,\nwe quantize each of its image features to a certain prototype in the codebook\nand obtain a K-dimensional histogram. Without assuming access to dataset\nlabels, the BoP representation provides a rich characterization of the dataset\nsemantic distribution. Furthermore, BoP representations cooperate well with\nJensen-Shannon divergence for measuring dataset-to-dataset similarity. Although\nvery simple, BoP consistently shows its advantage over existing representations\non a series of benchmarks for two dataset-level tasks.\n","authors":["Weijie Tu","Weijian Deng","Tom Gedeon","Liang Zheng"],"pdf_url":"https://arxiv.org/pdf/2303.13251v1.pdf","comment":"CVPR 2023 camera-ready"},{"id":"http://arxiv.org/abs/2303.13245v1","updated":"2023-03-23T13:24:16Z","published":"2023-03-23T13:24:16Z","title":"CrOC: Cross-View Online Clustering for Dense Visual Representation\n  Learning","summary":"  Learning dense visual representations without labels is an arduous task and\nmore so from scene-centric data. We propose to tackle this challenging problem\nby proposing a Cross-view consistency objective with an Online Clustering\nmechanism (CrOC) to discover and segment the semantics of the views. In the\nabsence of hand-crafted priors, the resulting method is more generalizable and\ndoes not require a cumbersome pre-processing step. More importantly, the\nclustering algorithm conjointly operates on the features of both views, thereby\nelegantly bypassing the issue of content not represented in both views and the\nambiguous matching of objects from one crop to the other. We demonstrate\nexcellent performance on linear and unsupervised segmentation transfer tasks on\nvarious datasets and similarly for video object segmentation. Our code and\npre-trained models are publicly available at https://github.com/stegmuel/CrOC.\n","authors":["Thomas Stegmüller","Tim Lebailly","Behzad Bozorgtabar","Tinne Tuytelaars","Jean-Philippe Thiran"],"pdf_url":"https://arxiv.org/pdf/2303.13245v1.pdf","comment":"Accepted at CVPR 2023, * denotes equal contribution"},{"id":"http://arxiv.org/abs/2303.13241v1","updated":"2023-03-23T13:18:05Z","published":"2023-03-23T13:18:05Z","title":"6D Object Pose Estimation from Approximate 3D Models for Orbital\n  Robotics","summary":"  We present a novel technique to estimate the 6D pose of objects from single\nimages where the 3D geometry of the object is only given approximately and not\nas a precise 3D model. To achieve this, we employ a dense 2D-to-3D\ncorrespondence predictor that regresses 3D model coordinates for every pixel.\nIn addition to the 3D coordinates, our model also estimates the pixel-wise\ncoordinate error to discard correspondences that are likely wrong. This allows\nus to generate multiple 6D pose hypotheses of the object, which we then refine\niteratively using a highly efficient region-based approach. We also introduce a\nnovel pixel-wise posterior formulation by which we can estimate the probability\nfor each hypothesis and select the most likely one. As we show in experiments,\nour approach is capable of dealing with extreme visual conditions including\noverexposure, high contrast, or low signal-to-noise ratio. This makes it a\npowerful technique for the particularly challenging task of estimating the pose\nof tumbling satellites for in-orbit robotic applications. Our method achieves\nstate-of-the-art performance on the SPEED+ dataset and has won the SPEC2021\npost-mortem competition.\n","authors":["Maximilian Ulmer","Maximilian Durner","Martin Sundermeyer","Manuel Stoiber","Rudolph Triebel"],"pdf_url":"https://arxiv.org/pdf/2303.13241v1.pdf","comment":"preprint"},{"id":"http://arxiv.org/abs/2211.12499v2","updated":"2023-03-23T13:16:06Z","published":"2022-11-22T18:59:46Z","title":"Instant Volumetric Head Avatars","summary":"  We present Instant Volumetric Head Avatars (INSTA), a novel approach for\nreconstructing photo-realistic digital avatars instantaneously. INSTA models a\ndynamic neural radiance field based on neural graphics primitives embedded\naround a parametric face model. Our pipeline is trained on a single monocular\nRGB portrait video that observes the subject under different expressions and\nviews. While state-of-the-art methods take up to several days to train an\navatar, our method can reconstruct a digital avatar in less than 10 minutes on\nmodern GPU hardware, which is orders of magnitude faster than previous\nsolutions. In addition, it allows for the interactive rendering of novel poses\nand expressions. By leveraging the geometry prior of the underlying parametric\nface model, we demonstrate that INSTA extrapolates to unseen poses. In\nquantitative and qualitative studies on various subjects, INSTA outperforms\nstate-of-the-art methods regarding rendering quality and training time.\n","authors":["Wojciech Zielonka","Timo Bolkart","Justus Thies"],"pdf_url":"https://arxiv.org/pdf/2211.12499v2.pdf","comment":"Website: https://zielon.github.io/insta/ Video:\n  https://youtu.be/HOgaeWTih7Q Accepted to CVPR2023"},{"id":"http://arxiv.org/abs/2210.03591v3","updated":"2023-03-23T13:15:27Z","published":"2022-10-07T14:46:32Z","title":"Modeling Inter-Class and Intra-Class Constraints in Novel Class\n  Discovery","summary":"  Novel class discovery (NCD) aims at learning a model that transfers the\ncommon knowledge from a class-disjoint labelled dataset to another unlabelled\ndataset and discovers new classes (clusters) within it. Many methods, as well\nas elaborate training pipelines and appropriate objectives, have been proposed\nand considerably boosted performance on NCD tasks. Despite all this, we find\nthat the existing methods do not sufficiently take advantage of the essence of\nthe NCD setting. To this end, in this paper, we propose to model both\ninter-class and intra-class constraints in NCD based on the symmetric\nKullback-Leibler divergence (sKLD). Specifically, we propose an inter-class\nsKLD constraint to effectively exploit the disjoint relationship between\nlabelled and unlabelled classes, enforcing the separability for different\nclasses in the embedding space. In addition, we present an intra-class sKLD\nconstraint to explicitly constrain the intra-relationship between a sample and\nits augmentations and ensure the stability of the training process at the same\ntime. We conduct extensive experiments on the popular CIFAR10, CIFAR100 and\nImageNet benchmarks and successfully demonstrate that our method can establish\na new state of the art and can achieve significant performance improvements,\ne.g., 3.5%/3.7% clustering accuracy improvements on CIFAR100-50 dataset split\nunder the task-aware/-agnostic evaluation protocol, over previous\nstate-of-the-art methods. Code is available at\nhttps://github.com/FanZhichen/NCD-IIC.\n","authors":["Wenbin Li","Zhichen Fan","Jing Huo","Yang Gao"],"pdf_url":"https://arxiv.org/pdf/2210.03591v3.pdf","comment":"Accepted to CVPR 2023"},{"id":"http://arxiv.org/abs/2303.13233v1","updated":"2023-03-23T13:06:38Z","published":"2023-03-23T13:06:38Z","title":"Visually-Prompted Language Model for Fine-Grained Scene Graph Generation\n  in an Open World","summary":"  Scene Graph Generation (SGG) aims to extract <subject, predicate, object>\nrelationships in images for vision understanding. Although recent works have\nmade steady progress on SGG, they still suffer long-tail distribution issues\nthat tail-predicates are more costly to train and hard to distinguish due to a\nsmall amount of annotated data compared to frequent predicates. Existing\nre-balancing strategies try to haddle it via prior rules but are still confined\nto pre-defined conditions, which are not scalable for various models and\ndatasets. In this paper, we propose a Cross-modal prediCate boosting (CaCao)\nframework, where a visually-prompted language model is learned to generate\ndiverse fine-grained predicates in a low-resource way. The proposed CaCao can\nbe applied in a plug-and-play fashion and automatically strengthen existing SGG\nto tackle the long-tailed problem. Based on that, we further introduce a novel\nEntangled cross-modal prompt approach for open-world predicate scene graph\ngeneration (Epic), where models can generalize to unseen predicates in a\nzero-shot manner. Comprehensive experiments on three benchmark datasets show\nthat CaCao consistently boosts the performance of multiple scene graph\ngeneration models in a model-agnostic way. Moreover, our Epic achieves\ncompetitive performance on open-world predicate prediction.\n","authors":["Qifan Yu","Juncheng Li","Yu Wu","Siliang Tang","Wei Ji","Yueting Zhuang"],"pdf_url":"https://arxiv.org/pdf/2303.13233v1.pdf","comment":"21 pages, 16 figures"},{"id":"http://arxiv.org/abs/2303.13232v1","updated":"2023-03-23T13:05:57Z","published":"2023-03-23T13:05:57Z","title":"Transforming Radiance Field with Lipschitz Network for Photorealistic 3D\n  Scene Stylization","summary":"  Recent advances in 3D scene representation and novel view synthesis have\nwitnessed the rise of Neural Radiance Fields (NeRFs). Nevertheless, it is not\ntrivial to exploit NeRF for the photorealistic 3D scene stylization task, which\naims to generate visually consistent and photorealistic stylized scenes from\nnovel views. Simply coupling NeRF with photorealistic style transfer (PST) will\nresult in cross-view inconsistency and degradation of stylized view syntheses.\nThrough a thorough analysis, we demonstrate that this non-trivial task can be\nsimplified in a new light: When transforming the appearance representation of a\npre-trained NeRF with Lipschitz mapping, the consistency and photorealism\nacross source views will be seamlessly encoded into the syntheses. That\nmotivates us to build a concise and flexible learning framework namely LipRF,\nwhich upgrades arbitrary 2D PST methods with Lipschitz mapping tailored for the\n3D scene. Technically, LipRF first pre-trains a radiance field to reconstruct\nthe 3D scene, and then emulates the style on each view by 2D PST as the prior\nto learn a Lipschitz network to stylize the pre-trained appearance. In view of\nthat Lipschitz condition highly impacts the expressivity of the neural network,\nwe devise an adaptive regularization to balance the reconstruction and\nstylization. A gradual gradient aggregation strategy is further introduced to\noptimize LipRF in a cost-efficient manner. We conduct extensive experiments to\nshow the high quality and robust performance of LipRF on both photorealistic 3D\nstylization and object appearance editing.\n","authors":["Zicheng Zhang","Yinglu Liu","Congying Han","Yingwei Pan","Tiande Guo","Ting Yao"],"pdf_url":"https://arxiv.org/pdf/2303.13232v1.pdf","comment":"CVPR 2023, Highlight"},{"id":"http://arxiv.org/abs/2210.00924v2","updated":"2023-03-23T13:02:42Z","published":"2022-10-03T13:30:40Z","title":"Multi-view object pose estimation from correspondence distributions and\n  epipolar geometry","summary":"  In many automation tasks involving manipulation of rigid objects, the poses\nof the objects must be acquired. Vision-based pose estimation using a single\nRGB or RGB-D sensor is especially popular due to its broad applicability.\nHowever, single-view pose estimation is inherently limited by depth ambiguity\nand ambiguities imposed by various phenomena like occlusion, self-occlusion,\nreflections, etc. Aggregation of information from multiple views can\npotentially resolve these ambiguities, but the current state-of-the-art\nmulti-view pose estimation method only uses multiple views to aggregate\nsingle-view pose estimates, and thus rely on obtaining good single-view\nestimates. We present a multi-view pose estimation method which aggregates\nlearned 2D-3D distributions from multiple views for both the initial estimate\nand optional refinement. Our method performs probabilistic sampling of 3D-3D\ncorrespondences under epipolar constraints using learned 2D-3D correspondence\ndistributions which are implicitly trained to respect visual ambiguities such\nas symmetry. Evaluation on the T-LESS dataset shows that our method reduces\npose estimation errors by 80-91% compared to the best single-view method, and\nwe present state-of-the-art results on T-LESS with four views, even compared\nwith methods using five and eight views.\n","authors":["Rasmus Laurvig Haugaard","Thorbjørn Mosekjær Iversen"],"pdf_url":"https://arxiv.org/pdf/2210.00924v2.pdf","comment":"7 pages, 2 figures, 1 table, ICRA 2023"},{"id":"http://arxiv.org/abs/2301.12900v2","updated":"2023-03-23T12:55:02Z","published":"2023-01-30T14:02:33Z","title":"DepGraph: Towards Any Structural Pruning","summary":"  Structural pruning enables model acceleration by removing\nstructurally-grouped parameters from neural networks. However, the\nparameter-grouping patterns vary widely across different models, making\narchitecture-specific pruners, which rely on manually-designed grouping\nschemes, non-generalizable to new architectures. In this work, we study a\nhighly-challenging yet barely-explored task, any structural pruning, to tackle\ngeneral structural pruning of arbitrary architecture like CNNs, RNNs, GNNs and\nTransformers. The most prominent obstacle towards this goal lies in the\nstructural coupling, which not only forces different layers to be pruned\nsimultaneously, but also expects all removed parameters to be consistently\nunimportant, thereby avoiding structural issues and significant performance\ndegradation after pruning. To address this problem, we propose a general and\n{fully automatic} method, \\emph{Dependency Graph} (DepGraph), to explicitly\nmodel the dependency between layers and comprehensively group coupled\nparameters for pruning. In this work, we extensively evaluate our method on\nseveral architectures and tasks, including ResNe(X)t, DenseNet, MobileNet and\nVision transformer for images, GAT for graph, DGCNN for 3D point cloud,\nalongside LSTM for language, and demonstrate that, even with a simple\nnorm-based criterion, the proposed method consistently yields gratifying\nperformances.\n","authors":["Gongfan Fang","Xinyin Ma","Mingli Song","Michael Bi Mi","Xinchao Wang"],"pdf_url":"https://arxiv.org/pdf/2301.12900v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.07150v2","updated":"2023-03-23T12:49:39Z","published":"2023-03-13T14:23:39Z","title":"Multi PILOT: Learned Feasible Multiple Acquisition Trajectories for\n  Dynamic MRI","summary":"  Dynamic Magnetic Resonance Imaging (MRI) is known to be a powerful and\nreliable technique for the dynamic imaging of internal organs and tissues,\nmaking it a leading diagnostic tool. A major difficulty in using MRI in this\nsetting is the relatively long acquisition time (and, hence, increased cost)\nrequired for imaging in high spatio-temporal resolution, leading to the\nappearance of related motion artifacts and decrease in resolution. Compressed\nSensing (CS) techniques have become a common tool to reduce MRI acquisition\ntime by subsampling images in the k-space according to some acquisition\ntrajectory. Several studies have particularly focused on applying deep learning\ntechniques to learn these acquisition trajectories in order to attain better\nimage reconstruction, rather than using some predefined set of trajectories. To\nthe best of our knowledge, learning acquisition trajectories has been only\nexplored in the context of static MRI. In this study, we consider acquisition\ntrajectory learning in the dynamic imaging setting. We design an end-to-end\npipeline for the joint optimization of multiple per-frame acquisition\ntrajectories along with a reconstruction neural network, and demonstrate\nimproved image reconstruction quality in shorter acquisition times. The code\nfor reproducing all experiments is accessible at\nhttps://github.com/tamirshor7/MultiPILOT.\n","authors":["Tamir Shor","Tomer Weiss","Dor Noti","Alex Bronstein"],"pdf_url":"https://arxiv.org/pdf/2303.07150v2.pdf","comment":"Accepted For MIDL 2023"},{"id":"http://arxiv.org/abs/2303.13227v1","updated":"2023-03-23T12:48:47Z","published":"2023-03-23T12:48:47Z","title":"Confidence-Aware and Self-Supervised Image Anomaly Localisation","summary":"  Universal anomaly detection still remains a challenging problem in machine\nlearning and medical image analysis. It is possible to learn an expected\ndistribution from a single class of normative samples, e.g., through epistemic\nuncertainty estimates, auto-encoding models, or from synthetic anomalies in a\nself-supervised way. The performance of self-supervised anomaly detection\napproaches is still inferior compared to methods that use examples from known\nunknown classes to shape the decision boundary. However, outlier exposure\nmethods often do not identify unknown unknowns. Here we discuss an improved\nself-supervised single-class training strategy that supports the approximation\nof probabilistic inference with loosen feature locality constraints. We show\nthat up-scaling of gradients with histogram-equalised images is beneficial for\nrecently proposed self-supervision tasks. Our method is integrated into several\nout-of-distribution (OOD) detection models and we show evidence that our method\noutperforms the state-of-the-art on various benchmark datasets. Source code\nwill be publicly available by the time of the conference.\n","authors":["Johanna P. Müller","Matthew Baugh","Jeremy Tan","Mischa Dombrowski","Bernhard Kainz"],"pdf_url":"https://arxiv.org/pdf/2303.13227v1.pdf","comment":"Under Review"},{"id":"http://arxiv.org/abs/2303.13223v1","updated":"2023-03-23T12:39:20Z","published":"2023-03-23T12:39:20Z","title":"Exploring Structured Semantic Prior for Multi Label Recognition with\n  Incomplete Labels","summary":"  Multi-label recognition (MLR) with incomplete labels is very challenging.\nRecent works strive to explore the image-to-label correspondence in the\nvision-language model, \\ie, CLIP~\\cite{radford2021clip}, to compensate for\ninsufficient annotations. In spite of promising performance, they generally\noverlook the valuable prior about the label-to-label correspondence. In this\npaper, we advocate remedying the deficiency of label supervision for the MLR\nwith incomplete labels by deriving a structured semantic prior about the\nlabel-to-label correspondence via a semantic prior prompter. We then present a\nnovel Semantic Correspondence Prompt Network (SCPNet), which can thoroughly\nexplore the structured semantic prior. A Prior-Enhanced Self-Supervised\nLearning method is further introduced to enhance the use of the prior.\nComprehensive experiments and analyses on several widely used benchmark\ndatasets show that our method significantly outperforms existing methods on all\ndatasets, well demonstrating the effectiveness and the superiority of our\nmethod. Our code will be available at https://github.com/jameslahm/SCPNet.\n","authors":["Zixuan Ding","Ao Wang","Hui Chen","Qiang Zhang","Pengzhang Liu","Yongjun Bao","Weipeng Yan","Jungong Han"],"pdf_url":"https://arxiv.org/pdf/2303.13223v1.pdf","comment":"Accepted by IEEE/CVF Conference on Computer Vision and Pattern\n  Recognition (CVPR) 2023"},{"id":"http://arxiv.org/abs/2303.13221v1","updated":"2023-03-23T12:34:52Z","published":"2023-03-23T12:34:52Z","title":"Explore the Power of Synthetic Data on Few-shot Object Detection","summary":"  Few-shot object detection (FSOD) aims to expand an object detector for novel\ncategories given only a few instances for training. The few training samples\nrestrict the performance of FSOD model. Recent text-to-image generation models\nhave shown promising results in generating high-quality images. How applicable\nthese synthetic images are for FSOD tasks remains under-explored. This work\nextensively studies how synthetic images generated from state-of-the-art\ntext-to-image generators benefit FSOD tasks. We focus on two perspectives: (1)\nHow to use synthetic data for FSOD? (2) How to find representative samples from\nthe large-scale synthetic dataset? We design a copy-paste-based pipeline for\nusing synthetic data. Specifically, saliency object detection is applied to the\noriginal generated image, and the minimum enclosing box is used for cropping\nthe main object based on the saliency map. After that, the cropped object is\nrandomly pasted on the image, which comes from the base dataset. We also study\nthe influence of the input text of text-to-image generator and the number of\nsynthetic images used. To construct a representative synthetic training\ndataset, we maximize the diversity of the selected images via a sample-based\nand cluster-based method. However, the severe problem of high false positives\n(FP) ratio of novel categories in FSOD can not be solved by using synthetic\ndata. We propose integrating CLIP, a zero-shot recognition model, into the FSOD\npipeline, which can filter 90% of FP by defining a threshold for the similarity\nscore between the detected object and the text of the predicted category.\nExtensive experiments on PASCAL VOC and MS COCO validate the effectiveness of\nour method, in which performance gain is up to 21.9% compared to the few-shot\nbaseline.\n","authors":["Shaobo Lin","Kun Wang","Xingyu Zeng","Rui Zhao"],"pdf_url":"https://arxiv.org/pdf/2303.13221v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2302.01047v2","updated":"2023-03-23T12:30:35Z","published":"2023-02-02T12:21:10Z","title":"Real-Time Evaluation in Online Continual Learning: A New Hope","summary":"  Current evaluations of Continual Learning (CL) methods typically assume that\nthere is no constraint on training time and computation. This is an unrealistic\nassumption for any real-world setting, which motivates us to propose: a\npractical real-time evaluation of continual learning, in which the stream does\nnot wait for the model to complete training before revealing the next data for\npredictions. To do this, we evaluate current CL methods with respect to their\ncomputational costs. We conduct extensive experiments on CLOC, a large-scale\ndataset containing 39 million time-stamped images with geolocation labels. We\nshow that a simple baseline outperforms state-of-the-art CL methods under this\nevaluation, questioning the applicability of existing methods in realistic\nsettings. In addition, we explore various CL components commonly used in the\nliterature, including memory sampling strategies and regularization approaches.\nWe find that all considered methods fail to be competitive against our simple\nbaseline. This surprisingly suggests that the majority of existing CL\nliterature is tailored to a specific class of streams that is not practical. We\nhope that the evaluation we provide will be the first step towards a paradigm\nshift to consider the computational cost in the development of online continual\nlearning methods.\n","authors":["Yasir Ghunaim","Adel Bibi","Kumail Alhamoud","Motasem Alfarra","Hasan Abed Al Kader Hammoud","Ameya Prabhu","Philip H. S. Torr","Bernard Ghanem"],"pdf_url":"https://arxiv.org/pdf/2302.01047v2.pdf","comment":"Accepted at CVPR'23 as Highlight (Top 2.5%)"},{"id":"http://arxiv.org/abs/2301.07702v2","updated":"2023-03-23T12:25:12Z","published":"2023-01-18T18:47:46Z","title":"Learning 3D-aware Image Synthesis with Unknown Pose Distribution","summary":"  Existing methods for 3D-aware image synthesis largely depend on the 3D pose\ndistribution pre-estimated on the training set. An inaccurate estimation may\nmislead the model into learning faulty geometry. This work proposes PoF3D that\nfrees generative radiance fields from the requirements of 3D pose priors. We\nfirst equip the generator with an efficient pose learner, which is able to\ninfer a pose from a latent code, to approximate the underlying true pose\ndistribution automatically. We then assign the discriminator a task to learn\npose distribution under the supervision of the generator and to differentiate\nreal and synthesized images with the predicted pose as the condition. The\npose-free generator and the pose-aware discriminator are jointly trained in an\nadversarial manner. Extensive results on a couple of datasets confirm that the\nperformance of our approach, regarding both image quality and geometry quality,\nis on par with state of the art. To our best knowledge, PoF3D demonstrates the\nfeasibility of learning high-quality 3D-aware image synthesis without using 3D\npose priors for the first time.\n","authors":["Zifan Shi","Yujun Shen","Yinghao Xu","Sida Peng","Yiyi Liao","Sheng Guo","Qifeng Chen","Dit-Yan Yeung"],"pdf_url":"https://arxiv.org/pdf/2301.07702v2.pdf","comment":"CVPR 2023. Project page: https://vivianszf.github.io/pof3d/"},{"id":"http://arxiv.org/abs/2303.13212v1","updated":"2023-03-23T12:13:29Z","published":"2023-03-23T12:13:29Z","title":"A Simple and Generic Framework for Feature Distillation via Channel-wise\n  Transformation","summary":"  Knowledge distillation is a popular technique for transferring the knowledge\nfrom a large teacher model to a smaller student model by mimicking. However,\ndistillation by directly aligning the feature maps between teacher and student\nmay enforce overly strict constraints on the student thus degrade the\nperformance of the student model. To alleviate the above feature misalignment\nissue, existing works mainly focus on spatially aligning the feature maps of\nthe teacher and the student, with pixel-wise transformation. In this paper, we\nnewly find that aligning the feature maps between teacher and student along the\nchannel-wise dimension is also effective for addressing the feature\nmisalignment issue. Specifically, we propose a learnable nonlinear channel-wise\ntransformation to align the features of the student and the teacher model.\nBased on it, we further propose a simple and generic framework for feature\ndistillation, with only one hyper-parameter to balance the distillation loss\nand the task specific loss. Extensive experimental results show that our method\nachieves significant performance improvements in various computer vision tasks\nincluding image classification (+3.28% top-1 accuracy for MobileNetV1 on\nImageNet-1K), object detection (+3.9% bbox mAP for ResNet50-based Faster-RCNN\non MS COCO), instance segmentation (+2.8% Mask mAP for ResNet50-based\nMask-RCNN), and semantic segmentation (+4.66% mIoU for ResNet18-based PSPNet in\nsemantic segmentation on Cityscapes), which demonstrates the effectiveness and\nthe versatility of the proposed method. The code will be made publicly\navailable.\n","authors":["Ziwei Liu","Yongtao Wang","Xiaojie Chu"],"pdf_url":"https://arxiv.org/pdf/2303.13212v1.pdf","comment":"13 pages"},{"id":"http://arxiv.org/abs/2303.13211v1","updated":"2023-03-23T12:11:24Z","published":"2023-03-23T12:11:24Z","title":"Don't FREAK Out: A Frequency-Inspired Approach to Detecting Backdoor\n  Poisoned Samples in DNNs","summary":"  In this paper we investigate the frequency sensitivity of Deep Neural\nNetworks (DNNs) when presented with clean samples versus poisoned samples. Our\nanalysis shows significant disparities in frequency sensitivity between these\ntwo types of samples. Building on these findings, we propose FREAK, a\nfrequency-based poisoned sample detection algorithm that is simple yet\neffective. Our experimental results demonstrate the efficacy of FREAK not only\nagainst frequency backdoor attacks but also against some spatial attacks. Our\nwork is just the first step in leveraging these insights. We believe that our\nanalysis and proposed defense mechanism will provide a foundation for future\nresearch and development of backdoor defenses.\n","authors":["Hasan Abed Al Kader Hammoud","Adel Bibi","Philip H. S. Torr","Bernard Ghanem"],"pdf_url":"https://arxiv.org/pdf/2303.13211v1.pdf","comment":"Accepted at CVPRW (The Art of Robustness)"},{"id":"http://arxiv.org/abs/2303.13209v1","updated":"2023-03-23T12:08:10Z","published":"2023-03-23T12:08:10Z","title":"Taking A Closer Look at Visual Relation: Unbiased Video Scene Graph\n  Generation with Decoupled Label Learning","summary":"  Current video-based scene graph generation (VidSGG) methods have been found\nto perform poorly on predicting predicates that are less represented due to the\ninherent biased distribution in the training data. In this paper, we take a\ncloser look at the predicates and identify that most visual relations (e.g.\nsit_above) involve both actional pattern (sit) and spatial pattern (above),\nwhile the distribution bias is much less severe at the pattern level. Based on\nthis insight, we propose a decoupled label learning (DLL) paradigm to address\nthe intractable visual relation prediction from the pattern-level perspective.\nSpecifically, DLL decouples the predicate labels and adopts separate\nclassifiers to learn actional and spatial patterns respectively. The patterns\nare then combined and mapped back to the predicate. Moreover, we propose a\nknowledge-level label decoupling method to transfer non-target knowledge from\nhead predicates to tail predicates within the same pattern to calibrate the\ndistribution of tail classes. We validate the effectiveness of DLL on the\ncommonly used VidSGG benchmark, i.e. VidVRD. Extensive experiments demonstrate\nthat the DLL offers a remarkably simple but highly effective solution to the\nlong-tailed problem, achieving the state-of-the-art VidSGG performance.\n","authors":["Wenqing Wang","Yawei Luo","Zhiqing Chen","Tao Jiang","Lei Chen","Yi Yang","Jun Xiao"],"pdf_url":"https://arxiv.org/pdf/2303.13209v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.05725v3","updated":"2023-03-23T12:00:33Z","published":"2023-03-10T06:12:36Z","title":"CVT-SLR: Contrastive Visual-Textual Transformation for Sign Language\n  Recognition with Variational Alignment","summary":"  Sign language recognition (SLR) is a weakly supervised task that annotates\nsign videos as textual glosses. Recent studies show that insufficient training\ncaused by the lack of large-scale available sign datasets becomes the main\nbottleneck for SLR. Most SLR works thereby adopt pretrained visual modules and\ndevelop two mainstream solutions. The multi-stream architectures extend\nmulti-cue visual features, yielding the current SOTA performances but requiring\ncomplex designs and might introduce potential noise. Alternatively, the\nadvanced single-cue SLR frameworks using explicit cross-modal alignment between\nvisual and textual modalities are simple and effective, potentially competitive\nwith the multi-cue framework. In this work, we propose a novel contrastive\nvisual-textual transformation for SLR, CVT-SLR, to fully explore the pretrained\nknowledge of both the visual and language modalities. Based on the single-cue\ncross-modal alignment framework, we propose a variational autoencoder (VAE) for\npretrained contextual knowledge while introducing the complete pretrained\nlanguage module. The VAE implicitly aligns visual and textual modalities while\nbenefiting from pretrained contextual knowledge as the traditional contextual\nmodule. Meanwhile, a contrastive cross-modal alignment algorithm is designed to\nexplicitly enhance the consistency constraints. Extensive experiments on public\ndatasets (PHOENIX-2014 and PHOENIX-2014T) demonstrate that our proposed CVT-SLR\nconsistently outperforms existing single-cue methods and even outperforms SOTA\nmulti-cue methods.\n","authors":["Jiangbin Zheng","Yile Wang","Cheng Tan","Siyuan Li","Ge Wang","Jun Xia","Yidong Chen","Stan Z. Li"],"pdf_url":"https://arxiv.org/pdf/2303.05725v3.pdf","comment":"Accepted to CVPR 2023 (Highlight Paper Top 2.5%)"},{"id":"http://arxiv.org/abs/2303.13203v1","updated":"2023-03-23T11:57:50Z","published":"2023-03-23T11:57:50Z","title":"A Confident Labelling Strategy Based on Deep Learning for Improving\n  Early Detection of Knee OsteoArthritis","summary":"  Knee OsteoArthritis (KOA) is a prevalent musculoskeletal disorder that causes\ndecreased mobility in seniors. The diagnosis provided by physicians is\nsubjective, however, as it relies on personal experience and the\nsemi-quantitative Kellgren-Lawrence (KL) scoring system. KOA has been\nsuccessfully diagnosed by Computer-Aided Diagnostic (CAD) systems that use deep\nlearning techniques like Convolutional Neural Networks (CNN). In this paper, we\npropose a novel Siamese-based network, and we introduce a new hybrid loss\nstrategy for the early detection of KOA. The model extends the classical\nSiamese network by integrating a collection of Global Average Pooling (GAP)\nlayers for feature extraction at each level. Then, to improve the\nclassification performance, a novel training strategy that partitions each\ntraining batch into low-, medium- and high-confidence subsets, and a specific\nhybrid loss function are used for each new label attributed to each sample. The\nfinal loss function is then derived by combining the latter loss functions with\noptimized weights. Our test results demonstrate that our proposed approach\nsignificantly improves the detection performance.\n","authors":["Zhe Wang","Aladine Chetouani","Rachid Jennane"],"pdf_url":"https://arxiv.org/pdf/2303.13203v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.13199v1","updated":"2023-03-23T11:54:41Z","published":"2023-03-23T11:54:41Z","title":"First Session Adaptation: A Strong Replay-Free Baseline for\n  Class-Incremental Learning","summary":"  In Class-Incremental Learning (CIL) an image classification system is exposed\nto new classes in each learning session and must be updated incrementally.\nMethods approaching this problem have updated both the classification head and\nthe feature extractor body at each session of CIL. In this work, we develop a\nbaseline method, First Session Adaptation (FSA), that sheds light on the\nefficacy of existing CIL approaches and allows us to assess the relative\nperformance contributions from head and body adaption. FSA adapts a pre-trained\nneural network body only on the first learning session and fixes it thereafter;\na head based on linear discriminant analysis (LDA), is then placed on top of\nthe adapted body, allowing exact updates through CIL. FSA is replay-free\ni.e.~it does not memorize examples from previous sessions of continual\nlearning. To empirically motivate FSA, we first consider a diverse selection of\n22 image-classification datasets, evaluating different heads and body\nadaptation techniques in high/low-shot offline settings. We find that the LDA\nhead performs well and supports CIL out-of-the-box. We also find that\nFeaturewise Layer Modulation (FiLM) adapters are highly effective in the\nfew-shot setting, and full-body adaption in the high-shot setting. Second, we\nempirically investigate various CIL settings including high-shot CIL and\nfew-shot CIL, including settings that have previously been used in the\nliterature. We show that FSA significantly improves over the state-of-the-art\nin 15 of the 16 settings considered. FSA with FiLM adapters is especially\nperformant in the few-shot setting. These results indicate that current\napproaches to continuous body adaptation are not working as expected. Finally,\nwe propose a measure that can be applied to a set of unlabelled inputs which is\npredictive of the benefits of body adaptation.\n","authors":["Aristeidis Panos","Yuriko Kobe","Daniel Olmeda Reino","Rahaf Aljundi","Richard E. Turner"],"pdf_url":"https://arxiv.org/pdf/2303.13199v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.13194v1","updated":"2023-03-23T11:52:17Z","published":"2023-03-23T11:52:17Z","title":"Complementary Pseudo Multimodal Feature for Point Cloud Anomaly\n  Detection","summary":"  Point cloud (PCD) anomaly detection steadily emerges as a promising research\narea. This study aims to improve PCD anomaly detection performance by combining\nhandcrafted PCD descriptions with powerful pre-trained 2D neural networks. To\nthis end, this study proposes Complementary Pseudo Multimodal Feature (CPMF)\nthat incorporates local geometrical information in 3D modality using\nhandcrafted PCD descriptors and global semantic information in the generated\npseudo 2D modality using pre-trained 2D neural networks. For global semantics\nextraction, CPMF projects the origin PCD into a pseudo 2D modality containing\nmulti-view images. These images are delivered to pre-trained 2D neural networks\nfor informative 2D modality feature extraction. The 3D and 2D modality features\nare aggregated to obtain the CPMF for PCD anomaly detection. Extensive\nexperiments demonstrate the complementary capacity between 2D and 3D modality\nfeatures and the effectiveness of CPMF, with 95.15% image-level AU-ROC and\n92.93% pixel-level PRO on the MVTec3D benchmark. Code is available on\nhttps://github.com/caoyunkang/CPMF.\n","authors":["Yunkang Cao","Xiaohao Xu","Weiming Shen"],"pdf_url":"https://arxiv.org/pdf/2303.13194v1.pdf","comment":"Submitted to Pattern Recognition. Code is available on\n  https://github.com/caoyunkang/CPMF"},{"id":"http://arxiv.org/abs/2303.13193v1","updated":"2023-03-23T11:50:44Z","published":"2023-03-23T11:50:44Z","title":"VADER: Video Alignment Differencing and Retrieval","summary":"  We propose VADER, a spatio-temporal matching, alignment, and change\nsummarization method to help fight misinformation spread via manipulated\nvideos. VADER matches and coarsely aligns partial video fragments to candidate\nvideos using a robust visual descriptor and scalable search over adaptively\nchunked video content. A transformer-based alignment module then refines the\ntemporal localization of the query fragment within the matched video. A\nspace-time comparator module identifies regions of manipulation between aligned\ncontent, invariant to any changes due to any residual temporal misalignments or\nartifacts arising from non-editorial changes of the content. Robustly matching\nvideo to a trusted source enables conclusions to be drawn on video provenance,\nenabling informed trust decisions on content encountered.\n","authors":["Alexander Black","Simon Jenni","Tu Bui","Md. Mehrab Tanjim","Stefano Petrangeli","Ritwik Sinha","Viswanathan Swaminathan","John Collomosse"],"pdf_url":"https://arxiv.org/pdf/2303.13193v1.pdf","comment":"Submitted to ICCV2023"},{"id":"http://arxiv.org/abs/2209.04747v4","updated":"2023-03-23T11:42:58Z","published":"2022-09-10T22:00:30Z","title":"Diffusion Models in Vision: A Survey","summary":"  Denoising diffusion models represent a recent emerging topic in computer\nvision, demonstrating remarkable results in the area of generative modeling. A\ndiffusion model is a deep generative model that is based on two stages, a\nforward diffusion stage and a reverse diffusion stage. In the forward diffusion\nstage, the input data is gradually perturbed over several steps by adding\nGaussian noise. In the reverse stage, a model is tasked at recovering the\noriginal input data by learning to gradually reverse the diffusion process,\nstep by step. Diffusion models are widely appreciated for the quality and\ndiversity of the generated samples, despite their known computational burdens,\ni.e. low speeds due to the high number of steps involved during sampling. In\nthis survey, we provide a comprehensive review of articles on denoising\ndiffusion models applied in vision, comprising both theoretical and practical\ncontributions in the field. First, we identify and present three generic\ndiffusion modeling frameworks, which are based on denoising diffusion\nprobabilistic models, noise conditioned score networks, and stochastic\ndifferential equations. We further discuss the relations between diffusion\nmodels and other deep generative models, including variational auto-encoders,\ngenerative adversarial networks, energy-based models, autoregressive models and\nnormalizing flows. Then, we introduce a multi-perspective categorization of\ndiffusion models applied in computer vision. Finally, we illustrate the current\nlimitations of diffusion models and envision some interesting directions for\nfuture research.\n","authors":["Florinel-Alin Croitoru","Vlad Hondru","Radu Tudor Ionescu","Mubarak Shah"],"pdf_url":"https://arxiv.org/pdf/2209.04747v4.pdf","comment":"Accepted in IEEE Transactions on Pattern Analysis and Machine\n  Intelligence. 25 pages, 3 figures"},{"id":"http://arxiv.org/abs/2303.13190v1","updated":"2023-03-23T11:42:35Z","published":"2023-03-23T11:42:35Z","title":"Marching-Primitives: Shape Abstraction from Signed Distance Function","summary":"  Representing complex objects with basic geometric primitives has long been a\ntopic in computer vision. Primitive-based representations have the merits of\ncompactness and computational efficiency in higher-level tasks such as physics\nsimulation, collision checking, and robotic manipulation. Unlike previous works\nwhich extract polygonal meshes from a signed distance function (SDF), in this\npaper, we present a novel method, named Marching-Primitives, to obtain a\nprimitive-based abstraction directly from an SDF. Our method grows geometric\nprimitives (such as superquadrics) iteratively by analyzing the connectivity of\nvoxels while marching at different levels of signed distance. For each valid\nconnected volume of interest, we march on the scope of voxels from which a\nprimitive is able to be extracted in a probabilistic sense and simultaneously\nsolve for the parameters of the primitive to capture the underlying local\ngeometry. We evaluate the performance of our method on both synthetic and\nreal-world datasets. The results show that the proposed method outperforms the\nstate-of-the-art in terms of accuracy, and is directly generalizable among\ndifferent categories and scales. The code is open-sourced at\nhttps://github.com/ChirikjianLab/Marching-Primitives.git.\n","authors":["Weixiao Liu","Yuwei Wu","Sipu Ruan","Gregory S. Chirikjian"],"pdf_url":"https://arxiv.org/pdf/2303.13190v1.pdf","comment":"Accepted to CVPR2023 Highlight"},{"id":"http://arxiv.org/abs/2302.06081v2","updated":"2023-03-23T11:38:53Z","published":"2023-02-13T03:34:49Z","title":"Correspondence-Free Domain Alignment for Unsupervised Cross-Domain Image\n  Retrieval","summary":"  Cross-domain image retrieval aims at retrieving images across different\ndomains to excavate cross-domain classificatory or correspondence\nrelationships. This paper studies a less-touched problem of cross-domain image\nretrieval, i.e., unsupervised cross-domain image retrieval, considering the\nfollowing practical assumptions: (i) no correspondence relationship, and (ii)\nno category annotations. It is challenging to align and bridge distinct domains\nwithout cross-domain correspondence. To tackle the challenge, we present a\nnovel Correspondence-free Domain Alignment (CoDA) method to effectively\neliminate the cross-domain gap through In-domain Self-matching Supervision\n(ISS) and Cross-domain Classifier Alignment (CCA). To be specific, ISS is\npresented to encapsulate discriminative information into the latent common\nspace by elaborating a novel self-matching supervision mechanism. To alleviate\nthe cross-domain discrepancy, CCA is proposed to align distinct domain-specific\nclassifiers. Thanks to the ISS and CCA, our method could encode the\ndiscrimination into the domain-invariant embedding space for unsupervised\ncross-domain image retrieval. To verify the effectiveness of the proposed\nmethod, extensive experiments are conducted on four benchmark datasets compared\nwith six state-of-the-art methods.\n","authors":["Xu Wang","Dezhong Peng","Ming Yan","Peng Hu"],"pdf_url":"https://arxiv.org/pdf/2302.06081v2.pdf","comment":"AAAI 2023"},{"id":"http://arxiv.org/abs/2303.13186v1","updated":"2023-03-23T11:36:14Z","published":"2023-03-23T11:36:14Z","title":"ScanERU: Interactive 3D Visual Grounding based on Embodied Reference\n  Understanding","summary":"  Aiming to link natural language descriptions to specific regions in a 3D\nscene represented as 3D point clouds, 3D visual grounding is a very fundamental\ntask for human-robot interaction. The recognition errors can significantly\nimpact the overall accuracy and then degrade the operation of AI systems.\nDespite their effectiveness, existing methods suffer from the difficulty of low\nrecognition accuracy in cases of multiple adjacent objects with similar\nappearances.To address this issue, this work intuitively introduces the\nhuman-robot interaction as a cue to facilitate the development of 3D visual\ngrounding. Specifically, a new task termed Embodied Reference Understanding\n(ERU) is first designed for this concern. Then a new dataset called ScanERU is\nconstructed to evaluate the effectiveness of this idea. Different from existing\ndatasets, our ScanERU is the first to cover semi-synthetic scene integration\nwith textual, real-world visual, and synthetic gestural information.\nAdditionally, this paper formulates a heuristic framework based on attention\nmechanisms and human body movements to enlighten the research of ERU.\nExperimental results demonstrate the superiority of the proposed method,\nespecially in the recognition of multiple identical objects. Our codes and\ndataset are ready to be available publicly.\n","authors":["Ziyang Lu","Yunqiang Pei","Guoqing Wang","Yang Yang","Zheng Wang","Heng Tao Shen"],"pdf_url":"https://arxiv.org/pdf/2303.13186v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.13182v1","updated":"2023-03-23T11:29:31Z","published":"2023-03-23T11:29:31Z","title":"CMG-Net: An End-to-End Contact-Based Multi-Finger Dexterous Grasping\n  Network","summary":"  In this paper, we propose a novel representation for grasping using contacts\nbetween multi-finger robotic hands and objects to be manipulated. This\nrepresentation significantly reduces the prediction dimensions and accelerates\nthe learning process. We present an effective end-to-end network, CMG-Net, for\ngrasping unknown objects in a cluttered environment by efficiently predicting\nmulti-finger grasp poses and hand configurations from a single-shot point\ncloud. Moreover, we create a synthetic grasp dataset that consists of five\nthousand cluttered scenes, 80 object categories, and 20 million annotations. We\nperform a comprehensive empirical study and demonstrate the effectiveness of\nour grasping representation and CMG-Net. Our work significantly outperforms the\nstate-of-the-art for three-finger robotic hands. We also demonstrate that the\nmodel trained using synthetic data performs very well for real robots.\n","authors":["Mingze Wei","Yaomin Huang","Zhiyuan Xu","Ning Liu","Zhengping Che","Xinyu Zhang","Chaomin Shen","Feifei Feng","Chun Shan","Jian Tang"],"pdf_url":"https://arxiv.org/pdf/2303.13182v1.pdf","comment":"The first two authors are with equal contributions. Paper accepted by\n  ICRA 2023"},{"id":"http://arxiv.org/abs/2206.06067v4","updated":"2023-03-23T11:17:31Z","published":"2022-06-13T11:52:13Z","title":"Better Teacher Better Student: Dynamic Prior Knowledge for Knowledge\n  Distillation","summary":"  Knowledge distillation (KD) has shown very promising capabilities in\ntransferring learning representations from large models (teachers) to small\nmodels (students). However, as the capacity gap between students and teachers\nbecomes larger, existing KD methods fail to achieve better results. Our work\nshows that the `prior knowledge' is vital to KD, especially when applying large\nteachers. Particularly, we propose the dynamic prior knowledge (DPK), which\nintegrates part of teacher's features as the prior knowledge before the feature\ndistillation. This means that our method also takes the teacher's feature as\n`input', not just `target'. Besides, we dynamically adjust the ratio of the\nprior knowledge during the training phase according to the feature gap, thus\nguiding the student in an appropriate difficulty. To evaluate the proposed\nmethod, we conduct extensive experiments on two image classification benchmarks\n(i.e. CIFAR100 and ImageNet) and an object detection benchmark (i.e. MS COCO.\nThe results demonstrate the superiority of our method in performance under\nvarying settings. Besides, our DPK makes the performance of the student model\npositively correlated with that of the teacher model, which means that we can\nfurther boost the accuracy of students by applying larger teachers. More\nimportantly, DPK provides a fast solution in teacher model selection for any\ngiven model.\n","authors":["Zengyu Qiu","Xinzhu Ma","Kunlin Yang","Chunya Liu","Jun Hou","Shuai Yi","Wanli Ouyang"],"pdf_url":"https://arxiv.org/pdf/2206.06067v4.pdf","comment":"ICLR'23 accepted"},{"id":"http://arxiv.org/abs/2211.13654v2","updated":"2023-03-23T11:14:35Z","published":"2022-11-24T15:09:33Z","title":"Cross Aggregation Transformer for Image Restoration","summary":"  Recently, Transformer architecture has been introduced into image restoration\nto replace convolution neural network (CNN) with surprising results.\nConsidering the high computational complexity of Transformer with global\nattention, some methods use the local square window to limit the scope of\nself-attention. However, these methods lack direct interaction among different\nwindows, which limits the establishment of long-range dependencies. To address\nthe above issue, we propose a new image restoration model, Cross Aggregation\nTransformer (CAT). The core of our CAT is the Rectangle-Window Self-Attention\n(Rwin-SA), which utilizes horizontal and vertical rectangle window attention in\ndifferent heads parallelly to expand the attention area and aggregate the\nfeatures cross different windows. We also introduce the Axial-Shift operation\nfor different window interactions. Furthermore, we propose the Locality\nComplementary Module to complement the self-attention mechanism, which\nincorporates the inductive bias of CNN (e.g., translation invariance and\nlocality) into Transformer, enabling global-local coupling. Extensive\nexperiments demonstrate that our CAT outperforms recent state-of-the-art\nmethods on several image restoration applications. The code and models are\navailable at https://github.com/zhengchen1999/CAT.\n","authors":["Zheng Chen","Yulun Zhang","Jinjin Gu","Yongbing Zhang","Linghe Kong","Xin Yuan"],"pdf_url":"https://arxiv.org/pdf/2211.13654v2.pdf","comment":"Accepted to NeurIPS 2022. Code is available at\n  https://github.com/zhengchen1999/CAT"},{"id":"http://arxiv.org/abs/2303.13175v1","updated":"2023-03-23T11:11:41Z","published":"2023-03-23T11:11:41Z","title":"Enhancement of theColor Image Compression Using a New Algorithm based on\n  Discrete Hermite Wavelet Transform","summary":"  The Internet has turned the entire world into a small village;this is because\nit has made it possible to share millions of images and videos. However,\nsending and receiving a huge amount of data is considered to be a main\nchallenge. To address this issue, a new algorithm is required to reduce image\nbits and represent the data in a compressed form. Nevertheless, image\ncompression is an important application for transferring large files and\nimages. This requires appropriate and efficient transfers in this field to\nachieve the task and reach the best results. In this work, we propose a new\nalgorithm based on discrete Hermite wavelets transformation (DHWT) that shows\nthe efficiency and quality of the color images. By compressing the color image,\nthis method analyzes it and divides it into approximate coefficients and detail\ncoefficients after adding the wavelets into MATLAB. With Multi-Resolution\nAnalyses (MRA), the appropriate filter is derived, and the mathematical aspects\nprove to be validated by testing a new filter and performing its operation.\nAfter the decomposition of the rows and upon the process of the reconstruction,\ntaking the inverse of the filter and dealing with the columns of the matrix,\nthe original matrix is improved by measuring the parameters of the image to\nachieve the best quality of the resulting image, such as the peak\nsignal-to-noise ratio (PSNR), compression ratio (CR), bits per pixel (BPP), and\nmean square error (MSE).\n","authors":["Hassan Mohamed Muhi-Aldeen","Asma A. Abdulrahman","Jabbar Abed Eleiwy","Fouad S. Tahir","Yurii Khlaponin"],"pdf_url":"https://arxiv.org/pdf/2303.13175v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.13174v1","updated":"2023-03-23T11:03:18Z","published":"2023-03-23T11:03:18Z","title":"3D-POP -- An automated annotation approach to facilitate markerless\n  2D-3D tracking of freely moving birds with marker-based motion capture","summary":"  Recent advances in machine learning and computer vision are revolutionizing\nthe field of animal behavior by enabling researchers to track the poses and\nlocations of freely moving animals without any marker attachment. However,\nlarge datasets of annotated images of animals for markerless pose tracking,\nespecially high-resolution images taken from multiple angles with accurate 3D\nannotations, are still scant. Here, we propose a method that uses a motion\ncapture (mo-cap) system to obtain a large amount of annotated data on animal\nmovement and posture (2D and 3D) in a semi-automatic manner. Our method is\nnovel in that it extracts the 3D positions of morphological keypoints (e.g\neyes, beak, tail) in reference to the positions of markers attached to the\nanimals. Using this method, we obtained, and offer here, a new dataset - 3D-POP\nwith approximately 300k annotated frames (4 million instances) in the form of\nvideos having groups of one to ten freely moving birds from 4 different camera\nviews in a 3.6m x 4.2m area. 3D-POP is the first dataset of flocking birds with\naccurate keypoint annotations in 2D and 3D along with bounding box and\nindividual identities and will facilitate the development of solutions for\nproblems of 2D to 3D markerless pose, trajectory tracking, and identification\nin birds.\n","authors":["Hemal Naik","Alex Hoi Hang Chan","Junran Yang","Mathilde Delacoux","Iain D. Couzin","Fumihiro Kano","Máté Nagy"],"pdf_url":"https://arxiv.org/pdf/2303.13174v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.13166v1","updated":"2023-03-23T10:36:10Z","published":"2023-03-23T10:36:10Z","title":"Take 5: Interpretable Image Classification with a Handful of Features","summary":"  Deep Neural Networks use thousands of mostly incomprehensible features to\nidentify a single class, a decision no human can follow. We propose an\ninterpretable sparse and low dimensional final decision layer in a deep neural\nnetwork with measurable aspects of interpretability and demonstrate it on\nfine-grained image classification. We argue that a human can only understand\nthe decision of a machine learning model, if the features are interpretable and\nonly very few of them are used for a single decision. For that matter, the\nfinal layer has to be sparse and, to make interpreting the features feasible,\nlow dimensional. We call a model with a Sparse Low-Dimensional Decision\nSLDD-Model. We show that a SLDD-Model is easier to interpret locally and\nglobally than a dense high-dimensional decision layer while being able to\nmaintain competitive accuracy. Additionally, we propose a loss function that\nimproves a model's feature diversity and accuracy. Our more interpretable\nSLDD-Model only uses 5 out of just 50 features per class, while maintaining 97%\nto 100% of the accuracy on four common benchmark datasets compared to the\nbaseline model with 2048 features.\n","authors":["Thomas Norrenbrock","Marco Rudolph","Bodo Rosenhahn"],"pdf_url":"https://arxiv.org/pdf/2303.13166v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.13158v1","updated":"2023-03-23T10:20:19Z","published":"2023-03-23T10:20:19Z","title":"Improvement of Color Image Analysis Using a New Hybrid Face Recognition\n  Algorithm based on Discrete Wavelets and Chebyshev Polynomials","summary":"  This work is unique in the use of discrete wavelets that were built from or\nderived from Chebyshev polynomials of the second and third kind, filter the\nDiscrete Second Chebyshev Wavelets Transform (DSCWT), and derive two effective\nfilters. The Filter Discrete Third Chebyshev Wavelets Transform (FDTCWT) is\nused in the process of analyzing color images and removing noise and impurities\nthat accompany the image, as well as because of the large amount of data that\nmakes up the image as it is taken. These data are massive, making it difficult\nto deal with each other during transmission. However to address this issue, the\nimage compression technique is used, with the image not losing information due\nto the readings that were obtained, and the results were satisfactory. Mean\nSquare Error (MSE), Peak Signal Noise Ratio (PSNR), Bit Per Pixel (BPP), and\nCompression Ratio (CR) Coronavirus is the initial treatment, while the\nprocessing stage is done with network training for Convolutional Neural\nNetworks (CNN) with Discrete Second Chebeshev Wavelets Convolutional Neural\nNetwork (DSCWCNN) and Discrete Third Chebeshev Wavelets Convolutional Neural\nNetwork (DTCWCNN) to create an efficient algorithm for face recognition, and\nthe best results were achieved in accuracy and in the least amount of time. Two\nsamples of color images that were made or implemented were used. The proposed\ntheory was obtained with fast and good results; the results are evident shown\nin the tables below.\n","authors":["Hassan Mohamed Muhi-Aldeen","Maha Ammar Mustafa","Asma A. Abdulrahman","Jabbar Abed Eleiwy","Fouad S. Tahir","Yurii Khlaponin"],"pdf_url":"https://arxiv.org/pdf/2303.13158v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.13148v1","updated":"2023-03-23T10:03:12Z","published":"2023-03-23T10:03:12Z","title":"Calibrated Out-of-Distribution Detection with a Generic Representation","summary":"  Out-of-distribution detection is a common issue in deploying vision models in\npractice and solving it is an essential building block in safety critical\napplications. Existing OOD detection solutions focus on improving the OOD\nrobustness of a classification model trained exclusively on in-distribution\n(ID) data. In this work, we take a different approach and propose to leverage\ngeneric pre-trained representations. We first investigate the behaviour of\nsimple classifiers built on top of such representations and show striking\nperformance gains compared to the ID trained representations. We propose a\nnovel OOD method, called GROOD, that achieves excellent performance, predicated\nby the use of a good generic representation. Only a trivial training process is\nrequired for adapting GROOD to a particular problem. The method is simple,\ngeneral, efficient, calibrated and with only a few hyper-parameters. The method\nachieves state-of-the-art performance on a number of OOD benchmarks, reaching\nnear perfect performance on several of them. The source code is available at\nhttps://github.com/vojirt/GROOD.\n","authors":["Tomas Vojir","Jan Sochman","Rahaf Aljundi","Jiri Matas"],"pdf_url":"https://arxiv.org/pdf/2303.13148v1.pdf","comment":"10 pages, submitted to conference"},{"id":"http://arxiv.org/abs/2210.01612v2","updated":"2023-03-23T10:01:33Z","published":"2022-10-04T13:51:59Z","title":"PlaneDepth: Self-supervised Depth Estimation via Orthogonal Planes","summary":"  Multiple near frontal-parallel planes based depth representation demonstrated\nimpressive results in self-supervised monocular depth estimation (MDE).\nWhereas, such a representation would cause the discontinuity of the ground as\nit is perpendicular to the frontal-parallel planes, which is detrimental to the\nidentification of drivable space in autonomous driving. In this paper, we\npropose the PlaneDepth, a novel orthogonal planes based presentation, including\nvertical planes and ground planes. PlaneDepth estimates the depth distribution\nusing a Laplacian Mixture Model based on orthogonal planes for an input image.\nThese planes are used to synthesize a reference view to provide the\nself-supervision signal. Further, we find that the widely used resizing and\ncropping data augmentation breaks the orthogonality assumptions, leading to\ninferior plane predictions. We address this problem by explicitly constructing\nthe resizing cropping transformation to rectify the predefined planes and\npredicted camera pose. Moreover, we propose an augmented self-distillation loss\nsupervised with a bilateral occlusion mask to boost the robustness of\northogonal planes representation for occlusions. Thanks to our orthogonal\nplanes representation, we can extract the ground plane in an unsupervised\nmanner, which is important for autonomous driving. Extensive experiments on the\nKITTI dataset demonstrate the effectiveness and efficiency of our method. The\ncode is available at https://github.com/svip-lab/PlaneDepth.\n","authors":["Ruoyu Wang","Zehao Yu","Shenghua Gao"],"pdf_url":"https://arxiv.org/pdf/2210.01612v2.pdf","comment":"Accepted by CVPR 2023. Code and models are available at:\n  https://github.com/svip-lab/PlaneDepth"},{"id":"http://arxiv.org/abs/2204.08696v3","updated":"2023-03-23T09:44:22Z","published":"2022-04-19T06:38:29Z","title":"CTCNet: A CNN-Transformer Cooperation Network for Face Image\n  Super-Resolution","summary":"  Recently, deep convolution neural networks (CNNs) steered face\nsuper-resolution methods have achieved great progress in restoring degraded\nfacial details by jointly training with facial priors. However, these methods\nhave some obvious limitations. On the one hand, multi-task joint learning\nrequires additional marking on the dataset, and the introduced prior network\nwill significantly increase the computational cost of the model. On the other\nhand, the limited receptive field of CNN will reduce the fidelity and\nnaturalness of the reconstructed facial images, resulting in suboptimal\nreconstructed images. In this work, we propose an efficient CNN-Transformer\nCooperation Network (CTCNet) for face super-resolution tasks, which uses the\nmulti-scale connected encoder-decoder architecture as the backbone.\nSpecifically, we first devise a novel Local-Global Feature Cooperation Module\n(LGCM), which is composed of a Facial Structure Attention Unit (FSAU) and a\nTransformer block, to promote the consistency of local facial detail and global\nfacial structure restoration simultaneously. Then, we design an efficient\nFeature Refinement Module (FRM) to enhance the encoded features. Finally, to\nfurther improve the restoration of fine facial details, we present a\nMulti-scale Feature Fusion Unit (MFFU) to adaptively fuse the features from\ndifferent stages in the encoder procedure. Extensive evaluations on various\ndatasets have assessed that the proposed CTCNet can outperform other\nstate-of-the-art methods significantly. Source code will be available at\nhttps://github.com/IVIPLab/CTCNet.\n","authors":["Guangwei Gao","Zixiang Xu","Juncheng Li","Jian Yang","Tieyong Zeng","Guo-Jun Qi"],"pdf_url":"https://arxiv.org/pdf/2204.08696v3.pdf","comment":"IEEE Transactions on Image Processing, 12 figures, 9 tables"},{"id":"http://arxiv.org/abs/2303.13133v1","updated":"2023-03-23T09:34:17Z","published":"2023-03-23T09:34:17Z","title":"Generative Image Inpainting with Segmentation Confusion Adversarial\n  Training and Contrastive Learning","summary":"  This paper presents a new adversarial training framework for image inpainting\nwith segmentation confusion adversarial training (SCAT) and contrastive\nlearning. SCAT plays an adversarial game between an inpainting generator and a\nsegmentation network, which provides pixel-level local training signals and can\nadapt to images with free-form holes. By combining SCAT with standard global\nadversarial training, the new adversarial training framework exhibits the\nfollowing three advantages simultaneously: (1) the global consistency of the\nrepaired image, (2) the local fine texture details of the repaired image, and\n(3) the flexibility of handling images with free-form holes. Moreover, we\npropose the textural and semantic contrastive learning losses to stabilize and\nimprove our inpainting model's training by exploiting the feature\nrepresentation space of the discriminator, in which the inpainting images are\npulled closer to the ground truth images but pushed farther from the corrupted\nimages. The proposed contrastive losses better guide the repaired images to\nmove from the corrupted image data points to the real image data points in the\nfeature representation space, resulting in more realistic completed images. We\nconduct extensive experiments on two benchmark datasets, demonstrating our\nmodel's effectiveness and superiority both qualitatively and quantitatively.\n","authors":["Zhiwen Zuo","Lei Zhao","Ailin Li","Zhizhong Wang","Zhanjie Zhang","Jiafu Chen","Wei Xing","Dongming Lu"],"pdf_url":"https://arxiv.org/pdf/2303.13133v1.pdf","comment":"Accepted to AAAI2023, Oral"},{"id":"http://arxiv.org/abs/2303.13132v1","updated":"2023-03-23T09:33:44Z","published":"2023-03-23T09:33:44Z","title":"Masked Image Training for Generalizable Deep Image Denoising","summary":"  When capturing and storing images, devices inevitably introduce noise.\nReducing this noise is a critical task called image denoising. Deep learning\nhas become the de facto method for image denoising, especially with the\nemergence of Transformer-based models that have achieved notable\nstate-of-the-art results on various image tasks. However, deep learning-based\nmethods often suffer from a lack of generalization ability. For example, deep\nmodels trained on Gaussian noise may perform poorly when tested on other noise\ndistributions. To address this issue, we present a novel approach to enhance\nthe generalization performance of denoising networks, known as masked training.\nOur method involves masking random pixels of the input image and reconstructing\nthe missing information during training. We also mask out the features in the\nself-attention layers to avoid the impact of training-testing inconsistency.\nOur approach exhibits better generalization ability than other deep learning\nmodels and is directly applicable to real-world scenarios. Additionally, our\ninterpretability analysis demonstrates the superiority of our method.\n","authors":["Haoyu Chen","Jinjin Gu","Yihao Liu","Salma Abdel Magid","Chao Dong","Qiong Wang","Hanspeter Pfister","Lei Zhu"],"pdf_url":"https://arxiv.org/pdf/2303.13132v1.pdf","comment":"Accepted to CVPR 2023"},{"id":"http://arxiv.org/abs/2303.13131v1","updated":"2023-03-23T09:33:10Z","published":"2023-03-23T09:33:10Z","title":"Watch Out for the Confusing Faces: Detecting Face Swapping with the\n  Probability Distribution of Face Identification Models","summary":"  Recently, face swapping has been developing rapidly and achieved a surprising\nreality, raising concerns about fake content. As a countermeasure, various\ndetection approaches have been proposed and achieved promising performance.\nHowever, most existing detectors struggle to maintain performance on unseen\nface swapping methods and low-quality images. Apart from the generalization\nproblem, current detection approaches have been shown vulnerable to evasion\nattacks crafted by detection-aware manipulators. Lack of robustness under\nadversary scenarios leaves threats for applying face swapping detection in real\nworld. In this paper, we propose a novel face swapping detection approach based\non face identification probability distributions, coined as IdP_FSD, to improve\nthe generalization and robustness. IdP_FSD is specially designed for detecting\nswapped faces whose identities belong to a finite set, which is meaningful in\nreal-world applications. Compared with previous general detection methods, we\nmake use of the available real faces with concerned identities and require no\nfake samples for training. IdP_FSD exploits face swapping's common nature that\nthe identity of swapped face combines that of two faces involved in swapping.\nWe reflect this nature with the confusion of a face identification model and\nmeasure the confusion with the maximum value of the output probability\ndistribution. What's more, to defend our detector under adversary scenarios, an\nattention-based finetuning scheme is proposed for the face identification\nmodels used in IdP_FSD. Extensive experiments show that the proposed IdP_FSD\nnot only achieves high detection performance on different benchmark datasets\nand image qualities but also raises the bar for manipulators to evade the\ndetection.\n","authors":["Yuxuan Duan","Xuhong Zhang","Chuer Yu","Zonghui Wang","Shouling Ji","Wenzhi Chen"],"pdf_url":"https://arxiv.org/pdf/2303.13131v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.13129v1","updated":"2023-03-23T09:31:56Z","published":"2023-03-23T09:31:56Z","title":"Task-Oriented Human-Object Interactions Generation with Implicit Neural\n  Representations","summary":"  Digital human motion synthesis is a vibrant research field with applications\nin movies, AR/VR, and video games. Whereas methods were proposed to generate\nnatural and realistic human motions, most only focus on modeling humans and\nlargely ignore object movements. Generating task-oriented human-object\ninteraction motions in simulation is challenging. For different intents of\nusing the objects, humans conduct various motions, which requires the human\nfirst to approach the objects and then make them move consistently with the\nhuman instead of staying still. Also, to deploy in downstream applications, the\nsynthesized motions are desired to be flexible in length, providing options to\npersonalize the predicted motions for various purposes. To this end, we propose\nTOHO: Task-Oriented Human-Object Interactions Generation with Implicit Neural\nRepresentations, which generates full human-object interaction motions to\nconduct specific tasks, given only the task type, the object, and a starting\nhuman status. TOHO generates human-object motions in three steps: 1) it first\nestimates the keyframe poses of conducting a task given the task type and\nobject information; 2) then, it infills the keyframes and generates continuous\nmotions; 3) finally, it applies a compact closed-form object motion estimation\nto generate the object motion. Our method generates continuous motions that are\nparameterized only by the temporal coordinate, which allows for upsampling or\ndownsampling of the sequence to arbitrary frames and adjusting the motion\nspeeds by designing the temporal coordinate vector. We demonstrate the\neffectiveness of our method, both qualitatively and quantitatively. This work\ntakes a step further toward general human-scene interaction simulation.\n","authors":["Quanzhou Li","Jingbo Wang","Chen Change Loy","Bo Dai"],"pdf_url":"https://arxiv.org/pdf/2303.13129v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.13126v1","updated":"2023-03-23T09:30:39Z","published":"2023-03-23T09:30:39Z","title":"MagicFusion: Boosting Text-to-Image Generation Performance by Fusing\n  Diffusion Models","summary":"  The advent of open-source AI communities has produced a cornucopia of\npowerful text-guided diffusion models that are trained on various datasets.\nWhile few explorations have been conducted on ensembling such models to combine\ntheir strengths. In this work, we propose a simple yet effective method called\nSaliency-aware Noise Blending (SNB) that can empower the fused text-guided\ndiffusion models to achieve more controllable generation. Specifically, we\nexperimentally find that the responses of classifier-free guidance are highly\nrelated to the saliency of generated images. Thus we propose to trust different\nmodels in their areas of expertise by blending the predicted noises of two\ndiffusion models in a saliency-aware manner. SNB is training-free and can be\ncompleted within a DDIM sampling process. Additionally, it can automatically\nalign the semantics of two noise spaces without requiring additional\nannotations such as masks. Extensive experiments show the impressive\neffectiveness of SNB in various applications. Project page is available at\nhttps://magicfusion.github.io/.\n","authors":["Jing Zhao","Heliang Zheng","Chaoyue Wang","Long Lan","Wenjing Yang"],"pdf_url":"https://arxiv.org/pdf/2303.13126v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.13123v1","updated":"2023-03-23T09:23:57Z","published":"2023-03-23T09:23:57Z","title":"Laplacian Segmentation Networks: Improved Epistemic Uncertainty from\n  Spatial Aleatoric Uncertainty","summary":"  Out of distribution (OOD) medical images are frequently encountered, e.g.\nbecause of site- or scanner differences, or image corruption. OOD images come\nwith a risk of incorrect image segmentation, potentially negatively affecting\ndownstream diagnoses or treatment. To ensure robustness to such incorrect\nsegmentations, we propose Laplacian Segmentation Networks (LSN) that jointly\nmodel epistemic (model) and aleatoric (data) uncertainty in image segmentation.\nWe capture data uncertainty with a spatially correlated logit distribution. For\nmodel uncertainty, we propose the first Laplace approximation of the weight\nposterior that scales to large neural networks with skip connections that have\nhigh-dimensional outputs. Empirically, we demonstrate that modelling spatial\npixel correlation allows the Laplacian Segmentation Network to successfully\nassign high epistemic uncertainty to out-of-distribution objects appearing\nwithin images.\n","authors":["Kilian Zepf","Selma Wanna","Marco Miani","Juston Moore","Jes Frellsen","Søren Hauberg","Aasa Feragen","Frederik Warburg"],"pdf_url":"https://arxiv.org/pdf/2303.13123v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.13122v1","updated":"2023-03-23T09:23:52Z","published":"2023-03-23T09:23:52Z","title":"Exploring Visual Prompts for Whole Slide Image Classification with\n  Multiple Instance Learning","summary":"  Multiple instance learning (MIL) has emerged as a popular method for\nclassifying histopathology whole slide images (WSIs). However, existing\napproaches typically rely on pre-trained models from large natural image\ndatasets, such as ImageNet, to generate instance features, which can be\nsub-optimal due to the significant differences between natural images and\nhistopathology images that lead to a domain shift. In this paper, we present a\nnovel, simple yet effective method for learning domain-specific knowledge\ntransformation from pre-trained models to histopathology images. Our approach\nentails using a prompt component to assist the pre-trained model in discerning\ndifferences between the pre-trained dataset and the target histopathology\ndataset, resulting in improved performance of MIL models. We validate our\nmethod on two publicly available datasets, Camelyon16 and TCGA-NSCLC. Extensive\nexperimental results demonstrate the significant performance improvement of our\nmethod for different MIL models and backbones. Upon publication of this paper,\nwe will release the source code for our method.\n","authors":["Yi Lin","Zhongchen Zhao","Zhengjie ZHU","Lisheng Wang","Kwang-Ting Cheng","Hao Chen"],"pdf_url":"https://arxiv.org/pdf/2303.13122v1.pdf","comment":"Submitted to MICCAI 2023"},{"id":"http://arxiv.org/abs/2303.13121v1","updated":"2023-03-23T09:23:11Z","published":"2023-03-23T09:23:11Z","title":"DetOFA: Efficient Training of Once-for-All Networks for Object Detection\n  by Using Pre-trained Supernet and Path Filter","summary":"  We address the challenge of training a large supernet for the object\ndetection task, using a relatively small amount of training data. Specifically,\nwe propose an efficient supernet-based neural architecture search (NAS) method\nthat uses transfer learning and search space pruning. First, the supernet is\npre-trained on a classification task, for which large datasets are available.\nSecond, the search space defined by the supernet is pruned by removing\ncandidate models that are predicted to perform poorly. To effectively remove\nthe candidates over a wide range of resource constraints, we particularly\ndesign a performance predictor, called path filter, which can accurately\npredict the relative performance of the models that satisfy similar resource\nconstraints. Hence, supernet training is more focused on the best-performing\ncandidates. Our path filter handles prediction for paths with different\nresource budgets. Compared to once-for-all, our proposed method reduces the\ncomputational cost of the optimal network architecture by 30% and 63%, while\nyielding better accuracy-floating point operations Pareto front (0.85 and 0.45\npoints of improvement on average precision for Pascal VOC and COCO,\nrespectively).\n","authors":["Yuiko Sakuma","Masato Ishii","Takuya Narihira"],"pdf_url":"https://arxiv.org/pdf/2303.13121v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.12274v2","updated":"2023-03-23T09:17:51Z","published":"2023-03-22T02:47:42Z","title":"A Hierarchical Hybrid Learning Framework for Multi-agent Trajectory\n  Prediction","summary":"  Accurate and robust trajectory prediction of neighboring agents is critical\nfor autonomous vehicles traversing in complex scenes. Most methods proposed in\nrecent years are deep learning-based due to their strength in encoding complex\ninteractions. However, unplausible predictions are often generated since they\nrely heavily on past observations and cannot effectively capture the transient\nand contingency interactions from sparse samples. In this paper, we propose a\nhierarchical hybrid framework of deep learning (DL) and reinforcement learning\n(RL) for multi-agent trajectory prediction, to cope with the challenge of\npredicting motions shaped by multi-scale interactions. In the DL stage, the\ntraffic scene is divided into multiple intermediate-scale heterogenous graphs\nbased on which Transformer-style GNNs are adopted to encode heterogenous\ninteractions at intermediate and global levels. In the RL stage, we divide the\ntraffic scene into local sub-scenes utilizing the key future points predicted\nin the DL stage. To emulate the motion planning procedure so as to produce\ntrajectory predictions, a Transformer-based Proximal Policy Optimization (PPO)\nincorporated with a vehicle kinematics model is devised to plan motions under\nthe dominant influence of microscopic interactions. A multi-objective reward is\ndesigned to balance between agent-centric accuracy and scene-wise\ncompatibility. Experimental results show that our proposal matches the\nstate-of-the-arts on the Argoverse forecasting benchmark. It's also revealed by\nthe visualized results that the hierarchical learning framework captures the\nmulti-scale interactions and improves the feasibility and compliance of the\npredicted trajectories.\n","authors":["Yujun Jiao","Mingze Miao","Zhishuai Yin","Chunyuan Lei","Xu Zhu","Linzhen Nie","Bo Tao"],"pdf_url":"https://arxiv.org/pdf/2303.12274v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.12644v2","updated":"2023-03-23T09:17:22Z","published":"2023-03-22T15:26:22Z","title":"Feature-Conditioned Cascaded Video Diffusion Models for Precise\n  Echocardiogram Synthesis","summary":"  Image synthesis is expected to provide value for the translation of machine\nlearning methods into clinical practice. Fundamental problems like model\nrobustness, domain transfer, causal modelling, and operator training become\napproachable through synthetic data. Especially, heavily operator-dependant\nmodalities like Ultrasound imaging require robust frameworks for image and\nvideo generation. So far, video generation has only been possible by providing\ninput data that is as rich as the output data, e.g., image sequence plus\nconditioning in, video out. However, clinical documentation is usually scarce\nand only single images are reported and stored, thus retrospective\npatient-specific analysis or the generation of rich training data becomes\nimpossible with current approaches. In this paper, we extend elucidated\ndiffusion models for video modelling to generate plausible video sequences from\nsingle images and arbitrary conditioning with clinical parameters. We explore\nthis idea within the context of echocardiograms by looking into the variation\nof the Left Ventricle Ejection Fraction, the most essential clinical metric\ngained from these examinations. We use the publicly available EchoNet-Dynamic\ndataset for all our experiments. Our image to sequence approach achieves an\n$R^2$ score of 93%, which is 38 points higher than recently proposed sequence\nto sequence generation methods. Code and models will be available at:\nhttps://github.com/HReynaud/EchoDiffusion.\n","authors":["Hadrien Reynaud","Mengyun Qiao","Mischa Dombrowski","Thomas Day","Reza Razavi","Alberto Gomez","Paul Leeson","Bernhard Kainz"],"pdf_url":"https://arxiv.org/pdf/2303.12644v2.pdf","comment":"Under Review"},{"id":"http://arxiv.org/abs/2303.09165v2","updated":"2023-03-23T09:02:33Z","published":"2023-03-16T09:03:52Z","title":"A New Benchmark: On the Utility of Synthetic Data with Blender for Bare\n  Supervised Learning and Downstream Domain Adaptation","summary":"  Deep learning in computer vision has achieved great success with the price of\nlarge-scale labeled training data. However, exhaustive data annotation is\nimpracticable for each task of all domains of interest, due to high labor costs\nand unguaranteed labeling accuracy. Besides, the uncontrollable data collection\nprocess produces non-IID training and test data, where undesired duplication\nmay exist. All these nuisances may hinder the verification of typical theories\nand exposure to new findings. To circumvent them, an alternative is to generate\nsynthetic data via 3D rendering with domain randomization. We in this work push\nforward along this line by doing profound and extensive research on bare\nsupervised learning and downstream domain adaptation. Specifically, under the\nwell-controlled, IID data setting enabled by 3D rendering, we systematically\nverify the typical, important learning insights, e.g., shortcut learning, and\ndiscover the new laws of various data regimes and network architectures in\ngeneralization. We further investigate the effect of image formation factors on\ngeneralization, e.g., object scale, material texture, illumination, camera\nviewpoint, and background in a 3D scene. Moreover, we use the\nsimulation-to-reality adaptation as a downstream task for comparing the\ntransferability between synthetic and real data when used for pre-training,\nwhich demonstrates that synthetic data pre-training is also promising to\nimprove real test results. Lastly, to promote future research, we develop a new\nlarge-scale synthetic-to-real benchmark for image classification, termed S2RDA,\nwhich provides more significant challenges for transfer from simulation to\nreality. The code and datasets are available at\nhttps://github.com/huitangtang/On_the_Utility_of_Synthetic_Data.\n","authors":["Hui Tang","Kui Jia"],"pdf_url":"https://arxiv.org/pdf/2303.09165v2.pdf","comment":"24 pages, 14 figures, 5 tables, accepted by the IEEE/CVF Conference\n  on Computer Vision and Pattern Recognition (CVPR), 2023. The proposed new\n  synthetic-to-real benchmark S2RDA is available at\n  https://pan.baidu.com/s/1fHHaqrEHbUZLXEg9XKpgSg?pwd=w9wa"},{"id":"http://arxiv.org/abs/2303.13113v1","updated":"2023-03-23T09:00:38Z","published":"2023-03-23T09:00:38Z","title":"Adaptive Regularization for Class-Incremental Learning","summary":"  Class-Incremental Learning updates a deep classifier with new categories\nwhile maintaining the previously observed class accuracy. Regularizing the\nneural network weights is a common method to prevent forgetting previously\nlearned classes while learning novel ones. However, existing regularizers use a\nconstant magnitude throughout the learning sessions, which may not reflect the\nvarying levels of difficulty of the tasks encountered during incremental\nlearning. This study investigates the necessity of adaptive regularization in\nClass-Incremental Learning, which dynamically adjusts the regularization\nstrength according to the complexity of the task at hand. We propose a Bayesian\nOptimization-based approach to automatically determine the optimal\nregularization magnitude for each learning task. Our experiments on two\ndatasets via two regularizers demonstrate the importance of adaptive\nregularization for achieving accurate and less forgetful visual incremental\nlearning.\n","authors":["Elif Ceren Gok","Murat Onur Yildirim","Mert Kilickaya","Joaquin Vanschoren"],"pdf_url":"https://arxiv.org/pdf/2303.13113v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.13111v1","updated":"2023-03-23T08:59:09Z","published":"2023-03-23T08:59:09Z","title":"A Permutable Hybrid Network for Volumetric Medical Image Segmentation","summary":"  The advent of Vision Transformer (ViT) has brought substantial advancements\nin 3D volumetric benchmarks, particularly in 3D medical image segmentation.\nConcurrently, Multi-Layer Perceptron (MLP) networks have regained popularity\namong researchers due to their comparable results to ViT, albeit with the\nexclusion of the heavy self-attention module. This paper introduces a\npermutable hybrid network for volumetric medical image segmentation, named\nPHNet, which exploits the advantages of convolution neural network (CNN) and\nMLP. PHNet addresses the intrinsic isotropy problem of 3D volumetric data by\nutilizing both 2D and 3D CNN to extract local information. Besides, we propose\nan efficient Multi-Layer Permute Perceptron module, named MLPP, which enhances\nthe original MLP by obtaining long-range dependence while retaining positional\ninformation. Extensive experimental results validate that PHNet outperforms the\nstate-of-the-art methods on two public datasets, namely, COVID-19-20 and\nSynapse. Moreover, the ablation study demonstrates the effectiveness of PHNet\nin harnessing the strengths of both CNN and MLP. The code will be accessible to\nthe public upon acceptance.\n","authors":["Yi Lin","Xiao Fang","Dong Zhang","Kwang-Ting Cheng","Hao Chen"],"pdf_url":"https://arxiv.org/pdf/2303.13111v1.pdf","comment":"Submitted to MICCAI 2023"},{"id":"http://arxiv.org/abs/2303.13110v1","updated":"2023-03-23T08:57:11Z","published":"2023-03-23T08:57:11Z","title":"OCELOT: Overlapped Cell on Tissue Dataset for Histopathology","summary":"  Cell detection is a fundamental task in computational pathology that can be\nused for extracting high-level medical information from whole-slide images. For\naccurate cell detection, pathologists often zoom out to understand the\ntissue-level structures and zoom in to classify cells based on their morphology\nand the surrounding context. However, there is a lack of efforts to reflect\nsuch behaviors by pathologists in the cell detection models, mainly due to the\nlack of datasets containing both cell and tissue annotations with overlapping\nregions. To overcome this limitation, we propose and publicly release OCELOT, a\ndataset purposely dedicated to the study of cell-tissue relationships for cell\ndetection in histopathology. OCELOT provides overlapping cell and tissue\nannotations on images acquired from multiple organs. Within this setting, we\nalso propose multi-task learning approaches that benefit from learning both\ncell and tissue tasks simultaneously. When compared against a model trained\nonly for the cell detection task, our proposed approaches improve cell\ndetection performance on 3 datasets: proposed OCELOT, public TIGER, and\ninternal CARP datasets. On the OCELOT test set in particular, we show up to\n6.79 improvement in F1-score. We believe the contributions of this paper,\nincluding the release of the OCELOT dataset at\nhttps://lunit-io.github.io/research/publications/ocelot are a crucial starting\npoint toward the important research direction of incorporating cell-tissue\nrelationships in computation pathology.\n","authors":["Jeongun Ryu","Aaron Valero Puche","JaeWoong Shin","Seonwook Park","Biagio Brattoli","Jinhee Lee","Wonkyung Jung","Soo Ick Cho","Kyunghyun Paeng","Chan-Young Ock","Donggeun Yoo","Sérgio Pereira"],"pdf_url":"https://arxiv.org/pdf/2303.13110v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.04238v3","updated":"2023-03-23T08:49:30Z","published":"2023-03-07T21:03:48Z","title":"Patch of Invisibility: Naturalistic Black-Box Adversarial Attacks on\n  Object Detectors","summary":"  Adversarial attacks on deep-learning models have been receiving increased\nattention in recent years. Work in this area has mostly focused on\ngradient-based techniques, so-called white-box attacks, wherein the attacker\nhas access to the targeted model's internal parameters; such an assumption is\nusually unrealistic in the real world. Some attacks additionally use the entire\npixel space to fool a given model, which is neither practical nor physical\n(i.e., real-world). On the contrary, we propose herein a gradient-free method\nthat uses the learned image manifold of a pretrained generative adversarial\nnetwork (GAN) to generate naturalistic physical adversarial patches for object\ndetectors. We show that our proposed method works both digitally and\nphysically.\n","authors":["Raz Lapid","Moshe Sipper"],"pdf_url":"https://arxiv.org/pdf/2303.04238v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.13102v1","updated":"2023-03-23T08:35:56Z","published":"2023-03-23T08:35:56Z","title":"Keypoint-Guided Optimal Transport","summary":"  Existing Optimal Transport (OT) methods mainly derive the optimal transport\nplan/matching under the criterion of transport cost/distance minimization,\nwhich may cause incorrect matching in some cases. In many applications,\nannotating a few matched keypoints across domains is reasonable or even\neffortless in annotation burden. It is valuable to investigate how to leverage\nthe annotated keypoints to guide the correct matching in OT. In this paper, we\npropose a novel KeyPoint-Guided model by ReLation preservation (KPG-RL) that\nsearches for the optimal matching (i.e., transport plan) guided by the\nkeypoints in OT. To impose the keypoints in OT, first, we propose a mask-based\nconstraint of the transport plan that preserves the matching of keypoint pairs.\nSecond, we propose to preserve the relation of each data point to the keypoints\nto guide the matching. The proposed KPG-RL model can be solved by Sinkhorn's\nalgorithm and is applicable even when distributions are supported in different\nspaces. We further utilize the relation preservation constraint in the\nKantorovich Problem and Gromov-Wasserstein model to impose the guidance of\nkeypoints in them. Meanwhile, the proposed KPG-RL model is extended to the\npartial OT setting. Moreover, we deduce the dual formulation of the KPG-RL\nmodel, which is solved using deep learning techniques. Based on the learned\ntransport plan from dual KPG-RL, we propose a novel manifold barycentric\nprojection to transport source data to the target domain. As applications, we\napply the proposed KPG-RL model to the heterogeneous domain adaptation and\nimage-to-image translation. Experiments verified the effectiveness of the\nproposed approach.\n","authors":["Xiang Gu","Yucheng Yang","Wei Zeng","Jian Sun","Zongben Xu"],"pdf_url":"https://arxiv.org/pdf/2303.13102v1.pdf","comment":"Under review"},{"id":"http://arxiv.org/abs/2303.13101v1","updated":"2023-03-23T08:34:24Z","published":"2023-03-23T08:34:24Z","title":"MMFormer: Multimodal Transformer Using Multiscale Self-Attention for\n  Remote Sensing Image Classification","summary":"  To benefit the complementary information between heterogeneous data, we\nintroduce a new Multimodal Transformer (MMFormer) for Remote Sensing (RS) image\nclassification using Hyperspectral Image (HSI) accompanied by another source of\ndata such as Light Detection and Ranging (LiDAR). Compared with traditional\nVision Transformer (ViT) lacking inductive biases of convolutions, we first\nintroduce convolutional layers to our MMFormer to tokenize patches from\nmultimodal data of HSI and LiDAR. Then we propose a Multi-scale Multi-head\nSelf-Attention (MSMHSA) module to address the problem of compatibility which\noften limits to fuse HSI with high spectral resolution and LiDAR with\nrelatively low spatial resolution. The proposed MSMHSA module can incorporate\nHSI to LiDAR data in a coarse-to-fine manner enabling us to learn a\nfine-grained representation. Extensive experiments on widely used benchmarks\n(e.g., Trento and MUUFL) demonstrate the effectiveness and superiority of our\nproposed MMFormer for RS image classification.\n","authors":["Bo Zhang","Zuheng Ming","Wei Feng","Yaqian Liu","Liang He","Kaixing Zhao"],"pdf_url":"https://arxiv.org/pdf/2303.13101v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.11108v2","updated":"2023-03-23T08:32:29Z","published":"2023-03-20T13:45:58Z","title":"I2Edit: Towards Multi-turn Interactive Image Editing via Dialogue","summary":"  Although there have been considerable research efforts on controllable facial\nimage editing, the desirable interactive setting where the users can interact\nwith the system to adjust their requirements dynamically hasn't been well\nexplored. This paper focuses on facial image editing via dialogue and\nintroduces a new benchmark dataset, Multi-turn Interactive Image Editing\n(I2Edit), for evaluating image editing quality and interaction ability in\nreal-world interactive facial editing scenarios. The dataset is constructed\nupon the CelebA-HQ dataset with images annotated with a multi-turn dialogue\nthat corresponds to the user editing requirements. I2Edit is challenging, as it\nneeds to 1) track the dynamically updated user requirements and edit the images\naccordingly, as well as 2) generate the appropriate natural language response\nto communicate with the user. To address these challenges, we propose a\nframework consisting of a dialogue module and an image editing module. The\nformer is for user edit requirements tracking and generating the corresponding\nindicative responses, while the latter edits the images conditioned on the\ntracked user edit requirements. In contrast to previous works that simply treat\nmulti-turn interaction as a sequence of single-turn interactions, we extract\nthe user edit requirements from the whole dialogue history instead of the\ncurrent single turn. The extracted global user edit requirements enable us to\ndirectly edit the input raw image to avoid error accumulation and attribute\nforgetting issues. Extensive quantitative and qualitative experiments on the\nI2Edit dataset demonstrate the advantage of our proposed framework over the\nprevious single-turn methods. We believe our new dataset could serve as a\nvaluable resource to push forward the exploration of real-world, complex\ninteractive image editing. Code and data will be made public.\n","authors":["Xing Cui","Zekun Li","Peipei Li","Yibo Hu","Hailin Shi","Zhaofeng He"],"pdf_url":"https://arxiv.org/pdf/2303.11108v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.13100v1","updated":"2023-03-23T08:32:10Z","published":"2023-03-23T08:32:10Z","title":"PointGame: Geometrically and Adaptively Masked Auto-Encoder on Point\n  Clouds","summary":"  Self-supervised learning is attracting large attention in point cloud\nunderstanding. However, exploring discriminative and transferable features\nstill remains challenging due to their nature of irregularity and sparsity. We\npropose a geometrically and adaptively masked auto-encoder for self-supervised\nlearning on point clouds, termed \\textit{PointGame}. PointGame contains two\ncore components: GATE and EAT. GATE stands for the geometrical and adaptive\ntoken embedding module; it not only absorbs the conventional wisdom of\ngeometric descriptors that captures the surface shape effectively, but also\nexploits adaptive saliency to focus on the salient part of a point cloud. EAT\nstands for the external attention-based Transformer encoder with linear\ncomputational complexity, which increases the efficiency of the whole pipeline.\nUnlike cutting-edge unsupervised learning models, PointGame leverages geometric\ndescriptors to perceive surface shapes and adaptively mines discriminative\nfeatures from training data. PointGame showcases clear advantages over its\ncompetitors on various downstream tasks under both global and local fine-tuning\nstrategies. The code and pre-trained models will be publicly available.\n","authors":["Yun Liu","Xuefeng Yan","Zhilei Chen","Zhiqi Li","Zeyong Wei","Mingqiang Wei"],"pdf_url":"https://arxiv.org/pdf/2303.13100v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2205.07680v2","updated":"2023-03-23T08:29:12Z","published":"2022-05-16T13:47:02Z","title":"BBDM: Image-to-image Translation with Brownian Bridge Diffusion Models","summary":"  Image-to-image translation is an important and challenging problem in\ncomputer vision and image processing. Diffusion models (DM) have shown great\npotentials for high-quality image synthesis, and have gained competitive\nperformance on the task of image-to-image translation. However, most of the\nexisting diffusion models treat image-to-image translation as conditional\ngeneration processes, and suffer heavily from the gap between distinct domains.\nIn this paper, a novel image-to-image translation method based on the Brownian\nBridge Diffusion Model (BBDM) is proposed, which models image-to-image\ntranslation as a stochastic Brownian bridge process, and learns the translation\nbetween two domains directly through the bidirectional diffusion process rather\nthan a conditional generation process. To the best of our knowledge, it is the\nfirst work that proposes Brownian Bridge diffusion process for image-to-image\ntranslation. Experimental results on various benchmarks demonstrate that the\nproposed BBDM model achieves competitive performance through both visual\ninspection and measurable metrics.\n","authors":["Bo Li","Kaitao Xue","Bin Liu","Yu-Kun Lai"],"pdf_url":"https://arxiv.org/pdf/2205.07680v2.pdf","comment":"18 pages, 13 figures"},{"id":"http://arxiv.org/abs/2303.13097v1","updated":"2023-03-23T08:25:46Z","published":"2023-03-23T08:25:46Z","title":"CP$^3$: Channel Pruning Plug-in for Point-based Networks","summary":"  Channel pruning can effectively reduce both computational cost and memory\nfootprint of the original network while keeping a comparable accuracy\nperformance. Though great success has been achieved in channel pruning for 2D\nimage-based convolutional networks (CNNs), existing works seldom extend the\nchannel pruning methods to 3D point-based neural networks (PNNs). Directly\nimplementing the 2D CNN channel pruning methods to PNNs undermine the\nperformance of PNNs because of the different representations of 2D images and\n3D point clouds as well as the network architecture disparity. In this paper,\nwe proposed CP$^3$, which is a Channel Pruning Plug-in for Point-based network.\nCP$^3$ is elaborately designed to leverage the characteristics of point clouds\nand PNNs in order to enable 2D channel pruning methods for PNNs. Specifically,\nit presents a coordinate-enhanced channel importance metric to reflect the\ncorrelation between dimensional information and individual channel features,\nand it recycles the discarded points in PNN's sampling process and reconsiders\ntheir potentially-exclusive information to enhance the robustness of channel\npruning. Experiments on various PNN architectures show that CP$^3$ constantly\nimproves state-of-the-art 2D CNN pruning approaches on different point cloud\ntasks. For instance, our compressed PointNeXt-S on ScanObjectNN achieves an\naccuracy of 88.52% with a pruning rate of 57.8%, outperforming the baseline\npruning methods with an accuracy gain of 1.94%.\n","authors":["Yaomin Huang","Ning Liu","Zhengping Che","Zhiyuan Xu","Chaomin Shen","Yaxin Peng","Guixu Zhang","Xinmei Liu","Feifei Feng","Jian Tang"],"pdf_url":"https://arxiv.org/pdf/2303.13097v1.pdf","comment":"Yaomin Huang and Ning Liu are with equal contributions. This paper\n  has been accepted by CVPR 2023"},{"id":"http://arxiv.org/abs/2303.13095v1","updated":"2023-03-23T08:21:16Z","published":"2023-03-23T08:21:16Z","title":"Modeling Entities as Semantic Points for Visual Information Extraction\n  in the Wild","summary":"  Recently, Visual Information Extraction (VIE) has been becoming increasingly\nimportant in both the academia and industry, due to the wide range of\nreal-world applications. Previously, numerous works have been proposed to\ntackle this problem. However, the benchmarks used to assess these methods are\nrelatively plain, i.e., scenarios with real-world complexity are not fully\nrepresented in these benchmarks. As the first contribution of this work, we\ncurate and release a new dataset for VIE, in which the document images are much\nmore challenging in that they are taken from real applications, and\ndifficulties such as blur, partial occlusion, and printing shift are quite\ncommon. All these factors may lead to failures in information extraction.\nTherefore, as the second contribution, we explore an alternative approach to\nprecisely and robustly extract key information from document images under such\ntough conditions. Specifically, in contrast to previous methods, which usually\neither incorporate visual information into a multi-modal architecture or train\ntext spotting and information extraction in an end-to-end fashion, we\nexplicitly model entities as semantic points, i.e., center points of entities\nare enriched with semantic information describing the attributes and\nrelationships of different entities, which could largely benefit entity\nlabeling and linking. Extensive experiments on standard benchmarks in this\nfield as well as the proposed dataset demonstrate that the proposed method can\nachieve significantly enhanced performance on entity labeling and linking,\ncompared with previous state-of-the-art models. Dataset is available at\nhttps://www.modelscope.cn/datasets/damo/SIBR/summary.\n","authors":["Zhibo Yang","Rujiao Long","Pengfei Wang","Sibo Song","Humen Zhong","Wenqing Cheng","Xiang Bai","Cong Yao"],"pdf_url":"https://arxiv.org/pdf/2303.13095v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.13090v1","updated":"2023-03-23T08:10:25Z","published":"2023-03-23T08:10:25Z","title":"Orthogonal Annotation Benefits Barely-supervised Medical Image\n  Segmentation","summary":"  Recent trends in semi-supervised learning have significantly boosted the\nperformance of 3D semi-supervised medical image segmentation. Compared with 2D\nimages, 3D medical volumes involve information from different directions, e.g.,\ntransverse, sagittal, and coronal planes, so as to naturally provide\ncomplementary views. These complementary views and the intrinsic similarity\namong adjacent 3D slices inspire us to develop a novel annotation way and its\ncorresponding semi-supervised model for effective segmentation. Specifically,\nwe firstly propose the orthogonal annotation by only labeling two orthogonal\nslices in a labeled volume, which significantly relieves the burden of\nannotation. Then, we perform registration to obtain the initial pseudo labels\nfor sparsely labeled volumes. Subsequently, by introducing unlabeled volumes,\nwe propose a dual-network paradigm named Dense-Sparse Co-training (DeSCO) that\nexploits dense pseudo labels in early stage and sparse labels in later stage\nand meanwhile forces consistent output of two networks. Experimental results on\nthree benchmark datasets validated our effectiveness in performance and\nefficiency in annotation. For example, with only 10 annotated slices, our\nmethod reaches a Dice up to 86.93% on KiTS19 dataset.\n","authors":["Heng Cai","Shumeng Li","Lei Qi","Qian Yu","Yinghuan Shi","Yang Gao"],"pdf_url":"https://arxiv.org/pdf/2303.13090v1.pdf","comment":"Accepted to CVPR 2023"},{"id":"http://arxiv.org/abs/2303.13089v1","updated":"2023-03-23T08:06:10Z","published":"2023-03-23T08:06:10Z","title":"Box-Level Active Detection","summary":"  Active learning selects informative samples for annotation within budget,\nwhich has proven efficient recently on object detection. However, the widely\nused active detection benchmarks conduct image-level evaluation, which is\nunrealistic in human workload estimation and biased towards crowded images.\nFurthermore, existing methods still perform image-level annotation, but equally\nscoring all targets within the same image incurs waste of budget and redundant\nlabels. Having revealed above problems and limitations, we introduce a\nbox-level active detection framework that controls a box-based budget per\ncycle, prioritizes informative targets and avoids redundancy for fair\ncomparison and efficient application.\n  Under the proposed box-level setting, we devise a novel pipeline, namely\nComplementary Pseudo Active Strategy (ComPAS). It exploits both human\nannotations and the model intelligence in a complementary fashion: an efficient\ninput-end committee queries labels for informative objects only; meantime\nwell-learned targets are identified by the model and compensated with\npseudo-labels. ComPAS consistently outperforms 10 competitors under 4 settings\nin a unified codebase. With supervision from labeled data only, it achieves\n100% supervised performance of VOC0712 with merely 19% box annotations. On the\nCOCO dataset, it yields up to 4.3% mAP improvement over the second-best method.\nComPAS also supports training with the unlabeled pool, where it surpasses 90%\nCOCO supervised performance with 85% label reduction. Our source code is\npublicly available at https://github.com/lyumengyao/blad.\n","authors":["Mengyao Lyu","Jundong Zhou","Hui Chen","Yijie Huang","Dongdong Yu","Yaqian Li","Yandong Guo","Yuchen Guo","Liuyu Xiang","Guiguang Ding"],"pdf_url":"https://arxiv.org/pdf/2303.13089v1.pdf","comment":"CVPR 2023 highlight"},{"id":"http://arxiv.org/abs/2211.14068v2","updated":"2023-03-23T08:05:52Z","published":"2022-11-25T12:40:45Z","title":"Fine-Grained Face Swapping via Regional GAN Inversion","summary":"  We present a novel paradigm for high-fidelity face swapping that faithfully\npreserves the desired subtle geometry and texture details. We rethink face\nswapping from the perspective of fine-grained face editing, \\textit{i.e.,\n``editing for swapping'' (E4S)}, and propose a framework that is based on the\nexplicit disentanglement of the shape and texture of facial components.\nFollowing the E4S principle, our framework enables both global and local\nswapping of facial features, as well as controlling the amount of partial\nswapping specified by the user. Furthermore, the E4S paradigm is inherently\ncapable of handling facial occlusions by means of facial masks. At the core of\nour system lies a novel Regional GAN Inversion (RGI) method, which allows the\nexplicit disentanglement of shape and texture. It also allows face swapping to\nbe performed in the latent space of StyleGAN. Specifically, we design a\nmulti-scale mask-guided encoder to project the texture of each facial component\ninto regional style codes. We also design a mask-guided injection module to\nmanipulate the feature maps with the style codes. Based on the disentanglement,\nface swapping is reformulated as a simplified problem of style and mask\nswapping. Extensive experiments and comparisons with current state-of-the-art\nmethods demonstrate the superiority of our approach in preserving texture and\nshape details, as well as working with high resolution images. The project page\nis http://e4s2022.github.io\n","authors":["Zhian Liu","Maomao Li","Yong Zhang","Cairong Wang","Qi Zhang","Jue Wang","Yongwei Nie"],"pdf_url":"https://arxiv.org/pdf/2211.14068v2.pdf","comment":"Accepted to CVPR 2023"},{"id":"http://arxiv.org/abs/2303.13087v1","updated":"2023-03-23T07:58:48Z","published":"2023-03-23T07:58:48Z","title":"Robust Generalization against Photon-Limited Corruptions via Worst-Case\n  Sharpness Minimization","summary":"  Robust generalization aims to tackle the most challenging data distributions\nwhich are rare in the training set and contain severe noises, i.e.,\nphoton-limited corruptions. Common solutions such as distributionally robust\noptimization (DRO) focus on the worst-case empirical risk to ensure low\ntraining error on the uncommon noisy distributions. However, due to the\nover-parameterized model being optimized on scarce worst-case data, DRO fails\nto produce a smooth loss landscape, thus struggling on generalizing well to the\ntest set. Therefore, instead of focusing on the worst-case risk minimization,\nwe propose SharpDRO by penalizing the sharpness of the worst-case distribution,\nwhich measures the loss changes around the neighbor of learning parameters.\nThrough worst-case sharpness minimization, the proposed method successfully\nproduces a flat loss curve on the corrupted distributions, thus achieving\nrobust generalization. Moreover, by considering whether the distribution\nannotation is available, we apply SharpDRO to two problem settings and design a\nworst-case selection process for robust generalization. Theoretically, we show\nthat SharpDRO has a great convergence guarantee. Experimentally, we simulate\nphoton-limited corruptions using CIFAR10/100 and ImageNet30 datasets and show\nthat SharpDRO exhibits a strong generalization ability against severe\ncorruptions and exceeds well-known baseline methods with large performance\ngains.\n","authors":["Zhuo Huang","Miaoxi Zhu","Xiaobo Xia","Li Shen","Jun Yu","Chen Gong","Bo Han","Bo Du","Tongliang Liu"],"pdf_url":"https://arxiv.org/pdf/2303.13087v1.pdf","comment":"CVPR 2023"},{"id":"http://arxiv.org/abs/2303.10848v2","updated":"2023-03-23T07:56:07Z","published":"2023-03-20T03:56:47Z","title":"Weakly-Supervised Text Instance Segmentation","summary":"  Text segmentation is a challenging vision task with many downstream\napplications. Current text segmentation methods require pixel-level\nannotations, which are expensive in the cost of human labor and limited in\napplication scenarios. In this paper, we take the first attempt to perform\nweakly-supervised text instance segmentation by bridging text recognition and\ntext segmentation. The insight is that text recognition methods provide precise\nattention position of each text instance, and the attention location can feed\nto both a text adaptive refinement head (TAR) and a text segmentation head.\nSpecifically, the proposed TAR generates pseudo labels by performing two-stage\niterative refinement operations on the attention location to fit the accurate\nboundaries of the corresponding text instance. Meanwhile, the text segmentation\nhead takes the rough attention location to predict segmentation masks which are\nsupervised by the aforementioned pseudo labels. In addition, we design a\nmask-augmented contrastive learning by treating our segmentation result as an\naugmented version of the input text image, thus improving the visual\nrepresentation and further enhancing the performance of both recognition and\nsegmentation. The experimental results demonstrate that the proposed method\nsignificantly outperforms weakly-supervised instance segmentation methods on\nICDAR13-FST (18.95$\\%$ improvement) and TextSeg (17.80$\\%$ improvement)\nbenchmarks.\n","authors":["Xinyan Zu","Haiyang Yu","Bin Li","Xiangyang Xue"],"pdf_url":"https://arxiv.org/pdf/2303.10848v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2302.01392v2","updated":"2023-03-23T07:15:53Z","published":"2023-02-02T20:06:58Z","title":"Multi-modal Gated Mixture of Local-to-Global Experts for Dynamic Image\n  Fusion","summary":"  Infrared and visible image fusion aims to integrate comprehensive information\nfrom multiple sources to achieve superior performances on various practical\ntasks, such as detection, over that of a single modality. However, most\nexisting methods directly combined the texture details and object contrast of\ndifferent modalities, ignoring the dynamic changes in reality, which diminishes\nthe visible texture in good lighting conditions and the infrared contrast in\nlow lighting conditions. To fill this gap, we propose a dynamic image fusion\nframework with a multi-modal gated mixture of local-to-global experts, termed\nMoE-Fusion, to dynamically extract effective and comprehensive information from\nthe respective modalities. Our model consists of a Mixture of Local Experts\n(MoLE) and a Mixture of Global Experts (MoGE) guided by a multi-modal gate. The\nMoLE performs specialized learning of multi-modal local features, prompting the\nfused images to retain the local information in a sample-adaptive manner, while\nthe MoGE focuses on the global information that complements the fused image\nwith overall texture detail and contrast. Extensive experiments show that our\nMoE-Fusion outperforms state-of-the-art methods in preserving multi-modal image\ntexture and contrast through the local-to-global dynamic learning paradigm, and\nalso achieves superior performance on detection tasks. Our code will be\navailable: https://github.com/SunYM2020/MoE-Fusion.\n","authors":["Yiming Sun","Bing Cao","Pengfei Zhu","Qinghua Hu"],"pdf_url":"https://arxiv.org/pdf/2302.01392v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2111.15077v2","updated":"2023-03-23T07:15:24Z","published":"2021-11-30T02:35:51Z","title":"Unsupervised Domain Generalization for Person Re-identification: A\n  Domain-specific Adaptive Framework","summary":"  Domain generalization (DG) has attracted much attention in person\nre-identification (ReID) recently. It aims to make a model trained on multiple\nsource domains generalize to an unseen target domain. Although achieving\npromising progress, existing methods usually need the source domains to be\nlabeled, which could be a significant burden for practical ReID tasks. In this\npaper, we turn to investigate unsupervised domain generalization for ReID, by\nassuming that no label is available for any source domains.\n  To address this challenging setting, we propose a simple and efficient\ndomain-specific adaptive framework, and realize it with an adaptive\nnormalization module designed upon the batch and instance normalization\ntechniques. In doing so, we successfully yield reliable pseudo-labels to\nimplement training and also enhance the domain generalization capability of the\nmodel as required. In addition, we show that our framework can even be applied\nto improve person ReID under the settings of supervised domain generalization\nand unsupervised domain adaptation, demonstrating competitive performance with\nrespect to relevant methods. Extensive experimental study on benchmark datasets\nis conducted to validate the proposed framework. A significance of our work\nlies in that it shows the potential of unsupervised domain generalization for\nperson ReID and sets a strong baseline for the further research on this topic.\n","authors":["Lei Qi","Jiaqi Liu","Lei Wang","Yinghuan Shi","Xin Geng"],"pdf_url":"https://arxiv.org/pdf/2111.15077v2.pdf","comment":"Accepted to Pattern Recognition (PR)"},{"id":"http://arxiv.org/abs/2303.13077v1","updated":"2023-03-23T07:14:48Z","published":"2023-03-23T07:14:48Z","title":"Improving the Performance of Spiking Neural Networks on Event-based\n  Datasets with Knowledge Transfer","summary":"  Spiking neural networks (SNNs) have rich spatial-temporal dynamics, which are\nsuitable for processing neuromorphic, event-based data. However, event-based\ndatasets are usually less annotated than static datasets used in traditional\ndeep learning. Small data scale makes SNNs prone to overfitting and limits the\nperformance of the SNN. To enhance the generalizability of SNNs on event-based\ndatasets, we propose a knowledge-transfer framework that leverages static\nimages to assist in the training on neuromorphic datasets. Our method proposes\ndomain loss and semantic loss to exploit both domain-invariant and unique\nfeatures of these two domains, providing SNNs with more generalized knowledge\nfor subsequent targeted training on neuromorphic data. Specifically, domain\nloss aligns the feature space and aims to capture common features between\nstatic and event-based images, while semantic loss emphasizes that the\ndifferences between samples from different categories should be as large as\npossible. Experimental results demonstrate that our method outperforms existing\nmethods on all mainstream neuromorphic vision datasets. In particular, we\nachieve significant performance improvement of 2.7\\% and 9.8\\% when using only\n10\\% training data of CIFAR10-DVS and N-Caltech 101 datasets, respectively.\n","authors":["Xiang He","Dongcheng Zhao","Yang Li","Guobin Shen","Qingqun Kong","Yi Zeng"],"pdf_url":"https://arxiv.org/pdf/2303.13077v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.13076v1","updated":"2023-03-23T07:13:57Z","published":"2023-03-23T07:13:57Z","title":"CORA: Adapting CLIP for Open-Vocabulary Detection with Region Prompting\n  and Anchor Pre-Matching","summary":"  Open-vocabulary detection (OVD) is an object detection task aiming at\ndetecting objects from novel categories beyond the base categories on which the\ndetector is trained. Recent OVD methods rely on large-scale visual-language\npre-trained models, such as CLIP, for recognizing novel objects. We identify\nthe two core obstacles that need to be tackled when incorporating these models\ninto detector training: (1) the distribution mismatch that happens when\napplying a VL-model trained on whole images to region recognition tasks; (2)\nthe difficulty of localizing objects of unseen classes. To overcome these\nobstacles, we propose CORA, a DETR-style framework that adapts CLIP for\nOpen-vocabulary detection by Region prompting and Anchor pre-matching. Region\nprompting mitigates the whole-to-region distribution gap by prompting the\nregion features of the CLIP-based region classifier. Anchor pre-matching helps\nlearning generalizable object localization by a class-aware matching mechanism.\nWe evaluate CORA on the COCO OVD benchmark, where we achieve 41.7 AP50 on novel\nclasses, which outperforms the previous SOTA by 2.4 AP50 even without resorting\nto extra training data. When extra training data is available, we train\nCORA$^+$ on both ground-truth base-category annotations and additional pseudo\nbounding box labels computed by CORA. CORA$^+$ achieves 43.1 AP50 on the COCO\nOVD benchmark and 28.1 box APr on the LVIS OVD benchmark.\n","authors":["Xiaoshi Wu","Feng Zhu","Rui Zhao","Hongsheng Li"],"pdf_url":"https://arxiv.org/pdf/2303.13076v1.pdf","comment":"11 pages, 4 figures. Accepted by CVPR 2023"},{"id":"http://arxiv.org/abs/2211.08332v3","updated":"2023-03-23T07:13:25Z","published":"2022-11-15T17:44:05Z","title":"Versatile Diffusion: Text, Images and Variations All in One Diffusion\n  Model","summary":"  Recent advances in diffusion models have set an impressive milestone in many\ngeneration tasks, and trending works such as DALL-E2, Imagen, and Stable\nDiffusion have attracted great interest. Despite the rapid landscape changes,\nrecent new approaches focus on extensions and performance rather than capacity,\nthus requiring separate models for separate tasks. In this work, we expand the\nexisting single-flow diffusion pipeline into a multi-task multimodal network,\ndubbed Versatile Diffusion (VD), that handles multiple flows of text-to-image,\nimage-to-text, and variations in one unified model. The pipeline design of VD\ninstantiates a unified multi-flow diffusion framework, consisting of sharable\nand swappable layer modules that enable the crossmodal generality beyond images\nand text. Through extensive experiments, we demonstrate that VD successfully\nachieves the following: a) VD outperforms the baseline approaches and handles\nall its base tasks with competitive quality; b) VD enables novel extensions\nsuch as disentanglement of style and semantics, dual- and multi-context\nblending, etc.; c) The success of our multi-flow multimodal framework over\nimages and text may inspire further diffusion-based universal AI research. Our\ncode and models are open-sourced at\nhttps://github.com/SHI-Labs/Versatile-Diffusion.\n","authors":["Xingqian Xu","Zhangyang Wang","Eric Zhang","Kai Wang","Humphrey Shi"],"pdf_url":"https://arxiv.org/pdf/2211.08332v3.pdf","comment":"Github link: https://github.com/SHI-Labs/Versatile-Diffusion"},{"id":"http://arxiv.org/abs/2303.00521v2","updated":"2023-03-23T06:57:56Z","published":"2023-03-01T13:52:40Z","title":"Quality-aware Pre-trained Models for Blind Image Quality Assessment","summary":"  Blind image quality assessment (BIQA) aims to automatically evaluate the\nperceived quality of a single image, whose performance has been improved by\ndeep learning-based methods in recent years. However, the paucity of labeled\ndata somewhat restrains deep learning-based BIQA methods from unleashing their\nfull potential. In this paper, we propose to solve the problem by a pretext\ntask customized for BIQA in a self-supervised learning manner, which enables\nlearning representations from orders of magnitude more data. To constrain the\nlearning process, we propose a quality-aware contrastive loss based on a simple\nassumption: the quality of patches from a distorted image should be similar,\nbut vary from patches from the same image with different degradations and\npatches from different images. Further, we improve the existing degradation\nprocess and form a degradation space with the size of roughly $2\\times10^7$.\nAfter pre-trained on ImageNet using our method, models are more sensitive to\nimage quality and perform significantly better on downstream BIQA tasks.\nExperimental results show that our method obtains remarkable improvements on\npopular BIQA datasets.\n","authors":["Kai Zhao","Kun Yuan","Ming Sun","Mading Li","Xing Wen"],"pdf_url":"https://arxiv.org/pdf/2303.00521v2.pdf","comment":"Accepted by CVPR 2023"},{"id":"http://arxiv.org/abs/2303.13071v1","updated":"2023-03-23T06:54:34Z","published":"2023-03-23T06:54:34Z","title":"PanoHead: Geometry-Aware 3D Full-Head Synthesis in 360$^{\\circ}$","summary":"  Synthesis and reconstruction of 3D human head has gained increasing interests\nin computer vision and computer graphics recently. Existing state-of-the-art 3D\ngenerative adversarial networks (GANs) for 3D human head synthesis are either\nlimited to near-frontal views or hard to preserve 3D consistency in large view\nangles. We propose PanoHead, the first 3D-aware generative model that enables\nhigh-quality view-consistent image synthesis of full heads in $360^\\circ$ with\ndiverse appearance and detailed geometry using only in-the-wild unstructured\nimages for training. At its core, we lift up the representation power of recent\n3D GANs and bridge the data alignment gap when training from in-the-wild images\nwith widely distributed views. Specifically, we propose a novel two-stage\nself-adaptive image alignment for robust 3D GAN training. We further introduce\na tri-grid neural volume representation that effectively addresses front-face\nand back-head feature entanglement rooted in the widely-adopted tri-plane\nformulation. Our method instills prior knowledge of 2D image segmentation in\nadversarial learning of 3D neural scene structures, enabling compositable head\nsynthesis in diverse backgrounds. Benefiting from these designs, our method\nsignificantly outperforms previous 3D GANs, generating high-quality 3D heads\nwith accurate geometry and diverse appearances, even with long wavy and afro\nhairstyles, renderable from arbitrary poses. Furthermore, we show that our\nsystem can reconstruct full 3D heads from single input images for personalized\nrealistic 3D avatars.\n","authors":["Sizhe An","Hongyi Xu","Yichun Shi","Guoxian Song","Umit Ogras","Linjie Luo"],"pdf_url":"https://arxiv.org/pdf/2303.13071v1.pdf","comment":"CVPR 2023. Project Page:https://sizhean.github.io/panohead"},{"id":"http://arxiv.org/abs/2303.13069v1","updated":"2023-03-23T06:53:14Z","published":"2023-03-23T06:53:14Z","title":"Human Guided Ground-truth Generation for Realistic Image\n  Super-resolution","summary":"  How to generate the ground-truth (GT) image is a critical issue for training\nrealistic image super-resolution (Real-ISR) models. Existing methods mostly\ntake a set of high-resolution (HR) images as GTs and apply various degradations\nto simulate their low-resolution (LR) counterparts. Though great progress has\nbeen achieved, such an LR-HR pair generation scheme has several limitations.\nFirst, the perceptual quality of HR images may not be high enough, limiting the\nquality of Real-ISR outputs. Second, existing schemes do not consider much\nhuman perception in GT generation, and the trained models tend to produce\nover-smoothed results or unpleasant artifacts. With the above considerations,\nwe propose a human guided GT generation scheme. We first elaborately train\nmultiple image enhancement models to improve the perceptual quality of HR\nimages, and enable one LR image having multiple HR counterparts. Human subjects\nare then involved to annotate the high quality regions among the enhanced HR\nimages as GTs, and label the regions with unpleasant artifacts as negative\nsamples. A human guided GT image dataset with both positive and negative\nsamples is then constructed, and a loss function is proposed to train the\nReal-ISR models. Experiments show that the Real-ISR models trained on our\ndataset can produce perceptually more realistic results with less artifacts.\nDataset and codes can be found at https://github.com/ChrisDud0257/HGGT\n","authors":["Du Chen","Jie Liang","Xindong Zhang","Ming Liu","Hui Zeng","Lei Zhang"],"pdf_url":"https://arxiv.org/pdf/2303.13069v1.pdf","comment":"10 pages. Already accpted by 2023 IEEE/CVF Conference on Computer\n  Vision and Pattern Recognition (CVPR)"},{"id":"http://arxiv.org/abs/2303.11048v2","updated":"2023-03-23T06:21:43Z","published":"2023-03-20T11:59:23Z","title":"Revisiting Transformer for Point Cloud-based 3D Scene Graph Generation","summary":"  In this paper, we propose the semantic graph Transformer (SGT) for 3D scene\ngraph generation. The task aims to parse a cloud point-based scene into a\nsemantic structural graph, with the core challenge of modeling the complex\nglobal structure. Existing methods based on graph convolutional networks (GCNs)\nsuffer from the over-smoothing dilemma and could only propagate information\nfrom limited neighboring nodes. In contrast, our SGT uses Transformer layers as\nthe base building block to allow global information passing, with two types of\nproposed Transformer layers tailored for the 3D scene graph generation task.\nSpecifically, we introduce the graph embedding layer to best utilize the global\ninformation in graph edges while maintaining comparable computation costs.\nAdditionally, we propose the semantic injection layer to leverage categorical\ntext labels and visual object knowledge. We benchmark our SGT on the\nestablished 3DSSG benchmark and achieve a 35.9% absolute improvement in\nrelationship prediction's R@50 and an 80.4% boost on the subset with complex\nscenes over the state-of-the-art. Our analyses further show SGT's superiority\nin the long-tailed and zero-shot scenarios. We will release the code and model.\n","authors":["Changsheng Lv","Mengshi Qi","Xia Li","Zhengyuan Yang","Huadong Ma"],"pdf_url":"https://arxiv.org/pdf/2303.11048v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.13062v1","updated":"2023-03-23T06:17:23Z","published":"2023-03-23T06:17:23Z","title":"SIEDOB: Semantic Image Editing by Disentangling Object and Background","summary":"  Semantic image editing provides users with a flexible tool to modify a given\nimage guided by a corresponding segmentation map. In this task, the features of\nthe foreground objects and the backgrounds are quite different. However, all\nprevious methods handle backgrounds and objects as a whole using a monolithic\nmodel. Consequently, they remain limited in processing content-rich images and\nsuffer from generating unrealistic objects and texture-inconsistent\nbackgrounds. To address this issue, we propose a novel paradigm,\n\\textbf{S}emantic \\textbf{I}mage \\textbf{E}diting by \\textbf{D}isentangling\n\\textbf{O}bject and \\textbf{B}ackground (\\textbf{SIEDOB}), the core idea of\nwhich is to explicitly leverages several heterogeneous subnetworks for objects\nand backgrounds. First, SIEDOB disassembles the edited input into background\nregions and instance-level objects. Then, we feed them into the dedicated\ngenerators. Finally, all synthesized parts are embedded in their original\nlocations and utilize a fusion network to obtain a harmonized result. Moreover,\nto produce high-quality edited images, we propose some innovative designs,\nincluding Semantic-Aware Self-Propagation Module, Boundary-Anchored Patch\nDiscriminator, and Style-Diversity Object Generator, and integrate them into\nSIEDOB. We conduct extensive experiments on Cityscapes and ADE20K-Room datasets\nand exhibit that our method remarkably outperforms the baselines, especially in\nsynthesizing realistic and diverse objects and texture-consistent backgrounds.\n","authors":["Wuyang Luo","Su Yang","Xinjian Zhang","Weishan Zhang"],"pdf_url":"https://arxiv.org/pdf/2303.13062v1.pdf","comment":"CVPR 2023 highlight paper"},{"id":"http://arxiv.org/abs/2303.13060v1","updated":"2023-03-23T06:16:14Z","published":"2023-03-23T06:16:14Z","title":"DiffPattern: Layout Pattern Generation via Discrete Diffusion","summary":"  Deep generative models dominate the existing literature in layout pattern\ngeneration. However, leaving the guarantee of legality to an inexplicable\nneural network could be problematic in several applications. In this paper, we\npropose \\tool{DiffPattern} to generate reliable layout patterns.\n\\tool{DiffPattern} introduces a novel diverse topology generation method via a\ndiscrete diffusion model with compute-efficiently lossless layout pattern\nrepresentation. Then a white-box pattern assessment is utilized to generate\nlegal patterns given desired design rules. Our experiments on several benchmark\nsettings show that \\tool{DiffPattern} significantly outperforms existing\nbaselines and is capable of synthesizing reliable layout patterns.\n","authors":["Zixiao Wang","Yunheng Shen","Wenqian Zhao","Yang Bai","Guojin Chen","Farzan Farnia","Bei Yu"],"pdf_url":"https://arxiv.org/pdf/2303.13060v1.pdf","comment":"DAC2023 Accepted"},{"id":"http://arxiv.org/abs/2303.10610v2","updated":"2023-03-23T06:12:48Z","published":"2023-03-19T09:15:45Z","title":"DiffMIC: Dual-Guidance Diffusion Network for Medical Image\n  Classification","summary":"  Diffusion Probabilistic Models have recently shown remarkable performance in\ngenerative image modeling, attracting significant attention in the computer\nvision community. However, while a substantial amount of diffusion-based\nresearch has focused on generative tasks, few studies have applied diffusion\nmodels to general medical image classification. In this paper, we propose the\nfirst diffusion-based model (named DiffMIC) to address general medical image\nclassification by eliminating unexpected noise and perturbations in medical\nimages and robustly capturing semantic representation. To achieve this goal, we\ndevise a dual conditional guidance strategy that conditions each diffusion step\nwith multiple granularities to improve step-wise regional attention.\nFurthermore, we propose learning the mutual information in each granularity by\nenforcing Maximum-Mean Discrepancy regularization during the diffusion forward\nprocess. We evaluate the effectiveness of our DiffMIC on three medical\nclassification tasks with different image modalities, including placental\nmaturity grading on ultrasound images, skin lesion classification using\ndermatoscopic images, and diabetic retinopathy grading using fundus images. Our\nexperimental results demonstrate that DiffMIC outperforms state-of-the-art\nmethods by a significant margin, indicating the universality and effectiveness\nof the proposed model. Our code will be publicly available at\nhttps://github.com/scott-yjyang/DiffMIC.\n","authors":["Yijun Yang","Huazhu Fu","Angelica I. Aviles-Rivero","Carola-Bibiane Schönlieb","Lei Zhu"],"pdf_url":"https://arxiv.org/pdf/2303.10610v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.00304v3","updated":"2023-03-23T05:59:24Z","published":"2023-03-01T08:00:46Z","title":"Renderable Neural Radiance Map for Visual Navigation","summary":"  We propose a novel type of map for visual navigation, a renderable neural\nradiance map (RNR-Map), which is designed to contain the overall visual\ninformation of a 3D environment. The RNR-Map has a grid form and consists of\nlatent codes at each pixel. These latent codes are embedded from image\nobservations, and can be converted to the neural radiance field which enables\nimage rendering given a camera pose. The recorded latent codes implicitly\ncontain visual information about the environment, which makes the RNR-Map\nvisually descriptive. This visual information in RNR-Map can be a useful\nguideline for visual localization and navigation. We develop localization and\nnavigation frameworks that can effectively utilize the RNR-Map. We evaluate the\nproposed frameworks on camera tracking, visual localization, and image-goal\nnavigation. Experimental results show that the RNR-Map-based localization\nframework can find the target location based on a single query image with fast\nspeed and competitive accuracy compared to other baselines. Also, this\nlocalization framework is robust to environmental changes, and even finds the\nmost visually similar places when a query image from a different environment is\ngiven. The proposed navigation framework outperforms the existing image-goal\nnavigation methods in difficult scenarios, under odometry and actuation noises.\nThe navigation framework shows 65.7% success rate in curved scenarios of the\nNRNS dataset, which is an improvement of 18.6% over the current\nstate-of-the-art. Project page: https://rllab-snu.github.io/projects/RNR-Map/\n","authors":["Obin Kwon","Jeongho Park","Songhwai Oh"],"pdf_url":"https://arxiv.org/pdf/2303.00304v3.pdf","comment":"Preprint version. CVPR 2023 accepted, highlight paper. Project page:\n  https://rllab-snu.github.io/projects/RNR-Map/"},{"id":"http://arxiv.org/abs/2303.13051v1","updated":"2023-03-23T05:53:34Z","published":"2023-03-23T05:53:34Z","title":"Hierarchical Semantic Contrast for Scene-aware Video Anomaly Detection","summary":"  Increasing scene-awareness is a key challenge in video anomaly detection\n(VAD). In this work, we propose a hierarchical semantic contrast (HSC) method\nto learn a scene-aware VAD model from normal videos. We first incorporate\nforeground object and background scene features with high-level semantics by\ntaking advantage of pre-trained video parsing models. Then, building upon the\nautoencoder-based reconstruction framework, we introduce both scene-level and\nobject-level contrastive learning to enforce the encoded latent features to be\ncompact within the same semantic classes while being separable across different\nclasses. This hierarchical semantic contrast strategy helps to deal with the\ndiversity of normal patterns and also increases their discrimination ability.\nMoreover, for the sake of tackling rare normal activities, we design a\nskeleton-based motion augmentation to increase samples and refine the model\nfurther. Extensive experiments on three public datasets and scene-dependent\nmixture datasets validate the effectiveness of our proposed method.\n","authors":["Shengyang Sun","Xiaojin Gong"],"pdf_url":"https://arxiv.org/pdf/2303.13051v1.pdf","comment":"Accepted by CVPR 2023"},{"id":"http://arxiv.org/abs/2210.06096v2","updated":"2023-03-23T05:50:55Z","published":"2022-10-12T11:19:55Z","title":"Masked Motion Encoding for Self-Supervised Video Representation Learning","summary":"  How to learn discriminative video representation from unlabeled videos is\nchallenging but crucial for video analysis. The latest attempts seek to learn a\nrepresentation model by predicting the appearance contents in the masked\nregions. However, simply masking and recovering appearance contents may not be\nsufficient to model temporal clues as the appearance contents can be easily\nreconstructed from a single frame. To overcome this limitation, we present\nMasked Motion Encoding (MME), a new pre-training paradigm that reconstructs\nboth appearance and motion information to explore temporal clues. In MME, we\nfocus on addressing two critical challenges to improve the representation\nperformance: 1) how to well represent the possible long-term motion across\nmultiple frames; and 2) how to obtain fine-grained temporal clues from sparsely\nsampled videos. Motivated by the fact that human is able to recognize an action\nby tracking objects' position changes and shape changes, we propose to\nreconstruct a motion trajectory that represents these two kinds of change in\nthe masked regions. Besides, given the sparse video input, we enforce the model\nto reconstruct dense motion trajectories in both spatial and temporal\ndimensions. Pre-trained with our MME paradigm, the model is able to anticipate\nlong-term and fine-grained motion details. Code is available at\nhttps://github.com/XinyuSun/MME.\n","authors":["Xinyu Sun","Peihao Chen","Liangwei Chen","Changhao Li","Thomas H. Li","Mingkui Tan","Chuang Gan"],"pdf_url":"https://arxiv.org/pdf/2210.06096v2.pdf","comment":"CVPR 2023 camera-ready version"},{"id":"http://arxiv.org/abs/2303.12670v2","updated":"2023-03-23T05:41:37Z","published":"2023-03-22T15:48:23Z","title":"Correlational Image Modeling for Self-Supervised Visual Pre-Training","summary":"  We introduce Correlational Image Modeling (CIM), a novel and surprisingly\neffective approach to self-supervised visual pre-training. Our CIM performs a\nsimple pretext task: we randomly crop image regions (exemplars) from an input\nimage (context) and predict correlation maps between the exemplars and the\ncontext. Three key designs enable correlational image modeling as a nontrivial\nand meaningful self-supervisory task. First, to generate useful\nexemplar-context pairs, we consider cropping image regions with various scales,\nshapes, rotations, and transformations. Second, we employ a bootstrap learning\nframework that involves online and target encoders. During pre-training, the\nformer takes exemplars as inputs while the latter converts the context. Third,\nwe model the output correlation maps via a simple cross-attention block, within\nwhich the context serves as queries and the exemplars offer values and keys. We\nshow that CIM performs on par or better than the current state of the art on\nself-supervised and transfer benchmarks.\n","authors":["Wei Li","Jiahao Xie","Chen Change Loy"],"pdf_url":"https://arxiv.org/pdf/2303.12670v2.pdf","comment":"Accepted by CVPR 2023"},{"id":"http://arxiv.org/abs/2301.01482v3","updated":"2023-03-23T05:34:15Z","published":"2023-01-04T08:22:34Z","title":"Adjust Sample Imbalance and Exclude Similar Object in Underwater Object\n  Tracking","summary":"  Although modern trackers have competitive performance when dealing with\nunderwater image degradation, there are still two problems when applying them\nto Underwater Object Tracking (UOT). On the one hand, the single object tracker\nis trained on the open-air datasets, which means that the tracker has a serious\nsample imbalance between underwater objects and open-air objects when applied\nto UOT. On the other hand, underwater targets such as fish and dolphins usually\nhave a similar appearance, it is challenging for the model itself to\ndiscriminate the weak discriminative features. The existing detection-based\npost processing is hard to distinguish the tracked target among similar\nobjects. In this paper, we propose UOSTrack, which consists of Underwater\nimages and Open-air sequences Hybrid Training (UOHT) and Motion-based Post\nProcessing (MBPP). UOHT is designed to adjust the sample imbalance underwater\ntracker. Specifically, Underwater Object Detection (UOD) image is converted\ninto imag pairs through customized data augmentation, so that the tracker has\nmore underwater domain training samples and learn the feature expression of\nunderwater objects. MBPP is proposed to exclude similar objects around the\ntarget. Specifically, it uses the estimation box predicted by the Kalman Filter\nand candidate boxes in each frame to reconfirm the target that is hidden in the\ncandidate area when the target is lost. UOSTrack has an average performance\nimprovement of 3.5% over OSTrack on Similar Object challenge of the UOT100 and\nUTB180 datasets. The average performance improvement of UOSTrack on UOT100 and\nUTB180 is 1% and 3%, respectively. Experiments on two UOT benchmarks\ndemonstrate the effectiveness of UOHT and MBPP, and the generalization and\napplicability of MBPP for UOT.\n","authors":["Yunfeng Li","Bo Wang","Ye Li","Wei Huo","Zhuoyan Liu"],"pdf_url":"https://arxiv.org/pdf/2301.01482v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.13043v1","updated":"2023-03-23T05:17:05Z","published":"2023-03-23T05:17:05Z","title":"Top-Down Visual Attention from Analysis by Synthesis","summary":"  Current attention algorithms (e.g., self-attention) are stimulus-driven and\nhighlight all the salient objects in an image. However, intelligent agents like\nhumans often guide their attention based on the high-level task at hand,\nfocusing only on task-related objects. This ability of task-guided top-down\nattention provides task-adaptive representation and helps the model generalize\nto various tasks. In this paper, we consider top-down attention from a classic\nAnalysis-by-Synthesis (AbS) perspective of vision. Prior work indicates a\nfunctional equivalence between visual attention and sparse reconstruction; we\nshow that an AbS visual system that optimizes a similar sparse reconstruction\nobjective modulated by a goal-directed top-down signal naturally simulates\ntop-down attention. We further propose Analysis-by-Synthesis Vision Transformer\n(AbSViT), which is a top-down modulated ViT model that variationally\napproximates AbS, and achieves controllable top-down attention. For real-world\napplications, AbSViT consistently improves over baselines on Vision-Language\ntasks such as VQA and zero-shot retrieval where language guides the top-down\nattention. AbSViT can also serve as a general backbone, improving performance\non classification, semantic segmentation, and model robustness.\n","authors":["Baifeng Shi","Trevor Darrell","Xin Wang"],"pdf_url":"https://arxiv.org/pdf/2303.13043v1.pdf","comment":"CVPR2023 highlight; Project page:\n  https://sites.google.com/view/absvit"},{"id":"http://arxiv.org/abs/2303.13040v1","updated":"2023-03-23T05:10:22Z","published":"2023-03-23T05:10:22Z","title":"Open-Vocabulary Object Detection using Pseudo Caption Labels","summary":"  Recent open-vocabulary detection methods aim to detect novel objects by\ndistilling knowledge from vision-language models (VLMs) trained on a vast\namount of image-text pairs. To improve the effectiveness of these methods,\nresearchers have utilized datasets with a large vocabulary that contains a\nlarge number of object classes, under the assumption that such data will enable\nmodels to extract comprehensive knowledge on the relationships between various\nobjects and better generalize to unseen object classes. In this study, we argue\nthat more fine-grained labels are necessary to extract richer knowledge about\nnovel objects, including object attributes and relationships, in addition to\ntheir names. To address this challenge, we propose a simple and effective\nmethod named Pseudo Caption Labeling (PCL), which utilizes an image captioning\nmodel to generate captions that describe object instances from diverse\nperspectives. The resulting pseudo caption labels offer dense samples for\nknowledge distillation. On the LVIS benchmark, our best model trained on the\nde-duplicated VisualGenome dataset achieves an AP of 34.5 and an APr of 30.6,\ncomparable to the state-of-the-art performance. PCL's simplicity and\nflexibility are other notable features, as it is a straightforward\npre-processing technique that can be used with any image captioning model\nwithout imposing any restrictions on model architecture or training process.\n","authors":["Han-Cheol Cho","Won Young Jhoo","Wooyoung Kang","Byungseok Roh"],"pdf_url":"https://arxiv.org/pdf/2303.13040v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.12145v2","updated":"2023-03-23T04:54:28Z","published":"2023-03-21T19:02:36Z","title":"Efficient Feature Distillation for Zero-shot Detection","summary":"  The large-scale vision-language models (e.g., CLIP) are leveraged by\ndifferent methods to detect unseen objects. However, most of these works\nrequire additional captions or images for training, which is not feasible in\nthe context of zero-shot detection. In contrast, the distillation-based method\nis an extra-data-free method, but it has its limitations. Specifically,\nexisting work creates distillation regions that are biased to the base\ncategories, which limits the distillation of novel category information and\nharms the distillation efficiency. Furthermore, directly using the raw feature\nfrom CLIP for distillation neglects the domain gap between the training data of\nCLIP and the detection datasets, which makes it difficult to learn the mapping\nfrom the image region to the vision-language feature space - an essential\ncomponent for detecting unseen objects. As a result, existing\ndistillation-based methods require an excessively long training schedule. To\nsolve these problems, we propose Efficient feature distillation for Zero-Shot\nDetection (EZSD). Firstly, EZSD adapts the CLIP's feature space to the target\ndetection domain by re-normalizing CLIP to bridge the domain gap; Secondly,\nEZSD uses CLIP to generate distillation proposals with potential novel\ninstances, to avoid the distillation being overly biased to the base\ncategories. Finally, EZSD takes advantage of semantic meaning for regression to\nfurther improve the model performance. As a result, EZSD achieves\nstate-of-the-art performance in the COCO zero-shot benchmark with a much\nshorter training schedule and outperforms previous work by 4% in LVIS overall\nsetting with 1/10 training time.\n","authors":["Zhuoming Liu","Xuefeng Hu","Ram Nevatia"],"pdf_url":"https://arxiv.org/pdf/2303.12145v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2202.07291v5","updated":"2023-03-23T04:44:42Z","published":"2022-02-15T10:17:02Z","title":"Exploring Discontinuity for Video Frame Interpolation","summary":"  Video frame interpolation (VFI) is the task that synthesizes the intermediate\nframe given two consecutive frames. Most of the previous studies have focused\non appropriate frame warping operations and refinement modules for the warped\nframes. These studies have been conducted on natural videos containing only\ncontinuous motions. However, many practical videos contain various unnatural\nobjects with discontinuous motions such as logos, user interfaces and\nsubtitles. We propose three techniques to make the existing deep learning-based\nVFI architectures robust to these elements. First is a novel data augmentation\nstrategy called figure-text mixing (FTM) which can make the models learn\ndiscontinuous motions during training stage without any extra dataset. Second,\nwe propose a simple but effective module that predicts a map called\ndiscontinuity map (D-map), which densely distinguishes between areas of\ncontinuous and discontinuous motions. Lastly, we propose loss functions to give\nsupervisions of the discontinuous motion areas which can be applied along with\nFTM and D-map. We additionally collect a special test benchmark called\nGraphical Discontinuous Motion (GDM) dataset consisting of some mobile games\nand chatting videos. Applied to the various state-of-the-art VFI networks, our\nmethod significantly improves the interpolation qualities on the videos from\nnot only GDM dataset, but also the existing benchmarks containing only\ncontinuous motions such as Vimeo90K, UCF101, and DAVIS.\n","authors":["Sangjin Lee","Hyeongmin Lee","Chajin Shin","Hanbin Son","Sangyoun Lee"],"pdf_url":"https://arxiv.org/pdf/2202.07291v5.pdf","comment":"highlight at CVPR 2023 (10% of accepted papers)"},{"id":"http://arxiv.org/abs/2303.13033v1","updated":"2023-03-23T04:41:44Z","published":"2023-03-23T04:41:44Z","title":"Federated Uncertainty-Aware Aggregation for Fundus Diabetic Retinopathy\n  Staging","summary":"  Deep learning models have shown promising performance in the field of\ndiabetic retinopathy (DR) staging. However, collaboratively training a DR\nstaging model across multiple institutions remains a challenge due to non-iid\ndata, client reliability, and confidence evaluation of the prediction. To\naddress these issues, we propose a novel federated uncertainty-aware\naggregation paradigm (FedUAA), which considers the reliability of each client\nand produces a confidence estimation for the DR staging. In our FedUAA, an\naggregated encoder is shared by all clients for learning a global\nrepresentation of fundus images, while a novel temperature-warmed uncertainty\nhead (TWEU) is utilized for each client for local personalized staging\ncriteria. Our TWEU employs an evidential deep layer to produce the uncertainty\nscore with the DR staging results for client reliability evaluation.\nFurthermore, we developed a novel uncertainty-aware weighting module (UAW) to\ndynamically adjust the weights of model aggregation based on the uncertainty\nscore distribution of each client. In our experiments, we collect five publicly\navailable datasets from different institutions to conduct a dataset for\nfederated DR staging to satisfy the real non-iid condition. The experimental\nresults demonstrate that our FedUAA achieves better DR staging performance with\nhigher reliability compared to other federated learning methods. Our proposed\nFedUAA paradigm effectively addresses the challenges of collaboratively\ntraining DR staging models across multiple institutions, and provides a robust\nand reliable solution for the deployment of DR diagnosis models in real-world\nclinical scenarios.\n","authors":["Meng Wang","Lianyu Wang","Xinxing Xu","Ke Zou","Yiming Qian","Rick Siow Mong Goh","Yong Liu","Huazhu Fu"],"pdf_url":"https://arxiv.org/pdf/2303.13033v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.13031v1","updated":"2023-03-23T04:40:33Z","published":"2023-03-23T04:40:33Z","title":"Learning a Practical SDR-to-HDRTV Up-conversion using New Dataset and\n  Degradation Models","summary":"  In media industry, the demand of SDR-to-HDRTV up-conversion arises when users\npossess HDR-WCG (high dynamic range-wide color gamut) TVs while most\noff-the-shelf footage is still in SDR (standard dynamic range). The research\ncommunity has started tackling this low-level vision task by learning-based\napproaches. When applied to real SDR, yet, current methods tend to produce dim\nand desaturated result, making nearly no improvement on viewing experience.\nDifferent from other network-oriented methods, we attribute such deficiency to\ntraining set (HDR-SDR pair). Consequently, we propose new HDRTV dataset (dubbed\nHDRTV4K) and new HDR-to-SDR degradation models. Then, it's used to train a\nluminance-segmented network (LSN) consisting of a global mapping trunk, and two\nTransformer branches on bright and dark luminance range. We also update\nassessment criteria by tailored metrics and subjective experiment. Finally,\nablation studies are conducted to prove the effectiveness. Our work is\navailable at: https://github.com/AndreGuo/HDRTVDM.\n","authors":["Cheng Guo","Leidong Fan","Ziyu Xue","and Xiuhua Jiang"],"pdf_url":"https://arxiv.org/pdf/2303.13031v1.pdf","comment":"Accepted by CVPR2023"},{"id":"http://arxiv.org/abs/2209.01540v3","updated":"2023-03-23T04:34:40Z","published":"2022-09-04T06:30:32Z","title":"An Empirical Study of End-to-End Video-Language Transformers with Masked\n  Visual Modeling","summary":"  Masked visual modeling (MVM) has been recently proven effective for visual\npre-training. While similar reconstructive objectives on video inputs (e.g.,\nmasked frame modeling) have been explored in video-language (VidL)\npre-training, previous studies fail to find a truly effective MVM strategy that\ncan largely benefit the downstream performance. In this work, we systematically\nexamine the potential of MVM in the context of VidL learning. Specifically, we\nbase our study on a fully end-to-end VIdeO-LanguagE Transformer (VIOLET), where\nthe supervision from MVM training can be backpropagated to the video pixel\nspace. In total, eight different reconstructive targets of MVM are explored,\nfrom low-level pixel values and oriented gradients to high-level depth maps,\noptical flow, discrete visual tokens, and latent visual features. We conduct\ncomprehensive experiments and provide insights into the factors leading to\neffective MVM training, resulting in an enhanced model VIOLETv2. Empirically,\nwe show VIOLETv2 pre-trained with MVM objective achieves notable improvements\non 13 VidL benchmarks, ranging from video question answering, video captioning,\nto text-to-video retrieval.\n","authors":["Tsu-Jui Fu","Linjie Li","Zhe Gan","Kevin Lin","William Yang Wang","Lijuan Wang","Zicheng Liu"],"pdf_url":"https://arxiv.org/pdf/2209.01540v3.pdf","comment":"CVPR'23; the first two authors contributed equally; code is available\n  at https://github.com/tsujuifu/pytorch_empirical-mvm"}],"Information Retrieval":[{"id":"http://arxiv.org/abs/2303.13484v1","updated":"2023-03-23T17:50:44Z","published":"2023-03-23T17:50:44Z","title":"Why and When: Understanding System Initiative during Conversational\n  Collaborative Search","summary":"  In the last decade, conversational search has attracted considerable\nattention. However, most research has focused on systems that can support a\n\\emph{single} searcher. In this paper, we explore how systems can support\n\\emph{multiple} searchers collaborating over an instant messaging platform\n(i.e., Slack). We present a ``Wizard of Oz'' study in which 27 participant\npairs collaborated on three information-seeking tasks over Slack. Participants\nwere unable to search on their own and had to gather information by interacting\nwith a \\emph{searchbot} directly from the Slack channel. The role of the\nsearchbot was played by a reference librarian. Conversational search systems\nmust be capable of engaging in \\emph{mixed-initiative} interaction by taking\nand relinquishing control of the conversation to fulfill different objectives.\nDiscourse analysis research suggests that conversational agents can take\n\\emph{two} levels of initiative: dialog- and task-level initiative. Agents take\ndialog-level initiative to establish mutual belief between agents and\ntask-level initiative to influence the goals of the other agents. During the\nstudy, participants were exposed to three experimental conditions in which the\nsearchbot could take different levels of initiative: (1) no initiative, (2)\nonly dialog-level initiative, and (3) both dialog- and task-level initiative.\nIn this paper, we focus on understanding the Wizard's actions. Specifically, we\nfocus on understanding the Wizard's motivations for taking initiative and their\nrationale for the timing of each intervention. To gain insights about the\nWizard's actions, we conducted a stimulated recall interview with the Wizard.\nWe present findings from a qualitative analysis of this interview data and\ndiscuss implications for designing conversational search systems to support\ncollaborative search.\n","authors":["Sandeep Avula","Bogeum Choi","Jaime Arguello"],"pdf_url":"https://arxiv.org/pdf/2303.13484v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.13419v1","updated":"2023-03-23T16:40:00Z","published":"2023-03-23T16:40:00Z","title":"Modular Retrieval for Generalization and Interpretation","summary":"  New retrieval tasks have always been emerging, thus urging the development of\nnew retrieval models. However, instantiating a retrieval model for each new\nretrieval task is resource-intensive and time-consuming, especially for a\nretrieval model that employs a large-scale pre-trained language model. To\naddress this issue, we shift to a novel retrieval paradigm called modular\nretrieval, which aims to solve new retrieval tasks by instead composing\nmultiple existing retrieval modules. Built upon the paradigm, we propose a\nretrieval model with modular prompt tuning named REMOP. It constructs retrieval\nmodules subject to task attributes with deep prompt tuning, and yields\nretrieval models subject to tasks with module composition. We validate that,\nREMOP inherently with modularity not only has appealing generalizability and\ninterpretability in preliminary explorations, but also achieves comparable\nperformance to state-of-the-art retrieval models on a zero-shot retrieval\nbenchmark.\\footnote{Our code is available at\n\\url{https://github.com/FreedomIntelligence/REMOP}}\n","authors":["Juhao Liang","Chen Zhang","Zhengyang Tang","Jie Fu","Dawei Song","Benyou Wang"],"pdf_url":"https://arxiv.org/pdf/2303.13419v1.pdf","comment":"preprint"},{"id":"http://arxiv.org/abs/2303.13416v1","updated":"2023-03-23T16:38:18Z","published":"2023-03-23T16:38:18Z","title":"A Unified Framework for Learned Sparse Retrieval","summary":"  Learned sparse retrieval (LSR) is a family of first-stage retrieval methods\nthat are trained to generate sparse lexical representations of queries and\ndocuments for use with an inverted index. Many LSR methods have been recently\nintroduced, with Splade models achieving state-of-the-art performance on\nMSMarco. Despite similarities in their model architectures, many LSR methods\nshow substantial differences in effectiveness and efficiency. Differences in\nthe experimental setups and configurations used make it difficult to compare\nthe methods and derive insights. In this work, we analyze existing LSR methods\nand identify key components to establish an LSR framework that unifies all LSR\nmethods under the same perspective. We then reproduce all prominent methods\nusing a common codebase and re-train them in the same environment, which allows\nus to quantify how components of the framework affect effectiveness and\nefficiency. We find that (1) including document term weighting is most\nimportant for a method's effectiveness, (2) including query weighting has a\nsmall positive impact, and (3) document expansion and query expansion have a\ncancellation effect. As a result, we show how removing query expansion from a\nstate-of-the-art model can reduce latency significantly while maintaining\neffectiveness on MSMarco and TripClick benchmarks. Our code is publicly\navailable at https://github.com/thongnt99/learned-sparse-retrieval\n","authors":["Thong Nguyen","Sean MacAvaney","Andrew Yates"],"pdf_url":"https://arxiv.org/pdf/2303.13416v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.13284v1","updated":"2023-03-23T14:06:26Z","published":"2023-03-23T14:06:26Z","title":"GETT-QA: Graph Embedding based T2T Transformer for Knowledge Graph\n  Question Answering","summary":"  In this work, we present an end-to-end Knowledge Graph Question Answering\n(KGQA) system named GETT-QA. GETT-QA uses T5, a popular text-to-text\npre-trained language model. The model takes a question in natural language as\ninput and produces a simpler form of the intended SPARQL query. In the simpler\nform, the model does not directly produce entity and relation IDs. Instead, it\nproduces corresponding entity and relation labels. The labels are grounded to\nKG entity and relation IDs in a subsequent step. To further improve the\nresults, we instruct the model to produce a truncated version of the KG\nembedding for each entity. The truncated KG embedding enables a finer search\nfor disambiguation purposes. We find that T5 is able to learn the truncated KG\nembeddings without any change of loss function, improving KGQA performance. As\na result, we report strong results for LC-QuAD 2.0 and SimpleQuestions-Wikidata\ndatasets on end-to-end KGQA over Wikidata.\n","authors":["Debayan Banerjee","Pranav Ajit Nair","Ricardo Usbeck","Chris Biemann"],"pdf_url":"https://arxiv.org/pdf/2303.13284v1.pdf","comment":"16 pages single column format accepted at ESWC 2023 research track"},{"id":"http://arxiv.org/abs/2303.13220v1","updated":"2023-03-23T12:34:30Z","published":"2023-03-23T12:34:30Z","title":"Parameter-Efficient Sparse Retrievers and Rerankers using Adapters","summary":"  Parameter-Efficient transfer learning with Adapters have been studied in\nNatural Language Processing (NLP) as an alternative to full fine-tuning.\nAdapters are memory-efficient and scale well with downstream tasks by training\nsmall bottle-neck layers added between transformer layers while keeping the\nlarge pretrained language model (PLMs) frozen. In spite of showing promising\nresults in NLP, these methods are under-explored in Information Retrieval.\nWhile previous studies have only experimented with dense retriever or in a\ncross lingual retrieval scenario, in this paper we aim to complete the picture\non the use of adapters in IR. First, we study adapters for SPLADE, a sparse\nretriever, for which adapters not only retain the efficiency and effectiveness\notherwise achieved by finetuning, but are memory-efficient and orders of\nmagnitude lighter to train. We observe that Adapters-SPLADE not only optimizes\njust 2\\% of training parameters, but outperforms fully fine-tuned counterpart\nand existing parameter-efficient dense IR models on IR benchmark datasets.\nSecondly, we address domain adaptation of neural retrieval thanks to adapters\non cross-domain BEIR datasets and TripClick. Finally, we also consider\nknowledge sharing between rerankers and first stage rankers. Overall, our study\ncomplete the examination of adapters for neural IR\n","authors":["Vaishali Pal","Carlos Lassance","Hervé Déjean","Stéphane Clinchant"],"pdf_url":"https://arxiv.org/pdf/2303.13220v1.pdf","comment":"accepted at ECIR'23"},{"id":"http://arxiv.org/abs/2303.13091v1","updated":"2023-03-23T08:11:42Z","published":"2023-03-23T08:11:42Z","title":"Limits of Predictability in Top-N Recommendation","summary":"  Top-N recommendation aims to recommend each consumer a small set of N items\nfrom a large collection of items, and its accuracy is one of the most common\nindexes to evaluate the performance of a recommendation system. While a large\nnumber of algorithms are proposed to push the Top-N accuracy by learning the\nuser preference from their history purchase data, a predictability question is\nnaturally raised - whether there is an upper limit of such Top-N accuracy. This\nwork investigates such predictability by studying the degree of regularity from\na specific set of user behavior data. Quantifying the predictability of Top-N\nrecommendations requires simultaneously quantifying the limits on the accuracy\nof the N behaviors with the highest probability. This greatly increases the\ndifficulty of the problem. To achieve this, we firstly excavate the\nassociations among N behaviors with the highest probability and describe the\nuser behavior distribution based on the information theory. Then, we adopt the\nFano inequality to scale and obtain the Top-N predictability. Extensive\nexperiments are conducted on the real-world data where significant improvements\nare observed compared to the state-of-the-art methods. We have not only\ncompleted the predictability calculation for N targets but also obtained\npredictability that is much closer to the true value than existing methods. We\nexpect our results to assist these research areas where the quantitative\nrequirement of Top-N predictability is required.\n","authors":["En Xu","Zhiwen Yu","Ying Zhang","Bin Guo","Lina Yao"],"pdf_url":"https://arxiv.org/pdf/2303.13091v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.12973v1","updated":"2023-03-23T00:42:48Z","published":"2023-03-23T00:42:48Z","title":"Uncertainty Calibration for Counterfactual Propensity Estimation in\n  Recommendation","summary":"  In recommendation systems, a large portion of the ratings are missing due to\nthe selection biases, which is known as Missing Not At Random. The\ncounterfactual inverse propensity scoring (IPS) was used to weight the\nimputation error of every observed rating. Although effective in multiple\nscenarios, we argue that the performance of IPS estimation is limited due to\nthe uncertainty miscalibration of propensity estimation. In this paper, we\npropose the uncertainty calibration for the propensity estimation in\nrecommendation systems with multiple representative uncertainty calibration\ntechniques. Theoretical analysis on the bias and generalization bound shows the\nsuperiority of the calibrated IPS estimator over the uncalibrated one.\nExperimental results on the coat and yahoo datasets shows that the uncertainty\ncalibration is improved and hence brings the better recommendation results.\n","authors":["Wenbo Hu","Xin Sun","Qiang liu","Shu Wu"],"pdf_url":"https://arxiv.org/pdf/2303.12973v1.pdf","comment":null}],"Machine Learning":[{"id":"http://arxiv.org/abs/2303.13519v1","updated":"2023-03-23T17:59:54Z","published":"2023-03-23T17:59:54Z","title":"Learning and Verification of Task Structure in Instructional Videos","summary":"  Given the enormous number of instructional videos available online, learning\na diverse array of multi-step task models from videos is an appealing goal. We\nintroduce a new pre-trained video model, VideoTaskformer, focused on\nrepresenting the semantics and structure of instructional videos. We pre-train\nVideoTaskformer using a simple and effective objective: predicting weakly\nsupervised textual labels for steps that are randomly masked out from an\ninstructional video (masked step modeling). Compared to prior work which learns\nstep representations locally, our approach involves learning them globally,\nleveraging video of the entire surrounding task as context. From these learned\nrepresentations, we can verify if an unseen video correctly executes a given\ntask, as well as forecast which steps are likely to be taken after a given\nstep. We introduce two new benchmarks for detecting mistakes in instructional\nvideos, to verify if there is an anomalous step and if steps are executed in\nthe right order. We also introduce a long-term forecasting benchmark, where the\ngoal is to predict long-range future steps from a given step. Our method\noutperforms previous baselines on these tasks, and we believe the tasks will be\na valuable way for the community to measure the quality of step\nrepresentations. Additionally, we evaluate VideoTaskformer on 3 existing\nbenchmarks -- procedural activity recognition, step classification, and step\nforecasting -- and demonstrate on each that our method outperforms existing\nbaselines and achieves new state-of-the-art performance.\n","authors":["Medhini Narasimhan","Licheng Yu","Sean Bell","Ning Zhang","Trevor Darrell"],"pdf_url":"https://arxiv.org/pdf/2303.13519v1.pdf","comment":"Wesbite at https://medhini.github.io/task_structure"},{"id":"http://arxiv.org/abs/2303.13518v1","updated":"2023-03-23T17:59:53Z","published":"2023-03-23T17:59:53Z","title":"Three ways to improve feature alignment for open vocabulary detection","summary":"  The core problem in zero-shot open vocabulary detection is how to align\nvisual and text features, so that the detector performs well on unseen classes.\nPrevious approaches train the feature pyramid and detection head from scratch,\nwhich breaks the vision-text feature alignment established during pretraining,\nand struggles to prevent the language model from forgetting unseen classes.\n  We propose three methods to alleviate these issues. Firstly, a simple scheme\nis used to augment the text embeddings which prevents overfitting to a small\nnumber of classes seen during training, while simultaneously saving memory and\ncomputation. Secondly, the feature pyramid network and the detection head are\nmodified to include trainable gated shortcuts, which encourages vision-text\nfeature alignment and guarantees it at the start of detection training.\nFinally, a self-training approach is used to leverage a larger corpus of\nimage-text pairs thus improving detection performance on classes with no human\nannotated bounding boxes.\n  Our three methods are evaluated on the zero-shot version of the LVIS\nbenchmark, each of them showing clear and significant benefits. Our final\nnetwork achieves the new stateof-the-art on the mAP-all metric and demonstrates\ncompetitive performance for mAP-rare, as well as superior transfer to COCO and\nObjects365.\n","authors":["Relja Arandjelović","Alex Andonian","Arthur Mensch","Olivier J. Hénaff","Jean-Baptiste Alayrac","Andrew Zisserman"],"pdf_url":"https://arxiv.org/pdf/2303.13518v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.13516v1","updated":"2023-03-23T17:59:42Z","published":"2023-03-23T17:59:42Z","title":"Ablating Concepts in Text-to-Image Diffusion Models","summary":"  Large-scale text-to-image diffusion models can generate high-fidelity images\nwith powerful compositional ability. However, these models are typically\ntrained on an enormous amount of Internet data, often containing copyrighted\nmaterial, licensed images, and personal photos. Furthermore, they have been\nfound to replicate the style of various living artists or memorize exact\ntraining samples. How can we remove such copyrighted concepts or images without\nretraining the model from scratch? To achieve this goal, we propose an\nefficient method of ablating concepts in the pretrained model, i.e., preventing\nthe generation of a target concept. Our algorithm learns to match the image\ndistribution for a target style, instance, or text prompt we wish to ablate to\nthe distribution corresponding to an anchor concept. This prevents the model\nfrom generating target concepts given its text condition. Extensive experiments\nshow that our method can successfully prevent the generation of the ablated\nconcept while preserving closely related concepts in the model.\n","authors":["Nupur Kumari","Bingliang Zhang","Sheng-Yu Wang","Eli Shechtman","Richard Zhang","Jun-Yan Zhu"],"pdf_url":"https://arxiv.org/pdf/2303.13516v1.pdf","comment":"project website: https://www.cs.cmu.edu/~concept-ablation/"},{"id":"http://arxiv.org/abs/2303.13515v1","updated":"2023-03-23T17:59:40Z","published":"2023-03-23T17:59:40Z","title":"Persistent Nature: A Generative Model of Unbounded 3D Worlds","summary":"  Despite increasingly realistic image quality, recent 3D image generative\nmodels often operate on 3D volumes of fixed extent with limited camera motions.\nWe investigate the task of unconditionally synthesizing unbounded nature\nscenes, enabling arbitrarily large camera motion while maintaining a persistent\n3D world model. Our scene representation consists of an extendable, planar\nscene layout grid, which can be rendered from arbitrary camera poses via a 3D\ndecoder and volume rendering, and a panoramic skydome. Based on this\nrepresentation, we learn a generative world model solely from single-view\ninternet photos. Our method enables simulating long flights through 3D\nlandscapes, while maintaining global scene consistency--for instance, returning\nto the starting point yields the same view of the scene. Our approach enables\nscene extrapolation beyond the fixed bounds of current 3D generative models,\nwhile also supporting a persistent, camera-independent world representation\nthat stands in contrast to auto-regressive 3D prediction models. Our project\npage: https://chail.github.io/persistent-nature/.\n","authors":["Lucy Chai","Richard Tucker","Zhengqi Li","Phillip Isola","Noah Snavely"],"pdf_url":"https://arxiv.org/pdf/2303.13515v1.pdf","comment":"CVPR camera ready version, project page:\n  https://chail.github.io/persistent-nature/"},{"id":"http://arxiv.org/abs/2303.13511v1","updated":"2023-03-23T17:59:10Z","published":"2023-03-23T17:59:10Z","title":"Neural Preset for Color Style Transfer","summary":"  In this paper, we present a Neural Preset technique to address the\nlimitations of existing color style transfer methods, including visual\nartifacts, vast memory requirement, and slow style switching speed. Our method\nis based on two core designs. First, we propose Deterministic Neural Color\nMapping (DNCM) to consistently operate on each pixel via an image-adaptive\ncolor mapping matrix, avoiding artifacts and supporting high-resolution inputs\nwith a small memory footprint. Second, we develop a two-stage pipeline by\ndividing the task into color normalization and stylization, which allows\nefficient style switching by extracting color styles as presets and reusing\nthem on normalized input images. Due to the unavailability of pairwise\ndatasets, we describe how to train Neural Preset via a self-supervised\nstrategy. Various advantages of Neural Preset over existing methods are\ndemonstrated through comprehensive evaluations. Besides, we show that our\ntrained model can naturally support multiple applications without fine-tuning,\nincluding low-light image enhancement, underwater image correction, image\ndehazing, and image harmonization.\n","authors":["Zhanghan Ke","Yuhao Liu","Lei Zhu","Nanxuan Zhao","Rynson W. H. Lau"],"pdf_url":"https://arxiv.org/pdf/2303.13511v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.13506v1","updated":"2023-03-23T17:58:43Z","published":"2023-03-23T17:58:43Z","title":"The Quantization Model of Neural Scaling","summary":"  We propose the $\\textit{Quantization Model}$ of neural scaling laws,\nexplaining both the observed power law dropoff of loss with model and data\nsize, and also the sudden emergence of new capabilities with scale. We derive\nthis model from what we call the $\\textit{Quantization Hypothesis}$, where\nlearned network capabilities are quantized into discrete chunks\n($\\textit{quanta}$). We show that when quanta are learned in order of\ndecreasing use frequency, then a power law in use frequencies explains observed\npower law scaling of loss. We validate this prediction on toy datasets, then\nstudy how scaling curves decompose for large language models. Using language\nmodel internals, we auto-discover diverse model capabilities (quanta) and find\ntentative evidence that the distribution over corresponding subproblems in the\nprediction of natural text is compatible with the power law predicted from the\nneural scaling exponent as predicted from our theory.\n","authors":["Eric J. Michaud","Ziming Liu","Uzay Girit","Max Tegmark"],"pdf_url":"https://arxiv.org/pdf/2303.13506v1.pdf","comment":"24 pages, 15 figures"},{"id":"http://arxiv.org/abs/2303.13501v1","updated":"2023-03-23T17:57:28Z","published":"2023-03-23T17:57:28Z","title":"Chordal Averaging on Flag Manifolds and Its Applications","summary":"  This paper presents a new, provably-convergent algorithm for computing the\nflag-mean and flag-median of a set of points on a flag manifold under the\nchordal metric. The flag manifold is a mathematical space consisting of flags,\nwhich are sequences of nested subspaces of a vector space that increase in\ndimension. The flag manifold is a superset of a wide range of known matrix\ngroups, including Stiefel and Grassmanians, making it a general object that is\nuseful in a wide variety computer vision problems.\n  To tackle the challenge of computing first order flag statistics, we first\ntransform the problem into one that involves auxiliary variables constrained to\nthe Stiefel manifold. The Stiefel manifold is a space of orthogonal frames, and\nleveraging the numerical stability and efficiency of Stiefel-manifold\noptimization enables us to compute the flag-mean effectively. Through a series\nof experiments, we show the competence of our method in Grassmann and rotation\naveraging, as well as principal component analysis.\n","authors":["Nathan Mankovich","Tolga Birdal"],"pdf_url":"https://arxiv.org/pdf/2303.13501v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.13500v1","updated":"2023-03-23T17:57:09Z","published":"2023-03-23T17:57:09Z","title":"A Closer Look at Model Adaptation using Feature Distortion and\n  Simplicity Bias","summary":"  Advances in the expressivity of pretrained models have increased interest in\nthe design of adaptation protocols which enable safe and effective transfer\nlearning. Going beyond conventional linear probing (LP) and fine tuning (FT)\nstrategies, protocols that can effectively control feature distortion, i.e.,\nthe failure to update features orthogonal to the in-distribution, have been\nfound to achieve improved out-of-distribution generalization (OOD). In order to\nlimit this distortion, the LP+FT protocol, which first learns a linear probe\nand then uses this initialization for subsequent FT, was proposed. However, in\nthis paper, we find when adaptation protocols (LP, FT, LP+FT) are also\nevaluated on a variety of safety objectives (e.g., calibration, robustness,\netc.), a complementary perspective to feature distortion is helpful to explain\nprotocol behavior. To this end, we study the susceptibility of protocols to\nsimplicity bias (SB), i.e. the well-known propensity of deep neural networks to\nrely upon simple features, as SB has recently been shown to underlie several\nproblems in robust generalization. Using a synthetic dataset, we demonstrate\nthe susceptibility of existing protocols to SB. Given the strong effectiveness\nof LP+FT, we then propose modified linear probes that help mitigate SB, and\nlead to better initializations for subsequent FT. We verify the effectiveness\nof the proposed LP+FT variants for decreasing SB in a controlled setting, and\ntheir ability to improve OOD generalization and safety on three adaptation\ndatasets.\n","authors":["Puja Trivedi","Danai Koutra","Jayaraman J. Thiagarajan"],"pdf_url":"https://arxiv.org/pdf/2303.13500v1.pdf","comment":"Accepted to ICLR 2023 as notable-25% (spotlight)"},{"id":"http://arxiv.org/abs/2208.05845v2","updated":"2023-03-23T17:56:52Z","published":"2022-08-11T14:28:21Z","title":"A Comprehensive Analysis of AI Biases in DeepFake Detection With\n  Massively Annotated Databases","summary":"  In recent years, image and video manipulations with Deepfake have become a\nsevere concern for security and society. Many detection models and datasets\nhave been proposed to detect Deepfake data reliably. However, there is an\nincreased concern that these models and training databases might be biased and,\nthus, cause Deepfake detectors to fail. In this work, we investigate the bias\nissue caused by public Deepfake datasets by (a) providing large-scale\ndemographic and non-demographic attribute annotations of 47 different\nattributes for five popular Deepfake datasets and (b) comprehensively analysing\nAI-bias of three state-of-the-art Deepfake detection backbone models on these\ndatasets. The investigation analyses the influence of a large variety of\ndistinctive attributes (from over 65M labels) on the detection performance,\nincluding demographic (age, gender, ethnicity) and non-demographic (hair, skin,\naccessories, etc.) information. The results indicate that investigated\ndatabases lack diversity and, more importantly, show that the utilised Deepfake\ndetection backbone models are strongly biased towards many investigated\nattributes. The Deepfake detection backbone methods, which are trained with\nbiased datasets, might output incorrect detection results, thereby leading to\ngeneralisability, fairness, and security issues. We hope that the findings of\nthis study and the annotation databases will help to evaluate and mitigate bias\nin future Deepfake detection techniques. The annotation datasets are publicly\navailable.\n","authors":["Ying Xu","Philipp Terhörst","Kiran Raja","Marius Pedersen"],"pdf_url":"https://arxiv.org/pdf/2208.05845v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.13496v1","updated":"2023-03-23T17:56:12Z","published":"2023-03-23T17:56:12Z","title":"The effectiveness of MAE pre-pretraining for billion-scale pretraining","summary":"  This paper revisits the standard pretrain-then-finetune paradigm used in\ncomputer vision for visual recognition tasks. Typically, state-of-the-art\nfoundation models are pretrained using large scale (weakly) supervised datasets\nwith billions of images. We introduce an additional pre-pretraining stage that\nis simple and uses the self-supervised MAE technique to initialize the model.\nWhile MAE has only been shown to scale with the size of models, we find that it\nscales with the size of the training dataset as well. Thus, our MAE-based\npre-pretraining scales with both model and data size making it applicable for\ntraining foundation models. Pre-pretraining consistently improves both the\nmodel convergence and the downstream transfer performance across a range of\nmodel scales (millions to billions of parameters), and dataset sizes (millions\nto billions of images). We measure the effectiveness of pre-pretraining on 10\ndifferent visual recognition tasks spanning image classification, video\nrecognition, object detection, low-shot classification and zero-shot\nrecognition. Our largest model achieves new state-of-the-art results on\niNaturalist-18 (91.3%), 1-shot ImageNet-1k (62.1%), and zero-shot transfer on\nFood-101 (96.0%). Our study reveals that model initialization plays a\nsignificant role, even for web-scale pretraining with billions of images.\n","authors":["Mannat Singh","Quentin Duval","Kalyan Vasudev Alwala","Haoqi Fan","Vaibhav Aggarwal","Aaron Adcock","Armand Joulin","Piotr Dollár","Christoph Feichtenhofer","Ross Girshick","Rohit Girdhar","Ishan Misra"],"pdf_url":"https://arxiv.org/pdf/2303.13496v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.13489v1","updated":"2023-03-23T17:53:44Z","published":"2023-03-23T17:53:44Z","title":"Boosting Reinforcement Learning and Planning with Demonstrations: A\n  Survey","summary":"  Although reinforcement learning has seen tremendous success recently, this\nkind of trial-and-error learning can be impractical or inefficient in complex\nenvironments. The use of demonstrations, on the other hand, enables agents to\nbenefit from expert knowledge rather than having to discover the best action to\ntake through exploration. In this survey, we discuss the advantages of using\ndemonstrations in sequential decision making, various ways to apply\ndemonstrations in learning-based decision making paradigms (for example,\nreinforcement learning and planning in the learned models), and how to collect\nthe demonstrations in various scenarios. Additionally, we exemplify a practical\npipeline for generating and utilizing demonstrations in the recently proposed\nManiSkill robot learning benchmark.\n","authors":["Tongzhou Mu","Hao Su"],"pdf_url":"https://arxiv.org/pdf/2303.13489v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.13482v1","updated":"2023-03-23T17:50:09Z","published":"2023-03-23T17:50:09Z","title":"TactoFind: A Tactile Only System for Object Retrieval","summary":"  We study the problem of object retrieval in scenarios where visual sensing is\nabsent, object shapes are unknown beforehand and objects can move freely, like\ngrabbing objects out of a drawer. Successful solutions require localizing free\nobjects, identifying specific object instances, and then grasping the\nidentified objects, only using touch feedback. Unlike vision, where cameras can\nobserve the entire scene, touch sensors are local and only observe parts of the\nscene that are in contact with the manipulator. Moreover, information gathering\nvia touch sensors necessitates applying forces on the touched surface which may\ndisturb the scene itself. Reasoning with touch, therefore, requires careful\nexploration and integration of information over time -- a challenge we tackle.\nWe present a system capable of using sparse tactile feedback from fingertip\ntouch sensors on a dexterous hand to localize, identify and grasp novel objects\nwithout any visual feedback. Videos are available at\nhttps://taochenshh.github.io/projects/tactofind.\n","authors":["Sameer Pai","Tao Chen","Megha Tippur","Edward Adelson","Abhishek Gupta","Pulkit Agrawal"],"pdf_url":"https://arxiv.org/pdf/2303.13482v1.pdf","comment":"Accepted in ICRA 2023"},{"id":"http://arxiv.org/abs/2102.04518v2","updated":"2023-03-23T17:38:09Z","published":"2021-02-08T20:36:41Z","title":"A* Search Without Expansions: Learning Heuristic Functions with Deep\n  Q-Networks","summary":"  Efficiently solving problems with large action spaces using A* search has\nbeen of importance to the artificial intelligence community for decades. This\nis because the computation and memory requirements of A* search grow linearly\nwith the size of the action space. This burden becomes even more apparent when\nA* search uses a heuristic function learned by computationally expensive\nfunction approximators, such as deep neural networks. To address this problem,\nwe introduce Q* search, a search algorithm that uses deep Q-networks to guide\nsearch in order to take advantage of the fact that the sum of the transition\ncosts and heuristic values of the children of a node can be computed with a\nsingle forward pass through a deep Q-network without explicitly generating\nthose children. This significantly reduces computation time and requires only\none node to be generated per iteration. We use Q* search to solve the Rubik's\ncube when formulated with a large action space that includes 1872 meta-actions\nand find that this 157-fold increase in the size of the action space incurs\nless than a 4-fold increase in computation time and less than a 3-fold increase\nin number of nodes generated when performing Q* search. Furthermore, Q* search\nis up to 129 times faster and generates up to 1288 times fewer nodes than A*\nsearch. Finally, although obtaining admissible heuristic functions from deep\nneural networks is an ongoing area of research, we prove that Q* search is\nguaranteed to find a shortest path given a heuristic function that neither\noverestimates the cost of a shortest path nor underestimates the transition\ncost.\n","authors":["Forest Agostinelli","Alexander Shmakov","Stephen McAleer","Roy Fox","Pierre Baldi"],"pdf_url":"https://arxiv.org/pdf/2102.04518v2.pdf","comment":"Added theoretical results to show that Q* search is an admissible\n  search algorithm. Added comparisons to deferred heuristic evaluation. Added\n  experiments with Lights Out and the 35-Pancake puzzle"},{"id":"http://arxiv.org/abs/2303.13462v1","updated":"2023-03-23T17:32:20Z","published":"2023-03-23T17:32:20Z","title":"Generalization with quantum geometry for learning unitaries","summary":"  Generalization is the ability of quantum machine learning models to make\naccurate predictions on new data by learning from training data. Here, we\nintroduce the data quantum Fisher information metric (DQFIM) to determine when\na model can generalize. For variational learning of unitaries, the DQFIM\nquantifies the amount of circuit parameters and training data needed to\nsuccessfully train and generalize. We apply the DQFIM to explain when a\nconstant number of training states and polynomial number of parameters are\nsufficient for generalization. Further, we can improve generalization by\nremoving symmetries from training data. Finally, we show that\nout-of-distribution generalization, where training and testing data are drawn\nfrom different data distributions, can be better than using the same\ndistribution. Our work opens up new approaches to improve generalization in\nquantum machine learning.\n","authors":["Tobias Haug","M. S. Kim"],"pdf_url":"https://arxiv.org/pdf/2303.13462v1.pdf","comment":"16 pages, 13 figures"},{"id":"http://arxiv.org/abs/2212.08049v5","updated":"2023-03-23T17:26:44Z","published":"2022-12-15T18:55:23Z","title":"Sliced Optimal Partial Transport","summary":"  Optimal transport (OT) has become exceedingly popular in machine learning,\ndata science, and computer vision. The core assumption in the OT problem is the\nequal total amount of mass in source and target measures, which limits its\napplication. Optimal Partial Transport (OPT) is a recently proposed solution to\nthis limitation. Similar to the OT problem, the computation of OPT relies on\nsolving a linear programming problem (often in high dimensions), which can\nbecome computationally prohibitive. In this paper, we propose an efficient\nalgorithm for calculating the OPT problem between two non-negative measures in\none dimension. Next, following the idea of sliced OT distances, we utilize\nslicing to define the sliced OPT distance. Finally, we demonstrate the\ncomputational and accuracy benefits of the sliced OPT-based method in various\nnumerical experiments. In particular, we show an application of our proposed\nSliced-OPT in noisy point cloud registration.\n","authors":["Yikun Bai","Bernard Schmitzer","Mathew Thorpe","Soheil Kolouri"],"pdf_url":"https://arxiv.org/pdf/2212.08049v5.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.13458v1","updated":"2023-03-23T17:26:12Z","published":"2023-03-23T17:26:12Z","title":"Optimization Dynamics of Equivariant and Augmented Neural Networks","summary":"  We investigate the optimization of multilayer perceptrons on symmetric data.\nWe compare the strategy of constraining the architecture to be equivariant to\nthat of using augmentation. We show that, under natural assumptions on the loss\nand non-linearities, the sets of equivariant stationary points are identical\nfor the two strategies, and that the set of equivariant layers is invariant\nunder the gradient flow for augmented models. Finally, we show that stationary\npoints may be unstable for augmented training although they are stable for the\nequivariant models\n","authors":["Axel Flinth","Fredrik Ohlsson"],"pdf_url":"https://arxiv.org/pdf/2303.13458v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2205.11912v2","updated":"2023-03-23T17:24:04Z","published":"2022-05-24T09:17:27Z","title":"Physics-Embedded Neural Networks: Graph Neural PDE Solvers with Mixed\n  Boundary Conditions","summary":"  Graph neural network (GNN) is a promising approach to learning and predicting\nphysical phenomena described in boundary value problems, such as partial\ndifferential equations (PDEs) with boundary conditions. However, existing\nmodels inadequately treat boundary conditions essential for the reliable\nprediction of such problems. In addition, because of the locally connected\nnature of GNNs, it is difficult to accurately predict the state after a long\ntime, where interaction between vertices tends to be global. We present our\napproach termed physics-embedded neural networks that considers boundary\nconditions and predicts the state after a long time using an implicit method.\nIt is built based on an E(n)-equivariant GNN, resulting in high generalization\nperformance on various shapes. We demonstrate that our model learns flow\nphenomena in complex shapes and outperforms a well-optimized classical solver\nand a state-of-the-art machine learning model in speed-accuracy trade-off.\nTherefore, our model can be a useful standard for realizing reliable, fast, and\naccurate GNN-based PDE solvers. The code is available at\nhttps://github.com/yellowshippo/penn-neurips2022.\n","authors":["Masanobu Horie","Naoto Mitsume"],"pdf_url":"https://arxiv.org/pdf/2205.11912v2.pdf","comment":"NeurIPS 2022"},{"id":"http://arxiv.org/abs/2303.13452v1","updated":"2023-03-23T17:19:26Z","published":"2023-03-23T17:19:26Z","title":"Human Behavior in the Time of COVID-19: Learning from Big Data","summary":"  Since the World Health Organization (WHO) characterized COVID-19 as a\npandemic in March 2020, there have been over 600 million confirmed cases of\nCOVID-19 and more than six million deaths as of October 2022. The relationship\nbetween the COVID-19 pandemic and human behavior is complicated. On one hand,\nhuman behavior is found to shape the spread of the disease. On the other hand,\nthe pandemic has impacted and even changed human behavior in almost every\naspect. To provide a holistic understanding of the complex interplay between\nhuman behavior and the COVID-19 pandemic, researchers have been employing big\ndata techniques such as natural language processing, computer vision, audio\nsignal processing, frequent pattern mining, and machine learning. In this\nstudy, we present an overview of the existing studies on using big data\ntechniques to study human behavior in the time of the COVID-19 pandemic. In\nparticular, we categorize these studies into three groups - using big data to\nmeasure, model, and leverage human behavior, respectively. The related tasks,\ndata, and methods are summarized accordingly. To provide more insights into how\nto fight the COVID-19 pandemic and future global catastrophes, we further\ndiscuss challenges and potential opportunities.\n","authors":["Hanjia Lyu","Arsal Imtiaz","Yufei Zhao","Jiebo Luo"],"pdf_url":"https://arxiv.org/pdf/2303.13452v1.pdf","comment":"Accepted for publication in the Horizons in Big Data 2022 article\n  collection of Frontiers in Big Data"},{"id":"http://arxiv.org/abs/2303.13450v1","updated":"2023-03-23T17:17:29Z","published":"2023-03-23T17:17:29Z","title":"Set-the-Scene: Global-Local Training for Generating Controllable NeRF\n  Scenes","summary":"  Recent breakthroughs in text-guided image generation have led to remarkable\nprogress in the field of 3D synthesis from text. By optimizing neural radiance\nfields (NeRF) directly from text, recent methods are able to produce remarkable\nresults. Yet, these methods are limited in their control of each object's\nplacement or appearance, as they represent the scene as a whole. This can be a\nmajor issue in scenarios that require refining or manipulating objects in the\nscene. To remedy this deficit, we propose a novel GlobalLocal training\nframework for synthesizing a 3D scene using object proxies. A proxy represents\nthe object's placement in the generated scene and optionally defines its coarse\ngeometry. The key to our approach is to represent each object as an independent\nNeRF. We alternate between optimizing each NeRF on its own and as part of the\nfull scene. Thus, a complete representation of each object can be learned,\nwhile also creating a harmonious scene with style and lighting match. We show\nthat using proxies allows a wide variety of editing options, such as adjusting\nthe placement of each independent object, removing objects from a scene, or\nrefining an object. Our results show that Set-the-Scene offers a powerful\nsolution for scene synthesis and manipulation, filling a crucial gap in\ncontrollable text-to-3D synthesis.\n","authors":["Dana Cohen-Bar","Elad Richardson","Gal Metzer","Raja Giryes","Daniel Cohen-Or"],"pdf_url":"https://arxiv.org/pdf/2303.13450v1.pdf","comment":"project page at https://danacohen95.github.io/Set-the-Scene/"},{"id":"http://arxiv.org/abs/2112.11947v3","updated":"2023-03-23T17:03:23Z","published":"2021-12-22T15:14:50Z","title":"Evaluating the Robustness of Deep Reinforcement Learning for Autonomous\n  Policies in a Multi-agent Urban Driving Environment","summary":"  Deep reinforcement learning is actively used for training autonomous car\npolicies in a simulated driving environment. Due to the large availability of\nvarious reinforcement learning algorithms and the lack of their systematic\ncomparison across different driving scenarios, we are unsure of which ones are\nmore effective for training autonomous car software in single-agent as well as\nmulti-agent driving environments. A benchmarking framework for the comparison\nof deep reinforcement learning in a vision-based autonomous driving will open\nup the possibilities for training better autonomous car driving policies. To\naddress these challenges, we provide an open and reusable benchmarking\nframework for systematic evaluation and comparative analysis of deep\nreinforcement learning algorithms for autonomous driving in a single- and\nmulti-agent environment. Using the framework, we perform a comparative study of\ndiscrete and continuous action space deep reinforcement learning algorithms. We\nalso propose a comprehensive multi-objective reward function designed for the\nevaluation of deep reinforcement learning-based autonomous driving agents. We\nrun the experiments in a vision-only high-fidelity urban driving simulated\nenvironments. The results indicate that only some of the deep reinforcement\nlearning algorithms perform consistently better across single and multi-agent\nscenarios when trained in various multi-agent-only environment settings. For\nexample, A3C- and TD3-based autonomous cars perform comparatively better in\nterms of more robust actions and minimal driving errors in both single and\nmulti-agent scenarios. We conclude that different deep reinforcement learning\nalgorithms exhibit different driving and testing performance in different\nscenarios, which underlines the need for their systematic comparative analysis.\nThe benchmarking framework proposed in this paper facilitates such a\ncomparison.\n","authors":["Aizaz Sharif","Dusica Marijan"],"pdf_url":"https://arxiv.org/pdf/2112.11947v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2212.09967v2","updated":"2023-03-23T16:53:13Z","published":"2022-12-20T02:45:09Z","title":"Learning Subgrid-scale Models with Neural Ordinary Differential\n  Equations","summary":"  We propose a new approach to learning the subgrid-scale model when simulating\npartial differential equations (PDEs) solved by the method of lines and their\nrepresentation in chaotic ordinary differential equations, based on neural\nordinary differential equations (NODEs). Solving systems with fine temporal and\nspatial grid scales is an ongoing computational challenge, and closure models\nare generally difficult to tune. Machine learning approaches have increased the\naccuracy and efficiency of computational fluid dynamics solvers. In this\napproach neural networks are used to learn the coarse- to fine-grid map, which\ncan be viewed as subgrid-scale parameterization. We propose a strategy that\nuses the NODE and partial knowledge to learn the source dynamics at a\ncontinuous level. Our method inherits the advantages of NODEs and can be used\nto parameterize subgrid scales, approximate coupling operators, and improve the\nefficiency of low-order solvers. Numerical results with the two-scale Lorenz 96\nODE, the convection-diffusion PDE, and the viscous Burgers' PDE are used to\nillustrate this approach.\n","authors":["Shinhoo Kang","Emil M. Constantinescu"],"pdf_url":"https://arxiv.org/pdf/2212.09967v2.pdf","comment":"27 pages, 20 figures, 2 tables"},{"id":"http://arxiv.org/abs/1909.12913v5","updated":"2023-03-23T16:43:29Z","published":"2019-09-18T15:46:48Z","title":"Student Engagement Detection Using Emotion Analysis, Eye Tracking and\n  Head Movement with Machine Learning","summary":"  With the increase of distance learning, in general, and e-learning, in\nparticular, having a system capable of determining the engagement of students\nis of primordial importance, and one of the biggest challenges, both for\nteachers, researchers and policy makers. Here, we present a system to detect\nthe engagement level of the students. It uses only information provided by the\ntypical built-in web-camera present in a laptop computer, and was designed to\nwork in real time. We combine information about the movements of the eyes and\nhead, and facial emotions to produce a concentration index with three classes\nof engagement: \"very engaged\", \"nominally engaged\" and \"not engaged at all\".\nThe system was tested in a typical e-learning scenario, and the results show\nthat it correctly identifies each period of time where students were \"very\nengaged\", \"nominally engaged\" and \"not engaged at all\". Additionally, the\nresults also show that the students with best scores also have higher\nconcentration indexes.\n","authors":["Prabin Sharma","Shubham Joshi","Subash Gautam","Sneha Maharjan","Salik Ram Khanal","Manuel Cabral Reis","João Barroso","Vítor Manuel de Jesus Filipe"],"pdf_url":"https://arxiv.org/pdf/1909.12913v5.pdf","comment":"9 pages, 9 Figures, 2 tables"},{"id":"http://arxiv.org/abs/2301.13734v2","updated":"2023-03-23T16:42:20Z","published":"2023-01-31T16:12:31Z","title":"Improving Monte Carlo Evaluation with Offline Data","summary":"  Monte Carlo (MC) methods are the most widely used methods to estimate the\nperformance of a policy. Given an interested policy, MC methods give estimates\nby repeatedly running this policy to collect samples and taking the average of\nthe outcomes. Samples collected during this process are called online samples.\nTo get an accurate estimate, MC methods consume massive online samples. When\nonline samples are expensive, e.g., online recommendations and inventory\nmanagement, we want to reduce the number of online samples while achieving the\nsame estimate accuracy. To this end, we use off-policy MC methods that evaluate\nthe interested policy by running a different policy called behavior policy. We\ndesign a tailored behavior policy such that the variance of the off-policy MC\nestimator is provably smaller than the ordinary MC estimator. Importantly, this\ntailored behavior policy can be efficiently learned from existing offline data,\ni,e., previously logged data, which are much cheaper than online samples. With\nreduced variance, our off-policy MC method requires fewer online samples to\nevaluate the performance of a policy compared with the ordinary MC method.\nMoreover, our off-policy MC estimator is always unbiased.\n","authors":["Shuze Liu","Shangtong Zhang"],"pdf_url":"https://arxiv.org/pdf/2301.13734v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.13418v1","updated":"2023-03-23T16:39:31Z","published":"2023-03-23T16:39:31Z","title":"GiveMeLabeledIssues: An Open Source Issue Recommendation System","summary":"  Developers often struggle to navigate an Open Source Software (OSS) project's\nissue-tracking system and find a suitable task. Proper issue labeling can aid\ntask selection, but current tools are limited to classifying the issues\naccording to their type (e.g., bug, question, good first issue, feature, etc.).\nIn contrast, this paper presents a tool (GiveMeLabeledIssues) that mines\nproject repositories and labels issues based on the skills required to solve\nthem. We leverage the domain of the APIs involved in the solution (e.g., User\nInterface (UI), Test, Databases (DB), etc.) as a proxy for the required skills.\nGiveMeLabeledIssues facilitates matching developers' skills to tasks, reducing\nthe burden on project maintainers. The tool obtained a precision of 83.9% when\npredicting the API domains involved in the issues. The replication package\ncontains instructions on executing the tool and including new projects. A demo\nvideo is available at https://www.youtube.com/watch?v=ic2quUue7i8\n","authors":["Joseph Vargovich","Fabio Santos","Jacob Penney","Marco A. Gerosa","Igor Steinmacher"],"pdf_url":"https://arxiv.org/pdf/2303.13418v1.pdf","comment":"MSR Data and Tool Showcase 2023"},{"id":"http://arxiv.org/abs/2209.06203v4","updated":"2023-03-23T16:29:28Z","published":"2022-09-13T17:56:13Z","title":"Normalizing Flows for Interventional Density Estimation","summary":"  Existing machine learning methods for causal inference usually estimate\nquantities expressed via the mean of potential outcomes (e.g., average\ntreatment effect). However, such quantities do not capture the full information\nabout the distribution of potential outcomes. In this work, we estimate the\ndensity of potential outcomes after interventions from observational data. For\nthis, we propose a novel, fully-parametric deep learning method called\nInterventional Normalizing Flows. Specifically, we combine two normalizing\nflows, namely (i) a nuisance flow for estimating nuisance parameters and (ii) a\ntarget flow for a parametric estimation of the density of potential outcomes.\nWe further develop a tractable optimization objective based on a one-step bias\ncorrection for an efficient and doubly robust estimation of the target flow\nparameters. As a result our Interventional Normalizing Flows offer a properly\nnormalized density estimator. Across various experiments, we demonstrate that\nour Interventional Normalizing Flows are expressive and highly effective, and\nscale well with both sample size and high-dimensional confounding. To the best\nof our knowledge, our Interventional Normalizing Flows are the first proper\nfully-parametric, deep learning method for density estimation of potential\noutcomes.\n","authors":["Valentyn Melnychuk","Dennis Frauen","Stefan Feuerriegel"],"pdf_url":"https://arxiv.org/pdf/2209.06203v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.13408v1","updated":"2023-03-23T16:29:27Z","published":"2023-03-23T16:29:27Z","title":"Paraphrasing evades detectors of AI-generated text, but retrieval is an\n  effective defense","summary":"  To detect the deployment of large language models for malicious use cases\n(e.g., fake content creation or academic plagiarism), several approaches have\nrecently been proposed for identifying AI-generated text via watermarks or\nstatistical irregularities. How robust are these detection algorithms to\nparaphrases of AI-generated text? To stress test these detectors, we first\ntrain an 11B parameter paraphrase generation model (DIPPER) that can paraphrase\nparagraphs, optionally leveraging surrounding text (e.g., user-written prompts)\nas context. DIPPER also uses scalar knobs to control the amount of lexical\ndiversity and reordering in the paraphrases. Paraphrasing text generated by\nthree large language models (including GPT3.5-davinci-003) with DIPPER\nsuccessfully evades several detectors, including watermarking, GPTZero,\nDetectGPT, and OpenAI's text classifier. For example, DIPPER drops the\ndetection accuracy of DetectGPT from 70.3% to 4.6% (at a constant false\npositive rate of 1%), without appreciably modifying the input semantics. To\nincrease the robustness of AI-generated text detection to paraphrase attacks,\nwe introduce a simple defense that relies on retrieving semantically-similar\ngenerations and must be maintained by a language model API provider. Given a\ncandidate text, our algorithm searches a database of sequences previously\ngenerated by the API, looking for sequences that match the candidate text\nwithin a certain threshold. We empirically verify our defense using a database\nof 15M generations from a fine-tuned T5-XXL model and find that it can detect\n80% to 97% of paraphrased generations across different settings, while only\nclassifying 1% of human-written sequences as AI-generated. We will open source\nour code, model and data for future research.\n","authors":["Kalpesh Krishna","Yixiao Song","Marzena Karpinska","John Wieting","Mohit Iyyer"],"pdf_url":"https://arxiv.org/pdf/2303.13408v1.pdf","comment":"Preprint (27 pages). Code, models, data will be added to\n  https://github.com/martiansideofthemoon/ai-detection-paraphrases"},{"id":"http://arxiv.org/abs/2301.07784v2","updated":"2023-03-23T16:29:23Z","published":"2023-01-18T20:54:40Z","title":"Sample-Efficient Multi-Objective Learning via Generalized Policy\n  Improvement Prioritization","summary":"  Multi-objective reinforcement learning (MORL) algorithms tackle sequential\ndecision problems where agents may have different preferences over (possibly\nconflicting) reward functions. Such algorithms often learn a set of policies\n(each optimized for a particular agent preference) that can later be used to\nsolve problems with novel preferences. We introduce a novel algorithm that uses\nGeneralized Policy Improvement (GPI) to define principled, formally-derived\nprioritization schemes that improve sample-efficient learning. They implement\nactive-learning strategies by which the agent can (i) identify the most\npromising preferences/objectives to train on at each moment, to more rapidly\nsolve a given MORL problem; and (ii) identify which previous experiences are\nmost relevant when learning a policy for a particular agent preference, via a\nnovel Dyna-style MORL method. We prove our algorithm is guaranteed to always\nconverge to an optimal solution in a finite number of steps, or an\n$\\epsilon$-optimal solution (for a bounded $\\epsilon$) if the agent is limited\nand can only identify possibly sub-optimal policies. We also prove that our\nmethod monotonically improves the quality of its partial solutions while\nlearning. Finally, we introduce a bound that characterizes the maximum utility\nloss (with respect to the optimal solution) incurred by the partial solutions\ncomputed by our method throughout learning. We empirically show that our method\noutperforms state-of-the-art MORL algorithms in challenging multi-objective\ntasks, both with discrete and continuous state and action spaces.\n","authors":["Lucas N. Alegre","Ana L. C. Bazzan","Diederik M. Roijers","Ann Nowé","Bruno C. da Silva"],"pdf_url":"https://arxiv.org/pdf/2301.07784v2.pdf","comment":"Accepted to AAMAS 2023"},{"id":"http://arxiv.org/abs/2303.13407v1","updated":"2023-03-23T16:28:26Z","published":"2023-03-23T16:28:26Z","title":"Adaptive Endpointing with Deep Contextual Multi-armed Bandits","summary":"  Current endpointing (EP) solutions learn in a supervised framework, which\ndoes not allow the model to incorporate feedback and improve in an online\nsetting. Also, it is a common practice to utilize costly grid-search to find\nthe best configuration for an endpointing model. In this paper, we aim to\nprovide a solution for adaptive endpointing by proposing an efficient method\nfor choosing an optimal endpointing configuration given utterance-level audio\nfeatures in an online setting, while avoiding hyperparameter grid-search. Our\nmethod does not require ground truth labels, and only uses online learning from\nreward signals without requiring annotated labels. Specifically, we propose a\ndeep contextual multi-armed bandit-based approach, which combines the\nrepresentational power of neural networks with the action exploration behavior\nof Thompson modeling algorithms. We compare our approach to several baselines,\nand show that our deep bandit models also succeed in reducing early cutoff\nerrors while maintaining low latency.\n","authors":["Do June Min","Andreas Stolcke","Anirudh Raju","Colin Vaz","Di He","Venkatesh Ravichandran","Viet Anh Trinh"],"pdf_url":"https://arxiv.org/pdf/2303.13407v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.13405v1","updated":"2023-03-23T16:28:15Z","published":"2023-03-23T16:28:15Z","title":"SC-MIL: Supervised Contrastive Multiple Instance Learning for Imbalanced\n  Classification in Pathology","summary":"  Multiple Instance learning (MIL) models have been extensively used in\npathology to predict biomarkers and risk-stratify patients from gigapixel-sized\nimages. Machine learning problems in medical imaging often deal with rare\ndiseases, making it important for these models to work in a label-imbalanced\nsetting. Furthermore, these imbalances can occur in out-of-distribution (OOD)\ndatasets when the models are deployed in the real-world. We leverage the idea\nthat decoupling feature and classifier learning can lead to improved decision\nboundaries for label imbalanced datasets. To this end, we investigate the\nintegration of supervised contrastive learning with multiple instance learning\n(SC-MIL). Specifically, we propose a joint-training MIL framework in the\npresence of label imbalance that progressively transitions from learning\nbag-level representations to optimal classifier learning. We perform\nexperiments with different imbalance settings for two well-studied problems in\ncancer pathology: subtyping of non-small cell lung cancer and subtyping of\nrenal cell carcinoma. SC-MIL provides large and consistent improvements over\nother techniques on both in-distribution (ID) and OOD held-out sets across\nmultiple imbalanced settings.\n","authors":["Dinkar Juyal","Siddhant Shingi","Syed Ashar Javed","Harshith Padigela","Chintan Shah","Anand Sampat","Archit Khosla","John Abel","Amaro Taylor-Weiner"],"pdf_url":"https://arxiv.org/pdf/2303.13405v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.13401v1","updated":"2023-03-23T16:22:59Z","published":"2023-03-23T16:22:59Z","title":"Optimization and Optimizers for Adversarial Robustness","summary":"  Empirical robustness evaluation (RE) of deep learning models against\nadversarial perturbations entails solving nontrivial constrained optimization\nproblems. Existing numerical algorithms that are commonly used to solve them in\npractice predominantly rely on projected gradient, and mostly handle\nperturbations modeled by the $\\ell_1$, $\\ell_2$ and $\\ell_\\infty$ distances. In\nthis paper, we introduce a novel algorithmic framework that blends a\ngeneral-purpose constrained-optimization solver PyGRANSO with Constraint\nFolding (PWCF), which can add more reliability and generality to the\nstate-of-the-art RE packages, e.g., AutoAttack. Regarding reliability, PWCF\nprovides solutions with stationarity measures and feasibility tests to assess\nthe solution quality. For generality, PWCF can handle perturbation models that\nare typically inaccessible to the existing projected gradient methods; the main\nrequirement is the distance metric to be almost everywhere differentiable.\nTaking advantage of PWCF and other existing numerical algorithms, we further\nexplore the distinct patterns in the solutions found for solving these\noptimization problems using various combinations of losses, perturbation\nmodels, and optimization algorithms. We then discuss the implications of these\npatterns on the current robustness evaluation and adversarial training.\n","authors":["Hengyue Liang","Buyun Liang","Le Peng","Ying Cui","Tim Mitchell","Ju Sun"],"pdf_url":"https://arxiv.org/pdf/2303.13401v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2106.06312v4","updated":"2023-03-23T16:19:38Z","published":"2021-06-11T11:09:53Z","title":"A Coupled Design of Exploiting Record Similarity for Practical Vertical\n  Federated Learning","summary":"  Federated learning is a learning paradigm to enable collaborative learning\nacross different parties without revealing raw data. Notably, vertical\nfederated learning (VFL), where parties share the same set of samples but only\nhold partial features, has a wide range of real-world applications. However,\nmost existing studies in VFL disregard the \"record linkage\" process. They\ndesign algorithms either assuming the data from different parties can be\nexactly linked or simply linking each record with its most similar neighboring\nrecord. These approaches may fail to capture the key features from other less\nsimilar records. Moreover, such improper linkage cannot be corrected by\ntraining since existing approaches provide no feedback on linkage during\ntraining. In this paper, we design a novel coupled training paradigm, FedSim,\nthat integrates one-to-many linkage into the training process. Besides enabling\nVFL in many real-world applications with fuzzy identifiers, FedSim also\nachieves better performance in traditional VFL tasks. Moreover, we\ntheoretically analyze the additional privacy risk incurred by sharing\nsimilarities. Our experiments on eight datasets with various similarity metrics\nshow that FedSim outperforms other state-of-the-art baselines. The codes of\nFedSim are available at https://github.com/Xtra-Computing/FedSim.\n","authors":["Zhaomin Wu","Qinbin Li","Bingsheng He"],"pdf_url":"https://arxiv.org/pdf/2106.06312v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2211.00024v2","updated":"2023-03-23T16:18:11Z","published":"2022-10-31T18:00:02Z","title":"A robust estimator of mutual information for deep learning\n  interpretability","summary":"  We develop the use of mutual information (MI), a well-established metric in\ninformation theory, to interpret the inner workings of deep learning models. To\naccurately estimate MI from a finite number of samples, we present GMM-MI\n(pronounced $``$Jimmie$\"$), an algorithm based on Gaussian mixture models that\ncan be applied to both discrete and continuous settings. GMM-MI is\ncomputationally efficient, robust to the choice of hyperparameters and provides\nthe uncertainty on the MI estimate due to the finite sample size. We\nextensively validate GMM-MI on toy data for which the ground truth MI is known,\ncomparing its performance against established mutual information estimators. We\nthen demonstrate the use of our MI estimator in the context of representation\nlearning, working with synthetic data and physical datasets describing highly\nnon-linear processes. We train deep learning models to encode high-dimensional\ndata within a meaningful compressed (latent) representation, and use GMM-MI to\nquantify both the level of disentanglement between the latent variables, and\ntheir association with relevant physical quantities, thus unlocking the\ninterpretability of the latent representation. We make GMM-MI publicly\navailable.\n","authors":["Davide Piras","Hiranya V. Peiris","Andrew Pontzen","Luisa Lucie-Smith","Ningyuan Guo","Brian Nord"],"pdf_url":"https://arxiv.org/pdf/2211.00024v2.pdf","comment":"30 pages, 8 figures. Minor changes to match version accepted for\n  publication in Machine Learning: Science and Technology. GMM-MI available at\n  https://github.com/dpiras/GMM-MI"},{"id":"http://arxiv.org/abs/2303.13391v1","updated":"2023-03-23T16:07:31Z","published":"2023-03-23T16:07:31Z","title":"Xplainer: From X-Ray Observations to Explainable Zero-Shot Diagnosis","summary":"  Automated diagnosis prediction from medical images is a valuable resource to\nsupport clinical decision-making. However, such systems usually need to be\ntrained on large amounts of annotated data, which often is scarce in the\nmedical domain. Zero-shot methods address this challenge by allowing a flexible\nadaption to new settings with different clinical findings without relying on\nlabeled data. Further, to integrate automated diagnosis in the clinical\nworkflow, methods should be transparent and explainable, increasing medical\nprofessionals' trust and facilitating correctness verification. In this work,\nwe introduce Xplainer, a novel framework for explainable zero-shot diagnosis in\nthe clinical setting. Xplainer adapts the classification-by-description\napproach of contrastive vision-language models to the multi-label medical\ndiagnosis task. Specifically, instead of directly predicting a diagnosis, we\nprompt the model to classify the existence of descriptive observations, which a\nradiologist would look for on an X-Ray scan, and use the descriptor\nprobabilities to estimate the likelihood of a diagnosis. Our model is\nexplainable by design, as the final diagnosis prediction is directly based on\nthe prediction of the underlying descriptors. We evaluate Xplainer on two chest\nX-ray datasets, CheXpert and ChestX-ray14, and demonstrate its effectiveness in\nimproving the performance and explainability of zero-shot diagnosis. Our\nresults suggest that Xplainer provides a more detailed understanding of the\ndecision-making process and can be a valuable tool for clinical diagnosis.\n","authors":["Chantal Pellegrini","Matthias Keicher","Ege Özsoy","Petra Jiraskova","Rickmer Braren","Nassir Navab"],"pdf_url":"https://arxiv.org/pdf/2303.13391v1.pdf","comment":"9 pages, 2 figures, 6 tables"},{"id":"http://arxiv.org/abs/2303.13386v1","updated":"2023-03-23T15:58:41Z","published":"2023-03-23T15:58:41Z","title":"Compositional Zero-Shot Domain Transfer with Text-to-Text Models","summary":"  Label scarcity is a bottleneck for improving task performance in specialised\ndomains. We propose a novel compositional transfer learning framework (DoT5 -\ndomain compositional zero-shot T5) for zero-shot domain transfer. Without\naccess to in-domain labels, DoT5 jointly learns domain knowledge (from MLM of\nunlabelled in-domain free text) and task knowledge (from task training on more\nreadily available general-domain data) in a multi-task manner. To improve the\ntransferability of task training, we design a strategy named NLGU: we\nsimultaneously train NLG for in-domain label-to-data generation which enables\ndata augmentation for self-finetuning and NLU for label prediction. We evaluate\nDoT5 on the biomedical domain and the resource-lean subdomain of radiology,\nfocusing on NLI, text summarisation and embedding learning. DoT5 demonstrates\nthe effectiveness of compositional transfer learning through multi-task\nlearning. In particular, DoT5 outperforms the current SOTA in zero-shot\ntransfer by over 7 absolute points in accuracy on RadNLI. We validate DoT5 with\nablations and a case study demonstrating its ability to solve challenging NLI\nexamples requiring in-domain expertise.\n","authors":["Fangyu Liu","Qianchu Liu","Shruthi Bannur","Fernando Pérez-García","Naoto Usuyama","Sheng Zhang","Tristan Naumann","Aditya Nori","Hoifung Poon","Javier Alvarez-Valle","Ozan Oktay","Stephanie L. Hyland"],"pdf_url":"https://arxiv.org/pdf/2303.13386v1.pdf","comment":"Accepted at TACL, pre-MIT Press publication version. 16 pages, 4\n  figures"},{"id":"http://arxiv.org/abs/2210.14064v3","updated":"2023-03-23T15:45:41Z","published":"2022-10-25T14:45:15Z","title":"Learning Low Dimensional State Spaces with Overparameterized Recurrent\n  Neural Nets","summary":"  Overparameterization in deep learning typically refers to settings where a\ntrained neural network (NN) has representational capacity to fit the training\ndata in many ways, some of which generalize well, while others do not. In the\ncase of Recurrent Neural Networks (RNNs), there exists an additional layer of\noverparameterization, in the sense that a model may exhibit many solutions that\ngeneralize well for sequence lengths seen in training, some of which\nextrapolate to longer sequences, while others do not. Numerous works have\nstudied the tendency of Gradient Descent (GD) to fit overparameterized NNs with\nsolutions that generalize well. On the other hand, its tendency to fit\noverparameterized RNNs with solutions that extrapolate has been discovered only\nrecently and is far less understood. In this paper, we analyze the\nextrapolation properties of GD when applied to overparameterized linear RNNs.\nIn contrast to recent arguments suggesting an implicit bias towards short-term\nmemory, we provide theoretical evidence for learning low-dimensional state\nspaces, which can also model long-term memory. Our result relies on a dynamical\ncharacterization which shows that GD (with small step size and near-zero\ninitialization) strives to maintain a certain form of balancedness, as well as\non tools developed in the context of the moment problem from statistics\n(recovery of a probability distribution from its moments). Experiments\ncorroborate our theory, demonstrating extrapolation via learning\nlow-dimensional state spaces with both linear and non-linear RNNs.\n","authors":["Edo Cohen-Karlik","Itamar Menuhin-Gruman","Raja Giryes","Nadav Cohen","Amir Globerson"],"pdf_url":"https://arxiv.org/pdf/2210.14064v3.pdf","comment":"Accepted to ICLR 2023, 9 pages, 2 figures plus supplementary"},{"id":"http://arxiv.org/abs/2302.04126v2","updated":"2023-03-23T15:45:10Z","published":"2023-02-08T15:24:17Z","title":"Predicting the performance of hybrid ventilation in buildings using a\n  multivariate attention-based biLSTM Encoder-Decoder neural network","summary":"  Hybrid ventilation is an energy-efficient solution to provide fresh air for\nmost climates, given that it has a reliable control system. To operate such\nsystems optimally, a high-fidelity control-oriented modesl is required. It\nshould enable near-real time forecast of the indoor air temperature based on\noperational conditions such as window opening and HVAC operating schedules.\nHowever, physics-based control-oriented models (i.e., white-box models) are\nlabour-intensive and computationally expensive. Alternatively, black-box models\nbased on artificial neural networks can be trained to be good estimators for\nbuilding dynamics. This paper investigates the capabilities of a deep neural\nnetwork (DNN), which is a multivariate multi-head attention-based long\nshort-term memory (LSTM) encoder-decoder neural network, to predict indoor air\ntemperature when windows are opened or closed. Training and test data are\ngenerated from a detailed multi-zone office building model (EnergyPlus).\nPseudo-random signals are used for the indoor air temperature setpoints and\nwindow opening instances. The results indicate that the DNN is able to\naccurately predict the indoor air temperature of five zones whenever windows\nare opened or closed. The prediction error plateaus after the 24th step ahead\nprediction (6 hr ahead prediction).\n","authors":["Gaurav Chaudhary","Hicham Johra","Laurent Georges","Bjørn Austbø"],"pdf_url":"https://arxiv.org/pdf/2302.04126v2.pdf","comment":"11 pages, 8 figures"},{"id":"http://arxiv.org/abs/2303.13363v1","updated":"2023-03-23T15:37:17Z","published":"2023-03-23T15:37:17Z","title":"FS-Real: Towards Real-World Cross-Device Federated Learning","summary":"  Federated Learning (FL) aims to train high-quality models in collaboration\nwith distributed clients while not uploading their local data, which attracts\nincreasing attention in both academia and industry. However, there is still a\nconsiderable gap between the flourishing FL research and real-world scenarios,\nmainly caused by the characteristics of heterogeneous devices and its scales.\nMost existing works conduct evaluations with homogeneous devices, which are\nmismatched with the diversity and variability of heterogeneous devices in\nreal-world scenarios. Moreover, it is challenging to conduct research and\ndevelopment at scale with heterogeneous devices due to limited resources and\ncomplex software stacks. These two key factors are important yet underexplored\nin FL research as they directly impact the FL training dynamics and final\nperformance, making the effectiveness and usability of FL algorithms unclear.\nTo bridge the gap, in this paper, we propose an efficient and scalable\nprototyping system for real-world cross-device FL, FS-Real. It supports\nheterogeneous device runtime, contains parallelism and robustness enhanced FL\nserver, and provides implementations and extensibility for advanced FL utility\nfeatures such as personalization, communication compression and asynchronous\naggregation. To demonstrate the usability and efficiency of FS-Real, we conduct\nextensive experiments with various device distributions, quantify and analyze\nthe effect of the heterogeneous device and various scales, and further provide\ninsights and open discussions about real-world FL scenarios. Our system is\nreleased to help to pave the way for further real-world FL research and broad\napplications involving diverse devices and scales.\n","authors":["Daoyuan Chen","Dawei Gao","Yuexiang Xie","Xuchen Pan","Zitao Li","Yaliang Li","Bolin Ding","Jingren Zhou"],"pdf_url":"https://arxiv.org/pdf/2303.13363v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.13340v1","updated":"2023-03-23T15:20:05Z","published":"2023-03-23T15:20:05Z","title":"Increasing Textual Context Size Boosts Medical Image-Text Matching","summary":"  This short technical report demonstrates a simple technique that yields state\nof the art results in medical image-text matching tasks. We analyze the use of\nOpenAI's CLIP, a general image-text matching model, and observe that CLIP's\nlimited textual input size has negative impact on downstream performance in the\nmedical domain where encoding longer textual contexts is often required. We\nthus train and release ClipMD, which is trained with a simple sliding window\ntechnique to encode textual captions. ClipMD was tested on two medical\nimage-text datasets and compared with other image-text matching models. The\nresults show that ClipMD outperforms other models on both datasets by a large\nmargin. We make our code and pretrained model publicly available.\n","authors":["Idan Glassberg","Tom Hope"],"pdf_url":"https://arxiv.org/pdf/2303.13340v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2209.12693v2","updated":"2023-03-23T15:17:22Z","published":"2022-09-23T11:37:02Z","title":"Leveraging the Potential of Novel Data in Power Line Communication of\n  Electricity Grids","summary":"  Electricity grids have become an essential part of daily life, even if they\nare often not noticed in everyday life. We usually only become particularly\naware of this dependence by the time the electricity grid is no longer\navailable. However, significant changes, such as the transition to renewable\nenergy (photovoltaic, wind turbines, etc.) and an increasing number of energy\nconsumers with complex load profiles (electric vehicles, home battery systems,\netc.), pose new challenges for the electricity grid. To address these\nchallenges, we propose two first-of-its-kind datasets based on measurements in\na broadband powerline communications (PLC) infrastructure. Both datasets FiN-1\nand FiN-2, were collected during real practical use in a part of the German\nlow-voltage grid that supplies around 4.4 million people and show more than 13\nbillion datapoints collected by more than 5100 sensors. In addition, we present\ndifferent use cases in asset management, grid state visualization, forecasting,\npredictive maintenance, and novelty detection to highlight the benefits of\nthese types of data. For these applications, we particularly highlight the use\nof novel machine learning architectures to extract rich information from\nreal-world data that cannot be captured using traditional approaches. By\npublishing the first large-scale real-world dataset, we aim to shed light on\nthe previously largely unrecognized potential of PLC data and emphasize\nmachine-learning-based research in low-voltage distribution networks by\npresenting a variety of different use cases.\n","authors":["Christoph Balada","Max Bondorf","Sheraz Ahmed","Andreas Dengela","Markus Zdrallek"],"pdf_url":"https://arxiv.org/pdf/2209.12693v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.13336v1","updated":"2023-03-23T15:17:15Z","published":"2023-03-23T15:17:15Z","title":"Audio Diffusion Model for Speech Synthesis: A Survey on Text To Speech\n  and Speech Enhancement in Generative AI","summary":"  Generative AI has demonstrated impressive performance in various fields,\namong which speech synthesis is an interesting direction. With the diffusion\nmodel as the most popular generative model, numerous works have attempted two\nactive tasks: text to speech and speech enhancement. This work conducts a\nsurvey on audio diffusion model, which is complementary to existing surveys\nthat either lack the recent progress of diffusion-based speech synthesis or\nhighlight an overall picture of applying diffusion model in multiple fields.\nSpecifically, this work first briefly introduces the background of audio and\ndiffusion model. As for the text-to-speech task, we divide the methods into\nthree categories based on the stage where diffusion model is adopted: acoustic\nmodel, vocoder and end-to-end framework. Moreover, we categorize various speech\nenhancement tasks by either certain signals are removed or added into the input\nspeech. Comparisons of experimental results and discussions are also covered in\nthis survey.\n","authors":["Chenshuang Zhang","Chaoning Zhang","Sheng Zheng","Mengchun Zhang","Maryam Qamar","Sung-Ho Bae","In So Kweon"],"pdf_url":"https://arxiv.org/pdf/2303.13336v1.pdf","comment":"18 pages"},{"id":"http://arxiv.org/abs/2002.02247v2","updated":"2023-03-23T15:13:14Z","published":"2020-02-06T13:25:26Z","title":"Almost Sure Convergence of Dropout Algorithms for Neural Networks","summary":"  We investigate the convergence and convergence rate of stochastic training\nalgorithms for Neural Networks (NNs) that have been inspired by Dropout (Hinton\net al., 2012). With the goal of avoiding overfitting during training of NNs,\ndropout algorithms consist in practice of multiplying the weight matrices of a\nNN componentwise by independently drawn random matrices with $\\{0, 1 \\}$-valued\nentries during each iteration of Stochastic Gradient Descent (SGD). This paper\npresents a probability theoretical proof that for fully-connected NNs with\ndifferentiable, polynomially bounded activation functions, if we project the\nweights onto a compact set when using a dropout algorithm, then the weights of\nthe NN converge to a unique stationary point of a projected system of Ordinary\nDifferential Equations (ODEs). After this general convergence guarantee, we go\non to investigate the convergence rate of dropout. Firstly, we obtain generic\nsample complexity bounds for finding $\\epsilon$-stationary points of smooth\nnonconvex functions using SGD with dropout that explicitly depend on the\ndropout probability. Secondly, we obtain an upper bound on the rate of\nconvergence of Gradient Descent (GD) on the limiting ODEs of dropout algorithms\nfor NNs with the shape of arborescences of arbitrary depth and with linear\nactivation functions. The latter bound shows that for an algorithm such as\nDropout or Dropconnect (Wan et al., 2013), the convergence rate can be impaired\nexponentially by the depth of the arborescence. In contrast, we experimentally\nobserve no such dependence for wide NNs with just a few dropout layers. We also\nprovide a heuristic argument for this observation. Our results suggest that\nthere is a change of scale of the effect of the dropout probability in the\nconvergence rate that depends on the relative size of the width of the NN\ncompared to its depth.\n","authors":["Albert Senen-Cerda","Jaron Sanders"],"pdf_url":"https://arxiv.org/pdf/2002.02247v2.pdf","comment":"52 pages, 3 figures. Added results pertaining to the convergence rate\n  of Dropout SGD to $\\epsilon$-stationary points and numerical experiments.\n  Updated the introduction, conclusion and appendix. Changed format to\n  one-column text"},{"id":"http://arxiv.org/abs/2210.17216v2","updated":"2023-03-23T15:10:19Z","published":"2022-10-31T10:55:30Z","title":"Symmetries, flat minima, and the conserved quantities of gradient flow","summary":"  Empirical studies of the loss landscape of deep networks have revealed that\nmany local minima are connected through low-loss valleys. Yet, little is known\nabout the theoretical origin of such valleys. We present a general framework\nfor finding continuous symmetries in the parameter space, which carve out\nlow-loss valleys. Our framework uses equivariances of the activation functions\nand can be applied to different layer architectures. To generalize this\nframework to nonlinear neural networks, we introduce a novel set of nonlinear,\ndata-dependent symmetries. These symmetries can transform a trained model such\nthat it performs similarly on new samples, which allows ensemble building that\nimproves robustness under certain adversarial attacks. We then show that\nconserved quantities associated with linear symmetries can be used to define\ncoordinates along low-loss valleys. The conserved quantities help reveal that\nusing common initialization methods, gradient flow only explores a small part\nof the global minimum. By relating conserved quantities to convergence rate and\nsharpness of the minimum, we provide insights on how initialization impacts\nconvergence and generalizability.\n","authors":["Bo Zhao","Iordan Ganev","Robin Walters","Rose Yu","Nima Dehmamy"],"pdf_url":"https://arxiv.org/pdf/2210.17216v2.pdf","comment":"To appear at ICLR 2023"},{"id":"http://arxiv.org/abs/2303.13326v1","updated":"2023-03-23T15:05:16Z","published":"2023-03-23T15:05:16Z","title":"Decentralized Adversarial Training over Graphs","summary":"  The vulnerability of machine learning models to adversarial attacks has been\nattracting considerable attention in recent years. Most existing studies focus\non the behavior of stand-alone single-agent learners. In comparison, this work\nstudies adversarial training over graphs, where individual agents are subjected\nto perturbations of varied strength levels across space. It is expected that\ninteractions by linked agents, and the heterogeneity of the attack models that\nare possible over the graph, can help enhance robustness in view of the\ncoordination power of the group. Using a min-max formulation of diffusion\nlearning, we develop a decentralized adversarial training framework for\nmulti-agent systems. We analyze the convergence properties of the proposed\nscheme for both convex and non-convex environments, and illustrate the enhanced\nrobustness to adversarial attacks.\n","authors":["Ying Cao","Elsa Rizk","Stefan Vlaski","Ali H. Sayed"],"pdf_url":"https://arxiv.org/pdf/2303.13326v1.pdf","comment":"arXiv admin note: text overlap with arXiv:2303.01936"},{"id":"http://arxiv.org/abs/2303.13323v1","updated":"2023-03-23T15:01:52Z","published":"2023-03-23T15:01:52Z","title":"Deep Generative Multi-Agent Imitation Model as a Computational Benchmark\n  for Evaluating Human Performance in Complex Interactive Tasks: A Case Study\n  in Football","summary":"  Evaluating the performance of human is a common need across many\napplications, such as in engineering and sports. When evaluating human\nperformance in completing complex and interactive tasks, the most common way is\nto use a metric having been proved efficient for that context, or to use\nsubjective measurement techniques. However, this can be an error prone and\nunreliable process since static metrics cannot capture all the complex contexts\nassociated with such tasks and biases exist in subjective measurement. The\nobjective of our research is to create data-driven AI agents as computational\nbenchmarks to evaluate human performance in solving difficult tasks involving\nmultiple humans and contextual factors. We demonstrate this within the context\nof football performance analysis. We train a generative model based on\nConditional Variational Recurrent Neural Network (VRNN) Model on a large player\nand ball tracking dataset. The trained model is used to imitate the\ninteractions between two teams and predict the performance from each team. Then\nthe trained Conditional VRNN Model is used as a benchmark to evaluate team\nperformance. The experimental results on Premier League football dataset\ndemonstrates the usefulness of our method to existing state-of-the-art static\nmetric used in football analytics.\n","authors":["Chaoyi Gu","Varuna De Silva"],"pdf_url":"https://arxiv.org/pdf/2303.13323v1.pdf","comment":"8 pages, 10 figures"},{"id":"http://arxiv.org/abs/2303.13299v1","updated":"2023-03-23T14:35:37Z","published":"2023-03-23T14:35:37Z","title":"Reckoning with the Disagreement Problem: Explanation Consensus as a\n  Training Objective","summary":"  As neural networks increasingly make critical decisions in high-stakes\nsettings, monitoring and explaining their behavior in an understandable and\ntrustworthy manner is a necessity. One commonly used type of explainer is post\nhoc feature attribution, a family of methods for giving each feature in an\ninput a score corresponding to its influence on a model's output. A major\nlimitation of this family of explainers in practice is that they can disagree\non which features are more important than others. Our contribution in this\npaper is a method of training models with this disagreement problem in mind. We\ndo this by introducing a Post hoc Explainer Agreement Regularization (PEAR)\nloss term alongside the standard term corresponding to accuracy, an additional\nterm that measures the difference in feature attribution between a pair of\nexplainers. We observe on three datasets that we can train a model with this\nloss term to improve explanation consensus on unseen data, and see improved\nconsensus between explainers other than those used in the loss term. We examine\nthe trade-off between improved consensus and model performance. And finally, we\nstudy the influence our method has on feature attribution explanations.\n","authors":["Avi Schwarzschild","Max Cembalest","Karthik Rao","Keegan Hines","John Dickerson"],"pdf_url":"https://arxiv.org/pdf/2303.13299v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.13297v1","updated":"2023-03-23T14:27:49Z","published":"2023-03-23T14:27:49Z","title":"Improving Generalization with Domain Convex Game","summary":"  Domain generalization (DG) tends to alleviate the poor generalization\ncapability of deep neural networks by learning model with multiple source\ndomains. A classical solution to DG is domain augmentation, the common belief\nof which is that diversifying source domains will be conducive to the\nout-of-distribution generalization. However, these claims are understood\nintuitively, rather than mathematically. Our explorations empirically reveal\nthat the correlation between model generalization and the diversity of domains\nmay be not strictly positive, which limits the effectiveness of domain\naugmentation. This work therefore aim to guarantee and further enhance the\nvalidity of this strand. To this end, we propose a new perspective on DG that\nrecasts it as a convex game between domains. We first encourage each\ndiversified domain to enhance model generalization by elaborately designing a\nregularization term based on supermodularity. Meanwhile, a sample filter is\nconstructed to eliminate low-quality samples, thereby avoiding the impact of\npotentially harmful information. Our framework presents a new avenue for the\nformal analysis of DG, heuristic analysis and extensive experiments demonstrate\nthe rationality and effectiveness.\n","authors":["Fangrui Lv","Jian Liang","Shuang Li","Jinming Zhang","Di Liu"],"pdf_url":"https://arxiv.org/pdf/2303.13297v1.pdf","comment":"accepted by CVPR 2023"},{"id":"http://arxiv.org/abs/2103.10427v4","updated":"2023-03-23T14:21:02Z","published":"2021-03-18T17:58:02Z","title":"The Low-Rank Simplicity Bias in Deep Networks","summary":"  Modern deep neural networks are highly over-parameterized compared to the\ndata on which they are trained, yet they often generalize remarkably well. A\nflurry of recent work has asked: why do deep networks not overfit to their\ntraining data? In this work, we make a series of empirical observations that\ninvestigate and extend the hypothesis that deeper networks are inductively\nbiased to find solutions with lower effective rank embeddings. We conjecture\nthat this bias exists because the volume of functions that maps to low\neffective rank embedding increases with depth. We show empirically that our\nclaim holds true on finite width linear and non-linear models on practical\nlearning paradigms and show that on natural data, these are often the solutions\nthat generalize well. We then show that the simplicity bias exists at both\ninitialization and after training and is resilient to hyper-parameters and\nlearning methods. We further demonstrate how linear over-parameterization of\ndeep non-linear models can be used to induce low-rank bias, improving\ngeneralization performance on CIFAR and ImageNet without changing the modeling\ncapacity.\n","authors":["Minyoung Huh","Hossein Mobahi","Richard Zhang","Brian Cheung","Pulkit Agrawal","Phillip Isola"],"pdf_url":"https://arxiv.org/pdf/2103.10427v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2209.10404v2","updated":"2023-03-23T14:14:00Z","published":"2022-09-21T14:51:42Z","title":"GP-net: Flexible Viewpoint Grasp Proposal","summary":"  We present the Grasp Proposal Network (GP-net), a Convolutional Neural\nNetwork model which can generate 6-DOF grasps from flexible viewpoints, e.g. as\nexperienced by mobile manipulators. To train GP-net, we synthetically generate\na dataset containing depth-images and ground-truth grasp information. In\nreal-world experiments we use the EGAD! grasping benchmark to evaluate GP-net\nagainst two commonly used algorithms, the Volumetric Grasping Network (VGN) and\nthe Grasp Pose Detection package (GPD), on a PAL TIAGo mobile manipulator. In\ncontrast to the state-of-the-art methods in robotic grasping, GP-net can be\nused for grasping objects from flexible, unknown viewpoints without the need to\ndefine the workspace and achieves a grasp success of 51.8% compared to 51.1%\nfor VGN and 33.6% for GPD. We provide a ROS package along with our code and\npre-trained models at https://aucoroboticsmu.github.io/GP-net/.\n","authors":["Anna Konrad","John McDonald","Rudi Villing"],"pdf_url":"https://arxiv.org/pdf/2209.10404v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2206.03792v3","updated":"2023-03-23T14:02:44Z","published":"2022-06-08T10:17:40Z","title":"Utilising the CLT Structure in Stochastic Gradient based Sampling :\n  Improved Analysis and Faster Algorithms","summary":"  We consider stochastic approximations of sampling algorithms, such as\nStochastic Gradient Langevin Dynamics (SGLD) and the Random Batch Method (RBM)\nfor Interacting Particle Dynamcs (IPD). We observe that the noise introduced by\nthe stochastic approximation is nearly Gaussian due to the Central Limit\nTheorem (CLT) while the driving Brownian motion is exactly Gaussian. We harness\nthis structure to absorb the stochastic approximation error inside the\ndiffusion process, and obtain improved convergence guarantees for these\nalgorithms. For SGLD, we prove the first stable convergence rate in KL\ndivergence without requiring uniform warm start, assuming the target density\nsatisfies a Log-Sobolev Inequality. Our result implies superior first-order\noracle complexity compared to prior works, under significantly milder\nassumptions. We also prove the first guarantees for SGLD under even weaker\nconditions such as H\\\"{o}lder smoothness and Poincare Inequality, thus bridging\nthe gap between the state-of-the-art guarantees for LMC and SGLD. Our analysis\nmotivates a new algorithm called covariance correction, which corrects for the\nadditional noise introduced by the stochastic approximation by rescaling the\nstrength of the diffusion. Finally, we apply our techniques to analyze RBM, and\nsignificantly improve upon the guarantees in prior works (such as removing\nexponential dependence on horizon), under minimal assumptions.\n","authors":["Aniket Das","Dheeraj Nagaraj","Anant Raj"],"pdf_url":"https://arxiv.org/pdf/2206.03792v3.pdf","comment":"Version 3 reduces the computational complexity of Covariance\n  Correction and relaxes the almost-sure noise growth assumption for smooth\n  SGLD"},{"id":"http://arxiv.org/abs/2208.07743v2","updated":"2023-03-23T13:54:52Z","published":"2022-08-16T13:33:19Z","title":"Langevin Diffusion Variational Inference","summary":"  Many methods that build powerful variational distributions based on\nunadjusted Langevin transitions exist. Most of these were developed using a\nwide range of different approaches and techniques. Unfortunately, the lack of a\nunified analysis and derivation makes developing new methods and reasoning\nabout existing ones a challenging task. We address this giving a single\nanalysis that unifies and generalizes these existing techniques. The main idea\nis to augment the target and variational by numerically simulating the\nunderdamped Langevin diffusion process and its time reversal. The benefits of\nthis approach are twofold: it provides a unified formulation for many existing\nmethods, and it simplifies the development of new ones. In fact, using our\nformulation we propose a new method that combines the strengths of previously\nexisting algorithms; it uses underdamped Langevin transitions and powerful\naugmentations parameterized by a score network. Our empirical evaluation shows\nthat our proposed method consistently outperforms relevant baselines in a wide\nrange of tasks.\n","authors":["Tomas Geffner","Justin Domke"],"pdf_url":"https://arxiv.org/pdf/2208.07743v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2210.01117v2","updated":"2023-03-23T13:42:27Z","published":"2022-10-03T17:58:04Z","title":"Omnigrok: Grokking Beyond Algorithmic Data","summary":"  Grokking, the unusual phenomenon for algorithmic datasets where\ngeneralization happens long after overfitting the training data, has remained\nelusive. We aim to understand grokking by analyzing the loss landscapes of\nneural networks, identifying the mismatch between training and test losses as\nthe cause for grokking. We refer to this as the \"LU mechanism\" because training\nand test losses (against model weight norm) typically resemble \"L\" and \"U\",\nrespectively. This simple mechanism can nicely explain many aspects of\ngrokking: data size dependence, weight decay dependence, the emergence of\nrepresentations, etc. Guided by the intuitive picture, we are able to induce\ngrokking on tasks involving images, language and molecules. In the reverse\ndirection, we are able to eliminate grokking for algorithmic datasets. We\nattribute the dramatic nature of grokking for algorithmic datasets to\nrepresentation learning.\n","authors":["Ziming Liu","Eric J. Michaud","Max Tegmark"],"pdf_url":"https://arxiv.org/pdf/2210.01117v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2301.11777v2","updated":"2023-03-23T13:28:58Z","published":"2023-01-27T15:30:25Z","title":"Interpreting learning in biological neural networks as zero-order\n  optimization method","summary":"  Recently, significant progress has been made regarding the statistical\nunderstanding of artificial neural networks (ANNs). ANNs are motivated by the\nfunctioning of the brain, but differ in several crucial aspects. In particular,\nthe locality in the updating rule of the connection parameters in biological\nneural networks (BNNs) makes it biologically implausible that the learning of\nthe brain is based on gradient descent. In this work, we look at the brain as a\nstatistical method for supervised learning. The main contribution is to relate\nthe local updating rule of the connection parameters in BNNs to a zero-order\noptimization method. It is shown that the expected values of the iterates\nimplement a modification of gradient descent.\n","authors":["Johannes Schmidt-Hieber"],"pdf_url":"https://arxiv.org/pdf/2301.11777v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.13228v1","updated":"2023-03-23T12:59:37Z","published":"2023-03-23T12:59:37Z","title":"Enriching Neural Network Training Dataset to Improve Worst-Case\n  Performance Guarantees","summary":"  Machine learning algorithms, especially Neural Networks (NNs), are a valuable\ntool used to approximate non-linear relationships, like the AC-Optimal Power\nFlow (AC-OPF), with considerable accuracy -- and achieving a speedup of several\norders of magnitude when deployed for use. Often in power systems literature,\nthe NNs are trained with a fixed dataset generated prior to the training\nprocess. In this paper, we show that adapting the NN training dataset during\ntraining can improve the NN performance and substantially reduce its worst-case\nviolations. This paper proposes an algorithm that identifies and enriches the\ntraining dataset with critical datapoints that reduce the worst-case violations\nand deliver a neural network with improved worst-case performance guarantees.\nWe demonstrate the performance of our algorithm in four test power systems,\nranging from 39-buses to 162-buses.\n","authors":["Rahul Nellikkath","Spyros Chatzivasileiadis"],"pdf_url":"https://arxiv.org/pdf/2303.13228v1.pdf","comment":"arXiv admin note: text overlap with arXiv:2212.10930"},{"id":"http://arxiv.org/abs/2303.07150v2","updated":"2023-03-23T12:49:39Z","published":"2023-03-13T14:23:39Z","title":"Multi PILOT: Learned Feasible Multiple Acquisition Trajectories for\n  Dynamic MRI","summary":"  Dynamic Magnetic Resonance Imaging (MRI) is known to be a powerful and\nreliable technique for the dynamic imaging of internal organs and tissues,\nmaking it a leading diagnostic tool. A major difficulty in using MRI in this\nsetting is the relatively long acquisition time (and, hence, increased cost)\nrequired for imaging in high spatio-temporal resolution, leading to the\nappearance of related motion artifacts and decrease in resolution. Compressed\nSensing (CS) techniques have become a common tool to reduce MRI acquisition\ntime by subsampling images in the k-space according to some acquisition\ntrajectory. Several studies have particularly focused on applying deep learning\ntechniques to learn these acquisition trajectories in order to attain better\nimage reconstruction, rather than using some predefined set of trajectories. To\nthe best of our knowledge, learning acquisition trajectories has been only\nexplored in the context of static MRI. In this study, we consider acquisition\ntrajectory learning in the dynamic imaging setting. We design an end-to-end\npipeline for the joint optimization of multiple per-frame acquisition\ntrajectories along with a reconstruction neural network, and demonstrate\nimproved image reconstruction quality in shorter acquisition times. The code\nfor reproducing all experiments is accessible at\nhttps://github.com/tamirshor7/MultiPILOT.\n","authors":["Tamir Shor","Tomer Weiss","Dor Noti","Alex Bronstein"],"pdf_url":"https://arxiv.org/pdf/2303.07150v2.pdf","comment":"Accepted For MIDL 2023"},{"id":"http://arxiv.org/abs/2012.10315v5","updated":"2023-03-23T12:48:40Z","published":"2020-12-18T16:00:08Z","title":"Kernel Methods for Unobserved Confounding: Negative Controls, Proxies,\n  and Instruments","summary":"  Negative control is a strategy for learning the causal relationship between\ntreatment and outcome in the presence of unmeasured confounding. The treatment\neffect can nonetheless be identified if two auxiliary variables are available:\na negative control treatment (which has no effect on the actual outcome), and a\nnegative control outcome (which is not affected by the actual treatment). These\nauxiliary variables can also be viewed as proxies for a traditional set of\ncontrol variables, and they bear resemblance to instrumental variables. I\npropose a family of algorithms based on kernel ridge regression for learning\nnonparametric treatment effects with negative controls. Examples include dose\nresponse curves, dose response curves with distribution shift, and\nheterogeneous treatment effects. Data may be discrete or continuous, and low,\nhigh, or infinite dimensional. I prove uniform consistency and provide finite\nsample rates of convergence. I estimate the dose response curve of cigarette\nsmoking on infant birth weight adjusting for unobserved confounding due to\nhousehold income, using a data set of singleton births in the state of\nPennsylvania between 1989 and 1991.\n","authors":["Rahul Singh"],"pdf_url":"https://arxiv.org/pdf/2012.10315v5.pdf","comment":null},{"id":"http://arxiv.org/abs/2302.01047v2","updated":"2023-03-23T12:30:35Z","published":"2023-02-02T12:21:10Z","title":"Real-Time Evaluation in Online Continual Learning: A New Hope","summary":"  Current evaluations of Continual Learning (CL) methods typically assume that\nthere is no constraint on training time and computation. This is an unrealistic\nassumption for any real-world setting, which motivates us to propose: a\npractical real-time evaluation of continual learning, in which the stream does\nnot wait for the model to complete training before revealing the next data for\npredictions. To do this, we evaluate current CL methods with respect to their\ncomputational costs. We conduct extensive experiments on CLOC, a large-scale\ndataset containing 39 million time-stamped images with geolocation labels. We\nshow that a simple baseline outperforms state-of-the-art CL methods under this\nevaluation, questioning the applicability of existing methods in realistic\nsettings. In addition, we explore various CL components commonly used in the\nliterature, including memory sampling strategies and regularization approaches.\nWe find that all considered methods fail to be competitive against our simple\nbaseline. This surprisingly suggests that the majority of existing CL\nliterature is tailored to a specific class of streams that is not practical. We\nhope that the evaluation we provide will be the first step towards a paradigm\nshift to consider the computational cost in the development of online continual\nlearning methods.\n","authors":["Yasir Ghunaim","Adel Bibi","Kumail Alhamoud","Motasem Alfarra","Hasan Abed Al Kader Hammoud","Ameya Prabhu","Philip H. S. Torr","Bernard Ghanem"],"pdf_url":"https://arxiv.org/pdf/2302.01047v2.pdf","comment":"Accepted at CVPR'23 as Highlight (Top 2.5%)"},{"id":"http://arxiv.org/abs/2303.13211v1","updated":"2023-03-23T12:11:24Z","published":"2023-03-23T12:11:24Z","title":"Don't FREAK Out: A Frequency-Inspired Approach to Detecting Backdoor\n  Poisoned Samples in DNNs","summary":"  In this paper we investigate the frequency sensitivity of Deep Neural\nNetworks (DNNs) when presented with clean samples versus poisoned samples. Our\nanalysis shows significant disparities in frequency sensitivity between these\ntwo types of samples. Building on these findings, we propose FREAK, a\nfrequency-based poisoned sample detection algorithm that is simple yet\neffective. Our experimental results demonstrate the efficacy of FREAK not only\nagainst frequency backdoor attacks but also against some spatial attacks. Our\nwork is just the first step in leveraging these insights. We believe that our\nanalysis and proposed defense mechanism will provide a foundation for future\nresearch and development of backdoor defenses.\n","authors":["Hasan Abed Al Kader Hammoud","Adel Bibi","Philip H. S. Torr","Bernard Ghanem"],"pdf_url":"https://arxiv.org/pdf/2303.13211v1.pdf","comment":"Accepted at CVPRW (The Art of Robustness)"},{"id":"http://arxiv.org/abs/2206.05564v2","updated":"2023-03-23T11:50:36Z","published":"2022-06-11T16:57:23Z","title":"gDDIM: Generalized denoising diffusion implicit models","summary":"  Our goal is to extend the denoising diffusion implicit model (DDIM) to\ngeneral diffusion models~(DMs) besides isotropic diffusions. Instead of\nconstructing a non-Markov noising process as in the original DDIM, we examine\nthe mechanism of DDIM from a numerical perspective. We discover that the DDIM\ncan be obtained by using some specific approximations of the score when solving\nthe corresponding stochastic differential equation. We present an\ninterpretation of the accelerating effects of DDIM that also explains the\nadvantages of a deterministic sampling scheme over the stochastic one for fast\nsampling. Building on this insight, we extend DDIM to general DMs, coined\ngeneralized DDIM (gDDIM), with a small but delicate modification in\nparameterizing the score network. We validate gDDIM in two non-isotropic DMs:\nBlurring diffusion model (BDM) and Critically-damped Langevin diffusion model\n(CLD). We observe more than 20 times acceleration in BDM. In the CLD, a\ndiffusion model by augmenting the diffusion process with velocity, our\nalgorithm achieves an FID score of 2.26, on CIFAR10, with only 50 number of\nscore function evaluations~(NFEs) and an FID score of 2.86 with only 27 NFEs.\nCode is available at https://github.com/qsh-zh/gDDIM\n","authors":["Qinsheng Zhang","Molei Tao","Yongxin Chen"],"pdf_url":"https://arxiv.org/pdf/2206.05564v2.pdf","comment":"26 pages, 9 figures, implementation https://github.com/qsh-zh/gDDIM"},{"id":"http://arxiv.org/abs/2209.04747v4","updated":"2023-03-23T11:42:58Z","published":"2022-09-10T22:00:30Z","title":"Diffusion Models in Vision: A Survey","summary":"  Denoising diffusion models represent a recent emerging topic in computer\nvision, demonstrating remarkable results in the area of generative modeling. A\ndiffusion model is a deep generative model that is based on two stages, a\nforward diffusion stage and a reverse diffusion stage. In the forward diffusion\nstage, the input data is gradually perturbed over several steps by adding\nGaussian noise. In the reverse stage, a model is tasked at recovering the\noriginal input data by learning to gradually reverse the diffusion process,\nstep by step. Diffusion models are widely appreciated for the quality and\ndiversity of the generated samples, despite their known computational burdens,\ni.e. low speeds due to the high number of steps involved during sampling. In\nthis survey, we provide a comprehensive review of articles on denoising\ndiffusion models applied in vision, comprising both theoretical and practical\ncontributions in the field. First, we identify and present three generic\ndiffusion modeling frameworks, which are based on denoising diffusion\nprobabilistic models, noise conditioned score networks, and stochastic\ndifferential equations. We further discuss the relations between diffusion\nmodels and other deep generative models, including variational auto-encoders,\ngenerative adversarial networks, energy-based models, autoregressive models and\nnormalizing flows. Then, we introduce a multi-perspective categorization of\ndiffusion models applied in computer vision. Finally, we illustrate the current\nlimitations of diffusion models and envision some interesting directions for\nfuture research.\n","authors":["Florinel-Alin Croitoru","Vlad Hondru","Radu Tudor Ionescu","Mubarak Shah"],"pdf_url":"https://arxiv.org/pdf/2209.04747v4.pdf","comment":"Accepted in IEEE Transactions on Pattern Analysis and Machine\n  Intelligence. 25 pages, 3 figures"},{"id":"http://arxiv.org/abs/2301.01217v4","updated":"2023-03-23T11:29:03Z","published":"2022-12-31T04:26:25Z","title":"Unlearnable Clusters: Towards Label-agnostic Unlearnable Examples","summary":"  There is a growing interest in developing unlearnable examples (UEs) against\nvisual privacy leaks on the Internet. UEs are training samples added with\ninvisible but unlearnable noise, which have been found can prevent unauthorized\ntraining of machine learning models. UEs typically are generated via a bilevel\noptimization framework with a surrogate model to remove (minimize) errors from\nthe original samples, and then applied to protect the data against unknown\ntarget models. However, existing UE generation methods all rely on an ideal\nassumption called label-consistency, where the hackers and protectors are\nassumed to hold the same label for a given sample. In this work, we propose and\npromote a more practical label-agnostic setting, where the hackers may exploit\nthe protected data quite differently from the protectors. E.g., a m-class\nunlearnable dataset held by the protector may be exploited by the hacker as a\nn-class dataset. Existing UE generation methods are rendered ineffective in\nthis challenging setting. To tackle this challenge, we present a novel\ntechnique called Unlearnable Clusters (UCs) to generate label-agnostic\nunlearnable examples with cluster-wise perturbations. Furthermore, we propose\nto leverage VisionandLanguage Pre-trained Models (VLPMs) like CLIP as the\nsurrogate model to improve the transferability of the crafted UCs to diverse\ndomains. We empirically verify the effectiveness of our proposed approach under\na variety of settings with different datasets, target models, and even\ncommercial platforms Microsoft Azure and Baidu PaddlePaddle. Code is available\nat \\url{https://github.com/jiamingzhang94/Unlearnable-Clusters}.\n","authors":["Jiaming Zhang","Xingjun Ma","Qi Yi","Jitao Sang","Yu-Gang Jiang","Yaowei Wang","Changsheng Xu"],"pdf_url":"https://arxiv.org/pdf/2301.01217v4.pdf","comment":"CVPR2023"},{"id":"http://arxiv.org/abs/2303.13177v1","updated":"2023-03-23T11:16:33Z","published":"2023-03-23T11:16:33Z","title":"It is all Connected: A New Graph Formulation for Spatio-Temporal\n  Forecasting","summary":"  With an ever-increasing number of sensors in modern society, spatio-temporal\ntime series forecasting has become a de facto tool to make informed decisions\nabout the future. Most spatio-temporal forecasting models typically comprise\ndistinct components that learn spatial and temporal dependencies. A common\nmethodology employs some Graph Neural Network (GNN) to capture relations\nbetween spatial locations, while another network, such as a recurrent neural\nnetwork (RNN), learns temporal correlations. By representing every recorded\nsample as its own node in a graph, rather than all measurements for a\nparticular location as a single node, temporal and spatial information is\nencoded in a similar manner. In this setting, GNNs can now directly learn both\ntemporal and spatial dependencies, jointly, while also alleviating the need for\nadditional temporal networks. Furthermore, the framework does not require\naligned measurements along the temporal dimension, meaning that it also\nnaturally facilitates irregular time series, different sampling frequencies or\nmissing data, without the need for data imputation. To evaluate the proposed\nmethodology, we consider wind speed forecasting as a case study, where our\nproposed framework outperformed other spatio-temporal models using GNNs with\neither Transformer or LSTM networks as temporal update functions.\n","authors":["Lars Ødegaard Bentsen","Narada Dilp Warakagoda","Roy Stenbro","Paal Engelstad"],"pdf_url":"https://arxiv.org/pdf/2303.13177v1.pdf","comment":"Pre-print"},{"id":"http://arxiv.org/abs/2206.08289v4","updated":"2023-03-23T10:54:32Z","published":"2022-06-16T16:46:32Z","title":"Switchable Representation Learning Framework with Self-compatibility","summary":"  Real-world visual search systems involve deployments on multiple platforms\nwith different computing and storage resources. Deploying a unified model that\nsuits the minimal-constrain platforms leads to limited accuracy. It is expected\nto deploy models with different capacities adapting to the resource\nconstraints, which requires features extracted by these models to be aligned in\nthe metric space. The method to achieve feature alignments is called\n``compatible learning''. Existing research mainly focuses on the one-to-one\ncompatible paradigm, which is limited in learning compatibility among multiple\nmodels. We propose a Switchable representation learning Framework with\nSelf-Compatibility (SFSC). SFSC generates a series of compatible sub-models\nwith different capacities through one training process. The optimization of\nsub-models faces gradients conflict, and we mitigate this problem from the\nperspective of the magnitude and direction. We adjust the priorities of\nsub-models dynamically through uncertainty estimation to co-optimize sub-models\nproperly. Besides, the gradients with conflicting directions are projected to\navoid mutual interference. SFSC achieves state-of-the-art performance on the\nevaluated datasets.\n","authors":["Shengsen Wu","Yan Bai","Yihang Lou","Xiongkun Linghu","Jianzhong He","Ling-Yu Duan"],"pdf_url":"https://arxiv.org/pdf/2206.08289v4.pdf","comment":"Accepted by CVPR 2023"},{"id":"http://arxiv.org/abs/2303.13166v1","updated":"2023-03-23T10:36:10Z","published":"2023-03-23T10:36:10Z","title":"Take 5: Interpretable Image Classification with a Handful of Features","summary":"  Deep Neural Networks use thousands of mostly incomprehensible features to\nidentify a single class, a decision no human can follow. We propose an\ninterpretable sparse and low dimensional final decision layer in a deep neural\nnetwork with measurable aspects of interpretability and demonstrate it on\nfine-grained image classification. We argue that a human can only understand\nthe decision of a machine learning model, if the features are interpretable and\nonly very few of them are used for a single decision. For that matter, the\nfinal layer has to be sparse and, to make interpreting the features feasible,\nlow dimensional. We call a model with a Sparse Low-Dimensional Decision\nSLDD-Model. We show that a SLDD-Model is easier to interpret locally and\nglobally than a dense high-dimensional decision layer while being able to\nmaintain competitive accuracy. Additionally, we propose a loss function that\nimproves a model's feature diversity and accuracy. Our more interpretable\nSLDD-Model only uses 5 out of just 50 features per class, while maintaining 97%\nto 100% of the accuracy on four common benchmark datasets compared to the\nbaseline model with 2048 features.\n","authors":["Thomas Norrenbrock","Marco Rudolph","Bodo Rosenhahn"],"pdf_url":"https://arxiv.org/pdf/2303.13166v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.13157v1","updated":"2023-03-23T10:18:06Z","published":"2023-03-23T10:18:06Z","title":"Adiabatic replay for continual learning","summary":"  Conventional replay-based approaches to continual learning (CL) require, for\neach learning phase with new data, the replay of samples representing all of\nthe previously learned knowledge in order to avoid catastrophic forgetting.\nSince the amount of learned knowledge grows over time in CL problems,\ngenerative replay spends an increasing amount of time just re-learning what is\nalready known. In this proof-of-concept study, we propose a replay-based CL\nstrategy that we term adiabatic replay (AR), which derives its efficiency from\nthe (reasonable) assumption that each new learning phase is adiabatic, i.e.,\nrepresents only a small addition to existing knowledge. Each new learning phase\ntriggers a sampling process that selectively replays, from the body of existing\nknowledge, just such samples that are similar to the new data, in contrast to\nreplaying all of it. Complete replay is not required since AR represents the\ndata distribution by GMMs, which are capable of selectively updating their\ninternal representation only where data statistics have changed. As long as\nadditions are adiabatic, the amount of to-be-replayed samples need not to\ndepend on the amount of previously acquired knowledge at all. We verify\nexperimentally that AR is superior to state-of-the-art deep generative replay\nusing VAEs.\n","authors":["Alexander Krawczyk","Alexander Gepperth"],"pdf_url":"https://arxiv.org/pdf/2303.13157v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2207.11143v3","updated":"2023-03-23T09:42:26Z","published":"2022-07-12T06:59:13Z","title":"Towards Global Optimality in Cooperative MARL with the Transformation\n  And Distillation Framework","summary":"  Decentralized execution is one core demand in cooperative multi-agent\nreinforcement learning (MARL). Recently, most popular MARL algorithms have\nadopted decentralized policies to enable decentralized execution and use\ngradient descent as their optimizer. However, there is hardly any theoretical\nanalysis of these algorithms taking the optimization method into consideration,\nand we find that various popular MARL algorithms with decentralized policies\nare suboptimal in toy tasks when gradient descent is chosen as their\noptimization method. In this paper, we theoretically analyze two common classes\nof algorithms with decentralized policies -- multi-agent policy gradient\nmethods and value-decomposition methods to prove their suboptimality when\ngradient descent is used. In addition, we propose the Transformation And\nDistillation (TAD) framework, which reformulates a multi-agent MDP as a special\nsingle-agent MDP with a sequential structure and enables decentralized\nexecution by distilling the learned policy on the derived ``single-agent\" MDP.\nThis approach uses a two-stage learning paradigm to address the optimization\nproblem in cooperative MARL, maintaining its performance guarantee.\nEmpirically, we implement TAD-PPO based on PPO, which can theoretically perform\noptimal policy learning in the finite multi-agent MDPs and shows significant\noutperformance on a large set of cooperative multi-agent tasks.\n","authors":["Jianing Ye","Chenghao Li","Jianhao Wang","Chongjie Zhang"],"pdf_url":"https://arxiv.org/pdf/2207.11143v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.13137v1","updated":"2023-03-23T09:38:52Z","published":"2023-03-23T09:38:52Z","title":"FedGH: Heterogeneous Federated Learning with Generalized Global Header","summary":"  Federated learning (FL) is an emerging machine learning paradigm that allows\nmultiple parties to train a shared model collaboratively in a\nprivacy-preserving manner. Existing horizontal FL methods generally assume that\nthe FL server and clients hold the same model structure. However, due to system\nheterogeneity and the need for personalization, enabling clients to hold models\nwith diverse structures has become an important direction. Existing\nmodel-heterogeneous FL approaches often require publicly available datasets and\nincur high communication and/or computational costs, which limit their\nperformances. To address these limitations, we propose the Federated Global\nprediction Header (FedGH) approach. It is a communication and\ncomputation-efficient model-heterogeneous FL framework which trains a shared\ngeneralized global prediction header with representations extracted by\nheterogeneous extractors for clients' models at the FL server. The trained\ngeneralized global prediction header learns from different clients. The\nacquired global knowledge is then transferred to clients to substitute each\nclient's local prediction header. We derive the non-convex convergence rate of\nFedGH. Extensive experiments on two real-world datasets demonstrate that FedGH\nachieves significantly more advantageous performance in both model-homogeneous\nand -heterogeneous FL scenarios compared to seven state-of-the-art personalized\nFL models, beating the best-performing baseline by up to 8.87% (for\nmodel-homogeneous FL) and 1.83% (for model-heterogeneous FL) in terms of\naverage test accuracy, while saving up to 85.53% of communication overhead.\n","authors":["Liping Yi","Gang Wang","Xiaoguang Liu","Zhuan Shi","Han Yu"],"pdf_url":"https://arxiv.org/pdf/2303.13137v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.13123v1","updated":"2023-03-23T09:23:57Z","published":"2023-03-23T09:23:57Z","title":"Laplacian Segmentation Networks: Improved Epistemic Uncertainty from\n  Spatial Aleatoric Uncertainty","summary":"  Out of distribution (OOD) medical images are frequently encountered, e.g.\nbecause of site- or scanner differences, or image corruption. OOD images come\nwith a risk of incorrect image segmentation, potentially negatively affecting\ndownstream diagnoses or treatment. To ensure robustness to such incorrect\nsegmentations, we propose Laplacian Segmentation Networks (LSN) that jointly\nmodel epistemic (model) and aleatoric (data) uncertainty in image segmentation.\nWe capture data uncertainty with a spatially correlated logit distribution. For\nmodel uncertainty, we propose the first Laplace approximation of the weight\nposterior that scales to large neural networks with skip connections that have\nhigh-dimensional outputs. Empirically, we demonstrate that modelling spatial\npixel correlation allows the Laplacian Segmentation Network to successfully\nassign high epistemic uncertainty to out-of-distribution objects appearing\nwithin images.\n","authors":["Kilian Zepf","Selma Wanna","Marco Miani","Juston Moore","Jes Frellsen","Søren Hauberg","Aasa Feragen","Frederik Warburg"],"pdf_url":"https://arxiv.org/pdf/2303.13123v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.10365v2","updated":"2023-03-23T09:19:56Z","published":"2023-03-18T08:48:16Z","title":"CroSel: Cross Selection of Confident Pseudo Labels for Partial-Label\n  Learning","summary":"  Partial-label learning (PLL) is an important weakly supervised learning\nproblem, which allows each training example to have a candidate label set\ninstead of a single ground-truth label. Identification-based methods have been\nwidely explored to tackle label ambiguity issues in PLL, which regard the true\nlabel as a latent variable to be identified. However, identifying the true\nlabels accurately and completely remains challenging, causing noise in pseudo\nlabels during model training. In this paper, we propose a new method called\nCroSel, which leverages historical prediction information from models to\nidentify true labels for most training examples. First, we introduce a cross\nselection strategy, which enables two deep models to select true labels of\npartially labeled data for each other. Besides, we propose a novel consistent\nregularization term called co-mix to avoid sample waste and tiny noise caused\nby false selection. In this way, CroSel can pick out the true labels of most\nexamples with high precision. Extensive experiments demonstrate the superiority\nof CroSel, which consistently outperforms previous state-of-the-art methods on\nbenchmark datasets. Additionally, our method achieves over 90\\% accuracy and\nquantity for selecting true labels on CIFAR-type datasets under various\nsettings.\n","authors":["Shiyu Tian","Hongxin Wei","Yiqun Wang","Lei Feng"],"pdf_url":"https://arxiv.org/pdf/2303.10365v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.13117v1","updated":"2023-03-23T09:07:30Z","published":"2023-03-23T09:07:30Z","title":"RLOR: A Flexible Framework of Deep Reinforcement Learning for Operation\n  Research","summary":"  Reinforcement learning has been applied in operation research and has shown\npromise in solving large combinatorial optimization problems. However, existing\nworks focus on developing neural network architectures for certain problems.\nThese works lack the flexibility to incorporate recent advances in\nreinforcement learning, as well as the flexibility of customizing model\narchitectures for operation research problems. In this work, we analyze the\nend-to-end autoregressive models for vehicle routing problems and show that\nthese models can benefit from the recent advances in reinforcement learning\nwith a careful re-implementation of the model architecture. In particular, we\nre-implemented the Attention Model and trained it with Proximal Policy\nOptimization (PPO) in CleanRL, showing at least 8 times speed up in training\ntime. We hereby introduce RLOR, a flexible framework for Deep Reinforcement\nLearning for Operation Research. We believe that a flexible framework is key to\ndeveloping deep reinforcement learning models for operation research problems.\nThe code of our work is publicly available at https://github.com/cpwan/RLOR.\n","authors":["Ching Pui Wan","Tung Li","Jason Min Wang"],"pdf_url":"https://arxiv.org/pdf/2303.13117v1.pdf","comment":"21 pages"},{"id":"http://arxiv.org/abs/2303.11789v2","updated":"2023-03-23T09:05:07Z","published":"2023-03-20T08:37:08Z","title":"Random Inverse Problems Over Graphs: Decentralized Online Learning","summary":"  We establish a framework of random inverse problems with real-time\nobservations over graphs, and present a decentralized online learning algorithm\nbased on online data streams, which unifies the distributed parameter\nestimation in Hilbert space and the least mean square problem in reproducing\nkernel Hilbert space (RKHS-LMS). We transform the algorithm convergence into\nthe asymptotic stability of randomly time-varying difference equations in\nHilbert space with L2-bounded martingale difference terms and develop the L2\n-asymptotic stability theory. It is shown that if the network graph is\nconnected and the sequence of forward operators satisfies the\ninfinitedimensional spatio-temporal persistence of excitation condition, then\nthe estimates of all nodes are mean square and almost surely strongly\nconsistent. By equivalently transferring the distributed learning problem in\nRKHS to the random inverse problem over graphs, we propose a decentralized\nonline learning algorithm in RKHS based on non-stationary and non-independent\nonline data streams, and prove that the algorithm is mean square and almost\nsurely strongly consistent if the operators induced by the random input data\nsatisfy the infinite-dimensional spatio-temporal persistence of excitation\ncondition.\n","authors":["Tao Li","Xiwei Zhang"],"pdf_url":"https://arxiv.org/pdf/2303.11789v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.13113v1","updated":"2023-03-23T09:00:38Z","published":"2023-03-23T09:00:38Z","title":"Adaptive Regularization for Class-Incremental Learning","summary":"  Class-Incremental Learning updates a deep classifier with new categories\nwhile maintaining the previously observed class accuracy. Regularizing the\nneural network weights is a common method to prevent forgetting previously\nlearned classes while learning novel ones. However, existing regularizers use a\nconstant magnitude throughout the learning sessions, which may not reflect the\nvarying levels of difficulty of the tasks encountered during incremental\nlearning. This study investigates the necessity of adaptive regularization in\nClass-Incremental Learning, which dynamically adjusts the regularization\nstrength according to the complexity of the task at hand. We propose a Bayesian\nOptimization-based approach to automatically determine the optimal\nregularization magnitude for each learning task. Our experiments on two\ndatasets via two regularizers demonstrate the importance of adaptive\nregularization for achieving accurate and less forgetful visual incremental\nlearning.\n","authors":["Elif Ceren Gok","Murat Onur Yildirim","Mert Kilickaya","Joaquin Vanschoren"],"pdf_url":"https://arxiv.org/pdf/2303.13113v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.13102v1","updated":"2023-03-23T08:35:56Z","published":"2023-03-23T08:35:56Z","title":"Keypoint-Guided Optimal Transport","summary":"  Existing Optimal Transport (OT) methods mainly derive the optimal transport\nplan/matching under the criterion of transport cost/distance minimization,\nwhich may cause incorrect matching in some cases. In many applications,\nannotating a few matched keypoints across domains is reasonable or even\neffortless in annotation burden. It is valuable to investigate how to leverage\nthe annotated keypoints to guide the correct matching in OT. In this paper, we\npropose a novel KeyPoint-Guided model by ReLation preservation (KPG-RL) that\nsearches for the optimal matching (i.e., transport plan) guided by the\nkeypoints in OT. To impose the keypoints in OT, first, we propose a mask-based\nconstraint of the transport plan that preserves the matching of keypoint pairs.\nSecond, we propose to preserve the relation of each data point to the keypoints\nto guide the matching. The proposed KPG-RL model can be solved by Sinkhorn's\nalgorithm and is applicable even when distributions are supported in different\nspaces. We further utilize the relation preservation constraint in the\nKantorovich Problem and Gromov-Wasserstein model to impose the guidance of\nkeypoints in them. Meanwhile, the proposed KPG-RL model is extended to the\npartial OT setting. Moreover, we deduce the dual formulation of the KPG-RL\nmodel, which is solved using deep learning techniques. Based on the learned\ntransport plan from dual KPG-RL, we propose a novel manifold barycentric\nprojection to transport source data to the target domain. As applications, we\napply the proposed KPG-RL model to the heterogeneous domain adaptation and\nimage-to-image translation. Experiments verified the effectiveness of the\nproposed approach.\n","authors":["Xiang Gu","Yucheng Yang","Wei Zeng","Jian Sun","Zongben Xu"],"pdf_url":"https://arxiv.org/pdf/2303.13102v1.pdf","comment":"Under review"},{"id":"http://arxiv.org/abs/2211.02420v2","updated":"2023-03-23T08:31:34Z","published":"2022-11-04T12:54:06Z","title":"Spatially Selective Deep Non-linear Filters for Speaker Extraction","summary":"  In a scenario with multiple persons talking simultaneously, the spatial\ncharacteristics of the signals are the most distinct feature for extracting the\ntarget signal. In this work, we develop a deep joint spatial-spectral\nnon-linear filter that can be steered in an arbitrary target direction. For\nthis we propose a simple and effective conditioning mechanism, which sets the\ninitial state of the filter's recurrent layers based on the target direction.\nWe show that this scheme is more effective than the baseline approach and\nincreases the flexibility of the filter at no performance cost. The resulting\nspatially selective non-linear filters can also be used for speech separation\nof an arbitrary number of speakers and enable very accurate multi-speaker\nlocalization as we demonstrate in this paper.\n","authors":["Kristina Tesch","Timo Gerkmann"],"pdf_url":"https://arxiv.org/pdf/2211.02420v2.pdf","comment":"\\copyright 2023 IEEE. Personal use of this material is permitted.\n  Permission from IEEE must be obtained for all other uses, in any current or\n  future media, including reprinting/republishing this material for advertising\n  or promotional purposes, creating new collective works, for resale or\n  redistribution to servers or lists, or reuse of any copyrighted component of\n  this work in other works"},{"id":"http://arxiv.org/abs/2303.13093v1","updated":"2023-03-23T08:17:10Z","published":"2023-03-23T08:17:10Z","title":"The Probabilistic Stability of Stochastic Gradient Descent","summary":"  A fundamental open problem in deep learning theory is how to define and\nunderstand the stability of stochastic gradient descent (SGD) close to a fixed\npoint. Conventional literature relies on the convergence of statistical\nmoments, esp., the variance, of the parameters to quantify the stability. We\nrevisit the definition of stability for SGD and use the \\textit{convergence in\nprobability} condition to define the \\textit{probabilistic stability} of SGD.\nThe proposed stability directly answers a fundamental question in deep learning\ntheory: how SGD selects a meaningful solution for a neural network from an\nenormous number of solutions that may overfit badly. To achieve this, we show\nthat only under the lens of probabilistic stability does SGD exhibit rich and\npractically relevant phases of learning, such as the phases of the complete\nloss of stability, incorrect learning, convergence to low-rank saddles, and\ncorrect learning. When applied to a neural network, these phase diagrams imply\nthat SGD prefers low-rank saddles when the underlying gradient is noisy,\nthereby improving the learning performance. This result is in sharp contrast to\nthe conventional wisdom that SGD prefers flatter minima to sharp ones, which we\nfind insufficient to explain the experimental data. We also prove that the\nprobabilistic stability of SGD can be quantified by the Lyapunov exponents of\nthe SGD dynamics, which can easily be measured in practice. Our work\npotentially opens a new venue for addressing the fundamental question of how\nthe learning algorithm affects the learning outcome in deep learning.\n","authors":["Liu Ziyin","Botao Li","Tomer Galanti","Masahito Ueda"],"pdf_url":"https://arxiv.org/pdf/2303.13093v1.pdf","comment":"preprint"},{"id":"http://arxiv.org/abs/2201.12358v2","updated":"2023-03-23T08:16:42Z","published":"2022-01-28T10:06:39Z","title":"Hypothesis Testing for Unknown Dynamical Systems and System Anomaly\n  Detection via Autoencoders","summary":"  We study the hypothesis testing problem for unknown dynamical systems. More\nspecifically, we observe sequential input and output data from a dynamical\nsystem with unknown parameters, and we aim to determine whether the collected\ndata is from a null distribution. Such a problem can have many applications.\nHere we formulate anomaly detection as hypothesis testing where the anomaly is\ndefined through the alternative hypothesis. Consequently, hypothesis testing\nalgorithms can detect faults in real-world systems such as robots, weather,\nenergy systems, and stock markets. Although recent works achieved\nstate-of-the-art performances in these tasks with deep learning models, we show\nthat a careful analysis using hypothesis testing and graphical models can not\nonly justify the effectiveness of autoencoder models, but also lead to a novel\nneural network design, termed DyAD (DYnamical system Anomaly Detection), with\nimproved performances. We then show that DyAD achieves state-of-the-art\nperformance on several existing datasets and a new dataset on battery anomaly\ndetection in electric vehicles.\n","authors":["Haowei He","Jingzhao Zhang","Yanan Wang","Benben Jiang","Shaobo Huang","Chen Wang","Yang Zhang","Xuebing Han","Dongxu Guo","Guannan He","Minggao Ouyang"],"pdf_url":"https://arxiv.org/pdf/2201.12358v2.pdf","comment":"16 pages, 6 figures"},{"id":"http://arxiv.org/abs/2303.13089v1","updated":"2023-03-23T08:06:10Z","published":"2023-03-23T08:06:10Z","title":"Box-Level Active Detection","summary":"  Active learning selects informative samples for annotation within budget,\nwhich has proven efficient recently on object detection. However, the widely\nused active detection benchmarks conduct image-level evaluation, which is\nunrealistic in human workload estimation and biased towards crowded images.\nFurthermore, existing methods still perform image-level annotation, but equally\nscoring all targets within the same image incurs waste of budget and redundant\nlabels. Having revealed above problems and limitations, we introduce a\nbox-level active detection framework that controls a box-based budget per\ncycle, prioritizes informative targets and avoids redundancy for fair\ncomparison and efficient application.\n  Under the proposed box-level setting, we devise a novel pipeline, namely\nComplementary Pseudo Active Strategy (ComPAS). It exploits both human\nannotations and the model intelligence in a complementary fashion: an efficient\ninput-end committee queries labels for informative objects only; meantime\nwell-learned targets are identified by the model and compensated with\npseudo-labels. ComPAS consistently outperforms 10 competitors under 4 settings\nin a unified codebase. With supervision from labeled data only, it achieves\n100% supervised performance of VOC0712 with merely 19% box annotations. On the\nCOCO dataset, it yields up to 4.3% mAP improvement over the second-best method.\nComPAS also supports training with the unlabeled pool, where it surpasses 90%\nCOCO supervised performance with 85% label reduction. Our source code is\npublicly available at https://github.com/lyumengyao/blad.\n","authors":["Mengyao Lyu","Jundong Zhou","Hui Chen","Yijie Huang","Dongdong Yu","Yaqian Li","Yandong Guo","Yuchen Guo","Liuyu Xiang","Guiguang Ding"],"pdf_url":"https://arxiv.org/pdf/2303.13089v1.pdf","comment":"CVPR 2023 highlight"},{"id":"http://arxiv.org/abs/2303.12497v2","updated":"2023-03-23T07:49:14Z","published":"2023-03-22T12:09:12Z","title":"Lower Bound on the Bayesian Risk via Information Measures","summary":"  This paper focuses on parameter estimation and introduces a new method for\nlower bounding the Bayesian risk. The method allows for the use of virtually\n\\emph{any} information measure, including R\\'enyi's $\\alpha$,\n$\\varphi$-Divergences, and Sibson's $\\alpha$-Mutual Information. The approach\nconsiders divergences as functionals of measures and exploits the duality\nbetween spaces of measures and spaces of functions. In particular, we show that\none can lower bound the risk with any information measure by upper bounding its\ndual via Markov's inequality. We are thus able to provide estimator-independent\nimpossibility results thanks to the Data-Processing Inequalities that\ndivergences satisfy. The results are then applied to settings of interest\ninvolving both discrete and continuous parameters, including the\n``Hide-and-Seek'' problem, and compared to the state-of-the-art techniques. An\nimportant observation is that the behaviour of the lower bound in the number of\nsamples is influenced by the choice of the information measure. We leverage\nthis by introducing a new divergence inspired by the ``Hockey-Stick''\nDivergence, which is demonstrated empirically to provide the largest\nlower-bound across all considered settings. If the observations are subject to\nprivatisation, stronger impossibility results can be obtained via Strong\nData-Processing Inequalities. The paper also discusses some generalisations and\nalternative directions.\n","authors":["Amedeo Roberto Esposito","Adrien Vandenbroucque","Michael Gastpar"],"pdf_url":"https://arxiv.org/pdf/2303.12497v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.05018v2","updated":"2023-03-23T07:20:34Z","published":"2023-03-09T03:40:34Z","title":"Improved Regret Bounds for Online Kernel Selection under Bandit Feedback","summary":"  In this paper, we improve the regret bound for online kernel selection under\nbandit feedback. Previous algorithm enjoys a $O((\\Vert\nf\\Vert^2_{\\mathcal{H}_i}+1)K^{\\frac{1}{3}}T^{\\frac{2}{3}})$ expected bound for\nLipschitz loss functions. We prove two types of regret bounds improving the\nprevious bound. For smooth loss functions, we propose an algorithm with a\n$O(U^{\\frac{2}{3}}K^{-\\frac{1}{3}}(\\sum^K_{i=1}L_T(f^\\ast_i))^{\\frac{2}{3}})$\nexpected bound where $L_T(f^\\ast_i)$ is the cumulative losses of optimal\nhypothesis in $\\mathbb{H}_{i}=\\{f\\in\\mathcal{H}_i:\\Vert\nf\\Vert_{\\mathcal{H}_i}\\leq U\\}$. The data-dependent bound keeps the previous\nworst-case bound and is smaller if most of candidate kernels match well with\nthe data. For Lipschitz loss functions, we propose an algorithm with a\n$O(U\\sqrt{KT}\\ln^{\\frac{2}{3}}{T})$ expected bound asymptotically improving the\nprevious bound. We apply the two algorithms to online kernel selection with\ntime constraint and prove new regret bounds matching or improving the previous\n$O(\\sqrt{T\\ln{K}} +\\Vert\nf\\Vert^2_{\\mathcal{H}_i}\\max\\{\\sqrt{T},\\frac{T}{\\sqrt{\\mathcal{R}}}\\})$\nexpected bound where $\\mathcal{R}$ is the time budget. Finally, we empirically\nverify our algorithms on online regression and classification tasks.\n","authors":["Junfan Li","Shizhong Liao"],"pdf_url":"https://arxiv.org/pdf/2303.05018v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.13080v1","updated":"2023-03-23T07:18:08Z","published":"2023-03-23T07:18:08Z","title":"MSAT: Biologically Inspired Multi-Stage Adaptive Threshold for\n  Conversion of Spiking Neural Networks","summary":"  Spiking Neural Networks (SNNs) can do inference with low power consumption\ndue to their spike sparsity. ANN-SNN conversion is an efficient way to achieve\ndeep SNNs by converting well-trained Artificial Neural Networks (ANNs).\nHowever, the existing methods commonly use constant threshold for conversion,\nwhich prevents neurons from rapidly delivering spikes to deeper layers and\ncauses high time delay. In addition, the same response for different inputs may\nresult in information loss during the information transmission. Inspired by the\nbiological model mechanism, we propose a multi-stage adaptive threshold (MSAT).\nSpecifically, for each neuron, the dynamic threshold varies with firing history\nand input properties and is positively correlated with the average membrane\npotential and negatively correlated with the rate of depolarization. The\nself-adaptation to membrane potential and input allows a timely adjustment of\nthe threshold to fire spike faster and transmit more information. Moreover, we\nanalyze the Spikes of Inactivated Neurons error which is pervasive in early\ntime steps and propose spike confidence accordingly as a measurement of\nconfidence about the neurons that correctly deliver spikes. We use such spike\nconfidence in early time steps to determine whether to elicit spike to\nalleviate this error. Combined with the proposed method, we examine the\nperformance on non-trivial datasets CIFAR-10, CIFAR-100, and ImageNet. We also\nconduct sentiment classification and speech recognition experiments on the IDBM\nand Google speech commands datasets respectively. Experiments show\nnear-lossless and lower latency ANN-SNN conversion. To the best of our\nknowledge, this is the first time to build a biologically inspired multi-stage\nadaptive threshold for converted SNN, with comparable performance to\nstate-of-the-art methods while improving energy efficiency.\n","authors":["Xiang He","Yang Li","Dongcheng Zhao","Qingqun Kong","Yi Zeng"],"pdf_url":"https://arxiv.org/pdf/2303.13080v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.08431v2","updated":"2023-03-23T07:02:17Z","published":"2023-03-15T08:08:02Z","title":"Policy Gradient Converges to the Globally Optimal Policy for Nearly\n  Linear-Quadratic Regulators","summary":"  Nonlinear control systems with partial information to the decision maker are\nprevalent in a variety of applications. As a step toward studying such\nnonlinear systems, this work explores reinforcement learning methods for\nfinding the optimal policy in the nearly linear-quadratic regulator systems. In\nparticular, we consider a dynamic system that combines linear and nonlinear\ncomponents, and is governed by a policy with the same structure. Assuming that\nthe nonlinear component comprises kernels with small Lipschitz coefficients, we\ncharacterize the optimization landscape of the cost function. Although the cost\nfunction is nonconvex in general, we establish the local strong convexity and\nsmoothness in the vicinity of the global optimizer. Additionally, we propose an\ninitialization mechanism to leverage these properties. Building on the\ndevelopments, we design a policy gradient algorithm that is guaranteed to\nconverge to the globally optimal policy with a linear rate.\n","authors":["Yinbin Han","Meisam Razaviyayn","Renyuan Xu"],"pdf_url":"https://arxiv.org/pdf/2303.08431v2.pdf","comment":"32 pages"},{"id":"http://arxiv.org/abs/2112.15287v5","updated":"2023-03-23T06:44:25Z","published":"2021-12-31T03:59:37Z","title":"Distributed Random Reshuffling over Networks","summary":"  In this paper, we consider distributed optimization problems where $n$\nagents, each possessing a local cost function, collaboratively minimize the\naverage of the local cost functions over a connected network. To solve the\nproblem, we propose a distributed random reshuffling (D-RR) algorithm that\ninvokes the random reshuffling (RR) update in each agent. We show that D-RR\ninherits favorable characteristics of RR for both smooth strongly convex and\nsmooth nonconvex objective functions. In particular, for smooth strongly convex\nobjective functions, D-RR achieves $\\mathcal{O}(1/T^2)$ rate of convergence\n(where $T$ counts epoch number) in terms of the squared distance between the\niterate and the global minimizer. When the objective function is assumed to be\nsmooth nonconvex, we show that D-RR drives the squared norm of gradient to $0$\nat a rate of $\\mathcal{O}(1/T^{2/3})$. These convergence results match those of\ncentralized RR (up to constant factors) and outperform the distributed\nstochastic gradient descent (DSGD) algorithm if we run a relatively large\nnumber of epochs. Finally, we conduct a set of numerical experiments to\nillustrate the efficiency of the proposed D-RR method on both strongly convex\nand nonconvex distributed optimization problems.\n","authors":["Kun Huang","Xiao Li","Andre Milzarek","Shi Pu","Junwen Qiu"],"pdf_url":"https://arxiv.org/pdf/2112.15287v5.pdf","comment":"20 pages, 13 figures"},{"id":"http://arxiv.org/abs/2208.08888v3","updated":"2023-03-23T06:35:45Z","published":"2022-08-15T12:33:09Z","title":"POCS-based Clustering Algorithm","summary":"  A novel clustering technique based on the projection onto convex set (POCS)\nmethod, called POCS-based clustering algorithm, is proposed in this paper. The\nproposed POCS-based clustering algorithm exploits a parallel projection method\nof POCS to find appropriate cluster prototypes in the feature space. The\nalgorithm considers each data point as a convex set and projects the cluster\nprototypes parallelly to the member data points. The projections are convexly\ncombined to minimize the objective function for data clustering purpose. The\nperformance of the proposed POCS-based clustering algorithm is verified through\nexperiments on various synthetic datasets. The experimental results show that\nthe proposed POCS-based clustering algorithm is competitive and efficient in\nterms of clustering error and execution speed when compared with other\nconventional clustering methods including Fuzzy C-Means (FCM) and K-means\nclustering algorithms.\n","authors":["Le-Anh Tran","Henock M. Deberneh","Truong-Dong Do","Thanh-Dat Nguyen","My-Ha Le","Dong-Chul Park"],"pdf_url":"https://arxiv.org/pdf/2208.08888v3.pdf","comment":"6 pages, 4 figures, IWIS 2022"},{"id":"http://arxiv.org/abs/2303.13056v1","updated":"2023-03-23T06:04:36Z","published":"2023-03-23T06:04:36Z","title":"Predicting the Initial Conditions of the Universe using Deep Learning","summary":"  Finding the initial conditions that led to the current state of the universe\nis challenging because it involves searching over a vast input space of initial\nconditions, along with modeling their evolution via tools such as N-body\nsimulations which are computationally expensive. Deep learning has emerged as\nan alternate modeling tool that can learn the mapping between the linear input\nof an N-body simulation and the final nonlinear displacements at redshift zero,\nwhich can significantly accelerate the forward modeling. However, this does not\nhelp reduce the search space for initial conditions. In this paper, we\ndemonstrate for the first time that a deep learning model can be trained for\nthe reverse mapping. We train a V-Net based convolutional neural network, which\noutputs the linear displacement of an N-body system, given the current time\nnonlinear displacement and the cosmological parameters of the system. We\ndemonstrate that this neural network accurately recovers the initial linear\ndisplacement field over a wide range of scales ($<1$-$2\\%$ error up to nearly\n$k = 1\\ \\mathrm{Mpc}^{-1}\\,h$), despite the ill-defined nature of the inverse\nproblem at smaller scales. Specifically, smaller scales are dominated by\nnonlinear effects which makes the backward dynamics much more susceptible to\nnumerical and computational errors leading to highly divergent backward\ntrajectories and a one-to-many backward mapping. The results of our method\nmotivate that neural network based models can act as good approximators of the\ninitial linear states and their predictions can serve as good starting points\nfor sampling-based methods to infer the initial states of the universe.\n","authors":["Vaibhav Jindal","Drew Jamieson","Albert Liang","Aarti Singh","Shirley Ho"],"pdf_url":"https://arxiv.org/pdf/2303.13056v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.13055v1","updated":"2023-03-23T05:59:56Z","published":"2023-03-23T05:59:56Z","title":"Reimagining Application User Interface (UI) Design using Deep Learning\n  Methods: Challenges and Opportunities","summary":"  In this paper, we present a review of the recent work in deep learning\nmethods for user interface design. The survey encompasses well known deep\nlearning techniques (deep neural networks, convolutional neural networks,\nrecurrent neural networks, autoencoders, and generative adversarial networks)\nand datasets widely used to design user interface applications. We highlight\nimportant problems and emerging research frontiers in this field. We believe\nthat the use of deep learning for user interface design automation tasks could\nbe one of the high potential fields for the advancement of the software\ndevelopment industry.\n","authors":["Subtain Malik","Muhammad Tariq Saeed","Marya Jabeen Zia","Shahzad Rasool","Liaquat Ali Khan","Mian Ilyas Ahmed"],"pdf_url":"https://arxiv.org/pdf/2303.13055v1.pdf","comment":"A review paper on studies of UI design techniques and deep learning"},{"id":"http://arxiv.org/abs/2303.12414v2","updated":"2023-03-23T05:59:55Z","published":"2023-03-22T09:23:29Z","title":"Delay-Aware Hierarchical Federated Learning","summary":"  Federated learning has gained popularity as a means of training models\ndistributed across the wireless edge. The paper introduces delay-aware\nfederated learning (DFL) to improve the efficiency of distributed machine\nlearning (ML) model training by addressing communication delays between edge\nand cloud. DFL employs multiple stochastic gradient descent iterations on\ndevice datasets during each global aggregation interval and intermittently\naggregates model parameters through edge servers in local subnetworks. The\ncloud server synchronizes the local models with the global deployed model\ncomputed via a local-global combiner at global synchronization. The convergence\nbehavior of DFL is theoretically investigated under a generalized data\nheterogeneity metric. A set of conditions is obtained to achieve the sub-linear\nconvergence rate of O(1/k). Based on these findings, an adaptive control\nalgorithm is developed for DFL, implementing policies to mitigate energy\nconsumption and edge-to-cloud communication latency while aiming for a\nsublinear convergence rate. Numerical evaluations show DFL's superior\nperformance in terms of faster global model convergence, reduced resource\nconsumption, and robustness against communication delays compared to existing\nFL algorithms. In summary, this proposed method offers improved efficiency and\nsatisfactory results when dealing with both convex and non-convex loss\nfunctions.\n","authors":["Frank Po-Chen Lin","Seyyedali Hosseinalipour","Christopher Brinton","Nicolò Michelusi"],"pdf_url":"https://arxiv.org/pdf/2303.12414v2.pdf","comment":"A condensed version of this paper was presented at IEEE Globecom 2020"},{"id":"http://arxiv.org/abs/2210.01919v2","updated":"2023-03-23T05:32:44Z","published":"2022-10-04T21:42:06Z","title":"Convex and Nonconvex Sublinear Regression with Application to\n  Data-driven Learning of Reach Sets","summary":"  We consider estimating a compact set from finite data by approximating the\nsupport function of that set via sublinear regression. Support functions\nuniquely characterize a compact set up to closure of convexification, and are\nsublinear (convex as well as positive homogeneous of degree one). Conversely,\nany sublinear function is the support function of a compact set. We leverage\nthis property to transcribe the task of learning a compact set to that of\nlearning its support function. We propose two algorithms to perform the\nsublinear regression, one via convex and another via nonconvex programming. The\nconvex programming approach involves solving a quadratic program (QP). The\nnonconvex programming approach involves training a input sublinear neural\nnetwork. We illustrate the proposed methods via numerical examples on learning\nthe reach sets of controlled dynamics subject to set-valued input uncertainties\nfrom trajectory data.\n","authors":["Shadi Haddad","Abhishek Halder"],"pdf_url":"https://arxiv.org/pdf/2210.01919v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.13047v1","updated":"2023-03-23T05:27:32Z","published":"2023-03-23T05:27:32Z","title":"Towards Better Dynamic Graph Learning: New Architecture and Unified\n  Library","summary":"  We propose DyGFormer, a new Transformer-based architecture for dynamic graph\nlearning that solely learns from the sequences of nodes' historical first-hop\ninteractions. DyGFormer incorporates two distinct designs: a neighbor\nco-occurrence encoding scheme that explores the correlations of the source node\nand destination node based on their sequences; a patching technique that\ndivides each sequence into multiple patches and feeds them to Transformer,\nallowing the model to effectively and efficiently benefit from longer\nhistories. We also introduce DyGLib, a unified library with standard training\npipelines, extensible coding interfaces, and comprehensive evaluating protocols\nto promote reproducible, scalable, and credible dynamic graph learning\nresearch. By performing extensive experiments on thirteen datasets from various\ndomains for transductive/inductive dynamic link prediction and dynamic node\nclassification tasks, we observe that: DyGFormer achieves state-of-the-art\nperformance on most of the datasets, demonstrating the effectiveness of\ncapturing nodes' correlations and long-term temporal dependencies; the results\nof baselines vary across different datasets and some findings are inconsistent\nwith previous reports, which may be caused by their diverse pipelines and\nproblematic implementations. We hope our work can provide new insights and\nfacilitate the development of the dynamic graph learning field. All the\nresources including datasets, data loaders, algorithms, and executing scripts\nare publicly available at https://github.com/yule-BUAA/DyGLib.\n","authors":["Le Yu","Leilei Sun","Bowen Du","Weifeng Lv"],"pdf_url":"https://arxiv.org/pdf/2303.13047v1.pdf","comment":"24 pages"},{"id":"http://arxiv.org/abs/2303.13035v1","updated":"2023-03-23T04:47:46Z","published":"2023-03-23T04:47:46Z","title":"SPeC: A Soft Prompt-Based Calibration on Mitigating Performance\n  Variability in Clinical Notes Summarization","summary":"  Electronic health records (EHRs) store an extensive array of patient\ninformation, encompassing medical histories, diagnoses, treatments, and test\noutcomes. These records are crucial for enabling healthcare providers to make\nwell-informed decisions regarding patient care. Summarizing clinical notes\nfurther assists healthcare professionals in pinpointing potential health risks\nand making better-informed decisions. This process contributes to reducing\nerrors and enhancing patient outcomes by ensuring providers have access to the\nmost pertinent and current patient data. Recent research has shown that\nincorporating prompts with large language models (LLMs) substantially boosts\nthe efficacy of summarization tasks. However, we show that this approach also\nleads to increased output variance, resulting in notably divergent outputs even\nwhen prompts share similar meanings. To tackle this challenge, we introduce a\nmodel-agnostic Soft Prompt-Based Calibration (SPeC) pipeline that employs soft\nprompts to diminish variance while preserving the advantages of prompt-based\nsummarization. Experimental findings on multiple clinical note tasks and LLMs\nindicate that our method not only bolsters performance but also effectively\ncurbs variance for various LLMs, providing a more uniform and dependable\nsolution for summarizing vital medical information.\n","authors":["Yu-Neng Chuang","Ruixiang Tang","Xiaoqian Jiang","Xia Hu"],"pdf_url":"https://arxiv.org/pdf/2303.13035v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.13034v1","updated":"2023-03-23T04:46:49Z","published":"2023-03-23T04:46:49Z","title":"Preference-Aware Constrained Multi-Objective Bayesian Optimization","summary":"  This paper addresses the problem of constrained multi-objective optimization\nover black-box objective functions with practitioner-specified preferences over\nthe objectives when a large fraction of the input space is infeasible (i.e.,\nviolates constraints). This problem arises in many engineering design problems\nincluding analog circuits and electric power system design. Our overall goal is\nto approximate the optimal Pareto set over the small fraction of feasible input\ndesigns. The key challenges include the huge size of the design space, multiple\nobjectives and large number of constraints, and the small fraction of feasible\ninput designs which can be identified only after performing expensive\nsimulations. We propose a novel and efficient preference-aware constrained\nmulti-objective Bayesian optimization approach referred to as PAC-MOO to\naddress these challenges. The key idea is to learn surrogate models for both\noutput objectives and constraints, and select the candidate input for\nevaluation in each iteration that maximizes the information gained about the\noptimal constrained Pareto front while factoring in the preferences over\nobjectives. Our experiments on two real-world analog circuit design\noptimization problems demonstrate the efficacy of PAC-MOO over prior methods.\n","authors":["Alaleh Ahmadianshalchi","Syrine Belakaria","Janardhan Rao Doppa"],"pdf_url":"https://arxiv.org/pdf/2303.13034v1.pdf","comment":"arXiv admin note: text overlap with arXiv:2110.06980"},{"id":"http://arxiv.org/abs/2303.00141v2","updated":"2023-03-23T04:26:19Z","published":"2023-03-01T00:13:52Z","title":"Containing a spread through sequential learning: to exploit or to\n  explore?","summary":"  The spread of an undesirable contact process, such as an infectious disease\n(e.g. COVID-19), is contained through testing and isolation of infected nodes.\nThe temporal and spatial evolution of the process (along with containment\nthrough isolation) render such detection as fundamentally different from active\nsearch detection strategies. In this work, through an active learning approach,\nwe design testing and isolation strategies to contain the spread and minimize\nthe cumulative infections under a given test budget. We prove that the\nobjective can be optimized, with performance guarantees, by greedily selecting\nthe nodes to test. We further design reward-based methodologies that\neffectively minimize an upper bound on the cumulative infections and are\ncomputationally more tractable in large networks. These policies, however, need\nknowledge about the nodes' infection probabilities which are dynamically\nchanging and have to be learned by sequential testing. We develop a\nmessage-passing framework for this purpose and, building on that, show novel\ntradeoffs between exploitation of knowledge through reward-based heuristics and\nexploration of the unknown through a carefully designed probabilistic testing.\nThe tradeoffs are fundamentally distinct from the classical counterparts under\nactive search or multi-armed bandit problems (MABs). We provably show the\nnecessity of exploration in a stylized network and show through simulations\nthat exploration can outperform exploitation in various synthetic and real-data\nnetworks depending on the parameters of the network and the spread.\n","authors":["Xingran Chen","Hesam Nikpey","Jungyeol Kim","Saswati Sarkar","Shirin Saeedi-Bidokhti"],"pdf_url":"https://arxiv.org/pdf/2303.00141v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.13024v1","updated":"2023-03-23T04:16:00Z","published":"2023-03-23T04:16:00Z","title":"Self-Supervised Clustering of Multivariate Time-Series Data for\n  Identifying TBI Physiological States","summary":"  Determining clinically relevant physiological states from multivariate time\nseries data with missing values is essential for providing appropriate\ntreatment for acute conditions such as Traumatic Brain Injury (TBI),\nrespiratory failure, and heart failure. Utilizing non-temporal clustering or\ndata imputation and aggregation techniques may lead to loss of valuable\ninformation and biased analyses. In our study, we apply the SLAC-Time\nalgorithm, an innovative self-supervision-based approach that maintains data\nintegrity by avoiding imputation or aggregation, offering a more useful\nrepresentation of acute patient states. By using SLAC-Time to cluster data in a\nlarge research dataset, we identified three distinct TBI physiological states\nand their specific feature profiles. We employed various clustering evaluation\nmetrics and incorporated input from a clinical domain expert to validate and\ninterpret the identified physiological states. Further, we discovered how\nspecific clinical events and interventions can influence patient states and\nstate transitions.\n","authors":["Hamid Ghaderi","Brandon Foreman","Amin Nayebi","Sindhu Tipirneni","Chandan K. Reddy","Vignesh Subbian"],"pdf_url":"https://arxiv.org/pdf/2303.13024v1.pdf","comment":"10 pages, 7 figures, 2 tables"},{"id":"http://arxiv.org/abs/2303.13022v1","updated":"2023-03-23T04:12:07Z","published":"2023-03-23T04:12:07Z","title":"ENVIDR: Implicit Differentiable Renderer with Neural Environment\n  Lighting","summary":"  Recent advances in neural rendering have shown great potential for\nreconstructing scenes from multiview images. However, accurately representing\nobjects with glossy surfaces remains a challenge for existing methods. In this\nwork, we introduce ENVIDR, a rendering and modeling framework for high-quality\nrendering and reconstruction of surfaces with challenging specular reflections.\nTo achieve this, we first propose a novel neural renderer with decomposed\nrendering components to learn the interaction between surface and environment\nlighting. This renderer is trained using existing physically based renderers\nand is decoupled from actual scene representations. We then propose an\nSDF-based neural surface model that leverages this learned neural renderer to\nrepresent general scenes. Our model additionally synthesizes indirect\nilluminations caused by inter-reflections from shiny surfaces by marching\nsurface-reflected rays. We demonstrate that our method outperforms state-of-art\nmethods on challenging shiny scenes, providing high-quality rendering of\nspecular reflections while also enabling material editing and scene relighting.\n","authors":["Ruofan Liang","Huiting Chen","Chunlin Li","Fan Chen","Selvakumar Panneer","Nandita Vijaykumar"],"pdf_url":"https://arxiv.org/pdf/2303.13022v1.pdf","comment":"Project page: https://nexuslrf.github.io/ENVIDR/"},{"id":"http://arxiv.org/abs/2211.08573v5","updated":"2023-03-23T03:41:06Z","published":"2022-11-15T23:35:15Z","title":"Realization of Causal Representation Learning to Adjust Confounding Bias\n  in Latent Space","summary":"  Causal DAGs(Directed Acyclic Graphs) are usually considered in a 2D plane.\nEdges indicate causal effects' directions and imply their corresponding\ntime-passings. Due to the natural restriction of statistical models, effect\nestimation is usually approximated by averaging the individuals' correlations,\ni.e., observational changes over a specific time. However, in the context of\nMachine Learning on large-scale questions with complex DAGs, such slight biases\ncan snowball to distort global models - More importantly, it has practically\nimpeded the development of AI, for instance, the weak generalizability of\ncausal models. In this paper, we redefine causal DAG as \\emph{do-DAG}, in which\nvariables' values are no longer time-stamp-dependent, and timelines can be seen\nas axes. By geometric explanation of multi-dimensional do-DAG, we identify the\n\\emph{Causal Representation Bias} and its necessary factors, differentiated\nfrom common confounding biases. Accordingly, a DL(Deep Learning)-based\nframework will be proposed as the general solution, along with a realization\nmethod and experiments to verify its feasibility.\n","authors":["Jia Li","Xiang Li","Xiaowei Jia","Michael Steinbach","Vipin Kumar"],"pdf_url":"https://arxiv.org/pdf/2211.08573v5.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.13015v1","updated":"2023-03-23T03:39:12Z","published":"2023-03-23T03:39:12Z","title":"Failure-tolerant Distributed Learning for Anomaly Detection in Wireless\n  Networks","summary":"  The analysis of distributed techniques is often focused upon their\nefficiency, without considering their robustness (or lack thereof). Such a\nconsideration is particularly important when devices or central servers can\nfail, which can potentially cripple distributed systems. When such failures\narise in wireless communications networks, important services that they\nuse/provide (like anomaly detection) can be left inoperable and can result in a\ncascade of security problems. In this paper, we present a novel method to\naddress these risks by combining both flat- and star-topologies, combining the\nperformance and reliability benefits of both. We refer to this method as\n\"Tol-FL\", due to its increased failure-tolerance as compared to the technique\nof Federated Learning. Our approach both limits device failure risks while\noutperforming prior methods by up to 8% in terms of anomaly detection AUROC in\na range of realistic settings that consider client as well as server failure,\nall while reducing communication costs. This performance demonstrates that\nTol-FL is a highly suitable method for distributed model training for anomaly\ndetection, especially in the domain of wireless networks.\n","authors":["Marc Katzef","Andrew C. Cullen","Tansu Alpcan","Christopher Leckie","Justin Kopacz"],"pdf_url":"https://arxiv.org/pdf/2303.13015v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2206.11488v3","updated":"2023-03-23T03:27:40Z","published":"2022-06-23T06:02:33Z","title":"On the Importance and Applicability of Pre-Training for Federated\n  Learning","summary":"  Pre-training is prevalent in nowadays deep learning to improve the learned\nmodel's performance. However, in the literature on federated learning (FL),\nneural networks are mostly initialized with random weights. These attract our\ninterest in conducting a systematic study to explore pre-training for FL.\nAcross multiple visual recognition benchmarks, we found that pre-training can\nnot only improve FL, but also close its accuracy gap to the counterpart\ncentralized learning, especially in the challenging cases of non-IID clients'\ndata. To make our findings applicable to situations where pre-trained models\nare not directly available, we explore pre-training with synthetic data or even\nwith clients' data in a decentralized manner, and found that they can already\nimprove FL notably. Interestingly, many of the techniques we explore are\ncomplementary to each other to further boost the performance, and we view this\nas a critical result toward scaling up deep FL for real-world applications. We\nconclude our paper with an attempt to understand the effect of pre-training on\nFL. We found that pre-training enables the learned global models under\ndifferent clients' data conditions to converge to the same loss basin, and\nmakes global aggregation in FL more stable. Nevertheless, pre-training seems to\nnot alleviate local model drifting, a fundamental problem in FL under non-IID\ndata.\n","authors":["Hong-You Chen","Cheng-Hao Tu","Ziwei Li","Han-Wei Shen","Wei-Lun Chao"],"pdf_url":"https://arxiv.org/pdf/2206.11488v3.pdf","comment":"Accepted to ICLR 2023"},{"id":"http://arxiv.org/abs/2303.13010v1","updated":"2023-03-23T03:13:04Z","published":"2023-03-23T03:13:04Z","title":"Semantic Image Attack for Visual Model Diagnosis","summary":"  In practice, metric analysis on a specific train and test dataset does not\nguarantee reliable or fair ML models. This is partially due to the fact that\nobtaining a balanced, diverse, and perfectly labeled dataset is typically\nexpensive, time-consuming, and error-prone. Rather than relying on a carefully\ndesigned test set to assess ML models' failures, fairness, or robustness, this\npaper proposes Semantic Image Attack (SIA), a method based on the adversarial\nattack that provides semantic adversarial images to allow model diagnosis,\ninterpretability, and robustness. Traditional adversarial training is a popular\nmethodology for robustifying ML models against attacks. However, existing\nadversarial methods do not combine the two aspects that enable the\ninterpretation and analysis of the model's flaws: semantic traceability and\nperceptual quality. SIA combines the two features via iterative gradient ascent\non a predefined semantic attribute space and the image space. We illustrate the\nvalidity of our approach in three scenarios for keypoint detection and\nclassification. (1) Model diagnosis: SIA generates a histogram of attributes\nthat highlights the semantic vulnerability of the ML model (i.e., attributes\nthat make the model fail). (2) Stronger attacks: SIA generates adversarial\nexamples with visually interpretable attributes that lead to higher attack\nsuccess rates than baseline methods. The adversarial training on SIA improves\nthe transferable robustness across different gradient-based attacks. (3)\nRobustness to imbalanced datasets: we use SIA to augment the underrepresented\nclasses, which outperforms strong augmentation and re-balancing baselines.\n","authors":["Jinqi Luo","Zhaoning Wang","Chen Henry Wu","Dong Huang","Fernando De la Torre"],"pdf_url":"https://arxiv.org/pdf/2303.13010v1.pdf","comment":"Initial version submitted to NeurIPS 2022"},{"id":"http://arxiv.org/abs/2303.13006v1","updated":"2023-03-23T03:02:09Z","published":"2023-03-23T03:02:09Z","title":"Controllable Inversion of Black-Box Face-Recognition Models via\n  Diffusion","summary":"  Face recognition models embed a face image into a low-dimensional identity\nvector containing abstract encodings of identity-specific facial features that\nallow individuals to be distinguished from one another. We tackle the\nchallenging task of inverting the latent space of pre-trained face recognition\nmodels without full model access (i.e. black-box setting). A variety of methods\nhave been proposed in literature for this task, but they have serious\nshortcomings such as a lack of realistic outputs, long inference times, and\nstrong requirements for the data set and accessibility of the face recognition\nmodel. Through an analysis of the black-box inversion problem, we show that the\nconditional diffusion model loss naturally emerges and that we can effectively\nsample from the inverse distribution even without an identity-specific loss.\nOur method, named identity denoising diffusion probabilistic model (ID3PM),\nleverages the stochastic nature of the denoising diffusion process to produce\nhigh-quality, identity-preserving face images with various backgrounds,\nlighting, poses, and expressions. We demonstrate state-of-the-art performance\nin terms of identity preservation and diversity both qualitatively and\nquantitatively. Our method is the first black-box face recognition model\ninversion method that offers intuitive control over the generation process and\ndoes not suffer from any of the common shortcomings from competing methods.\n","authors":["Manuel Kansy","Anton Raël","Graziana Mignone","Jacek Naruniec","Christopher Schroers","Markus Gross","Romann M. Weber"],"pdf_url":"https://arxiv.org/pdf/2303.13006v1.pdf","comment":"34 pages. Preprint. Under review"},{"id":"http://arxiv.org/abs/2303.13004v1","updated":"2023-03-23T02:58:14Z","published":"2023-03-23T02:58:14Z","title":"Adversarially Contrastive Estimation of Conditional Neural Processes","summary":"  Conditional Neural Processes~(CNPs) formulate distributions over functions\nand generate function observations with exact conditional likelihoods. CNPs,\nhowever, have limited expressivity for high-dimensional observations, since\ntheir predictive distribution is factorized into a product of unconstrained\n(typically) Gaussian outputs. Previously, this could be handled using latent\nvariables or autoregressive likelihood, but at the expense of intractable\ntraining and quadratically increased complexity. Instead, we propose\ncalibrating CNPs with an adversarial training scheme besides regular maximum\nlikelihood estimates. Specifically, we train an energy-based model (EBM) with\nnoise contrastive estimation, which enforces EBM to identify true observations\nfrom the generations of CNP. In this way, CNP must generate predictions closer\nto the ground-truth to fool EBM, instead of merely optimizing with respect to\nthe fixed-form likelihood. From generative function reconstruction to\ndownstream regression and classification tasks, we demonstrate that our method\nfits mainstream CNP members, showing effectiveness when unconstrained Gaussian\nlikelihood is defined, requiring minimal computation overhead while preserving\nfoundation properties of CNPs.\n","authors":["Zesheng Ye","Jing Du","Lina Yao"],"pdf_url":"https://arxiv.org/pdf/2303.13004v1.pdf","comment":"29 pages"},{"id":"http://arxiv.org/abs/2303.13003v1","updated":"2023-03-23T02:55:50Z","published":"2023-03-23T02:55:50Z","title":"Benchmarking the Reliability of Post-training Quantization: a Particular\n  Focus on Worst-case Performance","summary":"  Post-training quantization (PTQ) is a popular method for compressing deep\nneural networks (DNNs) without modifying their original architecture or\ntraining procedures. Despite its effectiveness and convenience, the reliability\nof PTQ methods in the presence of some extrem cases such as distribution shift\nand data noise remains largely unexplored. This paper first investigates this\nproblem on various commonly-used PTQ methods. We aim to answer several research\nquestions related to the influence of calibration set distribution variations,\ncalibration paradigm selection, and data augmentation or sampling strategies on\nPTQ reliability. A systematic evaluation process is conducted across a wide\nrange of tasks and commonly-used PTQ paradigms. The results show that most\nexisting PTQ methods are not reliable enough in term of the worst-case group\nperformance, highlighting the need for more robust methods. Our findings\nprovide insights for developing PTQ methods that can effectively handle\ndistribution shift scenarios and enable the deployment of quantized DNNs in\nreal-world applications.\n","authors":["Zhihang Yuan","Jiawei Liu","Jiaxiang Wu","Dawei Yang","Qiang Wu","Guangyu Sun","Wenyu Liu","Xinggang Wang","Bingzhe Wu"],"pdf_url":"https://arxiv.org/pdf/2303.13003v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.13002v1","updated":"2023-03-23T02:51:50Z","published":"2023-03-23T02:51:50Z","title":"Planning Goals for Exploration","summary":"  Dropped into an unknown environment, what should an agent do to quickly learn\nabout the environment and how to accomplish diverse tasks within it? We address\nthis question within the goal-conditioned reinforcement learning paradigm, by\nidentifying how the agent should set its goals at training time to maximize\nexploration. We propose \"Planning Exploratory Goals\" (PEG), a method that sets\ngoals for each training episode to directly optimize an intrinsic exploration\nreward. PEG first chooses goal commands such that the agent's goal-conditioned\npolicy, at its current level of training, will end up in states with high\nexploration potential. It then launches an exploration policy starting at those\npromising states. To enable this direct optimization, PEG learns world models\nand adapts sampling-based planning algorithms to \"plan goal commands\". In\nchallenging simulated robotics environments including a multi-legged ant robot\nin a maze, and a robot arm on a cluttered tabletop, PEG exploration enables\nmore efficient and effective training of goal-conditioned policies relative to\nbaselines and ablations. Our ant successfully navigates a long maze, and the\nrobot arm successfully builds a stack of three blocks upon command. Website:\nhttps://penn-pal-lab.github.io/peg/\n","authors":["Edward S. Hu","Richard Chang","Oleh Rybkin","Dinesh Jayaraman"],"pdf_url":"https://arxiv.org/pdf/2303.13002v1.pdf","comment":"Camera Ready version for ICLR2023 Spotlight"},{"id":"http://arxiv.org/abs/2303.12999v1","updated":"2023-03-23T02:42:10Z","published":"2023-03-23T02:42:10Z","title":"Automated Federated Learning in Mobile Edge Networks -- Fast Adaptation\n  and Convergence","summary":"  Federated Learning (FL) can be used in mobile edge networks to train machine\nlearning models in a distributed manner. Recently, FL has been interpreted\nwithin a Model-Agnostic Meta-Learning (MAML) framework, which brings FL\nsignificant advantages in fast adaptation and convergence over heterogeneous\ndatasets. However, existing research simply combines MAML and FL without\nexplicitly addressing how much benefit MAML brings to FL and how to maximize\nsuch benefit over mobile edge networks. In this paper, we quantify the benefit\nfrom two aspects: optimizing FL hyperparameters (i.e., sampled data size and\nthe number of communication rounds) and resource allocation (i.e., transmit\npower) in mobile edge networks. Specifically, we formulate the MAML-based FL\ndesign as an overall learning time minimization problem, under the constraints\nof model accuracy and energy consumption. Facilitated by the convergence\nanalysis of MAML-based FL, we decompose the formulated problem and then solve\nit using analytical solutions and the coordinate descent method. With the\nobtained FL hyperparameters and resource allocation, we design a MAML-based FL\nalgorithm, called Automated Federated Learning (AutoFL), that is able to\nconduct fast adaptation and convergence. Extensive experimental results verify\nthat AutoFL outperforms other benchmark algorithms regarding the learning time\nand convergence performance.\n","authors":["Chaoqun You","Kun Guo","Gang Feng","Peng Yang","Tony Q. S. Quek"],"pdf_url":"https://arxiv.org/pdf/2303.12999v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.12992v1","updated":"2023-03-23T02:09:18Z","published":"2023-03-23T02:09:18Z","title":"A Survey of Historical Learning: Learning Models with Learning History","summary":"  New knowledge originates from the old. The various types of elements,\ndeposited in the training history, are a large amount of wealth for improving\nlearning deep models. In this survey, we comprehensively review and summarize\nthe topic--``Historical Learning: Learning Models with Learning History'',\nwhich learns better neural models with the help of their learning history\nduring its optimization, from three detailed aspects: Historical Type (what),\nFunctional Part (where) and Storage Form (how). To our best knowledge, it is\nthe first survey that systematically studies the methodologies which make use\nof various historical statistics when training deep neural networks. The\ndiscussions with related topics like recurrent/memory networks, ensemble\nlearning, and reinforcement learning are demonstrated. We also expose future\nchallenges of this topic and encourage the community to pay attention to the\nthink of historical learning principles when designing algorithms. The paper\nlist related to historical learning is available at\n\\url{https://github.com/Martinser/Awesome-Historical-Learning.}\n","authors":["Xiang Li","Ge Wu","Lingfeng Yang","Wenhai Wang","Renjie Song","Jian Yang"],"pdf_url":"https://arxiv.org/pdf/2303.12992v1.pdf","comment":"Xiang Li and Ge Wu have equal contributions"},{"id":"http://arxiv.org/abs/2303.12982v1","updated":"2023-03-23T01:19:41Z","published":"2023-03-23T01:19:41Z","title":"Fault Prognosis of Turbofan Engines: Eventual Failure Prediction and\n  Remaining Useful Life Estimation","summary":"  In the era of industrial big data, prognostics and health management is\nessential to improve the prediction of future failures to minimize inventory,\nmaintenance, and human costs. Used for the 2021 PHM Data Challenge, the new\nCommercial Modular Aero-Propulsion System Simulation dataset from NASA is an\nopen-source benchmark containing simulated turbofan engine units flown under\nrealistic flight conditions. Deep learning approaches implemented previously\nfor this application attempt to predict the remaining useful life of the engine\nunits, but have not utilized labeled failure mode information, impeding\npractical usage and explainability. To address these limitations, a new\nprognostics approach is formulated with a customized loss function to\nsimultaneously predict the current health state, the eventual failing\ncomponent(s), and the remaining useful life. The proposed method incorporates\nprincipal component analysis to orthogonalize statistical time-domain features,\nwhich are inputs into supervised regressors such as random forests, extreme\nrandom forests, XGBoost, and artificial neural networks. The highest performing\nalgorithm, ANN-Flux, achieves AUROC and AUPR scores exceeding 0.95 for each\nclassification. In addition, ANN-Flux reduces the remaining useful life RMSE by\n38% for the same test split of the dataset compared to past work, with\nsignificantly less computational cost.\n","authors":["Joseph Cohen","Xun Huan","Jun Ni"],"pdf_url":"https://arxiv.org/pdf/2303.12982v1.pdf","comment":"Preprint with 10 pages, 5 figures. Submitted to International Journal\n  of Prognostics and Health Management (IJPHM)"},{"id":"http://arxiv.org/abs/2303.12981v1","updated":"2023-03-23T01:14:36Z","published":"2023-03-23T01:14:36Z","title":"Connected Superlevel Set in (Deep) Reinforcement Learning and its\n  Application to Minimax Theorems","summary":"  The aim of this paper is to improve the understanding of the optimization\nlandscape for policy optimization problems in reinforcement learning.\nSpecifically, we show that the superlevel set of the objective function with\nrespect to the policy parameter is always a connected set both in the tabular\nsetting and under policies represented by a class of neural networks. In\naddition, we show that the optimization objective as a function of the policy\nparameter and reward satisfies a stronger \"equiconnectedness\" property. To our\nbest knowledge, these are novel and previously unknown discoveries.\n  We present an application of the connectedness of these superlevel sets to\nthe derivation of minimax theorems for robust reinforcement learning. We show\nthat any minimax optimization program which is convex on one side and is\nequiconnected on the other side observes the minimax equality (i.e. has a Nash\nequilibrium). We find that this exact structure is exhibited by an interesting\nrobust reinforcement learning problem under an adversarial reward attack, and\nthe validity of its minimax equality immediately follows. This is the first\ntime such a result is established in the literature.\n","authors":["Sihan Zeng","Thinh T. Doan","Justin Romberg"],"pdf_url":"https://arxiv.org/pdf/2303.12981v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2212.12440v3","updated":"2023-03-23T01:07:14Z","published":"2022-12-23T16:14:53Z","title":"HAC-Net: A Hybrid Attention-Based Convolutional Neural Network for\n  Highly Accurate Protein-Ligand Binding Affinity Prediction","summary":"  Applying deep learning concepts from image detection and graph theory has\ngreatly advanced protein-ligand binding affinity prediction, a challenge with\nenormous ramifications for both drug discovery and protein engineering. We\nbuild upon these advances by designing a novel deep learning architecture\nconsisting of a 3-dimensional convolutional neural network utilizing\nchannel-wise attention and two graph convolutional networks utilizing\nattention-based aggregation of node features. HAC-Net (Hybrid Attention-Based\nConvolutional Neural Network) obtains state-of-the-art results on the PDBbind\nv.2016 core set, the most widely recognized benchmark in the field. We\nextensively assess the generalizability of our model using multiple train-test\nsplits, each of which maximizes differences between either protein structures,\nprotein sequences, or ligand extended-connectivity fingerprints of complexes in\nthe training and test sets. Furthermore, we perform 10-fold cross-validation\nwith a similarity cutoff between SMILES strings of ligands in the training and\ntest sets, and also evaluate the performance of HAC-Net on lower-quality data.\nWe envision that this model can be extended to a broad range of supervised\nlearning problems related to structure-based biomolecular property prediction.\nAll of our software is available as open source at\nhttps://github.com/gregory-kyro/HAC-Net/, and the HACNet Python package is\navailable through PyPI.\n","authors":["Gregory W. Kyro","Rafael I. Brent","Victor S. Batista"],"pdf_url":"https://arxiv.org/pdf/2212.12440v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2301.11241v2","updated":"2023-03-23T00:49:44Z","published":"2023-01-26T17:25:45Z","title":"On the Convergence of No-Regret Learning Dynamics in Time-Varying Games","summary":"  Most of the literature on learning in games has focused on the restrictive\nsetting where the underlying repeated game does not change over time. Much less\nis known about the convergence of no-regret learning algorithms in dynamic\nmultiagent settings. In this paper, we characterize the convergence of\noptimistic gradient descent (OGD) in time-varying games. Our framework yields\nsharp convergence bounds for the equilibrium gap of OGD in zero-sum games\nparameterized on natural variation measures of the sequence of games, subsuming\nknown results for static games. Furthermore, we establish improved second-order\nvariation bounds under strong convexity-concavity, as long as each game is\nrepeated multiple times. Our results also apply to time-varying general-sum\nmulti-player games via a bilinear formulation of correlated equilibria, which\nhas novel implications for meta-learning and for obtaining refined\nvariation-dependent regret bounds, addressing questions left open in prior\npapers. Finally, we leverage our framework to also provide new insights on\ndynamic regret guarantees in static games.\n","authors":["Ioannis Anagnostides","Ioannis Panageas","Gabriele Farina","Tuomas Sandholm"],"pdf_url":"https://arxiv.org/pdf/2301.11241v2.pdf","comment":"V2 clarifies connections with prior work"},{"id":"http://arxiv.org/abs/2303.12964v1","updated":"2023-03-23T00:11:17Z","published":"2023-03-23T00:11:17Z","title":"Continuous Indeterminate Probability Neural Network","summary":"  This paper introduces a general model called CIPNN - Continuous Indeterminate\nProbability Neural Network, and this model is based on IPNN, which is used for\ndiscrete latent random variables. Currently, posterior of continuous latent\nvariables is regarded as intractable, with the new theory proposed by IPNN this\nproblem can be solved. Our contributions are Four-fold. First, we derive the\nanalytical solution of the posterior calculation of continuous latent random\nvariables and propose a general classification model (CIPNN). Second, we\npropose a general auto-encoder called CIPAE - Continuous Indeterminate\nProbability Auto-Encoder, the decoder part is not a neural network and uses a\nfully probabilistic inference model for the first time. Third, we propose a new\nmethod to visualize the latent random variables, we use one of N dimensional\nlatent variables as a decoder to reconstruct the input image, which can work\neven for classification tasks, in this way, we can see what each latent\nvariable has learned. Fourth, IPNN has shown great classification capability,\nCIPNN has pushed this classification capability to infinity. Theoretical\nadvantages are reflected in experimental results.\n","authors":["Tao Yang"],"pdf_url":"https://arxiv.org/pdf/2303.12964v1.pdf","comment":"8 pages"},{"id":"http://arxiv.org/abs/2303.12963v1","updated":"2023-03-23T00:03:07Z","published":"2023-03-23T00:03:07Z","title":"Forecast-Aware Model Driven LSTM","summary":"  Poor air quality can have a significant impact on human health. The National\nOceanic and Atmospheric Administration (NOAA) air quality forecasting guidance\nis challenged by the increasing presence of extreme air quality events due to\nextreme weather events such as wild fires and heatwaves. These extreme air\nquality events further affect human health. Traditional methods used to correct\nmodel bias make assumptions about linearity and the underlying distribution.\nExtreme air quality events tend to occur without a strong signal leading up to\nthe event and this behavior tends to cause existing methods to either under or\nover compensate for the bias. Deep learning holds promise for air quality\nforecasting in the presence of extreme air quality events due to its ability to\ngeneralize and learn nonlinear problems. However, in the presence of these\nanomalous air quality events, standard deep network approaches that use a\nsingle network for generalizing to future forecasts, may not always provide the\nbest performance even with a full feature-set including geography and\nmeteorology. In this work we describe a method that combines unsupervised\nlearning and a forecast-aware bi-directional LSTM network to perform bias\ncorrection for operational air quality forecasting using AirNow station data\nfor ozone and PM2.5 in the continental US. Using an unsupervised clustering\nmethod trained on station geographical features such as latitude and longitude,\nurbanization, and elevation, the learned clusters direct training by\npartitioning the training data for the LSTM networks. LSTMs are forecast-aware\nand implemented using a unique way to perform learning forward and backwards in\ntime across forecasting days. When comparing the RMSE of the forecast model to\nthe RMSE of the bias corrected model, the bias corrected model shows\nsignificant improvement (27\\% lower RMSE for ozone) over the base forecast.\n","authors":["Sophia Hamer","Jennifer Sleeman","Ivanka Stajner"],"pdf_url":"https://arxiv.org/pdf/2303.12963v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.13714v1","updated":"2023-03-23T23:45:19Z","published":"2023-03-23T23:45:19Z","title":"High Fidelity Image Synthesis With Deep VAEs In Latent Space","summary":"  We present fast, realistic image generation on high-resolution, multimodal\ndatasets using hierarchical variational autoencoders (VAEs) trained on a\ndeterministic autoencoder's latent space. In this two-stage setup, the\nautoencoder compresses the image into its semantic features, which are then\nmodeled with a deep VAE. With this method, the VAE avoids modeling the\nfine-grained details that constitute the majority of the image's code length,\nallowing it to focus on learning its structural components. We demonstrate the\neffectiveness of our two-stage approach, achieving a FID of 9.34 on the\nImageNet-256 dataset which is comparable to BigGAN. We make our implementation\navailable online.\n","authors":["Troy Luhman","Eric Luhman"],"pdf_url":"https://arxiv.org/pdf/2303.13714v1.pdf","comment":"19 pages, 16 figures"},{"id":"http://arxiv.org/abs/2303.13712v1","updated":"2023-03-23T23:24:26Z","published":"2023-03-23T23:24:26Z","title":"Decision-aid or Controller? Steering Human Decision Makers with\n  Algorithms","summary":"  Algorithms are used to aid human decision makers by making predictions and\nrecommending decisions. Currently, these algorithms are trained to optimize\nprediction accuracy. What if they were optimized to control final decisions? In\nthis paper, we study a decision-aid algorithm that learns about the human\ndecision maker and provides ''personalized recommendations'' to influence final\ndecisions. We first consider fixed human decision functions which map\nobservable features and the algorithm's recommendations to final decisions. We\ncharacterize the conditions under which perfect control over final decisions is\nattainable. Under fairly general assumptions, the parameters of the human\ndecision function can be identified from past interactions between the\nalgorithm and the human decision maker, even when the algorithm was constrained\nto make truthful recommendations. We then consider a decision maker who is\naware of the algorithm's manipulation and responds strategically. By posing the\nsetting as a variation of the cheap talk game [Crawford and Sobel, 1982], we\nshow that all equilibria are partition equilibria where only coarse information\nis shared: the algorithm recommends an interval containing the ideal decision.\nWe discuss the potential applications of such algorithms and their social\nimplications.\n","authors":["Ruqing Xu","Sarah Dean"],"pdf_url":"https://arxiv.org/pdf/2303.13712v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2210.01936v3","updated":"2023-03-23T23:21:38Z","published":"2022-10-04T22:13:25Z","title":"When and why vision-language models behave like bags-of-words, and what\n  to do about it?","summary":"  Despite the success of large vision and language models (VLMs) in many\ndownstream applications, it is unclear how well they encode compositional\ninformation. Here, we create the Attribution, Relation, and Order (ARO)\nbenchmark to systematically evaluate the ability of VLMs to understand\ndifferent types of relationships, attributes, and order. ARO consists of Visual\nGenome Attribution, to test the understanding of objects' properties; Visual\nGenome Relation, to test for relational understanding; and COCO &\nFlickr30k-Order, to test for order sensitivity. ARO is orders of magnitude\nlarger than previous benchmarks of compositionality, with more than 50,000 test\ncases. We show where state-of-the-art VLMs have poor relational understanding,\ncan blunder when linking objects to their attributes, and demonstrate a severe\nlack of order sensitivity. VLMs are predominantly trained and evaluated on\nlarge datasets with rich compositional structure in the images and captions.\nYet, training on these datasets has not been enough to address the lack of\ncompositional understanding, and evaluating on these datasets has failed to\nsurface this deficiency. To understand why these limitations emerge and are not\nrepresented in the standard tests, we zoom into the evaluation and training\nprocedures. We demonstrate that it is possible to perform well on retrieval\nover existing datasets without using the composition and order information.\nGiven that contrastive pretraining optimizes for retrieval on datasets with\nsimilar shortcuts, we hypothesize that this can explain why the models do not\nneed to learn to represent compositional information. This finding suggests a\nnatural solution: composition-aware hard negative mining. We show that a\nsimple-to-implement modification of contrastive learning significantly improves\nthe performance on tasks requiring understanding of order and compositionality.\n","authors":["Mert Yuksekgonul","Federico Bianchi","Pratyusha Kalluri","Dan Jurafsky","James Zou"],"pdf_url":"https://arxiv.org/pdf/2210.01936v3.pdf","comment":"ICLR 2023 Oral (notable-top-5%)"},{"id":"http://arxiv.org/abs/2303.13703v1","updated":"2023-03-23T22:43:52Z","published":"2023-03-23T22:43:52Z","title":"End-to-End Diffusion Latent Optimization Improves Classifier Guidance","summary":"  Classifier guidance -- using the gradients of an image classifier to steer\nthe generations of a diffusion model -- has the potential to dramatically\nexpand the creative control over image generation and editing. However,\ncurrently classifier guidance requires either training new noise-aware models\nto obtain accurate gradients or using a one-step denoising approximation of the\nfinal generation, which leads to misaligned gradients and sub-optimal control.\nWe highlight this approximation's shortcomings and propose a novel guidance\nmethod: Direct Optimization of Diffusion Latents (DOODL), which enables\nplug-and-play guidance by optimizing diffusion latents w.r.t. the gradients of\na pre-trained classifier on the true generated pixels, using an invertible\ndiffusion process to achieve memory-efficient backpropagation. Showcasing the\npotential of more precise guidance, DOODL outperforms one-step classifier\nguidance on computational and human evaluation metrics across different forms\nof guidance: using CLIP guidance to improve generations of complex prompts from\nDrawBench, using fine-grained visual classifiers to expand the vocabulary of\nStable Diffusion, enabling image-conditioned generation with a CLIP visual\nencoder, and improving image aesthetics using an aesthetic scoring network.\n","authors":["Bram Wallace","Akash Gokul","Stefano Ermon","Nikhil Naik"],"pdf_url":"https://arxiv.org/pdf/2303.13703v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2206.03324v2","updated":"2023-03-23T22:13:46Z","published":"2022-06-05T17:22:29Z","title":"Efficient decentralized multi-agent learning in asymmetric bipartite\n  queueing systems","summary":"  We study decentralized multi-agent learning in bipartite queueing systems, a\nstandard model for service systems. In particular, N agents request service\nfrom K servers in a fully decentralized way, i.e, by running the same algorithm\nwithout communication. Previous decentralized algorithms are restricted to\nsymmetric systems, have performance that is degrading exponentially in the\nnumber of servers, require communication through shared randomness and unique\nagent identities, and are computationally demanding. In contrast, we provide a\nsimple learning algorithm that, when run decentrally by each agent, leads the\nqueueing system to have efficient performance in general asymmetric bipartite\nqueueing systems while also having additional robustness properties. Along the\nway, we provide the first provably efficient UCB-based algorithm for the\ncentralized case of the problem.\n","authors":["Daniel Freund","Thodoris Lykouris","Wentao Weng"],"pdf_url":"https://arxiv.org/pdf/2206.03324v2.pdf","comment":"A preliminary version of this work was accepted for presentation at\n  the Conference on Learning Theory (COLT) 2022. Compared to the first version\n  of the paper, the current version expands upon the related work and adds\n  intuition on the technical content"},{"id":"http://arxiv.org/abs/2212.08013v2","updated":"2023-03-23T21:38:16Z","published":"2022-12-15T18:18:38Z","title":"FlexiViT: One Model for All Patch Sizes","summary":"  Vision Transformers convert images to sequences by slicing them into patches.\nThe size of these patches controls a speed/accuracy tradeoff, with smaller\npatches leading to higher accuracy at greater computational cost, but changing\nthe patch size typically requires retraining the model. In this paper, we\ndemonstrate that simply randomizing the patch size at training time leads to a\nsingle set of weights that performs well across a wide range of patch sizes,\nmaking it possible to tailor the model to different compute budgets at\ndeployment time. We extensively evaluate the resulting model, which we call\nFlexiViT, on a wide range of tasks, including classification, image-text\nretrieval, open-world detection, panoptic segmentation, and semantic\nsegmentation, concluding that it usually matches, and sometimes outperforms,\nstandard ViT models trained at a single patch size in an otherwise identical\nsetup. Hence, FlexiViT training is a simple drop-in improvement for ViT that\nmakes it easy to add compute-adaptive capabilities to most models relying on a\nViT backbone architecture. Code and pre-trained models are available at\nhttps://github.com/google-research/big_vision\n","authors":["Lucas Beyer","Pavel Izmailov","Alexander Kolesnikov","Mathilde Caron","Simon Kornblith","Xiaohua Zhai","Matthias Minderer","Michael Tschannen","Ibrahim Alabdulmohsin","Filip Pavetic"],"pdf_url":"https://arxiv.org/pdf/2212.08013v2.pdf","comment":"Code and pre-trained models available at\n  https://github.com/google-research/big_vision. All authors made significant\n  technical contributions. CVPR 2023"},{"id":"http://arxiv.org/abs/2303.13683v1","updated":"2023-03-23T21:30:29Z","published":"2023-03-23T21:30:29Z","title":"OFA$^2$: A Multi-Objective Perspective for the Once-for-All Neural\n  Architecture Search","summary":"  Once-for-All (OFA) is a Neural Architecture Search (NAS) framework designed\nto address the problem of searching efficient architectures for devices with\ndifferent resources constraints by decoupling the training and the searching\nstages. The computationally expensive process of training the OFA neural\nnetwork is done only once, and then it is possible to perform multiple searches\nfor subnetworks extracted from this trained network according to each\ndeployment scenario. In this work we aim to give one step further in the search\nfor efficiency by explicitly conceiving the search stage as a multi-objective\noptimization problem. A Pareto frontier is then populated with efficient, and\nalready trained, neural architectures exhibiting distinct trade-offs among the\nconflicting objectives. This could be achieved by using any multi-objective\nevolutionary algorithm during the search stage, such as NSGA-II and SMS-EMOA.\nIn other words, the neural network is trained once, the searching for\nsubnetworks considering different hardware constraints is also done one single\ntime, and then the user can choose a suitable neural network according to each\ndeployment scenario. The conjugation of OFA and an explicit algorithm for\nmulti-objective optimization opens the possibility of a posteriori\ndecision-making in NAS, after sampling efficient subnetworks which are a very\ngood approximation of the Pareto frontier, given that those subnetworks are\nalready trained and ready to use. The source code and the final search\nalgorithm will be released at https://github.com/ito-rafael/once-for-all-2\n","authors":["Rafael C. Ito","Fernando J. Von Zuben"],"pdf_url":"https://arxiv.org/pdf/2303.13683v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2302.01574v2","updated":"2023-03-23T21:20:38Z","published":"2023-02-03T07:04:33Z","title":"An Operational Perspective to Fairness Interventions: Where and How to\n  Intervene","summary":"  As AI-based decision systems proliferate, their successful operationalization\nrequires balancing multiple desiderata: predictive performance, disparity\nacross groups, safeguarding sensitive group attributes (e.g., race), and\nengineering cost. We present a holistic framework for evaluating and\ncontextualizing fairness interventions with respect to the above desiderata.\nThe two key points of practical consideration are \\emph{where} (pre-, in-,\npost-processing) and \\emph{how} (in what way the sensitive group data is used)\nthe intervention is introduced. We demonstrate our framework with a case study\non predictive parity. In it, we first propose a novel method for achieving\npredictive parity fairness without using group data at inference time via\ndistibutionally robust optimization. Then, we showcase the effectiveness of\nthese methods in a benchmarking study of close to 400 variations across two\nmajor model types (XGBoost vs. Neural Net), ten datasets, and over twenty\nunique methodologies. Methodological insights derived from our empirical study\ninform the practical design of ML workflow with fairness as a central concern.\nWe find predictive parity is difficult to achieve without using group data, and\ndespite requiring group data during model training (but not inference),\ndistributionally robust methods we develop provide significant Pareto\nimprovement. Moreover, a plain XGBoost model often Pareto-dominates neural\nnetworks with fairness interventions, highlighting the importance of model\ninductive bias.\n","authors":["Brian Hsu","Xiaotong Chen","Ying Han","Hongseok Namkoong","Kinjal Basu"],"pdf_url":"https://arxiv.org/pdf/2302.01574v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2208.13049v3","updated":"2023-03-23T21:15:21Z","published":"2022-08-27T16:19:26Z","title":"TrojViT: Trojan Insertion in Vision Transformers","summary":"  Vision Transformers (ViTs) have demonstrated the state-of-the-art performance\nin various vision-related tasks. The success of ViTs motivates adversaries to\nperform backdoor attacks on ViTs. Although the vulnerability of traditional\nCNNs to backdoor attacks is well-known, backdoor attacks on ViTs are\nseldom-studied. Compared to CNNs capturing pixel-wise local features by\nconvolutions, ViTs extract global context information through patches and\nattentions. Na\\\"ively transplanting CNN-specific backdoor attacks to ViTs\nyields only a low clean data accuracy and a low attack success rate. In this\npaper, we propose a stealth and practical ViT-specific backdoor attack\n$TrojViT$. Rather than an area-wise trigger used by CNN-specific backdoor\nattacks, TrojViT generates a patch-wise trigger designed to build a Trojan\ncomposed of some vulnerable bits on the parameters of a ViT stored in DRAM\nmemory through patch salience ranking and attention-target loss. TrojViT\nfurther uses minimum-tuned parameter update to reduce the bit number of the\nTrojan. Once the attacker inserts the Trojan into the ViT model by flipping the\nvulnerable bits, the ViT model still produces normal inference accuracy with\nbenign inputs. But when the attacker embeds a trigger into an input, the ViT\nmodel is forced to classify the input to a predefined target class. We show\nthat flipping only few vulnerable bits identified by TrojViT on a ViT model\nusing the well-known RowHammer can transform the model into a backdoored one.\nWe perform extensive experiments of multiple datasets on various ViT models.\nTrojViT can classify $99.64\\%$ of test images to a target class by flipping\n$345$ bits on a ViT for ImageNet.\n","authors":["Mengxin Zheng","Qian Lou","Lei Jiang"],"pdf_url":"https://arxiv.org/pdf/2208.13049v3.pdf","comment":"10 pages, 4 figures, 11 tables"},{"id":"http://arxiv.org/abs/2303.13665v1","updated":"2023-03-23T20:44:36Z","published":"2023-03-23T20:44:36Z","title":"Clustering based on Mixtures of Sparse Gaussian Processes","summary":"  Creating low dimensional representations of a high dimensional data set is an\nimportant component in many machine learning applications. How to cluster data\nusing their low dimensional embedded space is still a challenging problem in\nmachine learning. In this article, we focus on proposing a joint formulation\nfor both clustering and dimensionality reduction. When a probabilistic model is\ndesired, one possible solution is to use the mixture models in which both\ncluster indicator and low dimensional space are learned. Our algorithm is based\non a mixture of sparse Gaussian processes, which is called Sparse Gaussian\nProcess Mixture Clustering (SGP-MIC). The main advantages to our approach over\nexisting methods are that the probabilistic nature of this model provides more\nadvantages over existing deterministic methods, it is straightforward to\nconstruct non-linear generalizations of the model, and applying a sparse model\nand an efficient variational EM approximation help to speed up the algorithm.\n","authors":["Zahra Moslehi","Abdolreza Mirzaei","Mehran Safayani"],"pdf_url":"https://arxiv.org/pdf/2303.13665v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.13664v1","updated":"2023-03-23T20:37:25Z","published":"2023-03-23T20:37:25Z","title":"Temperature Schedules for Self-Supervised Contrastive Methods on\n  Long-Tail Data","summary":"  Most approaches for self-supervised learning (SSL) are optimised on curated\nbalanced datasets, e.g. ImageNet, despite the fact that natural data usually\nexhibits long-tail distributions. In this paper, we analyse the behaviour of\none of the most popular variants of SSL, i.e. contrastive methods, on long-tail\ndata. In particular, we investigate the role of the temperature parameter\n$\\tau$ in the contrastive loss, by analysing the loss through the lens of\naverage distance maximisation, and find that a large $\\tau$ emphasises\ngroup-wise discrimination, whereas a small $\\tau$ leads to a higher degree of\ninstance discrimination. While $\\tau$ has thus far been treated exclusively as\na constant hyperparameter, in this work, we propose to employ a dynamic $\\tau$\nand show that a simple cosine schedule can yield significant improvements in\nthe learnt representations. Such a schedule results in a constant `task\nswitching' between an emphasis on instance discrimination and group-wise\ndiscrimination and thereby ensures that the model learns both group-wise\nfeatures, as well as instance-specific details. Since frequent classes benefit\nfrom the former, while infrequent classes require the latter, we find this\nmethod to consistently improve separation between the classes in long-tail data\nwithout any additional computational cost.\n","authors":["Anna Kukleva","Moritz Böhle","Bernt Schiele","Hilde Kuehne","Christian Rupprecht"],"pdf_url":"https://arxiv.org/pdf/2303.13664v1.pdf","comment":"ICLR 2023"},{"id":"http://arxiv.org/abs/2303.13657v1","updated":"2023-03-23T20:27:40Z","published":"2023-03-23T20:27:40Z","title":"Policy Evaluation in Distributional LQR","summary":"  Distributional reinforcement learning (DRL) enhances the understanding of the\neffects of the randomness in the environment by letting agents learn the\ndistribution of a random return, rather than its expected value as in standard\nRL. At the same time, a main challenge in DRL is that policy evaluation in DRL\ntypically relies on the representation of the return distribution, which needs\nto be carefully designed. In this paper, we address this challenge for a\nspecial class of DRL problems that rely on linear quadratic regulator (LQR) for\ncontrol, advocating for a new distributional approach to LQR, which we call\n\\emph{distributional LQR}. Specifically, we provide a closed-form expression of\nthe distribution of the random return which, remarkably, is applicable to all\nexogenous disturbances on the dynamics, as long as they are independent and\nidentically distributed (i.i.d.). While the proposed exact return distribution\nconsists of infinitely many random variables, we show that this distribution\ncan be approximated by a finite number of random variables, and the associated\napproximation error can be analytically bounded under mild assumptions. Using\nthe approximate return distribution, we propose a zeroth-order policy gradient\nalgorithm for risk-averse LQR using the Conditional Value at Risk (CVaR) as a\nmeasure of risk. Numerical experiments are provided to illustrate our\ntheoretical results.\n","authors":["Zifan Wang","Yulong Gao","Siyi Wang","Michael M. Zavlanos","Alessandro Abate","Karl H. Johansson"],"pdf_url":"https://arxiv.org/pdf/2303.13657v1.pdf","comment":"12pages"},{"id":"http://arxiv.org/abs/2303.13649v1","updated":"2023-03-23T20:13:44Z","published":"2023-03-23T20:13:44Z","title":"Adversarial Robustness and Feature Impact Analysis for Driver Drowsiness\n  Detection","summary":"  Drowsy driving is a major cause of road accidents, but drivers are dismissive\nof the impact that fatigue can have on their reaction times. To detect\ndrowsiness before any impairment occurs, a promising strategy is using Machine\nLearning (ML) to monitor Heart Rate Variability (HRV) signals. This work\npresents multiple experiments with different HRV time windows and ML models, a\nfeature impact analysis using Shapley Additive Explanations (SHAP), and an\nadversarial robustness analysis to assess their reliability when processing\nfaulty input data and perturbed HRV signals. The most reliable model was\nExtreme Gradient Boosting (XGB) and the optimal time window had between 120 and\n150 seconds. Furthermore, SHAP enabled the selection of the 18 most impactful\nfeatures and the training of new smaller models that achieved a performance as\ngood as the initial ones. Despite the susceptibility of all models to\nadversarial attacks, adversarial training enabled them to preserve\nsignificantly higher results, especially XGB. Therefore, ML models can\nsignificantly benefit from realistic adversarial training to provide a more\nrobust driver drowsiness detection.\n","authors":["João Vitorino","Lourenço Rodrigues","Eva Maia","Isabel Praça","André Lourenço"],"pdf_url":"https://arxiv.org/pdf/2303.13649v1.pdf","comment":"10 pages, 2 tables, 3 figures, AIME 2023 conference"},{"id":"http://arxiv.org/abs/2303.13637v1","updated":"2023-03-23T19:47:53Z","published":"2023-03-23T19:47:53Z","title":"Efficient and Direct Inference of Heart Rate Variability using Both\n  Signal Processing and Machine Learning","summary":"  Heart Rate Variability (HRV) measures the variation of the time between\nconsecutive heartbeats and is a major indicator of physical and mental health.\nRecent research has demonstrated that photoplethysmography (PPG) sensors can be\nused to infer HRV. However, many prior studies had high errors because they\nonly employed signal processing or machine learning (ML), or because they\nindirectly inferred HRV, or because there lacks large training datasets. Many\nprior studies may also require large ML models. The low accuracy and large\nmodel sizes limit their applications to small embedded devices and potential\nfuture use in healthcare. To address the above issues, we first collected a\nlarge dataset of PPG signals and HRV ground truth. With this dataset, we\ndeveloped HRV models that combine signal processing and ML to directly infer\nHRV. Evaluation results show that our method had errors between 3.5% to 25.7%\nand outperformed signal-processing-only and ML-only methods. We also explored\ndifferent ML models, which showed that Decision Trees and Multi-level\nPerceptrons have 13.0% and 9.1% errors on average with models at most hundreds\nof KB and inference time less than 1ms. Hence, they are more suitable for small\nembedded devices and potentially enable the future use of PPG-based HRV\nmonitoring in healthcare.\n","authors":["Yuntong Zhang","Jingye Xu","Mimi Xie","Dakai Zhu","Houbing Song","Wei Wang"],"pdf_url":"https://arxiv.org/pdf/2303.13637v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.13636v1","updated":"2023-03-23T19:47:36Z","published":"2023-03-23T19:47:36Z","title":"PPG-based Heart Rate Estimation with Efficient Sensor Sampling and\n  Learning Models","summary":"  Recent studies showed that Photoplethysmography (PPG) sensors embedded in\nwearable devices can estimate heart rate (HR) with high accuracy. However,\ndespite of prior research efforts, applying PPG sensor based HR estimation to\nembedded devices still faces challenges due to the energy-intensive\nhigh-frequency PPG sampling and the resource-intensive machine-learning models.\nIn this work, we aim to explore HR estimation techniques that are more suitable\nfor lower-power and resource-constrained embedded devices. More specifically,\nwe seek to design techniques that could provide high-accuracy HR estimation\nwith low-frequency PPG sampling, small model size, and fast inference time.\nFirst, we show that by combining signal processing and ML, it is possible to\nreduce the PPG sampling frequency from 125 Hz to only 25 Hz while providing\nhigher HR estimation accuracy. This combination also helps to reduce the ML\nmodel feature size, leading to smaller models. Additionally, we present a\ncomprehensive analysis on different ML models and feature sizes to compare\ntheir accuracy, model size, and inference time. The models explored include\nDecision Tree (DT), Random Forest (RF), K-nearest neighbor (KNN), Support\nvector machines (SVM), and Multi-layer perceptron (MLP). Experiments were\nconducted using both a widely-utilized dataset and our self-collected dataset.\nThe experimental results show that our method by combining signal processing\nand ML had only 5% error for HR estimation using low-frequency PPG data.\nMoreover, our analysis showed that DT models with 10 to 20 input features\nusually have good accuracy, while are several magnitude smaller in model sizes\nand faster in inference time.\n","authors":["Yuntong Zhang","Jingye Xu","Mimi Xie","Wei Wang","Keying Ye","Jing Wang","Dakai Zhu"],"pdf_url":"https://arxiv.org/pdf/2303.13636v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.13627v1","updated":"2023-03-23T19:32:31Z","published":"2023-03-23T19:32:31Z","title":"Associated Random Neural Networks for Collective Classification of Nodes\n  in Botnet Attacks","summary":"  Botnet attacks are a major threat to networked systems because of their\nability to turn the network nodes that they compromise into additional\nattackers, leading to the spread of high volume attacks over long periods. The\ndetection of such Botnets is complicated by the fact that multiple network IP\naddresses will be simultaneously compromised, so that Collective Classification\nof compromised nodes, in addition to the already available traditional methods\nthat focus on individual nodes, can be useful. Thus this work introduces a\ncollective Botnet attack classification technique that operates on traffic from\nan n-node IP network with a novel Associated Random Neural Network (ARNN) that\nidentifies the nodes which are compromised. The ARNN is a recurrent\narchitecture that incorporates two mutually associated, interconnected and\narchitecturally identical n-neuron random neural networks, that act\nsimultneously as mutual critics to reach the decision regarding which of n\nnodes have been compromised. A novel gradient learning descent algorithm is\npresented for the ARNN, and is shown to operate effectively both with\nconventional off-line training from prior data, and with on-line incremental\ntraining without prior off-line learning. Real data from a 107 node packet\nnetwork is used with over 700,000 packets to evaluate the ARNN, showing that it\nprovides accurate predictions. Comparisons with other well-known state of the\nart methods using the same learning and testing datasets, show that the ARNN\noffers significantly better performance.\n","authors":["Erol Gelenbe","Mert Nakıp"],"pdf_url":"https://arxiv.org/pdf/2303.13627v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.13610v1","updated":"2023-03-23T18:50:18Z","published":"2023-03-23T18:50:18Z","title":"Artificial-intelligence-based molecular classification of diffuse\n  gliomas using rapid, label-free optical imaging","summary":"  Molecular classification has transformed the management of brain tumors by\nenabling more accurate prognostication and personalized treatment. However,\ntimely molecular diagnostic testing for patients with brain tumors is limited,\ncomplicating surgical and adjuvant treatment and obstructing clinical trial\nenrollment. In this study, we developed DeepGlioma, a rapid ($< 90$ seconds),\nartificial-intelligence-based diagnostic screening system to streamline the\nmolecular diagnosis of diffuse gliomas. DeepGlioma is trained using a\nmultimodal dataset that includes stimulated Raman histology (SRH); a rapid,\nlabel-free, non-consumptive, optical imaging method; and large-scale, public\ngenomic data. In a prospective, multicenter, international testing cohort of\npatients with diffuse glioma ($n=153$) who underwent real-time SRH imaging, we\ndemonstrate that DeepGlioma can predict the molecular alterations used by the\nWorld Health Organization to define the adult-type diffuse glioma taxonomy (IDH\nmutation, 1p19q co-deletion and ATRX mutation), achieving a mean molecular\nclassification accuracy of $93.3\\pm 1.6\\%$. Our results represent how\nartificial intelligence and optical histology can be used to provide a rapid\nand scalable adjunct to wet lab methods for the molecular screening of patients\nwith diffuse glioma.\n","authors":["Todd C. Hollon","Cheng Jiang","Asadur Chowdury","Mustafa Nasir-Moin","Akhil Kondepudi","Alexander Aabedi","Arjun Adapa","Wajd Al-Holou","Jason Heth","Oren Sagher","Pedro Lowenstein","Maria Castro","Lisa Irina Wadiura","Georg Widhalm","Volker Neuschmelting","David Reinecke","Niklas von Spreckelsen","Mitchel S. Berger","Shawn L. Hervey-Jumper","John G. Golfinos","Matija Snuderl","Sandra Camelo-Piragua","Christian Freudiger","Honglak Lee","Daniel A. Orringer"],"pdf_url":"https://arxiv.org/pdf/2303.13610v1.pdf","comment":"Paper published in Nature Medicine"},{"id":"http://arxiv.org/abs/2212.08620v2","updated":"2023-03-23T18:45:37Z","published":"2022-12-16T17:57:41Z","title":"POTATO: The Portable Text Annotation Tool","summary":"  We present POTATO, the Portable text annotation tool, a free, fully\nopen-sourced annotation system that 1) supports labeling many types of text and\nmultimodal data; 2) offers easy-to-configure features to maximize the\nproductivity of both deployers and annotators (convenient templates for common\nML/NLP tasks, active learning, keypress shortcuts, keyword highlights,\ntooltips); and 3) supports a high degree of customization (editable UI,\ninserting pre-screening questions, attention and qualification tests).\nExperiments over two annotation tasks suggest that POTATO improves labeling\nspeed through its specially-designed productivity features, especially for long\ndocuments and complex tasks. POTATO is available at\nhttps://github.com/davidjurgens/potato and will continue to be updated.\n","authors":["Jiaxin Pei","Aparna Ananthasubramaniam","Xingyao Wang","Naitian Zhou","Jackson Sargent","Apostolos Dedeloudis","David Jurgens"],"pdf_url":"https://arxiv.org/pdf/2212.08620v2.pdf","comment":"EMNLP 2022 DEMO"},{"id":"http://arxiv.org/abs/2303.13604v1","updated":"2023-03-23T18:38:33Z","published":"2023-03-23T18:38:33Z","title":"Stochastic Submodular Bandits with Delayed Composite Anonymous Bandit\n  Feedback","summary":"  This paper investigates the problem of combinatorial multiarmed bandits with\nstochastic submodular (in expectation) rewards and full-bandit delayed\nfeedback, where the delayed feedback is assumed to be composite and anonymous.\nIn other words, the delayed feedback is composed of components of rewards from\npast actions, with unknown division among the sub-components. Three models of\ndelayed feedback: bounded adversarial, stochastic independent, and stochastic\nconditionally independent are studied, and regret bounds are derived for each\nof the delay models. Ignoring the problem dependent parameters, we show that\nregret bound for all the delay models is $\\tilde{O}(T^{2/3} + T^{1/3} \\nu)$ for\ntime horizon $T$, where $\\nu$ is a delay parameter defined differently in the\nthree cases, thus demonstrating an additive term in regret with delay in all\nthe three delay models. The considered algorithm is demonstrated to outperform\nother full-bandit approaches with delayed composite anonymous feedback.\n","authors":["Mohammad Pedramfar","Vaneet Aggarwal"],"pdf_url":"https://arxiv.org/pdf/2303.13604v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2209.15055v4","updated":"2023-03-23T18:14:14Z","published":"2022-09-29T18:57:51Z","title":"Implicit Bias of Large Depth Networks: a Notion of Rank for Nonlinear\n  Functions","summary":"  We show that the representation cost of fully connected neural networks with\nhomogeneous nonlinearities - which describes the implicit bias in function\nspace of networks with $L_2$-regularization or with losses such as the\ncross-entropy - converges as the depth of the network goes to infinity to a\nnotion of rank over nonlinear functions. We then inquire under which conditions\nthe global minima of the loss recover the `true' rank of the data: we show that\nfor too large depths the global minimum will be approximately rank 1\n(underestimating the rank); we then argue that there is a range of depths which\ngrows with the number of datapoints where the true rank is recovered. Finally,\nwe discuss the effect of the rank of a classifier on the topology of the\nresulting class boundaries and show that autoencoders with optimal nonlinear\nrank are naturally denoising.\n","authors":["Arthur Jacot"],"pdf_url":"https://arxiv.org/pdf/2209.15055v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.13590v1","updated":"2023-03-23T18:10:06Z","published":"2023-03-23T18:10:06Z","title":"Une comparaison des algorithmes d'apprentissage pour la survie avec\n  données manquantes","summary":"  Survival analysis is an essential tool for the study of health data. An\ninherent component of such data is the presence of missing values. In recent\nyears, researchers proposed new learning algorithms for survival tasks based on\nneural networks. Here, we studied the predictive performance of such algorithms\ncoupled with different methods for handling missing values on simulated data\nthat reflect a realistic situation, i.e., when individuals belong to unobserved\nclusters. We investigated different patterns of missing data. The results show\nthat, without further feature engineering, no single imputation method is\nbetter than the others in all cases. The proposed methodology can be used to\ncompare other missing data patterns and/or survival models. The Python code is\naccessible via the package survivalsim.\n  --\n  L'analyse de survie est un outil essentiel pour l'\\'etude des donn\\'ees de\nsant\\'e. Une composante inh\\'erente \\`a ces donn\\'ees est la pr\\'esence de\nvaleurs manquantes. Ces derni\\`eres ann\\'ees, de nouveaux algorithmes\nd'apprentissage pour la survie, bas\\'es sur les r\\'eseaux de neurones, ont\n\\'et\\'e con\\c{c}us. L'objectif de ce travail est d'\\'etudier la performance en\npr\\'ediction de ces algorithmes coupl\\'es \\`a diff\\'erentes m\\'ethodes pour\ng\\'erer les valeurs manquantes, sur des donn\\'ees simul\\'ees qui refl\\`etent\nune situation rencontr\\'ee en pratique, c'est-\\`a dire lorsque les individus\npeuvent \\^etre group\\'es selon leurs covariables. Diff\\'erents sch\\'emas de\ndonn\\'ees manquantes sont \\'etudi\\'es. Les r\\'esultats montrent que, sans\nl'ajout de variables suppl\\'ementaires, aucune m\\'ethode d'imputation n'est\nmeilleure que les autres dans tous les cas. La m\\'ethodologie propos\\'ee peut\n\\^etre utilis\\'ee pour comparer d'autres mod\\`eles de survie. Le code en Python\nest accessible via le package survivalsim.\n","authors":["Paul Dufossé","Sébastien Benzekry"],"pdf_url":"https://arxiv.org/pdf/2303.13590v1.pdf","comment":"in French language"},{"id":"http://arxiv.org/abs/2303.13589v1","updated":"2023-03-23T18:08:44Z","published":"2023-03-23T18:08:44Z","title":"A Closer Look at Scoring Functions and Generalization Prediction","summary":"  Generalization error predictors (GEPs) aim to predict model performance on\nunseen distributions by deriving dataset-level error estimates from\nsample-level scores. However, GEPs often utilize disparate mechanisms (e.g.,\nregressors, thresholding functions, calibration datasets, etc), to derive such\nerror estimates, which can obfuscate the benefits of a particular scoring\nfunction. Therefore, in this work, we rigorously study the effectiveness of\npopular scoring functions (confidence, local manifold smoothness, model\nagreement), independent of mechanism choice. We find, absent complex\nmechanisms, that state-of-the-art confidence- and smoothness- based scores fail\nto outperform simple model-agreement scores when estimating error under\ndistribution shifts and corruptions. Furthermore, on realistic settings where\nthe training data has been compromised (e.g., label noise, measurement noise,\nundersampling), we find that model-agreement scores continue to perform well\nand that ensemble diversity is important for improving its performance.\nFinally, to better understand the limitations of scoring functions, we\ndemonstrate that simplicity bias, or the propensity of deep neural networks to\nrely upon simple but brittle features, can adversely affect GEP performance.\nOverall, our work carefully studies the effectiveness of popular scoring\nfunctions in realistic settings and helps to better understand their\nlimitations.\n","authors":["Puja Trivedi","Danai Koutra","Jayaraman J. Thiagarajan"],"pdf_url":"https://arxiv.org/pdf/2303.13589v1.pdf","comment":"Accepted to ICASSP 2023"},{"id":"http://arxiv.org/abs/2303.13588v1","updated":"2023-03-23T18:08:11Z","published":"2023-03-23T18:08:11Z","title":"Efficient Symbolic Reasoning for Neural-Network Verification","summary":"  The neural network has become an integral part of modern software systems.\nHowever, they still suffer from various problems, in particular, vulnerability\nto adversarial attacks. In this work, we present a novel program reasoning\nframework for neural-network verification, which we refer to as symbolic\nreasoning. The key components of our framework are the use of the symbolic\ndomain and the quadratic relation. The symbolic domain has very flexible\nsemantics, and the quadratic relation is quite expressive. They allow us to\nencode many verification problems for neural networks as quadratic programs.\nOur scheme then relaxes the quadratic programs to semidefinite programs, which\ncan be efficiently solved. This framework allows us to verify various\nneural-network properties under different scenarios, especially those that\nappear challenging for non-symbolic domains. Moreover, it introduces new\nrepresentations and perspectives for the verification tasks. We believe that\nour framework can bring new theoretical insights and practical tools to\nverification problems for neural networks.\n","authors":["Zi Wang","Somesh Jha"," Krishnamurthy"," Dvijotham"],"pdf_url":"https://arxiv.org/pdf/2303.13588v1.pdf","comment":null}],"Multimedia":[{"id":"http://arxiv.org/abs/2303.13471v1","updated":"2023-03-23T17:43:11Z","published":"2023-03-23T17:43:11Z","title":"Egocentric Audio-Visual Object Localization","summary":"  Humans naturally perceive surrounding scenes by unifying sound and sight in a\nfirst-person view. Likewise, machines are advanced to approach human\nintelligence by learning with multisensory inputs from an egocentric\nperspective. In this paper, we explore the challenging egocentric audio-visual\nobject localization task and observe that 1) egomotion commonly exists in\nfirst-person recordings, even within a short duration; 2) The out-of-view sound\ncomponents can be created while wearers shift their attention. To address the\nfirst problem, we propose a geometry-aware temporal aggregation module to\nhandle the egomotion explicitly. The effect of egomotion is mitigated by\nestimating the temporal geometry transformation and exploiting it to update\nvisual representations. Moreover, we propose a cascaded feature enhancement\nmodule to tackle the second issue. It improves cross-modal localization\nrobustness by disentangling visually-indicated audio representation. During\ntraining, we take advantage of the naturally available audio-visual temporal\nsynchronization as the ``free'' self-supervision to avoid costly labeling. We\nalso annotate and create the Epic Sounding Object dataset for evaluation\npurposes. Extensive experiments show that our method achieves state-of-the-art\nlocalization performance in egocentric videos and can be generalized to diverse\naudio-visual scenes.\n","authors":["Chao Huang","Yapeng Tian","Anurag Kumar","Chenliang Xu"],"pdf_url":"https://arxiv.org/pdf/2303.13471v1.pdf","comment":"Accepted by CVPR 2023"},{"id":"http://arxiv.org/abs/2303.13397v1","updated":"2023-03-23T16:15:18Z","published":"2023-03-23T16:15:18Z","title":"DDT: A Diffusion-Driven Transformer-based Framework for Human Mesh\n  Recovery from a Video","summary":"  Human mesh recovery (HMR) provides rich human body information for various\nreal-world applications such as gaming, human-computer interaction, and virtual\nreality. Compared to single image-based methods, video-based methods can\nutilize temporal information to further improve performance by incorporating\nhuman body motion priors. However, many-to-many approaches such as VIBE suffer\nfrom motion smoothness and temporal inconsistency. While many-to-one approaches\nsuch as TCMR and MPS-Net rely on the future frames, which is non-causal and\ntime inefficient during inference. To address these challenges, a novel\nDiffusion-Driven Transformer-based framework (DDT) for video-based HMR is\npresented. DDT is designed to decode specific motion patterns from the input\nsequence, enhancing motion smoothness and temporal consistency. As a\nmany-to-many approach, the decoder of our DDT outputs the human mesh of all the\nframes, making DDT more viable for real-world applications where time\nefficiency is crucial and a causal model is desired. Extensive experiments are\nconducted on the widely used datasets (Human3.6M, MPI-INF-3DHP, and 3DPW),\nwhich demonstrated the effectiveness and efficiency of our DDT.\n","authors":["Ce Zheng","Guo-Jun Qi","Chen Chen"],"pdf_url":"https://arxiv.org/pdf/2303.13397v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.13371v1","updated":"2023-03-23T15:42:05Z","published":"2023-03-23T15:42:05Z","title":"Plug-and-Play Regulators for Image-Text Matching","summary":"  Exploiting fine-grained correspondence and visual-semantic alignments has\nshown great potential in image-text matching. Generally, recent approaches\nfirst employ a cross-modal attention unit to capture latent region-word\ninteractions, and then integrate all the alignments to obtain the final\nsimilarity. However, most of them adopt one-time forward association or\naggregation strategies with complex architectures or additional information,\nwhile ignoring the regulation ability of network feedback. In this paper, we\ndevelop two simple but quite effective regulators which efficiently encode the\nmessage output to automatically contextualize and aggregate cross-modal\nrepresentations. Specifically, we propose (i) a Recurrent Correspondence\nRegulator (RCR) which facilitates the cross-modal attention unit progressively\nwith adaptive attention factors to capture more flexible correspondence, and\n(ii) a Recurrent Aggregation Regulator (RAR) which adjusts the aggregation\nweights repeatedly to increasingly emphasize important alignments and dilute\nunimportant ones. Besides, it is interesting that RCR and RAR are\nplug-and-play: both of them can be incorporated into many frameworks based on\ncross-modal interaction to obtain significant benefits, and their cooperation\nachieves further improvements. Extensive experiments on MSCOCO and Flickr30K\ndatasets validate that they can bring an impressive and consistent R@1 gain on\nmultiple models, confirming the general effectiveness and generalization\nability of the proposed methods. Code and pre-trained models are available at:\nhttps://github.com/Paranioar/RCAR.\n","authors":["Haiwen Diao","Ying Zhang","Wei Liu","Xiang Ruan","Huchuan Lu"],"pdf_url":"https://arxiv.org/pdf/2303.13371v1.pdf","comment":"13 pages, 9 figures, Accepted by TIP2023"},{"id":"http://arxiv.org/abs/2303.13357v1","updated":"2023-03-23T15:36:12Z","published":"2023-03-23T15:36:12Z","title":"POTTER: Pooling Attention Transformer for Efficient Human Mesh Recovery","summary":"  Transformer architectures have achieved SOTA performance on the human mesh\nrecovery (HMR) from monocular images. However, the performance gain has come at\nthe cost of substantial memory and computational overhead. A lightweight and\nefficient model to reconstruct accurate human mesh is needed for real-world\napplications. In this paper, we propose a pure transformer architecture named\nPOoling aTtention TransformER (POTTER) for the HMR task from single images.\nObserving that the conventional attention module is memory and computationally\nexpensive, we propose an efficient pooling attention module, which\nsignificantly reduces the memory and computational cost without sacrificing\nperformance. Furthermore, we design a new transformer architecture by\nintegrating a High-Resolution (HR) stream for the HMR task. The high-resolution\nlocal and global features from the HR stream can be utilized for recovering\nmore accurate human mesh. Our POTTER outperforms the SOTA method METRO by only\nrequiring 7% of total parameters and 14% of the Multiply-Accumulate Operations\non the Human3.6M (PA-MPJPE metric) and 3DPW (all three metrics) datasets. The\nproject webpage is https://zczcwh.github.io/potter_page.\n","authors":["Ce Zheng","Xianpeng Liu","Guo-Jun Qi","Chen Chen"],"pdf_url":"https://arxiv.org/pdf/2303.13357v1.pdf","comment":"CVPR 2023"},{"id":"http://arxiv.org/abs/2303.13336v1","updated":"2023-03-23T15:17:15Z","published":"2023-03-23T15:17:15Z","title":"Audio Diffusion Model for Speech Synthesis: A Survey on Text To Speech\n  and Speech Enhancement in Generative AI","summary":"  Generative AI has demonstrated impressive performance in various fields,\namong which speech synthesis is an interesting direction. With the diffusion\nmodel as the most popular generative model, numerous works have attempted two\nactive tasks: text to speech and speech enhancement. This work conducts a\nsurvey on audio diffusion model, which is complementary to existing surveys\nthat either lack the recent progress of diffusion-based speech synthesis or\nhighlight an overall picture of applying diffusion model in multiple fields.\nSpecifically, this work first briefly introduces the background of audio and\ndiffusion model. As for the text-to-speech task, we divide the methods into\nthree categories based on the stage where diffusion model is adopted: acoustic\nmodel, vocoder and end-to-end framework. Moreover, we categorize various speech\nenhancement tasks by either certain signals are removed or added into the input\nspeech. Comparisons of experimental results and discussions are also covered in\nthis survey.\n","authors":["Chenshuang Zhang","Chaoning Zhang","Sheng Zheng","Mengchun Zhang","Maryam Qamar","Sung-Ho Bae","In So Kweon"],"pdf_url":"https://arxiv.org/pdf/2303.13336v1.pdf","comment":"18 pages"},{"id":"http://arxiv.org/abs/2303.13079v1","updated":"2023-03-23T07:15:28Z","published":"2023-03-23T07:15:28Z","title":"Implementation of communication media around a mixed reality experience\n  with HoloLens headset, as part of a digitalization of a nutrition workshop","summary":"  The release of Microsoft's HoloLens headset addresses new types of issues\nthat would have been difficult to design without such a hardware. This\nsemi-transparent visor headset allows the user who wears it to view the\nprojection of 3D virtual objects placed in its real environment. The user can\nalso interact with these 3D objects, which can interact with each other. The\nframework of this new technology is called mixed reality. We had the\nopportunity to numerically transform a conventional human nutrition workshop\nfor patients waiting for bariatric surgery by developing a software called\nHOLO_NUTRI using the HoloLens headset. Unlike our experience of user and\nconventional programmer specialized in the development of interactive 3D\ngraphics applications, we realized that such a mixed reality experience\nrequired specific programming concepts quite different from those of\nconventional software or those of virtual reality applications, but above all\nrequired a thorough reflection about communication for users. In this article,\nwe propose to explain our design of communication (graphic supports, tutorials\nof use of material, explanatory videos), a step which was crucial for the good\nprogress of our project. The software was used by thirty patients from Le\nPuy-en-Velay Hospital during 10 sessions of one hour and a half during which\npatients had to take in hand the headset and software HOLO_NUTRI. We also\nproposed a series of questions to patients to have an assessment of both the\nadequacy and the importance of this communication approach for such experience.\nAs the mixed reality technology is very recent but the number of applications\nbased on it significantly increases, the reflection on the implementation of\nthe elements of communication described in this article (videos, exercise of\nlearning for the use of the headset, communication leaflet, etc.) can help\ndevelopers of such applications.\n","authors":["Owen Kevin Appadoo","Hugo Rositi","Sylvie Valarier","Marie-Claire Ombret","Émilie Gadéa","Christine Barret-Grimault","Christophe Lohou"],"pdf_url":"https://arxiv.org/pdf/2303.13079v1.pdf","comment":"19 pages, 14 figures"},{"id":"http://arxiv.org/abs/2303.13031v1","updated":"2023-03-23T04:40:33Z","published":"2023-03-23T04:40:33Z","title":"Learning a Practical SDR-to-HDRTV Up-conversion using New Dataset and\n  Degradation Models","summary":"  In media industry, the demand of SDR-to-HDRTV up-conversion arises when users\npossess HDR-WCG (high dynamic range-wide color gamut) TVs while most\noff-the-shelf footage is still in SDR (standard dynamic range). The research\ncommunity has started tackling this low-level vision task by learning-based\napproaches. When applied to real SDR, yet, current methods tend to produce dim\nand desaturated result, making nearly no improvement on viewing experience.\nDifferent from other network-oriented methods, we attribute such deficiency to\ntraining set (HDR-SDR pair). Consequently, we propose new HDRTV dataset (dubbed\nHDRTV4K) and new HDR-to-SDR degradation models. Then, it's used to train a\nluminance-segmented network (LSN) consisting of a global mapping trunk, and two\nTransformer branches on bright and dark luminance range. We also update\nassessment criteria by tailored metrics and subjective experiment. Finally,\nablation studies are conducted to prove the effectiveness. Our work is\navailable at: https://github.com/AndreGuo/HDRTVDM.\n","authors":["Cheng Guo","Leidong Fan","Ziyu Xue","and Xiuhua Jiang"],"pdf_url":"https://arxiv.org/pdf/2303.13031v1.pdf","comment":"Accepted by CVPR2023"}]},"2023-03-24T00:00:00Z":{"Computation and Language":[{"id":"http://arxiv.org/abs/2303.14177v1","updated":"2023-03-24T17:38:58Z","published":"2023-03-24T17:38:58Z","title":"Scaling Expert Language Models with Unsupervised Domain Discovery","summary":"  Large language models are typically trained densely: all parameters are\nupdated with respect to all inputs. This requires synchronization of billions\nof parameters across thousands of GPUs. We introduce a simple but effective\nmethod to asynchronously train large, sparse language models on arbitrary text\ncorpora. Our method clusters a corpus into sets of related documents, trains a\nseparate expert language model on each cluster, and combines them in a sparse\nensemble for inference. This approach generalizes embarrassingly parallel\ntraining by automatically discovering the domains for each expert, and\neliminates nearly all the communication overhead of existing sparse language\nmodels. Our technique outperforms dense baselines on multiple corpora and\nfew-shot tasks, and our analysis shows that specializing experts to meaningful\nclusters is key to these gains. Performance also improves with the number of\nexperts and size of training data, suggesting this is a highly efficient and\naccessible approach to training large language models.\n","authors":["Suchin Gururangan","Margaret Li","Mike Lewis","Weijia Shi","Tim Althoff","Noah A. Smith","Luke Zettlemoyer"],"pdf_url":"https://arxiv.org/pdf/2303.14177v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.12712v2","updated":"2023-03-24T17:07:43Z","published":"2023-03-22T16:51:28Z","title":"Sparks of Artificial General Intelligence: Early experiments with GPT-4","summary":"  Artificial intelligence (AI) researchers have been developing and refining\nlarge language models (LLMs) that exhibit remarkable capabilities across a\nvariety of domains and tasks, challenging our understanding of learning and\ncognition. The latest model developed by OpenAI, GPT-4, was trained using an\nunprecedented scale of compute and data. In this paper, we report on our\ninvestigation of an early version of GPT-4, when it was still in active\ndevelopment by OpenAI. We contend that (this early version of) GPT-4 is part of\na new cohort of LLMs (along with ChatGPT and Google's PaLM for example) that\nexhibit more general intelligence than previous AI models. We discuss the\nrising capabilities and implications of these models. We demonstrate that,\nbeyond its mastery of language, GPT-4 can solve novel and difficult tasks that\nspan mathematics, coding, vision, medicine, law, psychology and more, without\nneeding any special prompting. Moreover, in all of these tasks, GPT-4's\nperformance is strikingly close to human-level performance, and often vastly\nsurpasses prior models such as ChatGPT. Given the breadth and depth of GPT-4's\ncapabilities, we believe that it could reasonably be viewed as an early (yet\nstill incomplete) version of an artificial general intelligence (AGI) system.\nIn our exploration of GPT-4, we put special emphasis on discovering its\nlimitations, and we discuss the challenges ahead for advancing towards deeper\nand more comprehensive versions of AGI, including the possible need for\npursuing a new paradigm that moves beyond next-word prediction. We conclude\nwith reflections on societal influences of the recent technological leap and\nfuture research directions.\n","authors":["Sébastien Bubeck","Varun Chandrasekaran","Ronen Eldan","Johannes Gehrke","Eric Horvitz","Ece Kamar","Peter Lee","Yin Tat Lee","Yuanzhi Li","Scott Lundberg","Harsha Nori","Hamid Palangi","Marco Tulio Ribeiro","Yi Zhang"],"pdf_url":"https://arxiv.org/pdf/2303.12712v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2205.11658v3","updated":"2023-03-24T17:00:56Z","published":"2022-05-23T22:45:53Z","title":"Penguins Don't Fly: Reasoning about Generics through Instantiations and\n  Exceptions","summary":"  Generics express generalizations about the world (e.g., birds can fly) that\nare not universally true (e.g., newborn birds and penguins cannot fly).\nCommonsense knowledge bases, used extensively in NLP, encode some generic\nknowledge but rarely enumerate such exceptions and knowing when a generic\nstatement holds or does not hold true is crucial for developing a comprehensive\nunderstanding of generics. We present a novel framework informed by linguistic\ntheory to generate exemplars -- specific cases when a generic holds true or\nfalse. We generate ~19k exemplars for ~650 generics and show that our framework\noutperforms a strong GPT-3 baseline by 12.8 precision points. Our analysis\nhighlights the importance of linguistic theory-based controllability for\ngenerating exemplars, the insufficiency of knowledge bases as a source of\nexemplars, and the challenges exemplars pose for the task of natural language\ninference.\n","authors":["Emily Allaway","Jena D. Hwang","Chandra Bhagavatula","Kathleen McKeown","Doug Downey","Yejin Choi"],"pdf_url":"https://arxiv.org/pdf/2205.11658v3.pdf","comment":"EACL 2023"},{"id":"http://arxiv.org/abs/2303.14128v1","updated":"2023-03-24T16:35:42Z","published":"2023-03-24T16:35:42Z","title":"The crime of being poor","summary":"  The criminalization of poverty has been widely denounced as a collective bias\nagainst the most vulnerable. NGOs and international organizations claim that\nthe poor are blamed for their situation, are more often associated with\ncriminal offenses than the wealthy strata of society and even incur criminal\noffenses simply as a result of being poor. While no evidence has been found in\nthe literature that correlates poverty and overall criminality rates, this\npaper offers evidence of a collective belief that associates both concepts.\nThis brief report measures the societal bias that correlates criminality with\nthe poor, as compared to the rich, by using Natural Language Processing (NLP)\ntechniques in Twitter. The paper quantifies the level of crime-poverty bias in\na panel of eight different English-speaking countries. The regional differences\nin the association between crime and poverty cannot be justified based on\ndifferent levels of inequality or unemployment, which the literature correlates\nto property crimes. The variation in the observed rates of crime-poverty bias\nfor different geographic locations could be influenced by cultural factors and\nthe tendency to overestimate the equality of opportunities and social mobility\nin specific countries. These results have consequences for policy-making and\nopen a new path of research for poverty mitigation with the focus not only on\nthe poor but on society as a whole. Acting on the collective bias against the\npoor would facilitate the approval of poverty reduction policies, as well as\nthe restoration of the dignity of the persons affected.\n","authors":["Georgina Curto","Svetlana Kiritchenko","Isar Nejadgholi","Kathleen C. Fraser"],"pdf_url":"https://arxiv.org/pdf/2303.14128v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.14116v1","updated":"2023-03-24T16:24:08Z","published":"2023-03-24T16:24:08Z","title":"Improving Prediction Performance and Model Interpretability through\n  Attention Mechanisms from Basic and Applied Research Perspectives","summary":"  With the dramatic advances in deep learning technology, machine learning\nresearch is focusing on improving the interpretability of model predictions as\nwell as prediction performance in both basic and applied research. While deep\nlearning models have much higher prediction performance than traditional\nmachine learning models, the specific prediction process is still difficult to\ninterpret and/or explain. This is known as the black-boxing of machine learning\nmodels and is recognized as a particularly important problem in a wide range of\nresearch fields, including manufacturing, commerce, robotics, and other\nindustries where the use of such technology has become commonplace, as well as\nthe medical field, where mistakes are not tolerated. This bulletin is based on\nthe summary of the author's dissertation. The research summarized in the\ndissertation focuses on the attention mechanism, which has been the focus of\nmuch attention in recent years, and discusses its potential for both basic\nresearch in terms of improving prediction performance and interpretability, and\napplied research in terms of evaluating it for real-world applications using\nlarge data sets beyond the laboratory environment. The dissertation also\nconcludes with a summary of the implications of these findings for subsequent\nresearch and future prospects in the field.\n","authors":["Shunsuke Kitada"],"pdf_url":"https://arxiv.org/pdf/2303.14116v1.pdf","comment":"The bulletin of Graduate School of Science and Engineering, Hosei\n  University, Vol.64 (03/2023). This article draws heavily from\n  arxiv:2009.12064, arxiv:2104.08763, arxiv:1905.07289, and arxiv:2204.11588"},{"id":"http://arxiv.org/abs/2301.10761v4","updated":"2023-03-24T15:35:49Z","published":"2023-01-25T18:55:05Z","title":"Fillers in Spoken Language Understanding: Computational and\n  Psycholinguistic Perspectives","summary":"  Disfluencies (i.e. interruptions in the regular flow of speech), are\nubiquitous to spoken discourse. Fillers (\"uh\", \"um\") are disfluencies that\noccur the most frequently compared to other kinds of disfluencies. Yet, to the\nbest of our knowledge, there isn't a resource that brings together the research\nperspectives influencing Spoken Language Understanding (SLU) on these speech\nevents. This aim of this article is to survey a breadth of perspectives in a\nholistic way; i.e. from considering underlying (psycho)linguistic theory, to\ntheir annotation and consideration in Automatic Speech Recognition (ASR) and\nSLU systems, to lastly, their study from a generation standpoint. This article\naims to present the perspectives in an approachable way to the SLU and\nConversational AI community, and discuss moving forward, what we believe are\nthe trends and challenges in each area.\n","authors":["Tanvi Dinkar","Chloé Clavel","Ioana Vasilescu"],"pdf_url":"https://arxiv.org/pdf/2301.10761v4.pdf","comment":"\\footnote{This article has been published in the journal \"Traitement\n  Automatique des Langues\" 63(3): 37-62, 2022,@ATALA. The original manuscript\n  is available on the web site www.atala.org}"},{"id":"http://arxiv.org/abs/2303.14070v1","updated":"2023-03-24T15:29:16Z","published":"2023-03-24T15:29:16Z","title":"ChatDoctor: A Medical Chat Model Fine-tuned on LLaMA Model using Medical\n  Domain Knowledge","summary":"  Recent large language models (LLMs) in the general domain, such as ChatGPT,\nhave shown remarkable success in following instructions and producing\nhuman-like responses. However, such language models have not been learned\nindividually and carefully for the medical domain, resulting in poor diagnostic\naccuracy and inability to give correct recommendations for medical diagnosis,\nmedications, etc. To address this issue, we collected more than 700 diseases\nand their corresponding symptoms, recommended medications, and required medical\ntests, and then generated 5K doctor-patient conversations. By fine-tuning\nmodels of doctor-patient conversations, these models emerge with great\npotential to understand patients' needs, provide informed advice, and offer\nvaluable assistance in a variety of medical-related fields. The integration of\nthese advanced language models into healthcare can revolutionize the way\nhealthcare professionals and patients communicate, ultimately improving the\noverall quality of care and patient outcomes. In addition, we will open all\nsource code, datasets and model weights to advance the further development of\ndialogue models in the medical field. In addition, the training data, code, and\nweights of this project are available at:\nhttps://github.com/Kent0n-Li/ChatDoctor.\n","authors":["Li Yunxiang","Li Zihan","Zhang Kai","Dan Ruilong","Zhang You"],"pdf_url":"https://arxiv.org/pdf/2303.14070v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.13284v2","updated":"2023-03-24T14:59:15Z","published":"2023-03-23T14:06:26Z","title":"GETT-QA: Graph Embedding based T2T Transformer for Knowledge Graph\n  Question Answering","summary":"  In this work, we present an end-to-end Knowledge Graph Question Answering\n(KGQA) system named GETT-QA. GETT-QA uses T5, a popular text-to-text\npre-trained language model. The model takes a question in natural language as\ninput and produces a simpler form of the intended SPARQL query. In the simpler\nform, the model does not directly produce entity and relation IDs. Instead, it\nproduces corresponding entity and relation labels. The labels are grounded to\nKG entity and relation IDs in a subsequent step. To further improve the\nresults, we instruct the model to produce a truncated version of the KG\nembedding for each entity. The truncated KG embedding enables a finer search\nfor disambiguation purposes. We find that T5 is able to learn the truncated KG\nembeddings without any change of loss function, improving KGQA performance. As\na result, we report strong results for LC-QuAD 2.0 and SimpleQuestions-Wikidata\ndatasets on end-to-end KGQA over Wikidata.\n","authors":["Debayan Banerjee","Pranav Ajit Nair","Ricardo Usbeck","Chris Biemann"],"pdf_url":"https://arxiv.org/pdf/2303.13284v2.pdf","comment":"16 pages single column format accepted at ESWC 2023 research track"},{"id":"http://arxiv.org/abs/2211.15783v2","updated":"2023-03-24T14:39:17Z","published":"2022-11-28T21:35:24Z","title":"Mathematically Modeling the Lexicon Entropy of Emergent Language","summary":"  We formulate a stochastic process, FiLex, as a mathematical model of lexicon\nentropy in deep learning-based emergent language systems. Defining a model\nmathematically allows it to generate clear predictions which can be directly\nand decisively tested. We empirically verify across four different environments\nthat FiLex predicts the correct correlation between hyperparameters (training\nsteps, lexicon size, learning rate, rollout buffer size, and Gumbel-Softmax\ntemperature) and the emergent language's entropy in 20 out of 20\nenvironment-hyperparameter combinations. Furthermore, our experiments reveal\nthat different environments show diverse relationships between their\nhyperparameters and entropy which demonstrates the need for a model which can\nmake well-defined predictions at a precise level of granularity.\n","authors":["Brendon Boldt","David Mortensen"],"pdf_url":"https://arxiv.org/pdf/2211.15783v2.pdf","comment":"12 pages, 3 figures; added link to GitHub repo"},{"id":"http://arxiv.org/abs/2303.14011v1","updated":"2023-03-24T14:07:03Z","published":"2023-03-24T14:07:03Z","title":"SPEC: Summary Preference Decomposition for Low-Resource Abstractive\n  Summarization","summary":"  Neural abstractive summarization has been widely studied and achieved great\nsuccess with large-scale corpora. However, the considerable cost of annotating\ndata motivates the need for learning strategies under low-resource settings. In\nthis paper, we investigate the problems of learning summarizers with only few\nexamples and propose corresponding methods for improvements. First, typical\ntransfer learning methods are prone to be affected by data properties and\nlearning objectives in the pretext tasks. Therefore, based on pretrained\nlanguage models, we further present a meta learning framework to transfer\nfew-shot learning processes from source corpora to the target corpus. Second,\nprevious methods learn from training examples without decomposing the content\nand preference. The generated summaries could therefore be constrained by the\npreference bias in the training set, especially under low-resource settings. As\nsuch, we propose decomposing the contents and preferences during learning\nthrough the parameter modulation, which enables control over preferences during\ninference. Third, given a target application, specifying required preferences\ncould be non-trivial because the preferences may be difficult to derive through\nobservations. Therefore, we propose a novel decoding method to automatically\nestimate suitable preferences and generate corresponding summary candidates\nfrom the few training examples. Extensive experiments demonstrate that our\nmethods achieve state-of-the-art performance on six diverse corpora with\n30.11%/33.95%/27.51% and 26.74%/31.14%/24.48% average improvements on\nROUGE-1/2/L under 10- and 100-example settings.\n","authors":["Yi-Syuan Chen","Yun-Zhu Song","Hong-Han Shuai"],"pdf_url":"https://arxiv.org/pdf/2303.14011v1.pdf","comment":"Accepted by IEEE/ACM Transactions on Audio, Speech, and Language\n  Processing (TASLP) 2022; Published Version"},{"id":"http://arxiv.org/abs/2303.13989v1","updated":"2023-03-24T13:25:46Z","published":"2023-03-24T13:25:46Z","title":"Paraphrase Detection: Human vs. Machine Content","summary":"  The growing prominence of large language models, such as GPT-4 and ChatGPT,\nhas led to increased concerns over academic integrity due to the potential for\nmachine-generated content and paraphrasing. Although studies have explored the\ndetection of human- and machine-paraphrased content, the comparison between\nthese types of content remains underexplored. In this paper, we conduct a\ncomprehensive analysis of various datasets commonly employed for paraphrase\ndetection tasks and evaluate an array of detection methods. Our findings\nhighlight the strengths and limitations of different detection methods in terms\nof performance on individual datasets, revealing a lack of suitable\nmachine-generated datasets that can be aligned with human expectations. Our\nmain finding is that human-authored paraphrases exceed machine-generated ones\nin terms of difficulty, diversity, and similarity implying that automatically\ngenerated texts are not yet on par with human-level performance. Transformers\nemerged as the most effective method across datasets with TF-IDF excelling on\nsemantically diverse corpora. Additionally, we identify four datasets as the\nmost diverse and challenging for paraphrase detection.\n","authors":["Jonas Becker","Jan Philip Wahle","Terry Ruas","Bela Gipp"],"pdf_url":"https://arxiv.org/pdf/2303.13989v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.13988v1","updated":"2023-03-24T13:24:41Z","published":"2023-03-24T13:24:41Z","title":"Machine Psychology: Investigating Emergent Capabilities and Behavior in\n  Large Language Models Using Psychological Methods","summary":"  Large language models (LLMs) are currently at the forefront of intertwining\nAI systems with human communication and everyday life. Due to rapid\ntechnological advances and their extreme versatility, LLMs nowadays have\nmillions of users and are at the cusp of being the main go-to technology for\ninformation retrieval, content generation, problem-solving, etc. Therefore, it\nis of great importance to thoroughly assess and scrutinize their capabilities.\nDue to increasingly complex and novel behavioral patterns in current LLMs, this\ncan be done by treating them as participants in psychology experiments that\nwere originally designed to test humans. For this purpose, the paper introduces\na new field of research called \"machine psychology\". The paper outlines how\ndifferent subfields of psychology can inform behavioral tests for LLMs. It\ndefines methodological standards for machine psychology research, especially by\nfocusing on policies for prompt designs. Additionally, it describes how\nbehavioral patterns discovered in LLMs are to be interpreted. In sum, machine\npsychology aims to discover emergent abilities in LLMs that cannot be detected\nby most traditional natural language processing benchmarks.\n","authors":["Thilo Hagendorff"],"pdf_url":"https://arxiv.org/pdf/2303.13988v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2212.04634v2","updated":"2023-03-24T13:20:05Z","published":"2022-12-09T02:19:07Z","title":"Open-world Story Generation with Structured Knowledge Enhancement: A\n  Comprehensive Survey","summary":"  Storytelling and narrative are fundamental to human experience, intertwined\nwith our social and cultural engagement. As such, researchers have long\nattempted to create systems that can generate stories automatically. In recent\nyears, powered by deep learning and massive data resources, automatic story\ngeneration has shown significant advances. However, considerable challenges,\nlike the need for global coherence in generated stories, still hamper\ngenerative models from reaching the same storytelling ability as human\nnarrators. To tackle these challenges, many studies seek to inject structured\nknowledge into the generation process, which is referred to as structured\nknowledge-enhanced story generation. Incorporating external knowledge can\nenhance the logical coherence among story events, achieve better knowledge\ngrounding, and alleviate over-generalization and repetition problems in\nstories. This survey provides the latest and comprehensive review of this\nresearch field: (i) we present a systematical taxonomy regarding how existing\nmethods integrate structured knowledge into story generation; (ii) we summarize\ninvolved story corpora, structured knowledge datasets, and evaluation metrics;\n(iii) we give multidimensional insights into the challenges of\nknowledge-enhanced story generation and cast light on promising directions for\nfuture study.\n","authors":["Yuxin Wang","Jieru Lin","Zhiwei Yu","Wei Hu","Börje F. Karlsson"],"pdf_url":"https://arxiv.org/pdf/2212.04634v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.13939v1","updated":"2023-03-24T11:52:25Z","published":"2023-03-24T11:52:25Z","title":"MUG: A General Meeting Understanding and Generation Benchmark","summary":"  Listening to long video/audio recordings from video conferencing and online\ncourses for acquiring information is extremely inefficient. Even after ASR\nsystems transcribe recordings into long-form spoken language documents, reading\nASR transcripts only partly speeds up seeking information. It has been observed\nthat a range of NLP applications, such as keyphrase extraction, topic\nsegmentation, and summarization, significantly improve users' efficiency in\ngrasping important information. The meeting scenario is among the most valuable\nscenarios for deploying these spoken language processing (SLP) capabilities.\nHowever, the lack of large-scale public meeting datasets annotated for these\nSLP tasks severely hinders their advancement. To prompt SLP advancement, we\nestablish a large-scale general Meeting Understanding and Generation Benchmark\n(MUG) to benchmark the performance of a wide range of SLP tasks, including\ntopic segmentation, topic-level and session-level extractive summarization and\ntopic title generation, keyphrase extraction, and action item detection. To\nfacilitate the MUG benchmark, we construct and release a large-scale meeting\ndataset for comprehensive long-form SLP development, the AliMeeting4MUG Corpus,\nwhich consists of 654 recorded Mandarin meeting sessions with diverse topic\ncoverage, with manual annotations for SLP tasks on manual transcripts of\nmeeting recordings. To the best of our knowledge, the AliMeeting4MUG Corpus is\nso far the largest meeting corpus in scale and facilitates most SLP tasks. In\nthis paper, we provide a detailed introduction of this corpus, SLP tasks and\nevaluation methods, baseline systems and their performance.\n","authors":["Qinglin Zhang","Chong Deng","Jiaqing Liu","Hai Yu","Qian Chen","Wen Wang","Zhijie Yan","Jinglin Liu","Yi Ren","Zhou Zhao"],"pdf_url":"https://arxiv.org/pdf/2303.13939v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.13932v1","updated":"2023-03-24T11:42:19Z","published":"2023-03-24T11:42:19Z","title":"Overview of the ICASSP 2023 General Meeting Understanding and Generation\n  Challenge (MUG)","summary":"  ICASSP2023 General Meeting Understanding and Generation Challenge (MUG)\nfocuses on prompting a wide range of spoken language processing (SLP) research\non meeting transcripts, as SLP applications are critical to improve users'\nefficiency in grasping important information in meetings. MUG includes five\ntracks, including topic segmentation, topic-level and session-level extractive\nsummarization, topic title generation, keyphrase extraction, and action item\ndetection. To facilitate MUG, we construct and release a large-scale meeting\ndataset, the AliMeeting4MUG Corpus.\n","authors":["Qinglin Zhang","Chong Deng","Jiaqing Liu","Hai Yu","Qian Chen","Wen Wang","Zhijie Yan","Jinglin Liu","Yi Ren","Zhou Zhao"],"pdf_url":"https://arxiv.org/pdf/2303.13932v1.pdf","comment":"Paper accepted to the 2023 IEEE International Conference on\n  Acoustics, Speech, and Signal Processing (ICASSP 2023), Rhodes, Greece"},{"id":"http://arxiv.org/abs/2301.10410v3","updated":"2023-03-24T10:51:05Z","published":"2023-01-25T05:16:43Z","title":"One Model for All Domains: Collaborative Domain-Prefix Tuning for\n  Cross-Domain NER","summary":"  Cross-domain NER is a challenging task to address the low-resource problem in\npractical scenarios. Previous typical solutions mainly obtain a NER model by\npre-trained language models (PLMs) with data from a rich-resource domain and\nadapt it to the target domain. Owing to the mismatch issue among entity types\nin different domains, previous approaches normally tune all parameters of PLMs,\nending up with an entirely new NER model for each domain. Moreover, current\nmodels only focus on leveraging knowledge in one general source domain while\nfailing to successfully transfer knowledge from multiple sources to the target.\nTo address these issues, we introduce Collaborative Domain-Prefix Tuning for\ncross-domain NER (CP-NER) based on text-to-text generative PLMs. Specifically,\nwe present text-to-text generation grounding domain-related instructors to\ntransfer knowledge to new domain NER tasks without structural modifications. We\nutilize frozen PLMs and conduct collaborative domain-prefix tuning to stimulate\nthe potential of PLMs to handle NER tasks across various domains. Experimental\nresults on the Cross-NER benchmark show that the proposed approach has flexible\ntransfer ability and performs better on both one-source and multiple-source\ncross-domain NER tasks. Codes will be available in\nhttps://github.com/zjunlp/DeepKE/tree/main/example/ner/cross.\n","authors":["Xiang Chen","Lei Li","Shuofei Qiao","Ningyu Zhang","Chuanqi Tan","Yong Jiang","Fei Huang","Huajun Chen"],"pdf_url":"https://arxiv.org/pdf/2301.10410v3.pdf","comment":"Work in progress"},{"id":"http://arxiv.org/abs/2111.09543v4","updated":"2023-03-24T09:17:17Z","published":"2021-11-18T06:48:00Z","title":"DeBERTaV3: Improving DeBERTa using ELECTRA-Style Pre-Training with\n  Gradient-Disentangled Embedding Sharing","summary":"  This paper presents a new pre-trained language model, DeBERTaV3, which\nimproves the original DeBERTa model by replacing mask language modeling (MLM)\nwith replaced token detection (RTD), a more sample-efficient pre-training task.\nOur analysis shows that vanilla embedding sharing in ELECTRA hurts training\nefficiency and model performance. This is because the training losses of the\ndiscriminator and the generator pull token embeddings in different directions,\ncreating the \"tug-of-war\" dynamics. We thus propose a new gradient-disentangled\nembedding sharing method that avoids the tug-of-war dynamics, improving both\ntraining efficiency and the quality of the pre-trained model. We have\npre-trained DeBERTaV3 using the same settings as DeBERTa to demonstrate its\nexceptional performance on a wide range of downstream natural language\nunderstanding (NLU) tasks. Taking the GLUE benchmark with eight tasks as an\nexample, the DeBERTaV3 Large model achieves a 91.37% average score, which is\n1.37% over DeBERTa and 1.91% over ELECTRA, setting a new state-of-the-art\n(SOTA) among the models with a similar structure. Furthermore, we have\npre-trained a multi-lingual model mDeBERTa and observed a larger improvement\nover strong baselines compared to English models. For example, the mDeBERTa\nBase achieves a 79.8% zero-shot cross-lingual accuracy on XNLI and a 3.6%\nimprovement over XLM-R Base, creating a new SOTA on this benchmark. We have\nmade our pre-trained models and inference code publicly available at\nhttps://github.com/microsoft/DeBERTa.\n","authors":["Pengcheng He","Jianfeng Gao","Weizhu Chen"],"pdf_url":"https://arxiv.org/pdf/2111.09543v4.pdf","comment":"16 pages, 10 tables, 2 Figures. The DeBERTaV3 model significantly\n  improves performance of the downstream NLU tasks over models with a similar\n  structure, e.g. DeBERTaV3 large achieves 91.37% average GLUE score which is\n  1.37% over DeBERTa large. XSmall has only 22M backbone parameters, but\n  significantly outperforms RoBERTa/XLNet-base. Paper is published as a\n  conference paper at ICLR 2023"},{"id":"http://arxiv.org/abs/2212.09196v2","updated":"2023-03-24T08:05:17Z","published":"2022-12-19T00:04:56Z","title":"Emergent Analogical Reasoning in Large Language Models","summary":"  The recent advent of large language models has reinvigorated debate over\nwhether human cognitive capacities might emerge in such generic models given\nsufficient training data. Of particular interest is the ability of these models\nto reason about novel problems zero-shot, without any direct training. In human\ncognition, this capacity is closely tied to an ability to reason by analogy.\nHere, we performed a direct comparison between human reasoners and a large\nlanguage model (the text-davinci-003 variant of GPT-3) on a range of analogical\ntasks, including a novel text-based matrix reasoning task closely modeled on\nRaven's Progressive Matrices. We found that GPT-3 displayed a surprisingly\nstrong capacity for abstract pattern induction, matching or even surpassing\nhuman capabilities in most settings. Our results indicate that large language\nmodels such as GPT-3 have acquired an emergent ability to find zero-shot\nsolutions to a broad range of analogy problems.\n","authors":["Taylor Webb","Keith J. Holyoak","Hongjing Lu"],"pdf_url":"https://arxiv.org/pdf/2212.09196v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.13839v1","updated":"2023-03-24T07:23:56Z","published":"2023-03-24T07:23:56Z","title":"HRDoc: Dataset and Baseline Method Toward Hierarchical Reconstruction of\n  Document Structures","summary":"  The problem of document structure reconstruction refers to converting digital\nor scanned documents into corresponding semantic structures. Most existing\nworks mainly focus on splitting the boundary of each element in a single\ndocument page, neglecting the reconstruction of semantic structure in\nmulti-page documents. This paper introduces hierarchical reconstruction of\ndocument structures as a novel task suitable for NLP and CV fields. To better\nevaluate the system performance on the new task, we built a large-scale dataset\nnamed HRDoc, which consists of 2,500 multi-page documents with nearly 2 million\nsemantic units. Every document in HRDoc has line-level annotations including\ncategories and relations obtained from rule-based extractors and human\nannotators. Moreover, we proposed an encoder-decoder-based hierarchical\ndocument structure parsing system (DSPS) to tackle this problem. By adopting a\nmulti-modal bidirectional encoder and a structure-aware GRU decoder with\nsoft-mask operation, the DSPS model surpass the baseline method by a large\nmargin. All scripts and datasets will be made publicly available at\nhttps://github.com/jfma-USTC/HRDoc.\n","authors":["Jiefeng Ma","Jun Du","Pengfei Hu","Zhenrong Zhang","Jianshu Zhang","Huihui Zhu","Cong Liu"],"pdf_url":"https://arxiv.org/pdf/2303.13839v1.pdf","comment":"8 pages, 6 figures. Accepted by AAAI-2023"},{"id":"http://arxiv.org/abs/2207.14251v2","updated":"2023-03-24T07:18:59Z","published":"2022-07-28T17:36:24Z","title":"Measuring Causal Effects of Data Statistics on Language Model's\n  `Factual' Predictions","summary":"  Large amounts of training data are one of the major reasons for the high\nperformance of state-of-the-art NLP models. But what exactly in the training\ndata causes a model to make a certain prediction? We seek to answer this\nquestion by providing a language for describing how training data influences\npredictions, through a causal framework. Importantly, our framework bypasses\nthe need to retrain expensive models and allows us to estimate causal effects\nbased on observational data alone. Addressing the problem of extracting factual\nknowledge from pretrained language models (PLMs), we focus on simple data\nstatistics such as co-occurrence counts and show that these statistics do\ninfluence the predictions of PLMs, suggesting that such models rely on shallow\nheuristics. Our causal framework and our results demonstrate the importance of\nstudying datasets and the benefits of causality for understanding NLP models.\n","authors":["Yanai Elazar","Nora Kassner","Shauli Ravfogel","Amir Feder","Abhilasha Ravichander","Marius Mosbach","Yonatan Belinkov","Hinrich Schütze","Yoav Goldberg"],"pdf_url":"https://arxiv.org/pdf/2207.14251v2.pdf","comment":"We received a criticism regarding the validity of the causal\n  formulation in this paper. We will address them in an upcoming version"},{"id":"http://arxiv.org/abs/2303.13824v1","updated":"2023-03-24T06:16:29Z","published":"2023-03-24T06:16:29Z","title":"$k$NN Prompting: Beyond-Context Learning with Calibration-Free Nearest\n  Neighbor Inference","summary":"  In-Context Learning (ICL), which formulates target tasks as prompt completion\nconditioned on in-context demonstrations, has become the prevailing utilization\nof LLMs. In this paper, we first disclose an actual predicament for this\ntypical usage that it can not scale up with training data due to context length\nrestriction. Besides, existing works have shown that ICL also suffers from\nvarious biases and requires delicate calibration treatment. To address both\nchallenges, we advocate a simple and effective solution, $k$NN Prompting, which\nfirst queries LLM with training data for distributed representations, then\npredicts test instances by simply referring to nearest neighbors. We conduct\ncomprehensive experiments to demonstrate its two-fold superiority: 1)\nCalibration-Free: $k$NN Prompting does not directly align LLM output\ndistribution with task-specific label space, instead leverages such\ndistribution to align test and training instances. It significantly outperforms\nstate-of-the-art calibration-based methods under comparable few-shot scenario.\n2) Beyond-Context: $k$NN Prompting can further scale up effectively with as\nmany training data as are available, continually bringing substantial\nimprovements. The scaling trend holds across 10 orders of magnitude ranging\nfrom 2 shots to 1024 shots as well as different LLMs scales ranging from 0.8B\nto 30B. It successfully bridges data scaling into model scaling, and brings new\npotentials for the gradient-free paradigm of LLM deployment. Code is publicly\navailable.\n","authors":["Benfeng Xu","Quan Wang","Zhendong Mao","Yajuan Lyu","Qiaoqiao She","Yongdong Zhang"],"pdf_url":"https://arxiv.org/pdf/2303.13824v1.pdf","comment":"ICLR 2023. Code is available at\n  https://github.com/BenfengXu/KNNPrompting"},{"id":"http://arxiv.org/abs/2303.13809v1","updated":"2023-03-24T05:05:03Z","published":"2023-03-24T05:05:03Z","title":"Error Analysis Prompting Enables Human-Like Translation Evaluation in\n  Large Language Models: A Case Study on ChatGPT","summary":"  Generative large language models (LLMs), e.g., ChatGPT, have demonstrated\nremarkable proficiency across several NLP tasks such as machine translation,\nquestion answering, text summarization, and natural language understanding.\nRecent research has shown that utilizing ChatGPT for assessing the quality of\nmachine translation (MT) achieves state-of-the-art performance at the system\nlevel but performs poorly at the segment level. To further improve the\nperformance of LLMs on MT quality assessment, we conducted an investigation\ninto several prompting methods. Our results indicate that by combining\nChain-of-Thoughts and Error Analysis, a new prompting method called\n\\textbf{\\texttt{Error Analysis Prompting}}, LLMs like ChatGPT can\n\\textit{generate human-like MT evaluations at both the system and segment\nlevel}. Additionally, we discovered some limitations of ChatGPT as an MT\nevaluator, such as unstable scoring and biases when provided with multiple\ntranslations in a single query. Our findings aim to provide a preliminary\nexperience for appropriately evaluating translation quality on ChatGPT while\noffering a variety of tricks in designing prompts for in-context learning. We\nanticipate that this report will shed new light on advancing the field of\ntranslation evaluation with LLMs by enhancing both the accuracy and reliability\nof metrics. The project can be found in\n\\url{https://github.com/Coldmist-Lu/ErrorAnalysis_Prompt}.\n","authors":["Qingyu Lu","Baopu Qiu","Liang Ding","Liping Xie","Dacheng Tao"],"pdf_url":"https://arxiv.org/pdf/2303.13809v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.13801v1","updated":"2023-03-24T04:51:22Z","published":"2023-03-24T04:51:22Z","title":"Toward Open-domain Slot Filling via Self-supervised Co-training","summary":"  Slot filling is one of the critical tasks in modern conversational systems.\nThe majority of existing literature employs supervised learning methods, which\nrequire labeled training data for each new domain. Zero-shot learning and weak\nsupervision approaches, among others, have shown promise as alternatives to\nmanual labeling. Nonetheless, these learning paradigms are significantly\ninferior to supervised learning approaches in terms of performance. To minimize\nthis performance gap and demonstrate the possibility of open-domain slot\nfilling, we propose a Self-supervised Co-training framework, called SCot, that\nrequires zero in-domain manually labeled training examples and works in three\nphases. Phase one acquires two sets of complementary pseudo labels\nautomatically. Phase two leverages the power of the pre-trained language model\nBERT, by adapting it for the slot filling task using these sets of pseudo\nlabels. In phase three, we introduce a self-supervised cotraining mechanism,\nwhere both models automatically select highconfidence soft labels to further\nimprove the performance of the other in an iterative fashion. Our thorough\nevaluations show that SCot outperforms state-of-the-art models by 45.57% and\n37.56% on SGD and MultiWoZ datasets, respectively. Moreover, our proposed\nframework SCot achieves comparable performance when compared to\nstate-of-the-art fully supervised models.\n","authors":["Adib Mosharrof","Moghis Fereidouni","A. B. Siddique"],"pdf_url":"https://arxiv.org/pdf/2303.13801v1.pdf","comment":"10 pages, 6 tables, 2 figures, ACM Web Conference 2023 (WWW'23)"},{"id":"http://arxiv.org/abs/2303.13797v1","updated":"2023-03-24T04:33:40Z","published":"2023-03-24T04:33:40Z","title":"Personalizing Task-oriented Dialog Systems via Zero-shot Generalizable\n  Reward Function","summary":"  Task-oriented dialog systems enable users to accomplish tasks using natural\nlanguage. State-of-the-art systems respond to users in the same way regardless\nof their personalities, although personalizing dialogues can lead to higher\nlevels of adoption and better user experiences. Building personalized dialog\nsystems is an important, yet challenging endeavor and only a handful of works\ntook on the challenge. Most existing works rely on supervised learning\napproaches and require laborious and expensive labeled training data for each\nuser profile. Additionally, collecting and labeling data for each user profile\nis virtually impossible. In this work, we propose a novel framework, P-ToD, to\npersonalize task-oriented dialog systems capable of adapting to a wide range of\nuser profiles in an unsupervised fashion using a zero-shot generalizable reward\nfunction. P-ToD uses a pre-trained GPT-2 as a backbone model and works in three\nphases. Phase one performs task-specific training. Phase two kicks off\nunsupervised personalization by leveraging the proximal policy optimization\nalgorithm that performs policy gradients guided by the zero-shot generalizable\nreward function. Our novel reward function can quantify the quality of the\ngenerated responses even for unseen profiles. The optional final phase\nfine-tunes the personalized model using a few labeled training examples. We\nconduct extensive experimental analysis using the personalized bAbI dialogue\nbenchmark for five tasks and up to 180 diverse user profiles. The experimental\nresults demonstrate that P-ToD, even when it had access to zero labeled\nexamples, outperforms state-of-the-art supervised personalization models and\nachieves competitive performance on BLEU and ROUGE metrics when compared to a\nstrong fully-supervised GPT-2 baseline\n","authors":["A. B. Siddique","M. H. Maqbool","Kshitija Taywade","Hassan Foroosh"],"pdf_url":"https://arxiv.org/pdf/2303.13797v1.pdf","comment":"11 pages, 4 tables, 31st ACM International Conference on Information\n  and Knowledge Management (CIKM'22)"},{"id":"http://arxiv.org/abs/2303.13780v1","updated":"2023-03-24T03:35:21Z","published":"2023-03-24T03:35:21Z","title":"Towards Making the Most of ChatGPT for Machine Translation","summary":"  ChatGPT shows remarkable capabilities for machine translation (MT). Several\nprior studies have shown that it achieves comparable results to commercial\nsystems for high-resource languages, but lags behind in complex tasks, e.g,\nlow-resource and distant-language-pairs translation. However, they usually\nadopt simple prompts which can not fully elicit the capability of ChatGPT. In\nthis report, we aim to further mine ChatGPT's translation ability by revisiting\nseveral aspects: temperature, task information, and domain information, and\ncorrespondingly propose two (simple but effective) prompts: Task-Specific\nPrompts (TSP) and Domain-Specific Prompts (DSP). We show that: 1) The\nperformance of ChatGPT depends largely on temperature, and a lower temperature\nusually can achieve better performance; 2) Emphasizing the task information\nfurther improves ChatGPT's performance, particularly in complex MT tasks; 3)\nIntroducing domain information can elicit ChatGPT's generalization ability and\nimprove its performance in the specific domain; 4) ChatGPT tends to generate\nhallucinations for non-English-centric MT tasks, which can be partially\naddressed by our proposed prompts but still need to be highlighted for the\nMT/NLP community. We also explore the effects of advanced in-context learning\nstrategies and find a (negative but interesting) observation: the powerful\nchain-of-thought prompt leads to word-by-word translation behavior, thus\nbringing significant translation degradation.\n","authors":["Keqin Peng","Liang Ding","Qihuang Zhong","Li Shen","Xuebo Liu","Min Zhang","Yuanxin Ouyang","Dacheng Tao"],"pdf_url":"https://arxiv.org/pdf/2303.13780v1.pdf","comment":"Work in progress, 9 pages"},{"id":"http://arxiv.org/abs/2303.13722v1","updated":"2023-03-24T00:26:07Z","published":"2023-03-24T00:26:07Z","title":"Natural language processing to automatically extract the presence and\n  severity of esophagitis in notes of patients undergoing radiotherapy","summary":"  Radiotherapy (RT) toxicities can impair survival and quality-of-life, yet\nremain under-studied. Real-world evidence holds potential to improve our\nunderstanding of toxicities, but toxicity information is often only in clinical\nnotes. We developed natural language processing (NLP) models to identify the\npresence and severity of esophagitis from notes of patients treated with\nthoracic RT. We fine-tuned statistical and pre-trained BERT-based models for\nthree esophagitis classification tasks: Task 1) presence of esophagitis, Task\n2) severe esophagitis or not, and Task 3) no esophagitis vs. grade 1 vs. grade\n2-3. Transferability was tested on 345 notes from patients with esophageal\ncancer undergoing RT.\n  Fine-tuning PubmedBERT yielded the best performance. The best macro-F1 was\n0.92, 0.82, and 0.74 for Task 1, 2, and 3, respectively. Selecting the most\ninformative note sections during fine-tuning improved macro-F1 by over 2% for\nall tasks. Silver-labeled data improved the macro-F1 by over 3% across all\ntasks. For the esophageal cancer notes, the best macro-F1 was 0.73, 0.74, and\n0.65 for Task 1, 2, and 3, respectively, without additional fine-tuning.\n  To our knowledge, this is the first effort to automatically extract\nesophagitis toxicity severity according to CTCAE guidelines from clinic notes.\nThe promising performance provides proof-of-concept for NLP-based automated\ndetailed toxicity monitoring in expanded domains.\n","authors":["Shan Chen","Marco Guevara","Nicolas Ramirez","Arpi Murray","Jeremy L. Warner","Hugo JWL Aerts","Timothy A. Miller","Guergana K. Savova","Raymond H. Mak","Danielle S. Bitterman"],"pdf_url":"https://arxiv.org/pdf/2303.13722v1.pdf","comment":"17 pages, 6 tables, 1figure, submiting to JCO-CCI for review"},{"id":"http://arxiv.org/abs/2303.13716v1","updated":"2023-03-24T00:01:24Z","published":"2023-03-24T00:01:24Z","title":"ReCOGS: How Incidental Details of a Logical Form Overshadow an\n  Evaluation of Semantic Interpretation","summary":"  Compositional generalization benchmarks seek to assess whether models can\naccurately compute meanings for novel sentences, but operationalize this in\nterms of logical form (LF) prediction. This raises the concern that\nsemantically irrelevant details of the chosen LFs could shape model\nperformance. We argue that this concern is realized for the COGS benchmark (Kim\nand Linzen, 2020). COGS poses generalization splits that appear impossible for\npresent-day models, which could be taken as an indictment of those models.\nHowever, we show that the negative results trace to incidental features of COGS\nLFs. Converting these LFs to semantically equivalent ones and factoring out\ncapabilities unrelated to semantic interpretation, we find that even baseline\nmodels get traction. A recent variable-free translation of COGS LFs suggests\nsimilar conclusions, but we observe this format is not semantically equivalent;\nit is incapable of accurately representing some COGS meanings. These findings\ninform our proposal for ReCOGS, a modified version of COGS that comes closer to\nassessing the target semantic capabilities while remaining very challenging.\nOverall, our results reaffirm the importance of compositional generalization\nand careful benchmark task design.\n","authors":["Zhengxuan Wu","Christopher D. Manning","Christopher Potts"],"pdf_url":"https://arxiv.org/pdf/2303.13716v1.pdf","comment":"10 pages, 5 figures"},{"id":"http://arxiv.org/abs/2303.08179v2","updated":"2023-03-24T23:05:31Z","published":"2023-03-14T18:58:08Z","title":"MEDBERT.de: A Comprehensive German BERT Model for the Medical Domain","summary":"  This paper presents medBERTde, a pre-trained German BERT model specifically\ndesigned for the German medical domain. The model has been trained on a large\ncorpus of 4.7 Million German medical documents and has been shown to achieve\nnew state-of-the-art performance on eight different medical benchmarks covering\na wide range of disciplines and medical document types. In addition to\nevaluating the overall performance of the model, this paper also conducts a\nmore in-depth analysis of its capabilities. We investigate the impact of data\ndeduplication on the model's performance, as well as the potential benefits of\nusing more efficient tokenization methods. Our results indicate that\ndomain-specific models such as medBERTde are particularly useful for longer\ntexts, and that deduplication of training data does not necessarily lead to\nimproved performance. Furthermore, we found that efficient tokenization plays\nonly a minor role in improving model performance, and attribute most of the\nimproved performance to the large amount of training data. To encourage further\nresearch, the pre-trained model weights and new benchmarks based on\nradiological data are made publicly available for use by the scientific\ncommunity.\n","authors":["Keno K. Bressem","Jens-Michalis Papaioannou","Paul Grundmann","Florian Borchert","Lisa C. Adams","Leonhard Liu","Felix Busch","Lina Xu","Jan P. Loyen","Stefan M. Niehues","Moritz Augustin","Lennart Grosser","Marcus R. Makowski","Hugo JWL. Aerts","Alexander Löser"],"pdf_url":"https://arxiv.org/pdf/2303.08179v2.pdf","comment":"Keno K. Bressem and Jens-Michalis Papaioannou and Paul Grundmann\n  contributed equally"},{"id":"http://arxiv.org/abs/2303.14286v1","updated":"2023-03-24T21:49:27Z","published":"2023-03-24T21:49:27Z","title":"Voice-Based Conversational Agents and Knowledge Graphs for Improving\n  News Search in Assisted Living","summary":"  As the healthcare sector is facing major challenges, such as aging\npopulations, staff shortages, and common chronic diseases, delivering\nhigh-quality care to individuals has become very difficult. Conversational\nagents have shown to be a promising technology to alleviate some of these\nissues. In the form of digital health assistants, they have the potential to\nimprove the everyday life of the elderly and chronically ill people. This\nincludes, for example, medication reminders, routine checks, or social\nchit-chat. In addition, conversational agents can satisfy the fundamental need\nof having access to information about daily news or local events, which enables\nindividuals to stay informed and connected with the world around them. However,\nfinding relevant news sources and navigating the plethora of news articles\navailable online can be overwhelming, particularly for those who may have\nlimited technological literacy or health-related impairments. To address this\nchallenge, we propose an innovative solution that combines knowledge graphs and\nconversational agents for news search in assisted living. By leveraging graph\ndatabases to semantically structure news data and implementing an intuitive\nvoice-based interface, our system can help care-dependent people to easily\ndiscover relevant news articles and give personalized recommendations. We\nexplain our design choices, provide a system architecture, share insights of an\ninitial user test, and give an outlook on planned future work.\n","authors":["Phillip Schneider","Nils Rehtanz","Kristiina Jokinen","Florian Matthes"],"pdf_url":"https://arxiv.org/pdf/2303.14286v1.pdf","comment":"10 pages, submitted to PETRA 2023"},{"id":"http://arxiv.org/abs/2303.14279v1","updated":"2023-03-24T21:26:27Z","published":"2023-03-24T21:26:27Z","title":"Depression detection in social media posts using affective and social\n  norm features","summary":"  We propose a deep architecture for depression detection from social media\nposts. The proposed architecture builds upon BERT to extract language\nrepresentations from social media posts and combines these representations\nusing an attentive bidirectional GRU network. We incorporate affective\ninformation, by augmenting the text representations with features extracted\nfrom a pretrained emotion classifier. Motivated by psychological literature we\npropose to incorporate profanity and morality features of posts and words in\nour architecture using a late fusion scheme. Our analysis indicates that\nmorality and profanity can be important features for depression detection. We\napply our model for depression detection on Reddit posts on the Pirina dataset,\nand further consider the setting of detecting depressed users, given multiple\nposts per user, proposed in the Reddit RSDD dataset. The inclusion of the\nproposed features yields state-of-the-art results in both settings, namely\n2.65% and 6.73% absolute improvement in F1 score respectively. Index Terms:\nDepression detection, BERT, Feature fusion, Emotion recognition, profanity,\nmorality\n","authors":["Ilias Triantafyllopoulos","Georgios Paraskevopoulos","Alexandros Potamianos"],"pdf_url":"https://arxiv.org/pdf/2303.14279v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2209.14627v2","updated":"2023-03-24T20:10:15Z","published":"2022-09-29T08:41:32Z","title":"An Equal-Size Hard EM Algorithm for Diverse Dialogue Generation","summary":"  Open-domain dialogue systems aim to interact with humans through natural\nlanguage texts in an open-ended fashion. Despite the recent success of super\nlarge dialogue systems such as ChatGPT, using medium-to-small-sized dialogue\nsystems remains the common practice as they are more lightweight and\naccessible; however, generating diverse dialogue responses is challenging,\nespecially with smaller models. In this work, we propose an Equal-size Hard\nExpectation--Maximization (EqHard-EM) algorithm to train a multi-decoder model\nfor diverse dialogue generation. Our algorithm assigns a sample to a decoder in\na hard manner and additionally imposes an equal-assignment constraint to ensure\nthat all decoders are well-trained. We provide detailed theoretical analysis to\njustify our approach. Further, experiments on two large-scale open-domain\ndialogue datasets verify that our EqHard-EM algorithm generates high-quality\ndiverse responses.\n","authors":["Yuqiao Wen","Yongchang Hao","Yanshuai Cao","Lili Mou"],"pdf_url":"https://arxiv.org/pdf/2209.14627v2.pdf","comment":"Accepted by ICLR 2023"},{"id":"http://arxiv.org/abs/2209.00099v2","updated":"2023-03-24T19:49:14Z","published":"2022-08-31T20:32:35Z","title":"Efficient Methods for Natural Language Processing: A Survey","summary":"  Recent work in natural language processing (NLP) has yielded appealing\nresults from scaling model parameters and training data; however, using only\nscale to improve performance means that resource consumption also grows. Such\nresources include data, time, storage, or energy, all of which are naturally\nlimited and unevenly distributed. This motivates research into efficient\nmethods that require fewer resources to achieve similar results. This survey\nsynthesizes and relates current methods and findings in efficient NLP. We aim\nto provide both guidance for conducting NLP under limited resources, and point\ntowards promising research directions for developing more efficient methods.\n","authors":["Marcos Treviso","Ji-Ung Lee","Tianchu Ji","Betty van Aken","Qingqing Cao","Manuel R. Ciosici","Michael Hassid","Kenneth Heafield","Sara Hooker","Colin Raffel","Pedro H. Martins","André F. T. Martins","Jessica Zosa Forde","Peter Milder","Edwin Simpson","Noam Slonim","Jesse Dodge","Emma Strubell","Niranjan Balasubramanian","Leon Derczynski","Iryna Gurevych","Roy Schwartz"],"pdf_url":"https://arxiv.org/pdf/2209.00099v2.pdf","comment":"Accepted at TACL, pre publication version"},{"id":"http://arxiv.org/abs/2303.14234v1","updated":"2023-03-24T18:59:06Z","published":"2023-03-24T18:59:06Z","title":"SIGMORPHON 2023 Shared Task of Interlinear Glossing: Baseline Model","summary":"  Language documentation is a critical aspect of language preservation, often\nincluding the creation of Interlinear Glossed Text (IGT). Creating IGT is\ntime-consuming and tedious, and automating the process can save valuable\nannotator effort.\n  This paper describes the baseline system for the SIGMORPHON 2023 Shared Task\nof Interlinear Glossing. In our system, we utilize a transformer architecture\nand treat gloss generation as a sequence labelling task.\n","authors":["Michael Ginn"],"pdf_url":"https://arxiv.org/pdf/2303.14234v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.14222v1","updated":"2023-03-24T18:30:50Z","published":"2023-03-24T18:30:50Z","title":"Lay Text Summarisation Using Natural Language Processing: A Narrative\n  Literature Review","summary":"  Summarisation of research results in plain language is crucial for promoting\npublic understanding of research findings. The use of Natural Language\nProcessing to generate lay summaries has the potential to relieve researchers'\nworkload and bridge the gap between science and society. The aim of this\nnarrative literature review is to describe and compare the different text\nsummarisation approaches used to generate lay summaries. We searched the\ndatabases Web of Science, Google Scholar, IEEE Xplore, Association for\nComputing Machinery Digital Library and arXiv for articles published until 6\nMay 2022. We included original studies on automatic text summarisation methods\nto generate lay summaries. We screened 82 articles and included eight relevant\npapers published between 2020 and 2021, all using the same dataset. The results\nshow that transformer-based methods such as Bidirectional Encoder\nRepresentations from Transformers (BERT) and Pre-training with Extracted\nGap-sentences for Abstractive Summarization (PEGASUS) dominate the landscape of\nlay text summarisation, with all but one study using these methods. A\ncombination of extractive and abstractive summarisation methods in a hybrid\napproach was found to be most effective. Furthermore, pre-processing approaches\nto input text (e.g. applying extractive summarisation) or determining which\nsections of a text to include, appear critical. Evaluation metrics such as\nRecall-Oriented Understudy for Gisting Evaluation (ROUGE) were used, which do\nnot consider readability. To conclude, automatic lay text summarisation is\nunder-explored. Future research should consider long document lay text\nsummarisation, including clinical trial reports, and the development of\nevaluation metrics that consider readability of the lay summary.\n","authors":["Oliver Vinzelberg","Mark David Jenkins","Gordon Morison","David McMinn","Zoe Tieges"],"pdf_url":"https://arxiv.org/pdf/2303.14222v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2207.03038v3","updated":"2023-03-24T09:55:12Z","published":"2022-07-07T01:47:19Z","title":"Dual-Stream Transformer for Generic Event Boundary Captioning","summary":"  This paper describes our champion solution for the CVPR2022 Generic Event\nBoundary Captioning (GEBC) competition. GEBC requires the captioning model to\nhave a comprehension of instantaneous status changes around the given video\nboundary, which makes it much more challenging than conventional video\ncaptioning task. In this paper, a Dual-Stream Transformer with improvements on\nboth video content encoding and captions generation is proposed: (1) We utilize\nthree pre-trained models to extract the video features from different\ngranularities. Moreover, we exploit the types of boundary as hints to help the\nmodel generate captions. (2) We particularly design an model, termed as\nDual-Stream Transformer, to learn discriminative representations for boundary\ncaptioning. (3) Towards generating content-relevant and human-like captions, we\nimprove the description quality by designing a word-level ensemble strategy.\nThe promising results on the GEBC test split demonstrate the efficacy of our\nproposed model.\n","authors":["Xin Gu","Hanhua Ye","Guang Chen","Yufei Wang","Libo Zhang","Longyin Wen"],"pdf_url":"https://arxiv.org/pdf/2207.03038v3.pdf","comment":"Accepted to CVPR 2022 Workshop"}],"Computer Vision and Pattern Recognition":[{"id":"http://arxiv.org/abs/2303.14191v1","updated":"2023-03-24T17:59:58Z","published":"2023-03-24T17:59:58Z","title":"Masked Scene Contrast: A Scalable Framework for Unsupervised 3D\n  Representation Learning","summary":"  As a pioneering work, PointContrast conducts unsupervised 3D representation\nlearning via leveraging contrastive learning over raw RGB-D frames and proves\nits effectiveness on various downstream tasks. However, the trend of\nlarge-scale unsupervised learning in 3D has yet to emerge due to two stumbling\nblocks: the inefficiency of matching RGB-D frames as contrastive views and the\nannoying mode collapse phenomenon mentioned in previous works. Turning the two\nstumbling blocks into empirical stepping stones, we first propose an efficient\nand effective contrastive learning framework, which generates contrastive views\ndirectly on scene-level point clouds by a well-curated data augmentation\npipeline and a practical view mixing strategy. Second, we introduce\nreconstructive learning on the contrastive learning framework with an exquisite\ndesign of contrastive cross masks, which targets the reconstruction of point\ncolor and surfel normal. Our Masked Scene Contrast (MSC) framework is capable\nof extracting comprehensive 3D representations more efficiently and\neffectively. It accelerates the pre-training procedure by at least 3x and still\nachieves an uncompromised performance compared with previous work. Besides, MSC\nalso enables large-scale 3D pre-training across multiple datasets, which\nfurther boosts the performance and achieves state-of-the-art fine-tuning\nresults on several downstream tasks, e.g., 75.5% mIoU on ScanNet semantic\nsegmentation validation set.\n","authors":["Xiaoyang Wu","Xin Wen","Xihui Liu","Hengshuang Zhao"],"pdf_url":"https://arxiv.org/pdf/2303.14191v1.pdf","comment":"Accepted by CVPR 2023, available at\n  https://github.com/Pointcept/Pointcept"},{"id":"http://arxiv.org/abs/2303.14190v1","updated":"2023-03-24T17:59:56Z","published":"2023-03-24T17:59:56Z","title":"WildLight: In-the-wild Inverse Rendering with a Flashlight","summary":"  This paper proposes a practical photometric solution for the challenging\nproblem of in-the-wild inverse rendering under unknown ambient lighting. Our\nsystem recovers scene geometry and reflectance using only multi-view images\ncaptured by a smartphone. The key idea is to exploit smartphone's built-in\nflashlight as a minimally controlled light source, and decompose image\nintensities into two photometric components -- a static appearance corresponds\nto ambient flux, plus a dynamic reflection induced by the moving flashlight.\nOur method does not require flash/non-flash images to be captured in pairs.\nBuilding on the success of neural light fields, we use an off-the-shelf method\nto capture the ambient reflections, while the flashlight component enables\nphysically accurate photometric constraints to decouple reflectance and\nillumination. Compared to existing inverse rendering methods, our setup is\napplicable to non-darkroom environments yet sidesteps the inherent difficulties\nof explicit solving ambient reflections. We demonstrate by extensive\nexperiments that our method is easy to implement, casual to set up, and\nconsistently outperforms existing in-the-wild inverse rendering techniques.\nFinally, our neural reconstruction can be easily exported to PBR textured\ntriangle mesh ready for industrial renderers.\n","authors":["Ziang Cheng","Junxuan Li","Hongdong Li"],"pdf_url":"https://arxiv.org/pdf/2303.14190v1.pdf","comment":"Accepted to CVPR23. Website:\n  https://junxuan-li.github.io/wildlight-website/"},{"id":"http://arxiv.org/abs/2303.13511v2","updated":"2023-03-24T17:59:21Z","published":"2023-03-23T17:59:10Z","title":"Neural Preset for Color Style Transfer","summary":"  In this paper, we present a Neural Preset technique to address the\nlimitations of existing color style transfer methods, including visual\nartifacts, vast memory requirement, and slow style switching speed. Our method\nis based on two core designs. First, we propose Deterministic Neural Color\nMapping (DNCM) to consistently operate on each pixel via an image-adaptive\ncolor mapping matrix, avoiding artifacts and supporting high-resolution inputs\nwith a small memory footprint. Second, we develop a two-stage pipeline by\ndividing the task into color normalization and stylization, which allows\nefficient style switching by extracting color styles as presets and reusing\nthem on normalized input images. Due to the unavailability of pairwise\ndatasets, we describe how to train Neural Preset via a self-supervised\nstrategy. Various advantages of Neural Preset over existing methods are\ndemonstrated through comprehensive evaluations. Notably, Neural Preset enables\nstable 4K color style transfer in real-time without artifacts. Besides, we show\nthat our trained model can naturally support multiple applications without\nfine-tuning, including low-light image enhancement, underwater image\ncorrection, image dehazing, and image harmonization. Project page with demos:\nhttps://zhkkke.github.io/NeuralPreset .\n","authors":["Zhanghan Ke","Yuhao Liu","Lei Zhu","Nanxuan Zhao","Rynson W. H. Lau"],"pdf_url":"https://arxiv.org/pdf/2303.13511v2.pdf","comment":"Project page with demos: https://zhkkke.github.io/NeuralPreset .\n  Artifact-free real-time 4K color style transfer via AI-generated presets.\n  CVPR 2023"},{"id":"http://arxiv.org/abs/2303.14189v1","updated":"2023-03-24T17:58:32Z","published":"2023-03-24T17:58:32Z","title":"FastViT: A Fast Hybrid Vision Transformer using Structural\n  Reparameterization","summary":"  The recent amalgamation of transformer and convolutional designs has led to\nsteady improvements in accuracy and efficiency of the models. In this work, we\nintroduce FastViT, a hybrid vision transformer architecture that obtains the\nstate-of-the-art latency-accuracy trade-off. To this end, we introduce a novel\ntoken mixing operator, RepMixer, a building block of FastViT, that uses\nstructural reparameterization to lower the memory access cost by removing\nskip-connections in the network. We further apply train-time\noverparametrization and large kernel convolutions to boost accuracy and\nempirically show that these choices have minimal effect on latency. We show\nthat - our model is 3.5x faster than CMT, a recent state-of-the-art hybrid\ntransformer architecture, 4.9x faster than EfficientNet, and 1.9x faster than\nConvNeXt on a mobile device for the same accuracy on the ImageNet dataset. At\nsimilar latency, our model obtains 4.2% better Top-1 accuracy on ImageNet than\nMobileOne. Our model consistently outperforms competing architectures across\nseveral tasks -- image classification, detection, segmentation and 3D mesh\nregression with significant improvement in latency on both a mobile device and\na desktop GPU. Furthermore, our model is highly robust to out-of-distribution\nsamples and corruptions, improving over competing robust models.\n","authors":["Pavan Kumar Anasosalu Vasu","James Gabriel","Jeff Zhu","Oncel Tuzel","Anurag Ranjan"],"pdf_url":"https://arxiv.org/pdf/2303.14189v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.14184v1","updated":"2023-03-24T17:54:22Z","published":"2023-03-24T17:54:22Z","title":"Make-It-3D: High-Fidelity 3D Creation from A Single Image with Diffusion\n  Prior","summary":"  In this work, we investigate the problem of creating high-fidelity 3D content\nfrom only a single image. This is inherently challenging: it essentially\ninvolves estimating the underlying 3D geometry while simultaneously\nhallucinating unseen textures. To address this challenge, we leverage prior\nknowledge from a well-trained 2D diffusion model to act as 3D-aware supervision\nfor 3D creation. Our approach, Make-It-3D, employs a two-stage optimization\npipeline: the first stage optimizes a neural radiance field by incorporating\nconstraints from the reference image at the frontal view and diffusion prior at\nnovel views; the second stage transforms the coarse model into textured point\nclouds and further elevates the realism with diffusion prior while leveraging\nthe high-quality textures from the reference image. Extensive experiments\ndemonstrate that our method outperforms prior works by a large margin,\nresulting in faithful reconstructions and impressive visual quality. Our method\npresents the first attempt to achieve high-quality 3D creation from a single\nimage for general objects and enables various applications such as text-to-3D\ncreation and texture editing.\n","authors":["Junshu Tang","Tengfei Wang","Bo Zhang","Ting Zhang","Ran Yi","Lizhuang Ma","Dong Chen"],"pdf_url":"https://arxiv.org/pdf/2303.14184v1.pdf","comment":"Project page: https://make-it-3d.github.io/"},{"id":"http://arxiv.org/abs/2302.14256v2","updated":"2023-03-24T17:43:20Z","published":"2023-02-28T02:27:36Z","title":"Remote Sensing Scene Classification with Masked Image Modeling (MIM)","summary":"  Remote sensing scene classification has been extensively studied for its\ncritical roles in geological survey, oil exploration, traffic management,\nearthquake prediction, wildfire monitoring, and intelligence monitoring. In the\npast, the Machine Learning (ML) methods for performing the task mainly used the\nbackbones pretrained in the manner of supervised learning (SL). As Masked Image\nModeling (MIM), a self-supervised learning (SSL) technique, has been shown as a\nbetter way for learning visual feature representation, it presents a new\nopportunity for improving ML performance on the scene classification task. This\nresearch aims to explore the potential of MIM pretrained backbones on four\nwell-known classification datasets: Merced, AID, NWPU-RESISC45, and Optimal-31.\nCompared to the published benchmarks, we show that the MIM pretrained Vision\nTransformer (ViTs) backbones outperform other alternatives (up to 18% on top 1\naccuracy) and that the MIM technique can learn better feature representation\nthan the supervised learning counterparts (up to 5% on top 1 accuracy).\nMoreover, we show that the general-purpose MIM-pretrained ViTs can achieve\ncompetitive performance as the specially designed yet complicated Transformer\nfor Remote Sensing (TRS) framework. Our experiment results also provide a\nperformance baseline for future studies.\n","authors":["Liya Wang","Alex Tien"],"pdf_url":"https://arxiv.org/pdf/2302.14256v2.pdf","comment":"arXiv admin note: text overlap with arXiv:2301.12058"},{"id":"http://arxiv.org/abs/2303.14176v1","updated":"2023-03-24T17:38:45Z","published":"2023-03-24T17:38:45Z","title":"A Hybrid ANN-SNN Architecture for Low-Power and Low-Latency Visual\n  Perception","summary":"  Spiking Neural Networks (SNN) are a class of bio-inspired neural networks\nthat promise to bring low-power and low-latency inference to edge devices\nthrough asynchronous and sparse processing. However, being temporal models,\nSNNs depend heavily on expressive states to generate predictions on par with\nclassical artificial neural networks (ANNs). These states converge only after\nlong transient periods, and quickly decay without input data, leading to higher\nlatency, power consumption, and lower accuracy. This work addresses this issue\nby initializing the state with an auxiliary ANN running at a low rate. The SNN\nthen uses the state to generate predictions with high temporal resolution until\nthe next initialization phase. Our hybrid ANN-SNN model thus combines the best\nof both worlds: It does not suffer from long state transients and state decay\nthanks to the ANN, and can generate predictions with high temporal resolution,\nlow latency, and low power thanks to the SNN. We show for the task of\nevent-based 2D and 3D human pose estimation that our method consumes 88% less\npower with only a 4% decrease in performance compared to its fully ANN\ncounterparts when run at the same inference rate. Moreover, when compared to\nSNNs, our method achieves a 74% lower error. This research thus provides a new\nunderstanding of how ANNs and SNNs can be used to maximize their respective\nbenefits.\n","authors":["Asude Aydin","Mathias Gehrig","Daniel Gehrig","Davide Scaramuzza"],"pdf_url":"https://arxiv.org/pdf/2303.14176v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.14175v1","updated":"2023-03-24T17:38:03Z","published":"2023-03-24T17:38:03Z","title":"Inherent Consistent Learning for Accurate Semi-supervised Medical Image\n  Segmentation","summary":"  Semi-supervised medical image segmentation has attracted much attention in\nrecent years because of the high cost of medical image annotations. In this\npaper, we propose a novel Inherent Consistent Learning (ICL) method, which aims\nto learn robust semantic category representations through the semantic\nconsistency guidance of labeled and unlabeled data to help segmentation. In\npractice, we introduce two external modules namely Supervised Semantic Proxy\nAdaptor (SSPA) and Unsupervised Semantic Consistent Learner (USCL) that based\non the attention mechanism to align the semantic category representations of\nlabeled and unlabeled data, as well as update the global semantic\nrepresentations over the entire training set. The proposed ICL is a\nplug-and-play scheme for various network architectures and the two modules are\nnot involved in the testing stage. Experimental results on three public\nbenchmarks show that the proposed method can outperform the state-of-the-art\nespecially when the number of annotated data is extremely limited. Code is\navailable at: https://github.com/zhuye98/ICL.git.\n","authors":["Ye Zhu","Jie Yang","Si-Qi Liu","Ruimao Zhang"],"pdf_url":"https://arxiv.org/pdf/2303.14175v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.00040v2","updated":"2023-03-24T17:35:07Z","published":"2023-02-28T19:29:05Z","title":"Towards Generalisable Video Moment Retrieval: Visual-Dynamic Injection\n  to Image-Text Pre-Training","summary":"  The correlation between the vision and text is essential for video moment\nretrieval (VMR), however, existing methods heavily rely on separate\npre-training feature extractors for visual and textual understanding. Without\nsufficient temporal boundary annotations, it is non-trivial to learn universal\nvideo-text alignments. In this work, we explore multi-modal correlations\nderived from large-scale image-text data to facilitate generalisable VMR. To\naddress the limitations of image-text pre-training models on capturing the\nvideo changes, we propose a generic method, referred to as Visual-Dynamic\nInjection (VDI), to empower the model's understanding of video moments. Whilst\nexisting VMR methods are focusing on building temporal-aware video features,\nbeing aware of the text descriptions about the temporal changes is also\ncritical but originally overlooked in pre-training by matching static images\nwith sentences. Therefore, we extract visual context and spatial dynamic\ninformation from video frames and explicitly enforce their alignments with the\nphrases describing video changes (e.g. verb). By doing so, the potentially\nrelevant visual and motion patterns in videos are encoded in the corresponding\ntext embeddings (injected) so to enable more accurate video-text alignments. We\nconduct extensive experiments on two VMR benchmark datasets (Charades-STA and\nActivityNet-Captions) and achieve state-of-the-art performances. Especially,\nVDI yields notable advantages when being tested on the out-of-distribution\nsplits where the testing samples involve novel scenes and vocabulary.\n","authors":["Dezhao Luo","Jiabo Huang","Shaogang Gong","Hailin Jin","Yang Liu"],"pdf_url":"https://arxiv.org/pdf/2303.00040v2.pdf","comment":"CVPR2023"},{"id":"http://arxiv.org/abs/2209.08633v2","updated":"2023-03-24T17:32:33Z","published":"2022-09-18T19:12:43Z","title":"CNN based Intelligent Streetlight Management Using Smart CCTV Camera and\n  Semantic Segmentation","summary":"  One of the most neglected sources of energy loss is streetlights which\ngenerate too much light in areas where it is not required. Energy waste has\nenormous economic and environmental effects. In addition, due to the\nconventional manual nature of the operation, streetlights are frequently seen\nbeing turned ON during the day and OFF in the evening, which is regrettable\neven in the twenty-first century. These issues require automated streetlight\ncontrol in order to be resolved. This study aims to develop a novel streetlight\ncontrolling method by combining a smart transport monitoring system powered by\ncomputer vision technology with a closed circuit television (CCTV) camera that\nallows the light-emitting diode (LED) streetlight to automatically light up\nwith the appropriate brightness by detecting the presence of pedestrians or\nvehicles and dimming the streetlight in their absence using semantic image\nsegmentation from the CCTV video streaming. Consequently, our model\ndistinguishes daylight and nighttime, which made it feasible to automate the\nprocess of turning the streetlight 'ON' and 'OFF' to save energy consumption\ncosts. According to the aforementioned approach, geolocation sensor data could\nbe utilized to make more informed streetlight management decisions. To complete\nthe tasks, we consider training the U-net model with ResNet-34 as its backbone.\nThe validity of the models is guaranteed with the use of assessment matrices.\nThe suggested concept is straightforward, economical, energy-efficient,\nlong-lasting, and more resilient than conventional alternatives.\n","authors":["Md Sakib Ullah Sourav","Huidong Wang","Mohammad Raziuddin Chowdhury","Rejwan Bin Sulaiman"],"pdf_url":"https://arxiv.org/pdf/2209.08633v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.14167v1","updated":"2023-03-24T17:28:07Z","published":"2023-03-24T17:28:07Z","title":"UrbanGIRAFFE: Representing Urban Scenes as Compositional Generative\n  Neural Feature Fields","summary":"  Generating photorealistic images with controllable camera pose and scene\ncontents is essential for many applications including AR/VR and simulation.\nDespite the fact that rapid progress has been made in 3D-aware generative\nmodels, most existing methods focus on object-centric images and are not\napplicable to generating urban scenes for free camera viewpoint control and\nscene editing. To address this challenging task, we propose UrbanGIRAFFE, which\nuses a coarse 3D panoptic prior, including the layout distribution of\nuncountable stuff and countable objects, to guide a 3D-aware generative model.\nOur model is compositional and controllable as it breaks down the scene into\nstuff, objects, and sky. Using stuff prior in the form of semantic voxel grids,\nwe build a conditioned stuff generator that effectively incorporates the coarse\nsemantic and geometry information. The object layout prior further allows us to\nlearn an object generator from cluttered scenes. With proper loss functions,\nour approach facilitates photorealistic 3D-aware image synthesis with diverse\ncontrollability, including large camera movement, stuff editing, and object\nmanipulation. We validate the effectiveness of our model on both synthetic and\nreal-world datasets, including the challenging KITTI-360 dataset.\n","authors":["Yuanbo Yang","Yifei Yang","Hanlei Guo","Rong Xiong","Yue Wang","Yiyi Liao"],"pdf_url":"https://arxiv.org/pdf/2303.14167v1.pdf","comment":"Project page: https://lv3d.github.io/urbanGIRAFFE"},{"id":"http://arxiv.org/abs/2211.10277v2","updated":"2023-03-24T17:22:32Z","published":"2022-11-18T15:09:03Z","title":"Task Residual for Tuning Vision-Language Models","summary":"  Large-scale vision-language models (VLMs) pre-trained on billion-level data\nhave learned general visual representations and broad visual concepts. In\nprinciple, the well-learned knowledge structure of the VLMs should be inherited\nappropriately when being transferred to downstream tasks with limited data.\nHowever, most existing efficient transfer learning (ETL) approaches for VLMs\neither damage or are excessively biased towards the prior knowledge, e.g.,\nprompt tuning (PT) discards the pre-trained text-based classifier and builds a\nnew one while adapter-style tuning (AT) fully relies on the pre-trained\nfeatures. To address this, we propose a new efficient tuning approach for VLMs\nnamed Task Residual Tuning (TaskRes), which performs directly on the text-based\nclassifier and explicitly decouples the prior knowledge of the pre-trained\nmodels and new knowledge regarding a target task. Specifically, TaskRes keeps\nthe original classifier weights from the VLMs frozen and obtains a new\nclassifier for the target task by tuning a set of prior-independent parameters\nas a residual to the original one, which enables reliable prior knowledge\npreservation and flexible task-specific knowledge exploration. The proposed\nTaskRes is simple yet effective, which significantly outperforms previous ETL\nmethods (e.g., PT and AT) on 11 benchmark datasets while requiring minimal\neffort for the implementation. Our code is available at\nhttps://github.com/geekyutao/TaskRes.\n","authors":["Tao Yu","Zhihe Lu","Xin Jin","Zhibo Chen","Xinchao Wang"],"pdf_url":"https://arxiv.org/pdf/2211.10277v2.pdf","comment":"Accepted to CVPR 2023"},{"id":"http://arxiv.org/abs/2303.14158v1","updated":"2023-03-24T17:13:49Z","published":"2023-03-24T17:13:49Z","title":"BundleSDF: Neural 6-DoF Tracking and 3D Reconstruction of Unknown\n  Objects","summary":"  We present a near real-time method for 6-DoF tracking of an unknown object\nfrom a monocular RGBD video sequence, while simultaneously performing neural 3D\nreconstruction of the object. Our method works for arbitrary rigid objects,\neven when visual texture is largely absent. The object is assumed to be\nsegmented in the first frame only. No additional information is required, and\nno assumption is made about the interaction agent. Key to our method is a\nNeural Object Field that is learned concurrently with a pose graph optimization\nprocess in order to robustly accumulate information into a consistent 3D\nrepresentation capturing both geometry and appearance. A dynamic pool of posed\nmemory frames is automatically maintained to facilitate communication between\nthese threads. Our approach handles challenging sequences with large pose\nchanges, partial and full occlusion, untextured surfaces, and specular\nhighlights. We show results on HO3D, YCBInEOAT, and BEHAVE datasets,\ndemonstrating that our method significantly outperforms existing approaches.\nProject page: https://bundlesdf.github.io\n","authors":["Bowen Wen","Jonathan Tremblay","Valts Blukis","Stephen Tyree","Thomas Muller","Alex Evans","Dieter Fox","Jan Kautz","Stan Birchfield"],"pdf_url":"https://arxiv.org/pdf/2303.14158v1.pdf","comment":"CVPR 2023"},{"id":"http://arxiv.org/abs/2303.14157v1","updated":"2023-03-24T17:12:38Z","published":"2023-03-24T17:12:38Z","title":"Efficient Scale-Invariant Generator with Column-Row Entangled Pixel\n  Synthesis","summary":"  Any-scale image synthesis offers an efficient and scalable solution to\nsynthesize photo-realistic images at any scale, even going beyond 2K\nresolution. However, existing GAN-based solutions depend excessively on\nconvolutions and a hierarchical architecture, which introduce inconsistency and\nthe $``$texture sticking$\"$ issue when scaling the output resolution. From\nanother perspective, INR-based generators are scale-equivariant by design, but\ntheir huge memory footprint and slow inference hinder these networks from being\nadopted in large-scale or real-time systems. In this work, we propose\n$\\textbf{C}$olumn-$\\textbf{R}$ow $\\textbf{E}$ntangled $\\textbf{P}$ixel\n$\\textbf{S}$ynthesis ($\\textbf{CREPS}$), a new generative model that is both\nefficient and scale-equivariant without using any spatial convolutions or\ncoarse-to-fine design. To save memory footprint and make the system scalable,\nwe employ a novel bi-line representation that decomposes layer-wise feature\nmaps into separate $``$thick$\"$ column and row encodings. Experiments on\nvarious datasets, including FFHQ, LSUN-Church, MetFaces, and Flickr-Scenery,\nconfirm CREPS' ability to synthesize scale-consistent and alias-free images at\nany arbitrary resolution with proper training and inference speed. Code is\navailable at https://github.com/VinAIResearch/CREPS.\n","authors":["Thuan Hoang Nguyen","Thanh Van Le","Anh Tran"],"pdf_url":"https://arxiv.org/pdf/2303.14157v1.pdf","comment":"Accepted to CVPR 2023"},{"id":"http://arxiv.org/abs/2303.14153v1","updated":"2023-03-24T17:04:26Z","published":"2023-03-24T17:04:26Z","title":"Local Contrastive Learning for Medical Image Recognition","summary":"  The proliferation of Deep Learning (DL)-based methods for radiographic image\nanalysis has created a great demand for expert-labeled radiology data. Recent\nself-supervised frameworks have alleviated the need for expert labeling by\nobtaining supervision from associated radiology reports. These frameworks,\nhowever, struggle to distinguish the subtle differences between different\npathologies in medical images. Additionally, many of them do not provide\ninterpretation between image regions and text, making it difficult for\nradiologists to assess model predictions. In this work, we propose Local Region\nContrastive Learning (LRCLR), a flexible fine-tuning framework that adds layers\nfor significant image region selection as well as cross-modality interaction.\nOur results on an external validation set of chest x-rays suggest that LRCLR\nidentifies significant local image regions and provides meaningful\ninterpretation against radiology text while improving zero-shot performance on\nseveral chest x-ray medical findings.\n","authors":["S. A. Rizvi","R. Tang","X. Jiang","X. Ma","X. Hu"],"pdf_url":"https://arxiv.org/pdf/2303.14153v1.pdf","comment":"10 pages, 5 figures, 1 table, AMIA conference submission"},{"id":"http://arxiv.org/abs/2303.14152v1","updated":"2023-03-24T17:03:40Z","published":"2023-03-24T17:03:40Z","title":"Fantastic Breaks: A Dataset of Paired 3D Scans of Real-World Broken\n  Objects and Their Complete Counterparts","summary":"  Automated shape repair approaches currently lack access to datasets that\ndescribe real-world damage geometry. We present Fantastic Breaks (and Where to\nFind Them:\nhttps://terascale-all-sensing-research-studio.github.io/FantasticBreaks), a\ndataset containing scanned, waterproofed, and cleaned 3D meshes for 78 broken\nobjects, paired and geometrically aligned with complete counterparts. Fantastic\nBreaks contains class and material labels, synthetic proxies of repair parts\nthat join to broken meshes to generate complete meshes, and manually annotated\nfracture boundaries. Through a detailed analysis of fracture geometry, we\nreveal differences between Fantastic Breaks and datasets of synthetically\nfractured objects generated using geometric and physics-based methods. We show\nexperimental results of shape repair with Fantastic Breaks using multiple\nlearning-based approaches pre-trained using a synthetic dataset and re-trained\nusing a subset of Fantastic Breaks.\n","authors":["Nikolas Lamb","Cameron Palmer","Benjamin Molloy","Sean Banerjee","Natasha Kholgade Banerjee"],"pdf_url":"https://arxiv.org/pdf/2303.14152v1.pdf","comment":"To be published at CVPR 2023"},{"id":"http://arxiv.org/abs/2206.11896v3","updated":"2023-03-24T16:57:36Z","published":"2022-06-23T17:59:53Z","title":"EventNeRF: Neural Radiance Fields from a Single Colour Event Camera","summary":"  Asynchronously operating event cameras find many applications due to their\nhigh dynamic range, vanishingly low motion blur, low latency and low data\nbandwidth. The field saw remarkable progress during the last few years, and\nexisting event-based 3D reconstruction approaches recover sparse point clouds\nof the scene. However, such sparsity is a limiting factor in many cases,\nespecially in computer vision and graphics, that has not been addressed\nsatisfactorily so far. Accordingly, this paper proposes the first approach for\n3D-consistent, dense and photorealistic novel view synthesis using just a\nsingle colour event stream as input. At its core is a neural radiance field\ntrained entirely in a self-supervised manner from events while preserving the\noriginal resolution of the colour event channels. Next, our ray sampling\nstrategy is tailored to events and allows for data-efficient training. At test,\nour method produces results in the RGB space at unprecedented quality. We\nevaluate our method qualitatively and numerically on several challenging\nsynthetic and real scenes and show that it produces significantly denser and\nmore visually appealing renderings than the existing methods. We also\ndemonstrate robustness in challenging scenarios with fast motion and under low\nlighting conditions. We release the newly recorded dataset and our source code\nto facilitate the research field, see https://4dqv.mpi-inf.mpg.de/EventNeRF.\n","authors":["Viktor Rudnev","Mohamed Elgharib","Christian Theobalt","Vladislav Golyanik"],"pdf_url":"https://arxiv.org/pdf/2206.11896v3.pdf","comment":"19 pages, 21 figures, 3 tables; CVPR 2023"},{"id":"http://arxiv.org/abs/2211.14306v2","updated":"2023-03-24T16:56:25Z","published":"2022-11-25T18:59:10Z","title":"RUST: Latent Neural Scene Representations from Unposed Imagery","summary":"  Inferring the structure of 3D scenes from 2D observations is a fundamental\nchallenge in computer vision. Recently popularized approaches based on neural\nscene representations have achieved tremendous impact and have been applied\nacross a variety of applications. One of the major remaining challenges in this\nspace is training a single model which can provide latent representations which\neffectively generalize beyond a single scene. Scene Representation Transformer\n(SRT) has shown promise in this direction, but scaling it to a larger set of\ndiverse scenes is challenging and necessitates accurately posed ground truth\ndata. To address this problem, we propose RUST (Really Unposed Scene\nrepresentation Transformer), a pose-free approach to novel view synthesis\ntrained on RGB images alone. Our main insight is that one can train a Pose\nEncoder that peeks at the target image and learns a latent pose embedding which\nis used by the decoder for view synthesis. We perform an empirical\ninvestigation into the learned latent pose structure and show that it allows\nmeaningful test-time camera transformations and accurate explicit pose\nreadouts. Perhaps surprisingly, RUST achieves similar quality as methods which\nhave access to perfect camera pose, thereby unlocking the potential for\nlarge-scale training of amortized neural scene representations.\n","authors":["Mehdi S. M. Sajjadi","Aravindh Mahendran","Thomas Kipf","Etienne Pot","Daniel Duckworth","Mario Lucic","Klaus Greff"],"pdf_url":"https://arxiv.org/pdf/2211.14306v2.pdf","comment":"CVPR 2023 Highlight. Project website: https://rust-paper.github.io/"},{"id":"http://arxiv.org/abs/2303.14139v1","updated":"2023-03-24T16:41:42Z","published":"2023-03-24T16:41:42Z","title":"MindDiffuser: Controlled Image Reconstruction from Human Brain Activity\n  with Semantic and Structural Diffusion","summary":"  Reconstructing visual stimuli from measured functional magnetic resonance\nimaging (fMRI) has been a meaningful and challenging task. Previous studies\nhave successfully achieved reconstructions with structures similar to the\noriginal images, such as the outlines and size of some natural images. However,\nthese reconstructions lack explicit semantic information and are difficult to\ndiscern. In recent years, many studies have utilized multi-modal pre-trained\nmodels with stronger generative capabilities to reconstruct images that are\nsemantically similar to the original ones. However, these images have\nuncontrollable structural information such as position and orientation. To\naddress both of the aforementioned issues simultaneously, we propose a\ntwo-stage image reconstruction model called MindDiffuser, utilizing Stable\nDiffusion. In Stage 1, the VQ-VAE latent representations and the CLIP text\nembeddings decoded from fMRI are put into the image-to-image process of Stable\nDiffusion, which yields a preliminary image that contains semantic and\nstructural information. In Stage 2, we utilize the low-level CLIP visual\nfeatures decoded from fMRI as supervisory information, and continually adjust\nthe two features in Stage 1 through backpropagation to align the structural\ninformation. The results of both qualitative and quantitative analyses\ndemonstrate that our proposed model has surpassed the current state-of-the-art\nmodels in terms of reconstruction results on Natural Scenes Dataset (NSD).\nFurthermore, the results of ablation experiments indicate that each component\nof our model is effective for image reconstruction.\n","authors":["Yizhuo Lu","Changde Du","Dianpeng Wang","Huiguang He"],"pdf_url":"https://arxiv.org/pdf/2303.14139v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.14133v1","updated":"2023-03-24T16:38:58Z","published":"2023-03-24T16:38:58Z","title":"Adversarial Attack and Defense for Medical Image Analysis: Methods and\n  Applications","summary":"  Deep learning techniques have achieved superior performance in computer-aided\nmedical image analysis, yet they are still vulnerable to imperceptible\nadversarial attacks, resulting in potential misdiagnosis in clinical practice.\nOppositely, recent years have also witnessed remarkable progress in defense\nagainst these tailored adversarial examples in deep medical diagnosis systems.\nIn this exposition, we present a comprehensive survey on recent advances in\nadversarial attack and defense for medical image analysis with a novel taxonomy\nin terms of the application scenario. We also provide a unified theoretical\nframework for different types of adversarial attack and defense methods for\nmedical image analysis. For a fair comparison, we establish a new benchmark for\nadversarially robust medical diagnosis models obtained by adversarial training\nunder various scenarios. To the best of our knowledge, this is the first survey\npaper that provides a thorough evaluation of adversarially robust medical\ndiagnosis models. By analyzing qualitative and quantitative results, we\nconclude this survey with a detailed discussion of current challenges for\nadversarial attack and defense in medical image analysis systems to shed light\non future research directions.\n","authors":["Junhao Dong","Junxi Chen","Xiaohua Xie","Jianhuang Lai","Hao Chen"],"pdf_url":"https://arxiv.org/pdf/2303.14133v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.14126v1","updated":"2023-03-24T16:33:06Z","published":"2023-03-24T16:33:06Z","title":"CIFAKE: Image Classification and Explainable Identification of\n  AI-Generated Synthetic Images","summary":"  Recent technological advances in synthetic data have enabled the generation\nof images with such high quality that human beings cannot tell the difference\nbetween real-life photographs and Artificial Intelligence (AI) generated\nimages. Given the critical necessity of data reliability and authentication,\nthis article proposes to enhance our ability to recognise AI-generated images\nthrough computer vision. Initially, a synthetic dataset is generated that\nmirrors the ten classes of the already available CIFAR-10 dataset with latent\ndiffusion which provides a contrasting set of images for comparison to real\nphotographs. The model is capable of generating complex visual attributes, such\nas photorealistic reflections in water. The two sets of data present as a\nbinary classification problem with regard to whether the photograph is real or\ngenerated by AI. This study then proposes the use of a Convolutional Neural\nNetwork (CNN) to classify the images into two categories; Real or Fake.\nFollowing hyperparameter tuning and the training of 36 individual network\ntopologies, the optimal approach could correctly classify the images with\n92.98% accuracy. Finally, this study implements explainable AI via Gradient\nClass Activation Mapping to explore which features within the images are useful\nfor classification. Interpretation reveals interesting concepts within the\nimage, in particular, noting that the actual entity itself does not hold useful\ninformation for classification; instead, the model focuses on small visual\nimperfections in the background of the images. The complete dataset engineered\nfor this study, referred to as the CIFAKE dataset, is made publicly available\nto the research community for future work.\n","authors":["Jordan J. Bird","Ahmad Lotfi"],"pdf_url":"https://arxiv.org/pdf/2303.14126v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.14123v1","updated":"2023-03-24T16:32:19Z","published":"2023-03-24T16:32:19Z","title":"Semantic Prompt for Few-Shot Image Recognition","summary":"  Few-shot learning is a challenging problem since only a few examples are\nprovided to recognize a new class. Several recent studies exploit additional\nsemantic information, e.g. text embeddings of class names, to address the issue\nof rare samples through combining semantic prototypes with visual prototypes.\nHowever, these methods still suffer from the spurious visual features learned\nfrom the rare support samples, resulting in limited benefits. In this paper, we\npropose a novel Semantic Prompt (SP) approach for few-shot learning. Instead of\nthe naive exploitation of semantic information for remedying classifiers, we\nexplore leveraging semantic information as prompts to tune the visual feature\nextraction network adaptively. Specifically, we design two complementary\nmechanisms to insert semantic prompts into the feature extractor: one is to\nenable the interaction between semantic prompts and patch embeddings along the\nspatial dimension via self-attention, another is to supplement visual features\nwith the transformed semantic prompts along the channel dimension. By combining\nthese two mechanisms, the feature extractor presents a better ability to attend\nto the class-specific features and obtains more generalized image\nrepresentations with merely a few support samples. Through extensive\nexperiments on four datasets, the proposed approach achieves promising results,\nimproving the 1-shot learning accuracy by 3.67% on average.\n","authors":["Wentao Chen","Chenyang Si","Zhang Zhang","Liang Wang","Zilei Wang","Tieniu Tan"],"pdf_url":"https://arxiv.org/pdf/2303.14123v1.pdf","comment":"Accepted by CVPR 2023"},{"id":"http://arxiv.org/abs/2303.14124v1","updated":"2023-03-24T16:32:19Z","published":"2023-03-24T16:32:19Z","title":"Towards Scalable Neural Representation for Diverse Videos","summary":"  Implicit neural representations (INR) have gained increasing attention in\nrepresenting 3D scenes and images, and have been recently applied to encode\nvideos (e.g., NeRV, E-NeRV). While achieving promising results, existing\nINR-based methods are limited to encoding a handful of short videos (e.g.,\nseven 5-second videos in the UVG dataset) with redundant visual content,\nleading to a model design that fits individual video frames independently and\nis not efficiently scalable to a large number of diverse videos. This paper\nfocuses on developing neural representations for a more practical setup --\nencoding long and/or a large number of videos with diverse visual content. We\nfirst show that instead of dividing videos into small subsets and encoding them\nwith separate models, encoding long and diverse videos jointly with a unified\nmodel achieves better compression results. Based on this observation, we\npropose D-NeRV, a novel neural representation framework designed to encode\ndiverse videos by (i) decoupling clip-specific visual content from motion\ninformation, (ii) introducing temporal reasoning into the implicit neural\nnetwork, and (iii) employing the task-oriented flow as intermediate output to\nreduce spatial redundancies. Our new model largely surpasses NeRV and\ntraditional video compression techniques on UCF101 and UVG datasets on the\nvideo compression task. Moreover, when used as an efficient data-loader, D-NeRV\nachieves 3%-10% higher accuracy than NeRV on action recognition tasks on the\nUCF101 dataset under the same compression ratios.\n","authors":["Bo He","Xitong Yang","Hanyu Wang","Zuxuan Wu","Hao Chen","Shuaiyi Huang","Yixuan Ren","Ser-Nam Lim","Abhinav Shrivastava"],"pdf_url":"https://arxiv.org/pdf/2303.14124v1.pdf","comment":"Accepted at CVPR 2023"},{"id":"http://arxiv.org/abs/2303.14116v1","updated":"2023-03-24T16:24:08Z","published":"2023-03-24T16:24:08Z","title":"Improving Prediction Performance and Model Interpretability through\n  Attention Mechanisms from Basic and Applied Research Perspectives","summary":"  With the dramatic advances in deep learning technology, machine learning\nresearch is focusing on improving the interpretability of model predictions as\nwell as prediction performance in both basic and applied research. While deep\nlearning models have much higher prediction performance than traditional\nmachine learning models, the specific prediction process is still difficult to\ninterpret and/or explain. This is known as the black-boxing of machine learning\nmodels and is recognized as a particularly important problem in a wide range of\nresearch fields, including manufacturing, commerce, robotics, and other\nindustries where the use of such technology has become commonplace, as well as\nthe medical field, where mistakes are not tolerated. This bulletin is based on\nthe summary of the author's dissertation. The research summarized in the\ndissertation focuses on the attention mechanism, which has been the focus of\nmuch attention in recent years, and discusses its potential for both basic\nresearch in terms of improving prediction performance and interpretability, and\napplied research in terms of evaluating it for real-world applications using\nlarge data sets beyond the laboratory environment. The dissertation also\nconcludes with a summary of the implications of these findings for subsequent\nresearch and future prospects in the field.\n","authors":["Shunsuke Kitada"],"pdf_url":"https://arxiv.org/pdf/2303.14116v1.pdf","comment":"The bulletin of Graduate School of Science and Engineering, Hosei\n  University, Vol.64 (03/2023). This article draws heavily from\n  arxiv:2009.12064, arxiv:2104.08763, arxiv:1905.07289, and arxiv:2204.11588"},{"id":"http://arxiv.org/abs/2303.14115v1","updated":"2023-03-24T16:23:49Z","published":"2023-03-24T16:23:49Z","title":"Principles of Forgetting in Domain-Incremental Semantic Segmentation in\n  Adverse Weather Conditions","summary":"  Deep neural networks for scene perception in automated vehicles achieve\nexcellent results for the domains they were trained on. However, in real-world\nconditions, the domain of operation and its underlying data distribution are\nsubject to change. Adverse weather conditions, in particular, can significantly\ndecrease model performance when such data are not available during\ntraining.Additionally, when a model is incrementally adapted to a new domain,\nit suffers from catastrophic forgetting, causing a significant drop in\nperformance on previously observed domains. Despite recent progress in reducing\ncatastrophic forgetting, its causes and effects remain obscure. Therefore, we\nstudy how the representations of semantic segmentation models are affected\nduring domain-incremental learning in adverse weather conditions. Our\nexperiments and representational analyses indicate that catastrophic forgetting\nis primarily caused by changes to low-level features in domain-incremental\nlearning and that learning more general features on the source domain using\npre-training and image augmentations leads to efficient feature reuse in\nsubsequent tasks, which drastically reduces catastrophic forgetting. These\nfindings highlight the importance of methods that facilitate generalized\nfeatures for effective continual learning algorithms.\n","authors":["Tobias Kalb","Jürgen Beyerer"],"pdf_url":"https://arxiv.org/pdf/2303.14115v1.pdf","comment":"Accepted at IEEE/CVF Conference on Computer Vision and Pattern\n  Recognition 2023"},{"id":"http://arxiv.org/abs/2303.14114v1","updated":"2023-03-24T16:22:06Z","published":"2023-03-24T16:22:06Z","title":"Object Motion Sensitivity: A Bio-inspired Solution to the Ego-motion\n  Problem for Event-based Cameras","summary":"  Neuromorphic (event-based) image sensors draw inspiration from the\nhuman-retina to create an electronic device that can process visual stimuli in\na way that closely resembles its biological counterpart. These sensors process\ninformation significantly different than the traditional RGB sensors.\nSpecifically, the sensory information generated by event-based image sensors\nare orders of magnitude sparser compared to that of RGB sensors. The first\ngeneration of neuromorphic image sensors, Dynamic Vision Sensor (DVS), are\ninspired by the computations confined to the photoreceptors and the first\nretinal synapse. In this work, we highlight the capability of the second\ngeneration of neuromorphic image sensors, Integrated Retinal Functionality in\nCMOS Image Sensors (IRIS), which aims to mimic full retinal computations from\nphotoreceptors to output of the retina (retinal ganglion cells) for targeted\nfeature-extraction. The feature of choice in this work is Object Motion\nSensitivity (OMS) that is processed locally in the IRIS sensor. We study the\ncapability of OMS in solving the ego-motion problem of the event-based cameras.\nOur results show that OMS can accomplish standard computer vision tasks with\nsimilar efficiency to conventional RGB and DVS solutions but offers drastic\nbandwidth reduction. This cuts the wireless and computing power budgets and\nopens up vast opportunities in high-speed, robust, energy-efficient, and\nlow-bandwidth real-time decision making.\n","authors":["Shay Snyder","Hunter Thompson","Md Abdullah-Al Kaiser","Gregory Schwartz","Akhilesh Jaiswal","Maryam Parsa"],"pdf_url":"https://arxiv.org/pdf/2303.14114v1.pdf","comment":"This document is 9 pages and has 6 figures, tables, and algorithms"},{"id":"http://arxiv.org/abs/2303.14109v1","updated":"2023-03-24T16:18:22Z","published":"2023-03-24T16:18:22Z","title":"Prediction of the morphological evolution of a splashing drop using an\n  encoder-decoder","summary":"  The impact of a drop on a solid surface is an important phenomenon that has\nvarious implications and applications. However, the multiphase nature of this\nphenomenon causes complications in the prediction of its morphological\nevolution, especially when the drop splashes. While most machine-learning-based\ndrop-impact studies have centred around physical parameters, this study used a\ncomputer-vision strategy by training an encoder-decoder to predict the drop\nmorphologies using image data. Herein, we show that this trained\nencoder-decoder is able to successfully generate videos that show the\nmorphologies of splashing and non-splashing drops. Remarkably, in each frame of\nthese generated videos, the spreading diameter of the drop was found to be in\ngood agreement with that of the actual videos. Moreover, there was also a high\naccuracy in splashing/non-splashing prediction. These findings demonstrate the\nability of the trained encoder-decoder to generate videos that can accurately\nrepresent the drop morphologies. This approach provides a faster and cheaper\nalternative to experimental and numerical studies.\n","authors":["Jingzu Yee","Daichi Igarashi","Shun Miyatake","Yoshiyuki Tagawa"],"pdf_url":"https://arxiv.org/pdf/2303.14109v1.pdf","comment":"25 pages, 14 figures"},{"id":"http://arxiv.org/abs/2303.14096v1","updated":"2023-03-24T16:03:21Z","published":"2023-03-24T16:03:21Z","title":"Enhancing Multiple Reliability Measures via Nuisance-extended\n  Information Bottleneck","summary":"  In practical scenarios where training data is limited, many predictive\nsignals in the data can be rather from some biases in data acquisition (i.e.,\nless generalizable), so that one cannot prevent a model from co-adapting on\nsuch (so-called) \"shortcut\" signals: this makes the model fragile in various\ndistribution shifts. To bypass such failure modes, we consider an adversarial\nthreat model under a mutual information constraint to cover a wider class of\nperturbations in training. This motivates us to extend the standard information\nbottleneck to additionally model the nuisance information. We propose an\nautoencoder-based training to implement the objective, as well as practical\nencoder designs to facilitate the proposed hybrid discriminative-generative\ntraining concerning both convolutional- and Transformer-based architectures.\nOur experimental results show that the proposed scheme improves robustness of\nlearned representations (remarkably without using any domain-specific\nknowledge), with respect to multiple challenging reliability measures. For\nexample, our model could advance the state-of-the-art on a recent challenging\nOBJECTS benchmark in novelty detection by $78.4\\% \\rightarrow 87.2\\%$ in AUROC,\nwhile simultaneously enjoying improved corruption, background and (certified)\nadversarial robustness. Code is available at\nhttps://github.com/jh-jeong/nuisance_ib.\n","authors":["Jongheon Jeong","Sihyun Yu","Hankook Lee","Jinwoo Shin"],"pdf_url":"https://arxiv.org/pdf/2303.14096v1.pdf","comment":"25 pages; CVPR 2023"},{"id":"http://arxiv.org/abs/2211.12380v2","updated":"2023-03-24T16:01:24Z","published":"2022-11-22T16:23:12Z","title":"OCTET: Object-aware Counterfactual Explanations","summary":"  Nowadays, deep vision models are being widely deployed in safety-critical\napplications, e.g., autonomous driving, and explainability of such models is\nbecoming a pressing concern. Among explanation methods, counterfactual\nexplanations aim to find minimal and interpretable changes to the input image\nthat would also change the output of the model to be explained. Such\nexplanations point end-users at the main factors that impact the decision of\nthe model. However, previous methods struggle to explain decision models\ntrained on images with many objects, e.g., urban scenes, which are more\ndifficult to work with but also arguably more critical to explain. In this\nwork, we propose to tackle this issue with an object-centric framework for\ncounterfactual explanation generation. Our method, inspired by recent\ngenerative modeling works, encodes the query image into a latent space that is\nstructured in a way to ease object-level manipulations. Doing so, it provides\nthe end-user with control over which search directions (e.g., spatial\ndisplacement of objects, style modification, etc.) are to be explored during\nthe counterfactual generation. We conduct a set of experiments on\ncounterfactual explanation benchmarks for driving scenes, and we show that our\nmethod can be adapted beyond classification, e.g., to explain semantic\nsegmentation models. To complete our analysis, we design and run a user study\nthat measures the usefulness of counterfactual explanations in understanding a\ndecision model. Code is available at https://github.com/valeoai/OCTET.\n","authors":["Mehdi Zemni","Mickaël Chen","Éloi Zablocki","Hédi Ben-Younes","Patrick Pérez","Matthieu Cord"],"pdf_url":"https://arxiv.org/pdf/2211.12380v2.pdf","comment":"CVPR 2023"},{"id":"http://arxiv.org/abs/2303.14095v1","updated":"2023-03-24T16:00:01Z","published":"2023-03-24T16:00:01Z","title":"PanoVPR: Towards Unified Perspective-to-Equirectangular Visual Place\n  Recognition via Sliding Windows across the Panoramic View","summary":"  Visual place recognition has received increasing attention in recent years as\na key technology in autonomous driving and robotics. The current mainstream\napproaches use either the perspective view retrieval perspective view (P2P)\nparadigm or the equirectangular image retrieval equirectangular image (E2E)\nparadigm. However, a natural and practical idea is that users only have\nconsumer-grade pinhole cameras to obtain query perspective images and retrieve\nthem in panoramic database images from map providers. To this end, we propose\nPanoVPR, a sliding-window-based perspective-to-equirectangular (P2E) visual\nplace recognition framework, which eliminates feature truncation caused by hard\ncropping by sliding windows over the whole equirectangular image and computing\nand comparing feature descriptors between windows. In addition, this unified\nframework allows for directly transferring the network structure used in\nperspective-to-perspective (P2P) methods without modification. To facilitate\ntraining and evaluation, we derive the pitts250k-P2E dataset from the pitts250k\nand achieve promising results, and we also establish a P2E dataset in a\nreal-world scenario by a mobile robot platform, which we refer to YQ360. Code\nand datasets will be made available at https://github.com/zafirshi/PanoVPR.\n","authors":["Ze Shi","Hao Shi","Kailun Yang","Zhe Yin","Yining Lin","Kaiwei Wang"],"pdf_url":"https://arxiv.org/pdf/2303.14095v1.pdf","comment":"Code and datasets will be made available at\n  https://github.com/zafirshi/PanoVPR"},{"id":"http://arxiv.org/abs/2303.14092v1","updated":"2023-03-24T15:57:39Z","published":"2023-03-24T15:57:39Z","title":"NeuFace: Realistic 3D Neural Face Rendering from Multi-view Images","summary":"  Realistic face rendering from multi-view images is beneficial to various\ncomputer vision and graphics applications. Due to the complex spatially-varying\nreflectance properties and geometry characteristics of faces, however, it\nremains challenging to recover 3D facial representations both faithfully and\nefficiently in the current studies. This paper presents a novel 3D face\nrendering model, namely NeuFace, to learn accurate and physically-meaningful\nunderlying 3D representations by neural rendering techniques. It naturally\nincorporates the neural BRDFs into physically based rendering, capturing\nsophisticated facial geometry and appearance clues in a collaborative manner.\nSpecifically, we introduce an approximated BRDF integration and a simple yet\nnew low-rank prior, which effectively lower the ambiguities and boost the\nperformance of the facial BRDFs. Extensive experiments demonstrate the\nsuperiority of NeuFace in human face rendering, along with a decent\ngeneralization ability to common objects.\n","authors":["Mingwu Zheng","Haiyu Zhang","Hongyu Yang","Di Huang"],"pdf_url":"https://arxiv.org/pdf/2303.14092v1.pdf","comment":"Accepted to CVPR 2023, code is released at\n  https://github.com/aejion/NeuFace"},{"id":"http://arxiv.org/abs/2301.05709v2","updated":"2023-03-24T15:53:21Z","published":"2023-01-12T19:58:54Z","title":"Self-Supervised Image-to-Point Distillation via Semantically Tolerant\n  Contrastive Loss","summary":"  An effective framework for learning 3D representations for perception tasks\nis distilling rich self-supervised image features via contrastive learning.\nHowever, image-to point representation learning for autonomous driving datasets\nfaces two main challenges: 1) the abundance of self-similarity, which results\nin the contrastive losses pushing away semantically similar point and image\nregions and thus disturbing the local semantic structure of the learned\nrepresentations, and 2) severe class imbalance as pretraining gets dominated by\nover-represented classes. We propose to alleviate the self-similarity problem\nthrough a novel semantically tolerant image-to-point contrastive loss that\ntakes into consideration the semantic distance between positive and negative\nimage regions to minimize contrasting semantically similar point and image\nregions. Additionally, we address class imbalance by designing a class-agnostic\nbalanced loss that approximates the degree of class imbalance through an\naggregate sample-to-samples semantic similarity measure. We demonstrate that\nour semantically-tolerant contrastive loss with class balancing improves\nstate-of-the art 2D-to-3D representation learning in all evaluation settings on\n3D semantic segmentation. Our method consistently outperforms state-of-the-art\n2D-to-3D representation learning frameworks across a wide range of 2D\nself-supervised pretrained models.\n","authors":["Anas Mahmoud","Jordan S. K. Hu","Tianshu Kuai","Ali Harakeh","Liam Paull","Steven L. Waslander"],"pdf_url":"https://arxiv.org/pdf/2301.05709v2.pdf","comment":"Accepted in CVPR 2023"},{"id":"http://arxiv.org/abs/2303.14089v1","updated":"2023-03-24T15:52:42Z","published":"2023-03-24T15:52:42Z","title":"Optimizing the Procedure of CT Segmentation Labeling","summary":"  In Computed Tomography, machine learning is often used for automated data\nprocessing. However, increasing model complexity is accompanied by increasingly\nlarge volume datasets, which in turn increases the cost of model training.\nUnlike most work that mitigates this by advancing model architectures and\ntraining algorithms, we consider the annotation procedure and its effect on the\nmodel performance. We assume three main virtues of a good dataset collected for\na model training to be label quality, diversity, and completeness. We compare\nthe effects of those virtues on the model performance using open medical CT\ndatasets and conclude, that quality is more important than diversity early\nduring labeling; the diversity, in turn, is more important than completeness.\nBased on this conclusion and additional experiments, we propose a labeling\nprocedure for the segmentation of tomographic images to minimize efforts spent\non labeling while maximizing the model performance.\n","authors":["Yaroslav Zharov","Tilo Baumbach","Vincent Heuveline"],"pdf_url":"https://arxiv.org/pdf/2303.14089v1.pdf","comment":"Under review"},{"id":"http://arxiv.org/abs/2303.14087v1","updated":"2023-03-24T15:52:20Z","published":"2023-03-24T15:52:20Z","title":"OPDMulti: Openable Part Detection for Multiple Objects","summary":"  Openable part detection is the task of detecting the openable parts of an\nobject in a single-view image, and predicting corresponding motion parameters.\nPrior work investigated the unrealistic setting where all input images only\ncontain a single openable object. We generalize this task to scenes with\nmultiple objects each potentially possessing openable parts, and create a\ncorresponding dataset based on real-world scenes. We then address this more\nchallenging scenario with OPDFormer: a part-aware transformer architecture. Our\nexperiments show that the OPDFormer architecture significantly outperforms\nprior work. The more realistic multiple-object scenarios we investigated remain\nchallenging for all methods, indicating opportunities for future work.\n","authors":["Xiaohao Sun","Hanxiao Jiang","Manolis Savva","Angel Xuan Chang"],"pdf_url":"https://arxiv.org/pdf/2303.14087v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.14081v1","updated":"2023-03-24T15:46:10Z","published":"2023-03-24T15:46:10Z","title":"CoLa-Diff: Conditional Latent Diffusion Model for Multi-Modal MRI\n  Synthesis","summary":"  MRI synthesis promises to mitigate the challenge of missing MRI modality in\nclinical practice. Diffusion model has emerged as an effective technique for\nimage synthesis by modelling complex and variable data distributions. However,\nmost diffusion-based MRI synthesis models are using a single modality. As they\noperate in the original image domain, they are memory-intensive and less\nfeasible for multi-modal synthesis. Moreover, they often fail to preserve the\nanatomical structure in MRI. Further, balancing the multiple conditions from\nmulti-modal MRI inputs is crucial for multi-modal synthesis. Here, we propose\nthe first diffusion-based multi-modality MRI synthesis model, namely\nConditioned Latent Diffusion Model (CoLa-Diff). To reduce memory consumption,\nwe design CoLa-Diff to operate in the latent space. We propose a novel network\narchitecture, e.g., similar cooperative filtering, to solve the possible\ncompression and noise in latent space. To better maintain the anatomical\nstructure, brain region masks are introduced as the priors of density\ndistributions to guide diffusion process. We further present auto-weight\nadaptation to employ multi-modal information effectively. Our experiments\ndemonstrate that CoLa-Diff outperforms other state-of-the-art MRI synthesis\nmethods, promising to serve as an effective tool for multi-modal MRI synthesis.\n","authors":["Lan Jiang","Ye Mao","Xi Chen","Xiangfeng Wang","Chao Li"],"pdf_url":"https://arxiv.org/pdf/2303.14081v1.pdf","comment":"8 pages"},{"id":"http://arxiv.org/abs/2303.14080v1","updated":"2023-03-24T15:44:42Z","published":"2023-03-24T15:44:42Z","title":"Best of Both Worlds: Multimodal Contrastive Learning with Tabular and\n  Imaging Data","summary":"  Medical datasets and especially biobanks, often contain extensive tabular\ndata with rich clinical information in addition to images. In practice,\nclinicians typically have less data, both in terms of diversity and scale, but\nstill wish to deploy deep learning solutions. Combined with increasing medical\ndataset sizes and expensive annotation costs, the necessity for unsupervised\nmethods that can pretrain multimodally and predict unimodally has risen.\n  To address these needs, we propose the first self-supervised contrastive\nlearning framework that takes advantage of images and tabular data to train\nunimodal encoders. Our solution combines SimCLR and SCARF, two leading\ncontrastive learning strategies, and is simple and effective. In our\nexperiments, we demonstrate the strength of our framework by predicting risks\nof myocardial infarction and coronary artery disease (CAD) using cardiac MR\nimages and 120 clinical features from 40,000 UK Biobank subjects. Furthermore,\nwe show the generalizability of our approach to natural images using the DVM\ncar advertisement dataset.\n  We take advantage of the high interpretability of tabular data and through\nattribution and ablation experiments find that morphometric tabular features,\ndescribing size and shape, have outsized importance during the contrastive\nlearning process and improve the quality of the learned embeddings. Finally, we\nintroduce a novel form of supervised contrastive learning, label as a feature\n(LaaF), by appending the ground truth label as a tabular feature during\nmultimodal pretraining, outperforming all supervised contrastive baselines.\n","authors":["Paul Hager","Martin J. Menten","Daniel Rueckert"],"pdf_url":"https://arxiv.org/pdf/2303.14080v1.pdf","comment":"Accepted in CVPR 2023"},{"id":"http://arxiv.org/abs/2303.14078v1","updated":"2023-03-24T15:42:54Z","published":"2023-03-24T15:42:54Z","title":"DistractFlow: Improving Optical Flow Estimation via Realistic\n  Distractions and Pseudo-Labeling","summary":"  We propose a novel data augmentation approach, DistractFlow, for training\noptical flow estimation models by introducing realistic distractions to the\ninput frames. Based on a mixing ratio, we combine one of the frames in the pair\nwith a distractor image depicting a similar domain, which allows for inducing\nvisual perturbations congruent with natural objects and scenes. We refer to\nsuch pairs as distracted pairs. Our intuition is that using semantically\nmeaningful distractors enables the model to learn related variations and attain\nrobustness against challenging deviations, compared to conventional\naugmentation schemes focusing only on low-level aspects and modifications. More\nspecifically, in addition to the supervised loss computed between the estimated\nflow for the original pair and its ground-truth flow, we include a second\nsupervised loss defined between the distracted pair's flow and the original\npair's ground-truth flow, weighted with the same mixing ratio. Furthermore,\nwhen unlabeled data is available, we extend our augmentation approach to\nself-supervised settings through pseudo-labeling and cross-consistency\nregularization. Given an original pair and its distracted version, we enforce\nthe estimated flow on the distracted pair to agree with the flow of the\noriginal pair. Our approach allows increasing the number of available training\npairs significantly without requiring additional annotations. It is agnostic to\nthe model architecture and can be applied to training any optical flow\nestimation models. Our extensive evaluations on multiple benchmarks, including\nSintel, KITTI, and SlowFlow, show that DistractFlow improves existing models\nconsistently, outperforming the latest state of the art.\n","authors":["Jisoo Jeong","Hong Cai","Risheek Garrepalli","Fatih Porikli"],"pdf_url":"https://arxiv.org/pdf/2303.14078v1.pdf","comment":"CVPR 2023"},{"id":"http://arxiv.org/abs/2303.14077v1","updated":"2023-03-24T15:41:40Z","published":"2023-03-24T15:41:40Z","title":"Improved Adversarial Training Through Adaptive Instance-wise Loss\n  Smoothing","summary":"  Deep neural networks can be easily fooled into making incorrect predictions\nthrough corruption of the input by adversarial perturbations:\nhuman-imperceptible artificial noise. So far adversarial training has been the\nmost successful defense against such adversarial attacks. This work focuses on\nimproving adversarial training to boost adversarial robustness. We first\nanalyze, from an instance-wise perspective, how adversarial vulnerability\nevolves during adversarial training. We find that during training an overall\nreduction of adversarial loss is achieved by sacrificing a considerable\nproportion of training samples to be more vulnerable to adversarial attack,\nwhich results in an uneven distribution of adversarial vulnerability among\ndata. Such \"uneven vulnerability\", is prevalent across several popular robust\ntraining methods and, more importantly, relates to overfitting in adversarial\ntraining. Motivated by this observation, we propose a new adversarial training\nmethod: Instance-adaptive Smoothness Enhanced Adversarial Training (ISEAT). It\njointly smooths both input and weight loss landscapes in an adaptive,\ninstance-specific, way to enhance robustness more for those samples with higher\nadversarial vulnerability. Extensive experiments demonstrate the superiority of\nour method over existing defense methods. Noticeably, our method, when combined\nwith the latest data augmentation and semi-supervised learning techniques,\nachieves state-of-the-art robustness against $\\ell_{\\infty}$-norm constrained\nattacks on CIFAR10 of 59.32% for Wide ResNet34-10 without extra data, and\n61.55% for Wide ResNet28-10 with extra data. Code is available at\nhttps://github.com/TreeLLi/Instance-adaptive-Smoothness-Enhanced-AT.\n","authors":["Lin Li","Michael Spratling"],"pdf_url":"https://arxiv.org/pdf/2303.14077v1.pdf","comment":"12 pages, work in submission"},{"id":"http://arxiv.org/abs/2303.14075v1","updated":"2023-03-24T15:38:58Z","published":"2023-03-24T15:38:58Z","title":"Search By Image: Deeply Exploring Beneficial Features for Beauty Product\n  Retrieval","summary":"  Searching by image is popular yet still challenging due to the extensive\ninterference arose from i) data variations (e.g., background, pose, visual\nangle, brightness) of real-world captured images and ii) similar images in the\nquery dataset. This paper studies a practically meaningful problem of beauty\nproduct retrieval (BPR) by neural networks. We broadly extract different types\nof image features, and raise an intriguing question that whether these features\nare beneficial to i) suppress data variations of real-world captured images,\nand ii) distinguish one image from others which look very similar but are\nintrinsically different beauty products in the dataset, therefore leading to an\nenhanced capability of BPR. To answer it, we present a novel variable-attention\nneural network to understand the combination of multiple features (termed\nVM-Net) of beauty product images. Considering that there are few publicly\nreleased training datasets for BPR, we establish a new dataset with more than\none million images classified into more than 20K categories to improve both the\ngeneralization and anti-interference abilities of VM-Net and other methods. We\nverify the performance of VM-Net and its competitors on the benchmark dataset\nPerfect-500K, where VM-Net shows clear improvements over the competitors in\nterms of MAP@7. The source code and dataset will be released upon publication.\n","authors":["Mingqiang Wei","Qian Sun","Haoran Xie","Dong Liang","Fu Lee Wang"],"pdf_url":"https://arxiv.org/pdf/2303.14075v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2212.01322v2","updated":"2023-03-24T15:26:35Z","published":"2022-12-02T17:29:32Z","title":"MIC: Masked Image Consistency for Context-Enhanced Domain Adaptation","summary":"  In unsupervised domain adaptation (UDA), a model trained on source data (e.g.\nsynthetic) is adapted to target data (e.g. real-world) without access to target\nannotation. Most previous UDA methods struggle with classes that have a similar\nvisual appearance on the target domain as no ground truth is available to learn\nthe slight appearance differences. To address this problem, we propose a Masked\nImage Consistency (MIC) module to enhance UDA by learning spatial context\nrelations of the target domain as additional clues for robust visual\nrecognition. MIC enforces the consistency between predictions of masked target\nimages, where random patches are withheld, and pseudo-labels that are generated\nbased on the complete image by an exponential moving average teacher. To\nminimize the consistency loss, the network has to learn to infer the\npredictions of the masked regions from their context. Due to its simple and\nuniversal concept, MIC can be integrated into various UDA methods across\ndifferent visual recognition tasks such as image classification, semantic\nsegmentation, and object detection. MIC significantly improves the\nstate-of-the-art performance across the different recognition tasks for\nsynthetic-to-real, day-to-nighttime, and clear-to-adverse-weather UDA. For\ninstance, MIC achieves an unprecedented UDA performance of 75.9 mIoU and 92.8%\non GTA-to-Cityscapes and VisDA-2017, respectively, which corresponds to an\nimprovement of +2.1 and +3.0 percent points over the previous state of the art.\nThe implementation is available at https://github.com/lhoyer/MIC.\n","authors":["Lukas Hoyer","Dengxin Dai","Haoran Wang","Luc Van Gool"],"pdf_url":"https://arxiv.org/pdf/2212.01322v2.pdf","comment":"CVPR 2023"},{"id":"http://arxiv.org/abs/2211.08834v2","updated":"2023-03-24T15:26:13Z","published":"2022-11-16T11:17:19Z","title":"A Generalized Framework for Video Instance Segmentation","summary":"  The handling of long videos with complex and occluded sequences has recently\nemerged as a new challenge in the video instance segmentation (VIS) community.\nHowever, existing methods have limitations in addressing this challenge. We\nargue that the biggest bottleneck in current approaches is the discrepancy\nbetween training and inference. To effectively bridge this gap, we propose a\nGeneralized framework for VIS, namely GenVIS, that achieves state-of-the-art\nperformance on challenging benchmarks without designing complicated\narchitectures or requiring extra post-processing. The key contribution of\nGenVIS is the learning strategy, which includes a query-based training pipeline\nfor sequential learning with a novel target label assignment. Additionally, we\nintroduce a memory that effectively acquires information from previous states.\nThanks to the new perspective, which focuses on building relationships between\nseparate frames or clips, GenVIS can be flexibly executed in both online and\nsemi-online manner. We evaluate our approach on popular VIS benchmarks,\nachieving state-of-the-art results on YouTube-VIS 2019/2021/2022 and Occluded\nVIS (OVIS). Notably, we greatly outperform the state-of-the-art on the long VIS\nbenchmark (OVIS), improving 5.6 AP with ResNet-50 backbone. Code is available\nat https://github.com/miranheo/GenVIS.\n","authors":["Miran Heo","Sukjun Hwang","Jeongseok Hyun","Hanjung Kim","Seoung Wug Oh","Joon-Young Lee","Seon Joo Kim"],"pdf_url":"https://arxiv.org/pdf/2211.08834v2.pdf","comment":"CVPR 2023"},{"id":"http://arxiv.org/abs/2301.11431v2","updated":"2023-03-24T14:53:54Z","published":"2023-01-26T21:31:32Z","title":"Semidefinite Relaxations for Robust Multiview Triangulation","summary":"  We propose an approach based on convex relaxations for certifiably optimal\nrobust multiview triangulation. To this end, we extend existing relaxation\napproaches to non-robust multiview triangulation by incorporating a least\nsquares cost function. We propose two formulations, one based on epipolar\nconstraints and one based on fractional reprojection constraints. The first is\nlower dimensional and remains tight under moderate noise and outlier levels,\nwhile the second is higher dimensional and therefore slower but remains tight\neven under extreme noise and outlier levels. We demonstrate through extensive\nexperiments that the proposed approaches allow us to compute provably optimal\nreconstructions even under significant noise and a large percentage of\noutliers.\n","authors":["Linus Härenstam-Nielsen","Niclas Zeller","Daniel Cremers"],"pdf_url":"https://arxiv.org/pdf/2301.11431v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.14044v1","updated":"2023-03-24T14:51:46Z","published":"2023-03-24T14:51:46Z","title":"MusicFace: Music-driven Expressive Singing Face Synthesis","summary":"  It is still an interesting and challenging problem to synthesize a vivid and\nrealistic singing face driven by music signal. In this paper, we present a\nmethod for this task with natural motions of the lip, facial expression, head\npose, and eye states. Due to the coupling of the mixed information of human\nvoice and background music in common signals of music audio, we design a\ndecouple-and-fuse strategy to tackle the challenge. We first decompose the\ninput music audio into human voice stream and background music stream. Due to\nthe implicit and complicated correlation between the two-stream input signals\nand the dynamics of the facial expressions, head motions and eye states, we\nmodel their relationship with an attention scheme, where the effects of the two\nstreams are fused seamlessly. Furthermore, to improve the expressiveness of the\ngenerated results, we propose to decompose head movements generation into speed\ngeneration and direction generation, and decompose eye states generation into\nthe short-time eye blinking generation and the long-time eye closing generation\nto model them separately. We also build a novel SingingFace Dataset to support\nthe training and evaluation of this task, and to facilitate future works on\nthis topic. Extensive experiments and user study show that our proposed method\nis capable of synthesizing vivid singing face, which is better than\nstate-of-the-art methods qualitatively and quantitatively.\n","authors":["Pengfei Liu","Wenjin Deng","Hengda Li","Jintai Wang","Yinglin Zheng","Yiwei Ding","Xiaohu Guo","Ming Zeng"],"pdf_url":"https://arxiv.org/pdf/2303.14044v1.pdf","comment":"Accepted to CVMJ"},{"id":"http://arxiv.org/abs/2303.14042v1","updated":"2023-03-24T14:51:20Z","published":"2023-03-24T14:51:20Z","title":"Class-Incremental Exemplar Compression for Class-Incremental Learning","summary":"  Exemplar-based class-incremental learning (CIL) finetunes the model with all\nsamples of new classes but few-shot exemplars of old classes in each\nincremental phase, where the \"few-shot\" abides by the limited memory budget. In\nthis paper, we break this \"few-shot\" limit based on a simple yet surprisingly\neffective idea: compressing exemplars by downsampling non-discriminative pixels\nand saving \"many-shot\" compressed exemplars in the memory. Without needing any\nmanual annotation, we achieve this compression by generating 0-1 masks on\ndiscriminative pixels from class activation maps (CAM). We propose an adaptive\nmask generation model called class-incremental masking (CIM) to explicitly\nresolve two difficulties of using CAM: 1) transforming the heatmaps of CAM to\n0-1 masks with an arbitrary threshold leads to a trade-off between the coverage\non discriminative pixels and the quantity of exemplars, as the total memory is\nfixed; and 2) optimal thresholds vary for different object classes, which is\nparticularly obvious in the dynamic environment of CIL. We optimize the CIM\nmodel alternatively with the conventional CIL model through a bilevel\noptimization problem. We conduct extensive experiments on high-resolution CIL\nbenchmarks including Food-101, ImageNet-100, and ImageNet-1000, and show that\nusing the compressed exemplars by CIM can achieve a new state-of-the-art CIL\naccuracy, e.g., 4.8 percentage points higher than FOSTER on 10-Phase\nImageNet-1000. Our code is available at https://github.com/xfflzl/CIM-CIL.\n","authors":["Zilin Luo","Yaoyao Liu","Bernt Schiele","Qianru Sun"],"pdf_url":"https://arxiv.org/pdf/2303.14042v1.pdf","comment":"Accepted to CVPR 2023"},{"id":"http://arxiv.org/abs/2303.14038v1","updated":"2023-03-24T14:49:22Z","published":"2023-03-24T14:49:22Z","title":"Accelerating Vision-Language Pretraining with Free Language Modeling","summary":"  The state of the arts in vision-language pretraining (VLP) achieves exemplary\nperformance but suffers from high training costs resulting from slow\nconvergence and long training time, especially on large-scale web datasets. An\nessential obstacle to training efficiency lies in the entangled prediction rate\n(percentage of tokens for reconstruction) and corruption rate (percentage of\ncorrupted tokens) in masked language modeling (MLM), that is, a proper\ncorruption rate is achieved at the cost of a large portion of output tokens\nbeing excluded from prediction loss. To accelerate the convergence of VLP, we\npropose a new pretraining task, namely, free language modeling (FLM), that\nenables a 100% prediction rate with arbitrary corruption rates. FLM\nsuccessfully frees the prediction rate from the tie-up with the corruption rate\nwhile allowing the corruption spans to be customized for each token to be\npredicted. FLM-trained models are encouraged to learn better and faster given\nthe same GPU time by exploiting bidirectional contexts more flexibly. Extensive\nexperiments show FLM could achieve an impressive 2.5x pretraining time\nreduction in comparison to the MLM-based methods, while keeping competitive\nperformance on both vision-language understanding and generation tasks. Code\nwill be public at https://github.com/TencentARC/FLM.\n","authors":["Teng Wang","Yixiao Ge","Feng Zheng","Ran Cheng","Ying Shan","Xiaohu Qie","Ping Luo"],"pdf_url":"https://arxiv.org/pdf/2303.14038v1.pdf","comment":"To appear in CVPR 2023"},{"id":"http://arxiv.org/abs/2211.13874v2","updated":"2023-03-24T14:44:50Z","published":"2022-11-25T03:21:05Z","title":"FFHQ-UV: Normalized Facial UV-Texture Dataset for 3D Face Reconstruction","summary":"  We present a large-scale facial UV-texture dataset that contains over 50,000\nhigh-quality texture UV-maps with even illuminations, neutral expressions, and\ncleaned facial regions, which are desired characteristics for rendering\nrealistic 3D face models under different lighting conditions. The dataset is\nderived from a large-scale face image dataset namely FFHQ, with the help of our\nfully automatic and robust UV-texture production pipeline. Our pipeline\nutilizes the recent advances in StyleGAN-based facial image editing approaches\nto generate multi-view normalized face images from single-image inputs. An\nelaborated UV-texture extraction, correction, and completion procedure is then\napplied to produce high-quality UV-maps from the normalized face images.\nCompared with existing UV-texture datasets, our dataset has more diverse and\nhigher-quality texture maps. We further train a GAN-based texture decoder as\nthe nonlinear texture basis for parametric fitting based 3D face\nreconstruction. Experiments show that our method improves the reconstruction\naccuracy over state-of-the-art approaches, and more importantly, produces\nhigh-quality texture maps that are ready for realistic renderings. The dataset,\ncode, and pre-trained texture decoder are publicly available at\nhttps://github.com/csbhr/FFHQ-UV.\n","authors":["Haoran Bai","Di Kang","Haoxian Zhang","Jinshan Pan","Linchao Bao"],"pdf_url":"https://arxiv.org/pdf/2211.13874v2.pdf","comment":"The dataset, code, and pre-trained texture decoder are publicly\n  available at https://github.com/csbhr/FFHQ-UV"},{"id":"http://arxiv.org/abs/2303.14027v1","updated":"2023-03-24T14:37:07Z","published":"2023-03-24T14:37:07Z","title":"Poincaré ResNet","summary":"  This paper introduces an end-to-end residual network that operates entirely\non the Poincar\\'e ball model of hyperbolic space. Hyperbolic learning has\nrecently shown great potential for visual understanding, but is currently only\nperformed in the penultimate layer(s) of deep networks. All visual\nrepresentations are still learned through standard Euclidean networks. In this\npaper we investigate how to learn hyperbolic representations of visual data\ndirectly from the pixel-level. We propose Poincar\\'e ResNet, a hyperbolic\ncounterpart of the celebrated residual network, starting from Poincar\\'e 2D\nconvolutions up to Poincar\\'e residual connections. We identify three\nroadblocks for training convolutional networks entirely in hyperbolic space and\npropose a solution for each: (i) Current hyperbolic network initializations\ncollapse to the origin, limiting their applicability in deeper networks. We\nprovide an identity-based initialization that preserves norms over many layers.\n(ii) Residual networks rely heavily on batch normalization, which comes with\nexpensive Fr\\'echet mean calculations in hyperbolic space. We introduce\nPoincar\\'e midpoint batch normalization as a faster and equally effective\nalternative. (iii) Due to the many intermediate operations in Poincar\\'e\nlayers, we lastly find that the computation graphs of deep learning libraries\nblow up, limiting our ability to train on deep hyperbolic networks. We provide\nmanual backward derivations of core hyperbolic operations to maintain\nmanageable computation graphs.\n","authors":["Max van Spengler","Erwin Berkhout","Pascal Mettes"],"pdf_url":"https://arxiv.org/pdf/2303.14027v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.14017v1","updated":"2023-03-24T14:18:40Z","published":"2023-03-24T14:18:40Z","title":"CF-Font: Content Fusion for Few-shot Font Generation","summary":"  Content and style disentanglement is an effective way to achieve few-shot\nfont generation. It allows to transfer the style of the font image in a source\ndomain to the style defined with a few reference images in a target domain.\nHowever, the content feature extracted using a representative font might not be\noptimal. In light of this, we propose a content fusion module (CFM) to project\nthe content feature into a linear space defined by the content features of\nbasis fonts, which can take the variation of content features caused by\ndifferent fonts into consideration. Our method also allows to optimize the\nstyle representation vector of reference images through a lightweight iterative\nstyle-vector refinement (ISR) strategy. Moreover, we treat the 1D projection of\na character image as a probability distribution and leverage the distance\nbetween two distributions as the reconstruction loss (namely projected\ncharacter loss, PCL). Compared to L2 or L1 reconstruction loss, the\ndistribution distance pays more attention to the global shape of characters. We\nhave evaluated our method on a dataset of 300 fonts with 6.5k characters each.\nExperimental results verify that our method outperforms existing\nstate-of-the-art few-shot font generation methods by a large margin. The source\ncode can be found at https://github.com/wangchi95/CF-Font.\n","authors":["Chi Wang","Min Zhou","Tiezheng Ge","Yuning Jiang","Hujun Bao","Weiwei Xu"],"pdf_url":"https://arxiv.org/pdf/2303.14017v1.pdf","comment":"Accepted by CVPR 2023"},{"id":"http://arxiv.org/abs/2303.14005v1","updated":"2023-03-24T13:59:58Z","published":"2023-03-24T13:59:58Z","title":"Category Query Learning for Human-Object Interaction Classification","summary":"  Unlike most previous HOI methods that focus on learning better human-object\nfeatures, we propose a novel and complementary approach called category query\nlearning. Such queries are explicitly associated to interaction categories,\nconverted to image specific category representation via a transformer decoder,\nand learnt via an auxiliary image-level classification task. This idea is\nmotivated by an earlier multi-label image classification method, but is for the\nfirst time applied for the challenging human-object interaction classification\ntask. Our method is simple, general and effective. It is validated on three\nrepresentative HOI baselines and achieves new state-of-the-art results on two\nbenchmarks.\n","authors":["Chi Xie","Fangao Zeng","Yue Hu","Shuang Liang","Yichen Wei"],"pdf_url":"https://arxiv.org/pdf/2303.14005v1.pdf","comment":"Accepted by CVPR 2023"},{"id":"http://arxiv.org/abs/2303.14004v1","updated":"2023-03-24T13:59:48Z","published":"2023-03-24T13:59:48Z","title":"Vulnerability of Face Morphing Attacks: A Case Study on Lookalike and\n  Identical Twins","summary":"  Face morphing attacks have emerged as a potential threat, particularly in\nautomatic border control scenarios. Morphing attacks permit more than one\nindividual to use travel documents that can be used to cross borders using\nautomatic border control gates. The potential for morphing attacks depends on\nthe selection of data subjects (accomplice and malicious actors). This work\ninvestigates lookalike and identical twins as the source of face morphing\ngeneration. We present a systematic study on benchmarking the vulnerability of\nFace Recognition Systems (FRS) to lookalike and identical twin morphing images.\nTherefore, we constructed new face morphing datasets using 16 pairs of\nidentical twin and lookalike data subjects. Morphing images from lookalike and\nidentical twins are generated using a landmark-based method. Extensive\nexperiments are carried out to benchmark the attack potential of lookalike and\nidentical twins. Furthermore, experiments are designed to provide insights into\nthe impact of vulnerability with normal face morphing compared with lookalike\nand identical twin face morphing.\n","authors":["Raghavendra Ramachandra","Sushma Venkatesh","Gaurav Jaswal","Guoqiang Li"],"pdf_url":"https://arxiv.org/pdf/2303.14004v1.pdf","comment":"Accepted in IWBF 2023"},{"id":"http://arxiv.org/abs/2303.14001v1","updated":"2023-03-24T13:56:45Z","published":"2023-03-24T13:56:45Z","title":"Grid-guided Neural Radiance Fields for Large Urban Scenes","summary":"  Purely MLP-based neural radiance fields (NeRF-based methods) often suffer\nfrom underfitting with blurred renderings on large-scale scenes due to limited\nmodel capacity. Recent approaches propose to geographically divide the scene\nand adopt multiple sub-NeRFs to model each region individually, leading to\nlinear scale-up in training costs and the number of sub-NeRFs as the scene\nexpands. An alternative solution is to use a feature grid representation, which\nis computationally efficient and can naturally scale to a large scene with\nincreased grid resolutions. However, the feature grid tends to be less\nconstrained and often reaches suboptimal solutions, producing noisy artifacts\nin renderings, especially in regions with complex geometry and texture. In this\nwork, we present a new framework that realizes high-fidelity rendering on large\nurban scenes while being computationally efficient. We propose to use a compact\nmultiresolution ground feature plane representation to coarsely capture the\nscene, and complement it with positional encoding inputs through another NeRF\nbranch for rendering in a joint learning fashion. We show that such an\nintegration can utilize the advantages of two alternative solutions: a\nlight-weighted NeRF is sufficient, under the guidance of the feature grid\nrepresentation, to render photorealistic novel views with fine details; and the\njointly optimized ground feature planes, can meanwhile gain further\nrefinements, forming a more accurate and compact feature space and output much\nmore natural rendering results.\n","authors":["Linning Xu","Yuanbo Xiangli","Sida Peng","Xingang Pan","Nanxuan Zhao","Christian Theobalt","Bo Dai","Dahua Lin"],"pdf_url":"https://arxiv.org/pdf/2303.14001v1.pdf","comment":"CVPR2023, Project page at https://city-super.github.io/gridnerf/"},{"id":"http://arxiv.org/abs/2303.13995v1","updated":"2023-03-24T13:49:05Z","published":"2023-03-24T13:49:05Z","title":"LINe: Out-of-Distribution Detection by Leveraging Important Neurons","summary":"  It is important to quantify the uncertainty of input samples, especially in\nmission-critical domains such as autonomous driving and healthcare, where\nfailure predictions on out-of-distribution (OOD) data are likely to cause big\nproblems. OOD detection problem fundamentally begins in that the model cannot\nexpress what it is not aware of. Post-hoc OOD detection approaches are widely\nexplored because they do not require an additional re-training process which\nmight degrade the model's performance and increase the training cost. In this\nstudy, from the perspective of neurons in the deep layer of the model\nrepresenting high-level features, we introduce a new aspect for analyzing the\ndifference in model outputs between in-distribution data and OOD data. We\npropose a novel method, Leveraging Important Neurons (LINe), for post-hoc Out\nof distribution detection.\n  Shapley value-based pruning reduces the effects of noisy outputs by selecting\nonly high-contribution neurons for predicting specific classes of input data\nand masking the rest. Activation clipping fixes all values above a certain\nthreshold into the same value, allowing LINe to treat all the class-specific\nfeatures equally and just consider the difference between the number of\nactivated feature differences between in-distribution and OOD data.\nComprehensive experiments verify the effectiveness of the proposed method by\noutperforming state-of-the-art post-hoc OOD detection methods on CIFAR-10,\nCIFAR-100, and ImageNet datasets.\n","authors":["Yong Hyun Ahn","Gyeong-Moon Park","Seong Tae Kim"],"pdf_url":"https://arxiv.org/pdf/2303.13995v1.pdf","comment":"IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)\n  2023"},{"id":"http://arxiv.org/abs/2302.11963v2","updated":"2023-03-24T13:40:27Z","published":"2023-02-23T12:23:35Z","title":"Investigating Catastrophic Overfitting in Fast Adversarial Training: A\n  Self-fitting Perspective","summary":"  Although fast adversarial training provides an efficient approach for\nbuilding robust networks, it may suffer from a serious problem known as\ncatastrophic overfitting (CO), where multi-step robust accuracy suddenly\ncollapses to zero. In this paper, we for the first time decouple single-step\nadversarial examples into data-information and self-information, which reveals\nan interesting phenomenon called \"self-fitting\". Self-fitting, i.e., the\nnetwork learns the self-information embedded in single-step perturbations,\nnaturally leads to the occurrence of CO. When self-fitting occurs, the network\nexperiences an obvious \"channel differentiation\" phenomenon that some\nconvolution channels accounting for recognizing self-information become\ndominant, while others for data-information are suppressed. In this way, the\nnetwork can only recognize images with sufficient self-information and loses\ngeneralization ability to other types of data. Based on self-fitting, we\nprovide new insights into the existing methods to mitigate CO and extend CO to\nmulti-step adversarial training. Our findings reveal a self-learning mechanism\nin adversarial training and open up new perspectives for suppressing different\nkinds of information to mitigate CO.\n","authors":["Zhengbao He","Tao Li","Sizhe Chen","Xiaolin Huang"],"pdf_url":"https://arxiv.org/pdf/2302.11963v2.pdf","comment":"Comment: The camera-ready version (accepted at CVPR Workshop of\n  Adversarial Machine Learning on Computer Vision: Art of Robustness, 2023)"},{"id":"http://arxiv.org/abs/2303.13959v1","updated":"2023-03-24T12:33:44Z","published":"2023-03-24T12:33:44Z","title":"StereoScene: BEV-Assisted Stereo Matching Empowers 3D Semantic Scene\n  Completion","summary":"  3D semantic scene completion (SSC) is an ill-posed task that requires\ninferring a dense 3D scene from incomplete observations. Previous methods\neither explicitly incorporate 3D geometric input or rely on learnt 3D prior\nbehind monocular RGB images. However, 3D sensors such as LiDAR are expensive\nand intrusive while monocular cameras face challenges in modeling precise\ngeometry due to the inherent ambiguity. In this work, we propose StereoScene\nfor 3D Semantic Scene Completion (SSC), which explores taking full advantage of\nlight-weight camera inputs without resorting to any external 3D sensors. Our\nkey insight is to leverage stereo matching to resolve geometric ambiguity. To\nimprove its robustness in unmatched areas, we introduce bird's-eye-view (BEV)\nrepresentation to inspire hallucination ability with rich context information.\nOn top of the stereo and BEV representations, a mutual interactive aggregation\n(MIA) module is carefully devised to fully unleash their power. Specifically, a\nBi-directional Interaction Transformer (BIT) augmented with confidence\nre-weighting is used to encourage reliable prediction through mutual guidance\nwhile a Dual Volume Aggregation (DVA) module is designed to facilitate\ncomplementary aggregation. Experimental results on SemanticKITTI demonstrate\nthat the proposed StereoScene outperforms the state-of-the-art camera-based\nmethods by a large margin with a relative improvement of 26.9% in geometry and\n38.6% in semantic.\n","authors":["Bohan Li","Yasheng Sun","Xin Jin","Wenjun Zeng","Zheng Zhu","Xiaoefeng Wang","Yunpeng Zhang","James Okae","Hang Xiao","Dalong Du"],"pdf_url":"https://arxiv.org/pdf/2303.13959v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.13957v1","updated":"2023-03-24T12:31:48Z","published":"2023-03-24T12:31:48Z","title":"Factorizers for Distributed Sparse Block Codes","summary":"  Distributed sparse block codes (SBCs) exhibit compact representations for\nencoding and manipulating symbolic data structures using fixed-with vectors.\nOne major challenge however is to disentangle, or factorize, such data\nstructures into their constituent elements without having to search through all\npossible combinations. This factorization becomes more challenging when queried\nby noisy SBCs wherein symbol representations are relaxed due to perceptual\nuncertainty and approximations made when modern neural networks are used to\ngenerate the query vectors. To address these challenges, we first propose a\nfast and highly accurate method for factorizing a more flexible and hence\ngeneralized form of SBCs, dubbed GSBCs. Our iterative factorizer introduces a\nthreshold-based nonlinear activation, a conditional random sampling, and an\n$\\ell_\\infty$-based similarity metric. Its random sampling mechanism in\ncombination with the search in superposition allows to analytically determine\nthe expected number of decoding iterations, which matches the empirical\nobservations up to the GSBC's bundling capacity. Secondly, the proposed\nfactorizer maintains its high accuracy when queried by noisy product vectors\ngenerated using deep convolutional neural networks (CNNs). This facilitates its\napplication in replacing the large fully connected layer (FCL) in CNNs, whereby\nC trainable class vectors, or attribute combinations, can be implicitly\nrepresented by our factorizer having F-factor codebooks, each with\n$\\sqrt[\\leftroot{-2}\\uproot{2}F]{C}$ fixed codevectors. We provide a\nmethodology to flexibly integrate our factorizer in the classification layer of\nCNNs with a novel loss function. We demonstrate the feasibility of our method\non four deep CNN architectures over CIFAR-100, ImageNet-1K, and RAVEN datasets.\nIn all use cases, the number of parameters and operations are significantly\nreduced compared to the FCL.\n","authors":["Michael Hersche","Aleksandar Terzic","Geethan Karunaratne","Jovin Langenegger","Angéline Pouget","Giovanni Cherubini","Luca Benini","Abu Sebastian","Abbas Rahimi"],"pdf_url":"https://arxiv.org/pdf/2303.13957v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.09728v2","updated":"2023-03-24T12:22:55Z","published":"2023-03-17T02:01:11Z","title":"The Cascaded Forward Algorithm for Neural Network Training","summary":"  Backpropagation algorithm has been widely used as a mainstream learning\nprocedure for neural networks in the past decade, and has played a significant\nrole in the development of deep learning. However, there exist some limitations\nassociated with this algorithm, such as getting stuck in local minima and\nexperiencing vanishing/exploding gradients, which have led to questions about\nits biological plausibility. To address these limitations, alternative\nalgorithms to backpropagation have been preliminarily explored, with the\nForward-Forward (FF) algorithm being one of the most well-known. In this paper\nwe propose a new learning framework for neural networks, namely Cascaded\nForward (CaFo) algorithm, which does not rely on BP optimization as that in FF.\nUnlike FF, our framework directly outputs label distributions at each cascaded\nblock, which does not require generation of additional negative samples and\nthus leads to a more efficient process at both training and testing. Moreover,\nin our framework each block can be trained independently, so it can be easily\ndeployed into parallel acceleration systems. The proposed method is evaluated\non four public image classification benchmarks, and the experimental results\nillustrate significant improvement in prediction accuracy in comparison with\nthe baseline.\n","authors":["Gongpei Zhao","Tao Wang","Yidong Li","Yi Jin","Congyan Lang","Haibin Ling"],"pdf_url":"https://arxiv.org/pdf/2303.09728v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.13955v1","updated":"2023-03-24T12:22:34Z","published":"2023-03-24T12:22:34Z","title":"PIAT: Parameter Interpolation based Adversarial Training for Image\n  Classification","summary":"  Adversarial training has been demonstrated to be the most effective approach\nto defend against adversarial attacks. However, existing adversarial training\nmethods show apparent oscillations and overfitting issue in the training\nprocess, degrading the defense efficacy. In this work, we propose a novel\nframework, termed Parameter Interpolation based Adversarial Training (PIAT),\nthat makes full use of the historical information during training.\nSpecifically, at the end of each epoch, PIAT tunes the model parameters as the\ninterpolation of the parameters of the previous and current epochs. Besides, we\nsuggest to use the Normalized Mean Square Error (NMSE) to further improve the\nrobustness by aligning the clean and adversarial examples. Compared with other\nregularization methods, NMSE focuses more on the relative magnitude of the\nlogits rather than the absolute magnitude. Extensive experiments on several\nbenchmark datasets and various networks show that our method could prominently\nimprove the model robustness and reduce the generalization error. Moreover, our\nframework is general and could further boost the robust accuracy when combined\nwith other adversarial training methods.\n","authors":["Kun He","Xin Liu","Yichen Yang","Zhou Qin","Weigao Wen","Hui Xue","John E. Hopcroft"],"pdf_url":"https://arxiv.org/pdf/2303.13955v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2301.05175v2","updated":"2023-03-24T12:21:23Z","published":"2023-01-12T18:01:28Z","title":"Scene-Aware 3D Multi-Human Motion Capture from a Single Camera","summary":"  In this work, we consider the problem of estimating the 3D position of\nmultiple humans in a scene as well as their body shape and articulation from a\nsingle RGB video recorded with a static camera. In contrast to expensive\nmarker-based or multi-view systems, our lightweight setup is ideal for private\nusers as it enables an affordable 3D motion capture that is easy to install and\ndoes not require expert knowledge. To deal with this challenging setting, we\nleverage recent advances in computer vision using large-scale pre-trained\nmodels for a variety of modalities, including 2D body joints, joint angles,\nnormalized disparity maps, and human segmentation masks. Thus, we introduce the\nfirst non-linear optimization-based approach that jointly solves for the\nabsolute 3D position of each human, their articulated pose, their individual\nshapes as well as the scale of the scene. In particular, we estimate the scene\ndepth and person unique scale from normalized disparity predictions using the\n2D body joints and joint angles. Given the per-frame scene depth, we\nreconstruct a point-cloud of the static scene in 3D space. Finally, given the\nper-frame 3D estimates of the humans and scene point-cloud, we perform a\nspace-time coherent optimization over the video to ensure temporal, spatial and\nphysical plausibility. We evaluate our method on established multi-person 3D\nhuman pose benchmarks where we consistently outperform previous methods and we\nqualitatively demonstrate that our method is robust to in-the-wild conditions\nincluding challenging scenes with people of different sizes.\n","authors":["Diogo Luvizon","Marc Habermann","Vladislav Golyanik","Adam Kortylewski","Christian Theobalt"],"pdf_url":"https://arxiv.org/pdf/2301.05175v2.pdf","comment":"Accepted to Eurographics 2023. See also github:\n  https://github.com/dluvizon/scene-aware-3d-multi-human project page:\n  https://github.com/dluvizon/scene-aware-3d-multi-human"},{"id":"http://arxiv.org/abs/2303.13953v1","updated":"2023-03-24T12:18:10Z","published":"2023-03-24T12:18:10Z","title":"AssetField: Assets Mining and Reconfiguration in Ground Feature Plane\n  Representation","summary":"  Both indoor and outdoor environments are inherently structured and\nrepetitive. Traditional modeling pipelines keep an asset library storing unique\nobject templates, which is both versatile and memory efficient in practice.\nInspired by this observation, we propose AssetField, a novel neural scene\nrepresentation that learns a set of object-aware ground feature planes to\nrepresent the scene, where an asset library storing template feature patches\ncan be constructed in an unsupervised manner. Unlike existing methods which\nrequire object masks to query spatial points for object editing, our ground\nfeature plane representation offers a natural visualization of the scene in the\nbird-eye view, allowing a variety of operations (e.g. translation, duplication,\ndeformation) on objects to configure a new scene. With the template feature\npatches, group editing is enabled for scenes with many recurring items to avoid\nrepetitive work on object individuals. We show that AssetField not only\nachieves competitive performance for novel-view synthesis but also generates\nrealistic renderings for new scene configurations.\n","authors":["Yuanbo Xiangli","Linning Xu","Xingang Pan","Nanxuan Zhao","Bo Dai","Dahua Lin"],"pdf_url":"https://arxiv.org/pdf/2303.13953v1.pdf","comment":"Project page can be found in https://city-super.github.io/assetfield/"},{"id":"http://arxiv.org/abs/2303.13952v1","updated":"2023-03-24T12:14:54Z","published":"2023-03-24T12:14:54Z","title":"CCL: Continual Contrastive Learning for LiDAR Place Recognition","summary":"  Place recognition is an essential and challenging task in loop closing and\nglobal localization for robotics and autonomous driving applications.\nBenefiting from the recent advances in deep learning techniques, the\nperformance of LiDAR place recognition (LPR) has been greatly improved.\nHowever, current deep learning-based methods suffer from two major problems:\npoor generalization ability and catastrophic forgetting. In this paper, we\npropose a continual contrastive learning method, named CCL, to tackle the\ncatastrophic forgetting problem and generally improve the robustness of LPR\napproaches. Our CCL constructs a contrastive feature pool and utilizes\ncontrastive loss to train more transferable representations of places. When\ntransferred into new environments, our CCL continuously reviews the contrastive\nmemory bank and applies a distribution-based knowledge distillation to maintain\nthe retrieval ability of the past data while continually learning to recognize\nnew places from the new data. We thoroughly evaluate our approach on Oxford,\nMulRan, and PNV datasets using three different LPR methods. The experimental\nresults show that our CCL consistently improves the performance of different\nmethods in different environments outperforming the state-of-the-art continual\nlearning method. The implementation of our method has been released at\nhttps://github.com/cloudcjf/CCL.\n","authors":["Jiafeng Cui","Xieyuanli Chen"],"pdf_url":"https://arxiv.org/pdf/2303.13952v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2212.09478v2","updated":"2023-03-24T12:13:52Z","published":"2022-12-19T14:11:52Z","title":"MM-Diffusion: Learning Multi-Modal Diffusion Models for Joint Audio and\n  Video Generation","summary":"  We propose the first joint audio-video generation framework that brings\nengaging watching and listening experiences simultaneously, towards\nhigh-quality realistic videos. To generate joint audio-video pairs, we propose\na novel Multi-Modal Diffusion model (i.e., MM-Diffusion), with two-coupled\ndenoising autoencoders. In contrast to existing single-modal diffusion models,\nMM-Diffusion consists of a sequential multi-modal U-Net for a joint denoising\nprocess by design. Two subnets for audio and video learn to gradually generate\naligned audio-video pairs from Gaussian noises. To ensure semantic consistency\nacross modalities, we propose a novel random-shift based attention block\nbridging over the two subnets, which enables efficient cross-modal alignment,\nand thus reinforces the audio-video fidelity for each other. Extensive\nexperiments show superior results in unconditional audio-video generation, and\nzero-shot conditional tasks (e.g., video-to-audio). In particular, we achieve\nthe best FVD and FAD on Landscape and AIST++ dancing datasets. Turing tests of\n10k votes further demonstrate dominant preferences for our model. The code and\npre-trained models can be downloaded at\nhttps://github.com/researchmm/MM-Diffusion.\n","authors":["Ludan Ruan","Yiyang Ma","Huan Yang","Huiguo He","Bei Liu","Jianlong Fu","Nicholas Jing Yuan","Qin Jin","Baining Guo"],"pdf_url":"https://arxiv.org/pdf/2212.09478v2.pdf","comment":"Accepted by CVPR 2023"},{"id":"http://arxiv.org/abs/2211.13969v2","updated":"2023-03-24T12:11:49Z","published":"2022-11-25T09:31:41Z","title":"Unsupervised Continual Semantic Adaptation through Neural Rendering","summary":"  An increasing amount of applications rely on data-driven models that are\ndeployed for perception tasks across a sequence of scenes. Due to the mismatch\nbetween training and deployment data, adapting the model on the new scenes is\noften crucial to obtain good performance. In this work, we study continual\nmulti-scene adaptation for the task of semantic segmentation, assuming that no\nground-truth labels are available during deployment and that performance on the\nprevious scenes should be maintained. We propose training a Semantic-NeRF\nnetwork for each scene by fusing the predictions of a segmentation model and\nthen using the view-consistent rendered semantic labels as pseudo-labels to\nadapt the model. Through joint training with the segmentation model, the\nSemantic-NeRF model effectively enables 2D-3D knowledge transfer. Furthermore,\ndue to its compact size, it can be stored in a long-term memory and\nsubsequently used to render data from arbitrary viewpoints to reduce\nforgetting. We evaluate our approach on ScanNet, where we outperform both a\nvoxel-based baseline and a state-of-the-art unsupervised domain adaptation\nmethod.\n","authors":["Zhizheng Liu","Francesco Milano","Jonas Frey","Roland Siegwart","Hermann Blum","Cesar Cadena"],"pdf_url":"https://arxiv.org/pdf/2211.13969v2.pdf","comment":"Accepted by the IEEE/CVF Conference on Computer Vision and Pattern\n  Recognition (CVPR) 2023. Zhizheng Liu and Francesco Milano share first\n  authorship. Hermann Blum and Cesar Cadena share senior authorship. 18 pages,\n  8 figures, 9 tables"},{"id":"http://arxiv.org/abs/2303.13933v1","updated":"2023-03-24T11:42:45Z","published":"2023-03-24T11:42:45Z","title":"DisC-Diff: Disentangled Conditional Diffusion Model for Multi-Contrast\n  MRI Super-Resolution","summary":"  Multi-contrast magnetic resonance imaging (MRI) is the most common management\ntool used to characterize neurological disorders based on brain tissue\ncontrasts. However, acquiring high-resolution MRI scans is time-consuming and\ninfeasible under specific conditions. Hence, multi-contrast super-resolution\nmethods have been developed to improve the quality of low-resolution contrasts\nby leveraging complementary information from multi-contrast MRI. Current deep\nlearning-based super-resolution methods have limitations in estimating\nrestoration uncertainty and avoiding mode collapse. Although the diffusion\nmodel has emerged as a promising approach for image enhancement, capturing\ncomplex interactions between multiple conditions introduced by multi-contrast\nMRI super-resolution remains a challenge for clinical applications. In this\npaper, we propose a disentangled conditional diffusion model, DisC-Diff, for\nmulti-contrast brain MRI super-resolution. It utilizes the sampling-based\ngeneration and simple objective function of diffusion models to estimate\nuncertainty in restorations effectively and ensure a stable optimization\nprocess. Moreover, DisC-Diff leverages a disentangled multi-stream network to\nfully exploit complementary information from multi-contrast MRI, improving\nmodel interpretation under multiple conditions of multi-contrast inputs. We\nvalidated the effectiveness of DisC-Diff on two datasets: the IXI dataset,\nwhich contains 578 normal brains, and a clinical dataset with 316 pathological\nbrains. Our experimental results demonstrate that DisC-Diff outperforms other\nstate-of-the-art methods both quantitatively and visually.\n","authors":["Ye Mao","Lan Jiang","Xi Chen","Chao Li"],"pdf_url":"https://arxiv.org/pdf/2303.13933v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.13931v1","updated":"2023-03-24T11:40:50Z","published":"2023-03-24T11:40:50Z","title":"MSdocTr-Lite: A Lite Transformer for Full Page Multi-script Handwriting\n  Recognition","summary":"  The Transformer has quickly become the dominant architecture for various\npattern recognition tasks due to its capacity for long-range representation.\nHowever, transformers are data-hungry models and need large datasets for\ntraining. In Handwritten Text Recognition (HTR), collecting a massive amount of\nlabeled data is a complicated and expensive task. In this paper, we propose a\nlite transformer architecture for full-page multi-script handwriting\nrecognition. The proposed model comes with three advantages: First, to solve\nthe common problem of data scarcity, we propose a lite transformer model that\ncan be trained on a reasonable amount of data, which is the case of most HTR\npublic datasets, without the need for external data. Second, it can learn the\nreading order at page-level thanks to a curriculum learning strategy, allowing\nit to avoid line segmentation errors, exploit a larger context and reduce the\nneed for costly segmentation annotations. Third, it can be easily adapted to\nother scripts by applying a simple transfer-learning process using only\npage-level labeled images. Extensive experiments on different datasets with\ndifferent scripts (French, English, Spanish, and Arabic) show the effectiveness\nof the proposed model.\n","authors":["Marwa Dhiaf","Ahmed Cheikh Rouhou","Yousri Kessentini","Sinda Ben Salem"],"pdf_url":"https://arxiv.org/pdf/2303.13931v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.12930v2","updated":"2023-03-24T11:14:02Z","published":"2023-03-22T22:00:17Z","title":"Dense-Localizing Audio-Visual Events in Untrimmed Videos: A Large-Scale\n  Benchmark and Baseline","summary":"  Existing audio-visual event localization (AVE) handles manually trimmed\nvideos with only a single instance in each of them. However, this setting is\nunrealistic as natural videos often contain numerous audio-visual events with\ndifferent categories. To better adapt to real-life applications, in this paper\nwe focus on the task of dense-localizing audio-visual events, which aims to\njointly localize and recognize all audio-visual events occurring in an\nuntrimmed video. The problem is challenging as it requires fine-grained\naudio-visual scene and context understanding. To tackle this problem, we\nintroduce the first Untrimmed Audio-Visual (UnAV-100) dataset, which contains\n10K untrimmed videos with over 30K audio-visual events. Each video has 2.8\naudio-visual events on average, and the events are usually related to each\nother and might co-occur as in real-life scenes. Next, we formulate the task\nusing a new learning-based framework, which is capable of fully integrating\naudio and visual modalities to localize audio-visual events with various\nlengths and capture dependencies between them in a single pass. Extensive\nexperiments demonstrate the effectiveness of our method as well as the\nsignificance of multi-scale cross-modal perception and dependency modeling for\nthis task.\n","authors":["Tiantian Geng","Teng Wang","Jinming Duan","Runmin Cong","Feng Zheng"],"pdf_url":"https://arxiv.org/pdf/2303.12930v2.pdf","comment":"Accepted by CVPR2023"},{"id":"http://arxiv.org/abs/2303.13918v1","updated":"2023-03-24T11:13:33Z","published":"2023-03-24T11:13:33Z","title":"Removing confounding information from fetal ultrasound images","summary":"  Confounding information in the form of text or markings embedded in medical\nimages can severely affect the training of diagnostic deep learning algorithms.\nHowever, data collected for clinical purposes often have such markings embedded\nin them. In dermatology, known examples include drawings or rulers that are\noverrepresented in images of malignant lesions. In this paper, we encounter\ntext and calipers placed on the images found in national databases containing\nfetal screening ultrasound scans, which correlate with standard planes to be\npredicted. In order to utilize the vast amounts of data available in these\ndatabases, we develop and validate a series of methods for minimizing the\nconfounding effects of embedded text and calipers on deep learning algorithms\ndesigned for ultrasound, using standard plane classification as a test case.\n","authors":["Kamil Mikolaj","Manxi Lin","Zahra Bashir","Morten Bo Søndergaard Svendsen","Martin Tolsgaard","Anders Nymark","Aasa Feragen"],"pdf_url":"https://arxiv.org/pdf/2303.13918v1.pdf","comment":"Fetal ultrasound, confounders, shortcut learning"},{"id":"http://arxiv.org/abs/2303.13917v1","updated":"2023-03-24T11:12:37Z","published":"2023-03-24T11:12:37Z","title":"Convolutional Neural Networks for the classification of glitches in\n  gravitational-wave data streams","summary":"  We investigate the use of Convolutional Neural Networks (including the modern\nConvNeXt network family) to classify transient noise signals (i.e.~glitches)\nand gravitational waves in data from the Advanced LIGO detectors. First, we use\nmodels with a supervised learning approach, both trained from scratch using the\nGravity Spy dataset and employing transfer learning by fine-tuning pre-trained\nmodels in this dataset. Second, we also explore a self-supervised approach,\npre-training models with automatically generated pseudo-labels. Our findings\nare very close to existing results for the same dataset, reaching values for\nthe F1 score of 97.18% (94.15%) for the best supervised (self-supervised)\nmodel. We further test the models using actual gravitational-wave signals from\nLIGO-Virgo's O3 run. Although trained using data from previous runs (O1 and\nO2), the models show good performance, in particular when using transfer\nlearning. We find that transfer learning improves the scores without the need\nfor any training on real signals apart from the less than 50 chirp examples\nfrom hardware injections present in the Gravity Spy dataset. This motivates the\nuse of transfer learning not only for glitch classification but also for signal\nclassification.\n","authors":["Tiago S. Fernandes","Samuel J. Vieira","Antonio Onofre","Juan Calderón Bustillo","Alejandro Torres-Forné","José A. Font"],"pdf_url":"https://arxiv.org/pdf/2303.13917v1.pdf","comment":"15 pages, 14 figures"},{"id":"http://arxiv.org/abs/2303.13916v1","updated":"2023-03-24T11:12:05Z","published":"2023-03-24T11:12:05Z","title":"Self-Supervised Reversed Image Signal Processing via Reference-Guided\n  Dynamic Parameter Selection","summary":"  Unprocessed sensor outputs (RAW images) potentially improve both low-level\nand high-level computer vision algorithms, but the lack of large-scale RAW\nimage datasets is a barrier to research. Thus, reversed Image Signal Processing\n(ISP) which converts existing RGB images into RAW images has been studied.\nHowever, most existing methods require camera-specific metadata or paired RGB\nand RAW images to model the conversion, and they are not always available. In\naddition, there are issues in handling diverse ISPs and recovering global\nillumination. To tackle these limitations, we propose a self-supervised\nreversed ISP method that does not require metadata and paired images. The\nproposed method converts a RGB image into a RAW-like image taken in the same\nenvironment with the same sensor as a reference RAW image by dynamically\nselecting parameters of the reversed ISP pipeline based on the reference RAW\nimage. The parameter selection is trained via pseudo paired data created from\nunpaired RGB and RAW images. We show that the proposed method is able to learn\nvarious reversed ISPs with comparable accuracy to other state-of-the-art\nsupervised methods and convert unknown RGB images from COCO and Flickr1M to\ntarget RAW-like images more accurately in terms of pixel distribution. We also\ndemonstrate that our generated RAW images improve performance on real RAW image\nobject detection task.\n","authors":["Junji Otsuka","Masakazu Yoshimura","Takeshi Ohashi"],"pdf_url":"https://arxiv.org/pdf/2303.13916v1.pdf","comment":"19 pages, 12 figures"},{"id":"http://arxiv.org/abs/2303.13913v1","updated":"2023-03-24T10:59:17Z","published":"2023-03-24T10:59:17Z","title":"GarmentTracking: Category-Level Garment Pose Tracking","summary":"  Garments are important to humans. A visual system that can estimate and track\nthe complete garment pose can be useful for many downstream tasks and\nreal-world applications. In this work, we present a complete package to address\nthe category-level garment pose tracking task: (1) A recording system\nVR-Garment, with which users can manipulate virtual garment models in\nsimulation through a VR interface. (2) A large-scale dataset VR-Folding, with\ncomplex garment pose configurations in manipulation like flattening and\nfolding. (3) An end-to-end online tracking framework GarmentTracking, which\npredicts complete garment pose both in canonical space and task space given a\npoint cloud sequence. Extensive experiments demonstrate that the proposed\nGarmentTracking achieves great performance even when the garment has large\nnon-rigid deformation. It outperforms the baseline approach on both speed and\naccuracy. We hope our proposed solution can serve as a platform for future\nresearch. Codes and datasets are available in\nhttps://garment-tracking.robotflow.ai.\n","authors":["Han Xue","Wenqiang Xu","Jieyi Zhang","Tutian Tang","Yutong Li","Wenxin Du","Ruolin Ye","Cewu Lu"],"pdf_url":"https://arxiv.org/pdf/2303.13913v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2211.11738v2","updated":"2023-03-24T10:51:26Z","published":"2022-11-21T18:57:47Z","title":"SPARF: Neural Radiance Fields from Sparse and Noisy Poses","summary":"  Neural Radiance Field (NeRF) has recently emerged as a powerful\nrepresentation to synthesize photorealistic novel views. While showing\nimpressive performance, it relies on the availability of dense input views with\nhighly accurate camera poses, thus limiting its application in real-world\nscenarios. In this work, we introduce Sparse Pose Adjusting Radiance Field\n(SPARF), to address the challenge of novel-view synthesis given only few\nwide-baseline input images (as low as 3) with noisy camera poses. Our approach\nexploits multi-view geometry constraints in order to jointly learn the NeRF and\nrefine the camera poses. By relying on pixel matches extracted between the\ninput views, our multi-view correspondence objective enforces the optimized\nscene and camera poses to converge to a global and geometrically accurate\nsolution. Our depth consistency loss further encourages the reconstructed scene\nto be consistent from any viewpoint. Our approach sets a new state of the art\nin the sparse-view regime on multiple challenging datasets.\n","authors":["Prune Truong","Marie-Julie Rakotosaona","Fabian Manhardt","Federico Tombari"],"pdf_url":"https://arxiv.org/pdf/2211.11738v2.pdf","comment":"Code will be released soon. Published at CVPR 2023 as a Highlight"},{"id":"http://arxiv.org/abs/2303.13900v1","updated":"2023-03-24T10:19:34Z","published":"2023-03-24T10:19:34Z","title":"A Three-Player GAN for Super-Resolution in Magnetic Resonance Imaging","summary":"  Learning based single image super resolution (SISR) task is well investigated\nin 2D images. However, SISR for 3D Magnetics Resonance Images (MRI) is more\nchallenging compared to 2D, mainly due to the increased number of neural\nnetwork parameters, the larger memory requirement and the limited amount of\navailable training data. Current SISR methods for 3D volumetric images are\nbased on Generative Adversarial Networks (GANs), especially Wasserstein GANs\ndue to their training stability. Other common architectures in the 2D domain,\ne.g. transformer models, require large amounts of training data and are\ntherefore not suitable for the limited 3D data. However, Wasserstein GANs can\nbe problematic because they may not converge to a global optimum and thus\nproduce blurry results. Here, we propose a new method for 3D SR based on the\nGAN framework. Specifically, we use instance noise to balance the GAN training.\nFurthermore, we use a relativistic GAN loss function and an updating feature\nextractor during the training process. We show that our method produces highly\naccurate results. We also show that we need very few training samples. In\nparticular, we need less than 30 samples instead of thousands of training\nsamples that are typically required in previous studies. Finally, we show\nimproved out-of-sample results produced by our model.\n","authors":["Qi Wang","Lucas Mahler","Julius Steiglechner","Florian Birk","Klaus Scheffler","Gabriele Lohmann"],"pdf_url":"https://arxiv.org/pdf/2303.13900v1.pdf","comment":"10 pages, 5 figures"},{"id":"http://arxiv.org/abs/2303.13899v1","updated":"2023-03-24T10:19:14Z","published":"2023-03-24T10:19:14Z","title":"Robust Test-Time Adaptation in Dynamic Scenarios","summary":"  Test-time adaptation (TTA) intends to adapt the pretrained model to test\ndistributions with only unlabeled test data streams. Most of the previous TTA\nmethods have achieved great success on simple test data streams such as\nindependently sampled data from single or multiple distributions. However,\nthese attempts may fail in dynamic scenarios of real-world applications like\nautonomous driving, where the environments gradually change and the test data\nis sampled correlatively over time. In this work, we explore such practical\ntest data streams to deploy the model on the fly, namely practical test-time\nadaptation (PTTA). To do so, we elaborate a Robust Test-Time Adaptation (RoTTA)\nmethod against the complex data stream in PTTA. More specifically, we present a\nrobust batch normalization scheme to estimate the normalization statistics.\nMeanwhile, a memory bank is utilized to sample category-balanced data with\nconsideration of timeliness and uncertainty. Further, to stabilize the training\nprocedure, we develop a time-aware reweighting strategy with a teacher-student\nmodel. Extensive experiments prove that RoTTA enables continual testtime\nadaptation on the correlatively sampled data streams. Our method is easy to\nimplement, making it a good choice for rapid deployment. The code is publicly\navailable at https://github.com/BIT-DA/RoTTA\n","authors":["Longhui Yuan","Binhui Xie","Shuang Li"],"pdf_url":"https://arxiv.org/pdf/2303.13899v1.pdf","comment":"Accepted by CVPR 2023"},{"id":"http://arxiv.org/abs/2303.13898v1","updated":"2023-03-24T10:18:28Z","published":"2023-03-24T10:18:28Z","title":"Remind of the Past: Incremental Learning with Analogical Prompts","summary":"  Although data-free incremental learning methods are memory-friendly,\naccurately estimating and counteracting representation shifts is challenging in\nthe absence of historical data. This paper addresses this thorny problem by\nproposing a novel incremental learning method inspired by human analogy\ncapabilities. Specifically, we design an analogy-making mechanism to remap the\nnew data into the old class by prompt tuning. It mimics the feature\ndistribution of the target old class on the old model using only samples of new\nclasses. The learnt prompts are further used to estimate and counteract the\nrepresentation shift caused by fine-tuning for the historical prototypes. The\nproposed method sets up new state-of-the-art performance on four incremental\nlearning benchmarks under both the class and domain incremental learning\nsettings. It consistently outperforms data-replay methods by only saving\nfeature prototypes for each class. It has almost hit the empirical upper bound\nby joint training on the Core50 benchmark. The code will be released at\n\\url{https://github.com/ZhihengCV/A-Prompts}.\n","authors":["Zhiheng Ma","Xiaopeng Hong","Beinan Liu","Yabin Wang","Pinyue Guo","Huiyun Li"],"pdf_url":"https://arxiv.org/pdf/2303.13898v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.13896v1","updated":"2023-03-24T10:05:22Z","published":"2023-03-24T10:05:22Z","title":"Regularization of polynomial networks for image recognition","summary":"  Deep Neural Networks (DNNs) have obtained impressive performance across\ntasks, however they still remain as black boxes, e.g., hard to theoretically\nanalyze. At the same time, Polynomial Networks (PNs) have emerged as an\nalternative method with a promising performance and improved interpretability\nbut have yet to reach the performance of the powerful DNN baselines. In this\nwork, we aim to close this performance gap. We introduce a class of PNs, which\nare able to reach the performance of ResNet across a range of six benchmarks.\nWe demonstrate that strong regularization is critical and conduct an extensive\nstudy of the exact regularization schemes required to match performance. To\nfurther motivate the regularization schemes, we introduce D-PolyNets that\nachieve a higher-degree of expansion than previously proposed polynomial\nnetworks. D-PolyNets are more parameter-efficient while achieving a similar\nperformance as other polynomial networks. We expect that our new models can\nlead to an understanding of the role of elementwise activation functions (which\nare no longer required for training PNs). The source code is available at\nhttps://github.com/grigorisg9gr/regularized_polynomials.\n","authors":["Grigorios G Chrysos","Bohan Wang","Jiankang Deng","Volkan Cevher"],"pdf_url":"https://arxiv.org/pdf/2303.13896v1.pdf","comment":"Accepted at CVPR'23"},{"id":"http://arxiv.org/abs/2211.14860v2","updated":"2023-03-24T10:01:31Z","published":"2022-11-27T15:29:39Z","title":"Foiling Explanations in Deep Neural Networks","summary":"  Deep neural networks (DNNs) have greatly impacted numerous fields over the\npast decade. Yet despite exhibiting superb performance over many problems,\ntheir black-box nature still poses a significant challenge with respect to\nexplainability. Indeed, explainable artificial intelligence (XAI) is crucial in\nseveral fields, wherein the answer alone -- sans a reasoning of how said answer\nwas derived -- is of little value. This paper uncovers a troubling property of\nexplanation methods for image-based DNNs: by making small visual changes to the\ninput image -- hardly influencing the network's output -- we demonstrate how\nexplanations may be arbitrarily manipulated through the use of evolution\nstrategies. Our novel algorithm, AttaXAI, a model-agnostic, adversarial attack\non XAI algorithms, only requires access to the output logits of a classifier\nand to the explanation map; these weak assumptions render our approach highly\nuseful where real-world models and data are concerned. We compare our method's\nperformance on two benchmark datasets -- CIFAR100 and ImageNet -- using four\ndifferent pretrained deep-learning models: VGG16-CIFAR100, VGG16-ImageNet,\nMobileNet-CIFAR100, and Inception-v3-ImageNet. We find that the XAI methods can\nbe manipulated without the use of gradients or other model internals. Our novel\nalgorithm is successfully able to manipulate an image in a manner imperceptible\nto the human eye, such that the XAI method outputs a specific explanation map.\nTo our knowledge, this is the first such method in a black-box setting, and we\nbelieve it has significant value where explainability is desired, required, or\nlegally mandatory.\n","authors":["Snir Vitrack Tamam","Raz Lapid","Moshe Sipper"],"pdf_url":"https://arxiv.org/pdf/2211.14860v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.13887v1","updated":"2023-03-24T09:54:34Z","published":"2023-03-24T09:54:34Z","title":"Effective black box adversarial attack with handcrafted kernels","summary":"  We propose a new, simple framework for crafting adversarial examples for\nblack box attacks. The idea is to simulate the substitution model with a\nnon-trainable model compounded of just one layer of handcrafted convolutional\nkernels and then train the generator neural network to maximize the distance of\nthe outputs for the original and generated adversarial image. We show that\nfooling the prediction of the first layer causes the whole network to be fooled\nand decreases its accuracy on adversarial inputs. Moreover, we do not train the\nneural network to obtain the first convolutional layer kernels, but we create\nthem using the technique of F-transform. Therefore, our method is very time and\nresource effective.\n","authors":["Petr Dvořáček","Petr Hurtik","Petra Števuliáková"],"pdf_url":"https://arxiv.org/pdf/2303.13887v1.pdf","comment":"12 pages, 5 figures, 3 tables, IWANN conference"},{"id":"http://arxiv.org/abs/2303.13885v1","updated":"2023-03-24T09:51:13Z","published":"2023-03-24T09:51:13Z","title":"ARKitTrack: A New Diverse Dataset for Tracking Using Mobile RGB-D Data","summary":"  Compared with traditional RGB-only visual tracking, few datasets have been\nconstructed for RGB-D tracking. In this paper, we propose ARKitTrack, a new\nRGB-D tracking dataset for both static and dynamic scenes captured by\nconsumer-grade LiDAR scanners equipped on Apple's iPhone and iPad. ARKitTrack\ncontains 300 RGB-D sequences, 455 targets, and 229.7K video frames in total.\nAlong with the bounding box annotations and frame-level attributes, we also\nannotate this dataset with 123.9K pixel-level target masks. Besides, the camera\nintrinsic and camera pose of each frame are provided for future developments.\nTo demonstrate the potential usefulness of this dataset, we further present a\nunified baseline for both box-level and pixel-level tracking, which integrates\nRGB features with bird's-eye-view representations to better explore\ncross-modality 3D geometry. In-depth empirical analysis has verified that the\nARKitTrack dataset can significantly facilitate RGB-D tracking and that the\nproposed baseline method compares favorably against the state of the arts. The\ncode and dataset is available at https://arkittrack.github.io.\n","authors":["Haojie Zhao","Junsong Chen","Lijun Wang","Huchuan Lu"],"pdf_url":"https://arxiv.org/pdf/2303.13885v1.pdf","comment":"Accepted by CVPR2023"},{"id":"http://arxiv.org/abs/2303.13874v1","updated":"2023-03-24T09:32:50Z","published":"2023-03-24T09:32:50Z","title":"Query-Dependent Video Representation for Moment Retrieval and Highlight\n  Detection","summary":"  Recently, video moment retrieval and highlight detection (MR/HD) are being\nspotlighted as the demand for video understanding is drastically increased. The\nkey objective of MR/HD is to localize the moment and estimate clip-wise\naccordance level, i.e., saliency score, to the given text query. Although the\nrecent transformer-based models brought some advances, we found that these\nmethods do not fully exploit the information of a given query. For example, the\nrelevance between text query and video contents is sometimes neglected when\npredicting the moment and its saliency. To tackle this issue, we introduce\nQuery-Dependent DETR (QD-DETR), a detection transformer tailored for MR/HD. As\nwe observe the insignificant role of a given query in transformer\narchitectures, our encoding module starts with cross-attention layers to\nexplicitly inject the context of text query into video representation. Then, to\nenhance the model's capability of exploiting the query information, we\nmanipulate the video-query pairs to produce irrelevant pairs. Such negative\n(irrelevant) video-query pairs are trained to yield low saliency scores, which\nin turn, encourages the model to estimate precise accordance between\nquery-video pairs. Lastly, we present an input-adaptive saliency predictor\nwhich adaptively defines the criterion of saliency scores for the given\nvideo-query pairs. Our extensive studies verify the importance of building the\nquery-dependent representation for MR/HD. Specifically, QD-DETR outperforms\nstate-of-the-art methods on QVHighlights, TVSum, and Charades-STA datasets.\nCodes are available at github.com/wjun0830/QD-DETR.\n","authors":["WonJun Moon","Sangeek Hyun","SangUk Park","Dongchan Park","Jae-Pil Heo"],"pdf_url":"https://arxiv.org/pdf/2303.13874v1.pdf","comment":"Accepted to CVPR 2023. Code is available at\n  https://github.com/wjun0830/QD-DETR"},{"id":"http://arxiv.org/abs/2303.13873v1","updated":"2023-03-24T09:30:09Z","published":"2023-03-24T09:30:09Z","title":"Fantasia3D: Disentangling Geometry and Appearance for High-quality\n  Text-to-3D Content Creation","summary":"  Automatic 3D content creation has achieved rapid progress recently due to the\navailability of pre-trained, large language models and image diffusion models,\nforming the emerging topic of text-to-3D content creation. Existing text-to-3D\nmethods commonly use implicit scene representations, which couple the geometry\nand appearance via volume rendering and are suboptimal in terms of recovering\nfiner geometries and achieving photorealistic rendering; consequently, they are\nless effective for generating high-quality 3D assets. In this work, we propose\na new method of Fantasia3D for high-quality text-to-3D content creation. Key to\nFantasia3D is the disentangled modeling and learning of geometry and\nappearance. For geometry learning, we rely on a hybrid scene representation,\nand propose to encode surface normal extracted from the representation as the\ninput of the image diffusion model. For appearance modeling, we introduce the\nspatially varying bidirectional reflectance distribution function (BRDF) into\nthe text-to-3D task, and learn the surface material for photorealistic\nrendering of the generated surface. Our disentangled framework is more\ncompatible with popular graphics engines, supporting relighting, editing, and\nphysical simulation of the generated 3D assets. We conduct thorough experiments\nthat show the advantages of our method over existing ones under different\ntext-to-3D task settings. Project page and source codes:\nhttps://fantasia3d.github.io/.\n","authors":["Rui Chen","Yongwei Chen","Ningxin Jiao","Kui Jia"],"pdf_url":"https://arxiv.org/pdf/2303.13873v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.13113v2","updated":"2023-03-24T09:23:37Z","published":"2023-03-23T09:00:38Z","title":"Adaptive Regularization for Class-Incremental Learning","summary":"  Class-Incremental Learning updates a deep classifier with new categories\nwhile maintaining the previously observed class accuracy. Regularizing the\nneural network weights is a common method to prevent forgetting previously\nlearned classes while learning novel ones. However, existing regularizers use a\nconstant magnitude throughout the learning sessions, which may not reflect the\nvarying levels of difficulty of the tasks encountered during incremental\nlearning. This study investigates the necessity of adaptive regularization in\nClass-Incremental Learning, which dynamically adjusts the regularization\nstrength according to the complexity of the task at hand. We propose a Bayesian\nOptimization-based approach to automatically determine the optimal\nregularization magnitude for each learning task. Our experiments on two\ndatasets via two regularizers demonstrate the importance of adaptive\nregularization for achieving accurate and less forgetful visual incremental\nlearning.\n","authors":["Elif Ceren Gok Yildirim","Murat Onur Yildirim","Mert Kilickaya","Joaquin Vanschoren"],"pdf_url":"https://arxiv.org/pdf/2303.13113v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2301.04467v2","updated":"2023-03-24T09:18:39Z","published":"2023-01-10T17:51:55Z","title":"FrustumFormer: Adaptive Instance-aware Resampling for Multi-view 3D\n  Detection","summary":"  The transformation of features from 2D perspective space to 3D space is\nessential to multi-view 3D object detection. Recent approaches mainly focus on\nthe design of view transformation, either pixel-wisely lifting perspective view\nfeatures into 3D space with estimated depth or grid-wisely constructing BEV\nfeatures via 3D projection, treating all pixels or grids equally. However,\nchoosing what to transform is also important but has rarely been discussed\nbefore. The pixels of a moving car are more informative than the pixels of the\nsky. To fully utilize the information contained in images, the view\ntransformation should be able to adapt to different image regions according to\ntheir contents. In this paper, we propose a novel framework named\nFrustumFormer, which pays more attention to the features in instance regions\nvia adaptive instance-aware resampling. Specifically, the model obtains\ninstance frustums on the bird's eye view by leveraging image view object\nproposals. An adaptive occupancy mask within the instance frustum is learned to\nrefine the instance location. Moreover, the temporal frustum intersection could\nfurther reduce the localization uncertainty of objects. Comprehensive\nexperiments on the nuScenes dataset demonstrate the effectiveness of\nFrustumFormer, and we achieve a new state-of-the-art performance on the\nbenchmark. Codes and models will be made available at\nhttps://github.com/Robertwyq/Frustum.\n","authors":["Yuqi Wang","Yuntao Chen","Zhaoxiang Zhang"],"pdf_url":"https://arxiv.org/pdf/2301.04467v2.pdf","comment":"Accepted to CVPR 2023"},{"id":"http://arxiv.org/abs/2303.13111v2","updated":"2023-03-24T09:14:05Z","published":"2023-03-23T08:59:09Z","title":"A Permutable Hybrid Network for Volumetric Medical Image Segmentation","summary":"  The advent of Vision Transformer (ViT) has brought substantial advancements\nin 3D volumetric benchmarks, particularly in 3D medical image segmentation.\nConcurrently, Multi-Layer Perceptron (MLP) networks have regained popularity\namong researchers due to their comparable results to ViT, albeit with the\nexclusion of the heavy self-attention module. This paper introduces a\npermutable hybrid network for volumetric medical image segmentation, named\nPHNet, which exploits the advantages of convolution neural network (CNN) and\nMLP. PHNet addresses the intrinsic isotropy problem of 3D volumetric data by\nutilizing both 2D and 3D CNN to extract local information. Besides, we propose\nan efficient Multi-Layer Permute Perceptron module, named MLPP, which enhances\nthe original MLP by obtaining long-range dependence while retaining positional\ninformation. Extensive experimental results validate that PHNet outperforms the\nstate-of-the-art methods on two public datasets, namely, COVID-19-20 and\nSynapse. Moreover, the ablation study demonstrates the effectiveness of PHNet\nin harnessing the strengths of both CNN and MLP. The code will be accessible to\nthe public upon acceptance.\n","authors":["Yi Lin","Xiao Fang","Dong Zhang","Kwang-Ting Cheng","Hao Chen"],"pdf_url":"https://arxiv.org/pdf/2303.13111v2.pdf","comment":"Submitted to MICCAI 2023"},{"id":"http://arxiv.org/abs/2303.13868v1","updated":"2023-03-24T09:11:36Z","published":"2023-03-24T09:11:36Z","title":"Physically Adversarial Infrared Patches with Learnable Shapes and\n  Locations","summary":"  Owing to the extensive application of infrared object detectors in the\nsafety-critical tasks, it is necessary to evaluate their robustness against\nadversarial examples in the real world. However, current few physical infrared\nattacks are complicated to implement in practical application because of their\ncomplex transformation from digital world to physical world. To address this\nissue, in this paper, we propose a physically feasible infrared attack method\ncalled \"adversarial infrared patches\". Considering the imaging mechanism of\ninfrared cameras by capturing objects' thermal radiation, adversarial infrared\npatches conduct attacks by attaching a patch of thermal insulation materials on\nthe target object to manipulate its thermal distribution. To enhance\nadversarial attacks, we present a novel aggregation regularization to guide the\nsimultaneous learning for the patch' shape and location on the target object.\nThus, a simple gradient-based optimization can be adapted to solve for them. We\nverify adversarial infrared patches in different object detection tasks with\nvarious object detectors. Experimental results show that our method achieves\nmore than 90\\% Attack Success Rate (ASR) versus the pedestrian detector and\nvehicle detector in the physical environment, where the objects are captured in\ndifferent angles, distances, postures, and scenes. More importantly,\nadversarial infrared patch is easy to implement, and it only needs 0.5 hours to\nbe constructed in the physical world, which verifies its effectiveness and\nefficiency.\n","authors":["Wei Xingxing","Yu Jie","Huang Yao"],"pdf_url":"https://arxiv.org/pdf/2303.13868v1.pdf","comment":"accepted by CVPR2023"},{"id":"http://arxiv.org/abs/2303.13867v1","updated":"2023-03-24T09:10:14Z","published":"2023-03-24T09:10:14Z","title":"Few Shot Medical Image Segmentation with Cross Attention Transformer","summary":"  Medical image segmentation has made significant progress in recent years.\nDeep learning-based methods are recognized as data-hungry techniques, requiring\nlarge amounts of data with manual annotations. However, manual annotation is\nexpensive in the field of medical image analysis, which requires\ndomain-specific expertise. To address this challenge, few-shot learning has the\npotential to learn new classes from only a few examples. In this work, we\npropose a novel framework for few-shot medical image segmentation, termed\nCAT-Net, based on cross masked attention Transformer. Our proposed network\nmines the correlations between the support image and query image, limiting them\nto focus only on useful foreground information and boosting the representation\ncapacity of both the support prototype and query features. We further design an\niterative refinement framework that refines the query image segmentation\niteratively and promotes the support feature in turn. We validated the proposed\nmethod on three public datasets: Abd-CT, Abd-MRI, and Card-MRI. Experimental\nresults demonstrate the superior performance of our method compared to\nstate-of-the-art methods and the effectiveness of each component. we will\nrelease the source codes of our method upon acceptance.\n","authors":["Yi Lin","Yufan Chen","Kwang-Ting Cheng","Hao Chen"],"pdf_url":"https://arxiv.org/pdf/2303.13867v1.pdf","comment":"Submitted to MICCAI 2023"},{"id":"http://arxiv.org/abs/2303.13862v1","updated":"2023-03-24T08:58:08Z","published":"2023-03-24T08:58:08Z","title":"Two-level Graph Network for Few-Shot Class-Incremental Learning","summary":"  Few-shot class-incremental learning (FSCIL) aims to design machine learning\nalgorithms that can continually learn new concepts from a few data points,\nwithout forgetting knowledge of old classes. The difficulty lies in that\nlimited data from new classes not only lead to significant overfitting issues\nbut also exacerbates the notorious catastrophic forgetting problems. However,\nexisting FSCIL methods ignore the semantic relationships between sample-level\nand class-level. % Using the advantage that graph neural network (GNN) can mine\nrich information among few samples, In this paper, we designed a two-level\ngraph network for FSCIL named Sample-level and Class-level Graph Neural Network\n(SCGN). Specifically, a pseudo incremental learning paradigm is designed in\nSCGN, which synthesizes virtual few-shot tasks as new tasks to optimize SCGN\nmodel parameters in advance. Sample-level graph network uses the relationship\nof a few samples to aggregate similar samples and obtains refined class-level\nfeatures. Class-level graph network aims to mitigate the semantic conflict\nbetween prototype features of new classes and old classes. SCGN builds\ntwo-level graph networks to guarantee the latent semantic of each few-shot\nclass can be effectively represented in FSCIL. Experiments on three popular\nbenchmark datasets show that our method significantly outperforms the baselines\nand sets new state-of-the-art results with remarkable advantages.\n","authors":["Hao Chen","Linyan Li","Fan Lyu","Fuyuan Hu","Zhenping Xia","Fenglei Xu"],"pdf_url":"https://arxiv.org/pdf/2303.13862v1.pdf","comment":"arXiv admin note: text overlap with arXiv:2203.06953 by other authors"},{"id":"http://arxiv.org/abs/2212.09877v3","updated":"2023-03-24T08:56:44Z","published":"2022-12-19T21:57:35Z","title":"LayoutDETR: Detection Transformer Is a Good Multimodal Layout Designer","summary":"  Graphic layout designs play an essential role in visual communication. Yet\nhandcrafting layout designs is skill-demanding, time-consuming, and\nnon-scalable to batch production. Generative models emerge to make design\nautomation scalable but it remains non-trivial to produce designs that comply\nwith designers' multimodal desires, i.e., constrained by background images and\ndriven by foreground content. We propose LayoutDETR that inherits the high\nquality and realism from generative modeling, while reformulating content-aware\nrequirements as a detection problem: we learn to detect in a background image\nthe reasonable locations, scales, and spatial relations for multimodal\nforeground elements in a layout. Our solution sets a new state-of-the-art\nperformance for layout generation on public benchmarks and on our newly-curated\nad banner dataset. We integrate our solution into a graphical system that\nfacilitates user studies, and show that users prefer our designs over baselines\nby significant margins. Our code, models, dataset, graphical system, and demos\nare available at https://github.com/salesforce/LayoutDETR.\n","authors":["Ning Yu","Chia-Chih Chen","Zeyuan Chen","Rui Meng","Gang Wu","Paul Josel","Juan Carlos Niebles","Caiming Xiong","Ran Xu"],"pdf_url":"https://arxiv.org/pdf/2212.09877v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2212.06820v2","updated":"2023-03-24T08:43:48Z","published":"2022-12-13T18:57:33Z","title":"Structured 3D Features for Reconstructing Controllable Avatars","summary":"  We introduce Structured 3D Features, a model based on a novel implicit 3D\nrepresentation that pools pixel-aligned image features onto dense 3D points\nsampled from a parametric, statistical human mesh surface. The 3D points have\nassociated semantics and can move freely in 3D space. This allows for optimal\ncoverage of the person of interest, beyond just the body shape, which in turn,\nadditionally helps modeling accessories, hair, and loose clothing. Owing to\nthis, we present a complete 3D transformer-based attention framework which,\ngiven a single image of a person in an unconstrained pose, generates an\nanimatable 3D reconstruction with albedo and illumination decomposition, as a\nresult of a single end-to-end model, trained semi-supervised, and with no\nadditional postprocessing. We show that our S3F model surpasses the previous\nstate-of-the-art on various tasks, including monocular 3D reconstruction, as\nwell as albedo and shading estimation. Moreover, we show that the proposed\nmethodology allows novel view synthesis, relighting, and re-posing the\nreconstruction, and can naturally be extended to handle multiple input images\n(e.g. different views of a person, or the same view, in different poses, in\nvideo). Finally, we demonstrate the editing capabilities of our model for 3D\nvirtual try-on applications.\n","authors":["Enric Corona","Mihai Zanfir","Thiemo Alldieck","Eduard Gabriel Bazavan","Andrei Zanfir","Cristian Sminchisescu"],"pdf_url":"https://arxiv.org/pdf/2212.06820v2.pdf","comment":"Accepted at CVPR 2023. Project page:\n  https://enriccorona.github.io/s3f/, Video:\n  https://www.youtube.com/watch?v=mcZGcQ6L-2s"},{"id":"http://arxiv.org/abs/2210.15929v3","updated":"2023-03-24T08:43:35Z","published":"2022-10-28T06:20:55Z","title":"Being Comes from Not-being: Open-vocabulary Text-to-Motion Generation\n  with Wordless Training","summary":"  Text-to-motion generation is an emerging and challenging problem, which aims\nto synthesize motion with the same semantics as the input text. However, due to\nthe lack of diverse labeled training data, most approaches either limit to\nspecific types of text annotations or require online optimizations to cater to\nthe texts during inference at the cost of efficiency and stability. In this\npaper, we investigate offline open-vocabulary text-to-motion generation in a\nzero-shot learning manner that neither requires paired training data nor extra\nonline optimization to adapt for unseen texts. Inspired by the prompt learning\nin NLP, we pretrain a motion generator that learns to reconstruct the full\nmotion from the masked motion. During inference, instead of changing the motion\ngenerator, our method reformulates the input text into a masked motion as the\nprompt for the motion generator to ``reconstruct'' the motion. In constructing\nthe prompt, the unmasked poses of the prompt are synthesized by a text-to-pose\ngenerator. To supervise the optimization of the text-to-pose generator, we\npropose the first text-pose alignment model for measuring the alignment between\ntexts and 3D poses. And to prevent the pose generator from overfitting to\nlimited training texts, we further propose a novel wordless training mechanism\nthat optimizes the text-to-pose generator without any training texts. The\ncomprehensive experimental results show that our method obtains a significant\nimprovement against the baseline methods. The code is available at\nhttps://github.com/junfanlin/oohmg.\n","authors":["Junfan Lin","Jianlong Chang","Lingbo Liu","Guanbin Li","Liang Lin","Qi Tian","Chang Wen Chen"],"pdf_url":"https://arxiv.org/pdf/2210.15929v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.11675v2","updated":"2023-03-24T08:41:24Z","published":"2023-03-21T08:36:59Z","title":"BoPR: Body-aware Part Regressor for Human Shape and Pose Estimation","summary":"  This paper presents a novel approach for estimating human body shape and pose\nfrom monocular images that effectively addresses the challenges of occlusions\nand depth ambiguity. Our proposed method BoPR, the Body-aware Part Regressor,\nfirst extracts features of both the body and part regions using an\nattention-guided mechanism. We then utilize these features to encode extra\npart-body dependency for per-part regression, with part features as queries and\nbody feature as a reference. This allows our network to infer the spatial\nrelationship of occluded parts with the body by leveraging visible parts and\nbody reference information. Our method outperforms existing state-of-the-art\nmethods on two benchmark datasets, and our experiments show that it\nsignificantly surpasses existing methods in terms of depth ambiguity and\nocclusion handling. These results provide strong evidence of the effectiveness\nof our approach.The code and data are available for research purposes at\nhttps://github.com/cyk990422/BoPR.\n","authors":["Yongkang Cheng","Shaoli Huang","Jifeng Ning","Ying Shan"],"pdf_url":"https://arxiv.org/pdf/2303.11675v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.12408v2","updated":"2023-03-24T08:36:20Z","published":"2023-03-22T09:17:01Z","title":"Balanced Spherical Grid for Egocentric View Synthesis","summary":"  We present EgoNeRF, a practical solution to reconstruct large-scale\nreal-world environments for VR assets. Given a few seconds of casually captured\n360 video, EgoNeRF can efficiently build neural radiance fields which enable\nhigh-quality rendering from novel viewpoints. Motivated by the recent\nacceleration of NeRF using feature grids, we adopt spherical coordinate instead\nof conventional Cartesian coordinate. Cartesian feature grid is inefficient to\nrepresent large-scale unbounded scenes because it has a spatially uniform\nresolution, regardless of distance from viewers. The spherical parameterization\nbetter aligns with the rays of egocentric images, and yet enables factorization\nfor performance enhancement. However, the na\\\"ive spherical grid suffers from\nirregularities at two poles, and also cannot represent unbounded scenes. To\navoid singularities near poles, we combine two balanced grids, which results in\na quasi-uniform angular grid. We also partition the radial grid exponentially\nand place an environment map at infinity to represent unbounded scenes.\nFurthermore, with our resampling technique for grid-based methods, we can\nincrease the number of valid samples to train NeRF volume. We extensively\nevaluate our method in our newly introduced synthetic and real-world egocentric\n360 video datasets, and it consistently achieves state-of-the-art performance.\n","authors":["Changwoon Choi","Sang Min Kim","Young Min Kim"],"pdf_url":"https://arxiv.org/pdf/2303.12408v2.pdf","comment":"Accepted to CVPR 2023"},{"id":"http://arxiv.org/abs/2203.11009v2","updated":"2023-03-24T08:32:24Z","published":"2022-03-21T14:23:18Z","title":"Continual Spatio-Temporal Graph Convolutional Networks","summary":"  Graph-based reasoning over skeleton data has emerged as a promising approach\nfor human action recognition. However, the application of prior graph-based\nmethods, which predominantly employ whole temporal sequences as their input, to\nthe setting of online inference entails considerable computational redundancy.\nIn this paper, we tackle this issue by reformulating the Spatio-Temporal Graph\nConvolutional Neural Network as a Continual Inference Network, which can\nperform step-by-step predictions in time without repeat frame processing. To\nevaluate our method, we create a continual version of ST-GCN, CoST-GCN,\nalongside two derived methods with different self-attention mechanisms, CoAGCN\nand CoS-TR. We investigate weight transfer strategies and architectural\nmodifications for inference acceleration, and perform experiments on the NTU\nRGB+D 60, NTU RGB+D 120, and Kinetics Skeleton 400 datasets. Retaining similar\npredictive accuracy, we observe up to 109x reduction in time complexity,\non-hardware accelerations of 26x, and reductions in maximum allocated memory of\n52% during online inference.\n","authors":["Lukas Hedegaard","Negar Heidari","Alexandros Iosifidis"],"pdf_url":"https://arxiv.org/pdf/2203.11009v2.pdf","comment":"12 pages, 6 figures, 5 tables"},{"id":"http://arxiv.org/abs/2303.13855v1","updated":"2023-03-24T08:32:00Z","published":"2023-03-24T08:32:00Z","title":"Deformable Model Driven Neural Rendering for High-fidelity 3D\n  Reconstruction of Human Heads Under Low-View Settings","summary":"  We propose a robust method for learning neural implicit functions that can\nreconstruct 3D human heads with high-fidelity geometry from low-view inputs. We\nrepresent 3D human heads as the zero level-set of a composed signed distance\nfield that consists of a smooth template, a non-rigid deformation, and a\nhigh-frequency displacement field. The template represents identity-independent\nand expression-neutral features, which is trained on multiple individuals,\nalong with the deformation network. The displacement field encodes\nidentity-dependent geometric details, trained for each specific individual. We\ntrain our network in two stages using a coarse-to-fine strategy without 3D\nsupervision. Our experiments demonstrate that the geometry decomposition and\ntwo-stage training make our method robust and our model outperforms existing\nmethods in terms of reconstruction accuracy and novel view synthesis under\nlow-view settings. Additionally, the pre-trained template serves a good\ninitialization for our model to adapt to unseen individuals.\n","authors":["Baixin Xu","Jiarui Zhang","Kwan-Yee Lin","Chen Qian","Ying He"],"pdf_url":"https://arxiv.org/pdf/2303.13855v1.pdf","comment":"21 pages. Visit our project page at\n  https://github.com/xubaixinxbx/High-fidelity-3D-Reconstruction-of-Human-Heads"},{"id":"http://arxiv.org/abs/2208.08037v2","updated":"2023-03-24T08:31:19Z","published":"2022-08-17T02:43:23Z","title":"LayoutFormer++: Conditional Graphic Layout Generation via Constraint\n  Serialization and Decoding Space Restriction","summary":"  Conditional graphic layout generation, which generates realistic layouts\naccording to user constraints, is a challenging task that has not been\nwell-studied yet. First, there is limited discussion about how to handle\ndiverse user constraints flexibly and uniformly. Second, to make the layouts\nconform to user constraints, existing work often sacrifices generation quality\nsignificantly. In this work, we propose LayoutFormer++ to tackle the above\nproblems. First, to flexibly handle diverse constraints, we propose a\nconstraint serialization scheme, which represents different user constraints as\nsequences of tokens with a predefined format. Then, we formulate conditional\nlayout generation as a sequence-to-sequence transformation, and leverage\nencoder-decoder framework with Transformer as the basic architecture.\nFurthermore, to make the layout better meet user requirements without harming\nquality, we propose a decoding space restriction strategy. Specifically, we\nprune the predicted distribution by ignoring the options that definitely\nviolate user constraints and likely result in low-quality layouts, and make the\nmodel samples from the restricted distribution. Experiments demonstrate that\nLayoutFormer++ outperforms existing approaches on all the tasks in terms of\nboth better generation quality and less constraint violation.\n","authors":["Zhaoyun Jiang","Jiaqi Guo","Shizhao Sun","Huayu Deng","Zhongkai Wu","Vuksan Mijovic","Zijiang James Yang","Jian-Guang Lou","Dongmei Zhang"],"pdf_url":"https://arxiv.org/pdf/2208.08037v2.pdf","comment":"Accepted by CVPR2023"},{"id":"http://arxiv.org/abs/2303.13853v1","updated":"2023-03-24T08:22:41Z","published":"2023-03-24T08:22:41Z","title":"2PCNet: Two-Phase Consistency Training for Day-to-Night Unsupervised\n  Domain Adaptive Object Detection","summary":"  Object detection at night is a challenging problem due to the absence of\nnight image annotations. Despite several domain adaptation methods, achieving\nhigh-precision results remains an issue. False-positive error propagation is\nstill observed in methods using the well-established student-teacher framework,\nparticularly for small-scale and low-light objects. This paper proposes a\ntwo-phase consistency unsupervised domain adaptation network, 2PCNet, to\naddress these issues. The network employs high-confidence bounding-box\npredictions from the teacher in the first phase and appends them to the\nstudent's region proposals for the teacher to re-evaluate in the second phase,\nresulting in a combination of high and low confidence pseudo-labels. The night\nimages and pseudo-labels are scaled-down before being used as input to the\nstudent, providing stronger small-scale pseudo-labels. To address errors that\narise from low-light regions and other night-related attributes in images, we\npropose a night-specific augmentation pipeline called NightAug. This pipeline\ninvolves applying random augmentations, such as glare, blur, and noise, to\ndaytime images. Experiments on publicly available datasets demonstrate that our\nmethod achieves superior results to state-of-the-art methods by 20\\%, and to\nsupervised models trained directly on the target data.\n","authors":["Mikhail Kennerley","Jian-Gang Wang","Bharadwaj Veeravalli","Robby T. Tan"],"pdf_url":"https://arxiv.org/pdf/2303.13853v1.pdf","comment":"Accepted into CVPR'23"},{"id":"http://arxiv.org/abs/2303.13852v1","updated":"2023-03-24T08:20:16Z","published":"2023-03-24T08:20:16Z","title":"Weakly-supervised Single-view Image Relighting","summary":"  We present a learning-based approach to relight a single image of Lambertian\nand low-frequency specular objects. Our method enables inserting objects from\nphotographs into new scenes and relighting them under the new environment\nlighting, which is essential for AR applications. To relight the object, we\nsolve both inverse rendering and re-rendering. To resolve the ill-posed inverse\nrendering, we propose a weakly-supervised method by a low-rank constraint. To\nfacilitate the weakly-supervised training, we contribute Relit, a large-scale\n(750K images) dataset of videos with aligned objects under changing\nilluminations. For re-rendering, we propose a differentiable specular rendering\nlayer to render low-frequency non-Lambertian materials under various\nilluminations of spherical harmonics. The whole pipeline is end-to-end and\nefficient, allowing for a mobile app implementation of AR object insertion.\nExtensive evaluations demonstrate that our method achieves state-of-the-art\nperformance. Project page: https://renjiaoyi.github.io/relighting/.\n","authors":["Renjiao Yi","Chenyang Zhu","Kai Xu"],"pdf_url":"https://arxiv.org/pdf/2303.13852v1.pdf","comment":"21 pages, with supplementary material"},{"id":"http://arxiv.org/abs/2303.12564v2","updated":"2023-03-24T07:49:32Z","published":"2023-03-22T13:46:15Z","title":"RaBit: Parametric Modeling of 3D Biped Cartoon Characters with a\n  Topological-consistent Dataset","summary":"  Assisting people in efficiently producing visually plausible 3D characters\nhas always been a fundamental research topic in computer vision and computer\ngraphics. Recent learning-based approaches have achieved unprecedented accuracy\nand efficiency in the area of 3D real human digitization. However, none of the\nprior works focus on modeling 3D biped cartoon characters, which are also in\ngreat demand in gaming and filming. In this paper, we introduce 3DBiCar, the\nfirst large-scale dataset of 3D biped cartoon characters, and RaBit, the\ncorresponding parametric model. Our dataset contains 1,500 topologically\nconsistent high-quality 3D textured models which are manually crafted by\nprofessional artists. Built upon the data, RaBit is thus designed with a\nSMPL-like linear blend shape model and a StyleGAN-based neural UV-texture\ngenerator, simultaneously expressing the shape, pose, and texture. To\ndemonstrate the practicality of 3DBiCar and RaBit, various applications are\nconducted, including single-view reconstruction, sketch-based modeling, and 3D\ncartoon animation. For the single-view reconstruction setting, we find a\nstraightforward global mapping from input images to the output UV-based texture\nmaps tends to lose detailed appearances of some local parts (e.g., nose, ears).\nThus, a part-sensitive texture reasoner is adopted to make all important local\nareas perceived. Experiments further demonstrate the effectiveness of our\nmethod both qualitatively and quantitatively. 3DBiCar and RaBit are available\nat gaplab.cuhk.edu.cn/projects/RaBit.\n","authors":["Zhongjin Luo","Shengcai Cai","Jinguo Dong","Ruibo Ming","Liangdong Qiu","Xiaohang Zhan","Xiaoguang Han"],"pdf_url":"https://arxiv.org/pdf/2303.12564v2.pdf","comment":"CVPR 2023, Project page: https://gaplab.cuhk.edu.cn/projects/RaBit/"},{"id":"http://arxiv.org/abs/2303.13846v1","updated":"2023-03-24T07:43:57Z","published":"2023-03-24T07:43:57Z","title":"Feature Separation and Recalibration for Adversarial Robustness","summary":"  Deep neural networks are susceptible to adversarial attacks due to the\naccumulation of perturbations in the feature level, and numerous works have\nboosted model robustness by deactivating the non-robust feature activations\nthat cause model mispredictions. However, we claim that these malicious\nactivations still contain discriminative cues and that with recalibration, they\ncan capture additional useful information for correct model predictions. To\nthis end, we propose a novel, easy-to-plugin approach named Feature Separation\nand Recalibration (FSR) that recalibrates the malicious, non-robust activations\nfor more robust feature maps through Separation and Recalibration. The\nSeparation part disentangles the input feature map into the robust feature with\nactivations that help the model make correct predictions and the non-robust\nfeature with activations that are responsible for model mispredictions upon\nadversarial attack. The Recalibration part then adjusts the non-robust\nactivations to restore the potentially useful cues for model predictions.\nExtensive experiments verify the superiority of FSR compared to traditional\ndeactivation techniques and demonstrate that it improves the robustness of\nexisting adversarial training methods by up to 8.57% with small computational\noverhead. Codes are available at https://github.com/wkim97/FSR.\n","authors":["Woo Jae Kim","Yoonki Cho","Junsik Jung","Sung-Eui Yoon"],"pdf_url":"https://arxiv.org/pdf/2303.13846v1.pdf","comment":"CVPR 2023 (Highlight)"},{"id":"http://arxiv.org/abs/2303.13845v1","updated":"2023-03-24T07:39:08Z","published":"2023-03-24T07:39:08Z","title":"Anomaly Detection under Distribution Shift","summary":"  Anomaly detection (AD) is a crucial machine learning task that aims to learn\npatterns from a set of normal training samples to identify abnormal samples in\ntest data. Most existing AD studies assume that the training and test data are\ndrawn from the same data distribution, but the test data can have large\ndistribution shifts arising in many real-world applications due to different\nnatural variations such as new lighting conditions, object poses, or background\nappearances, rendering existing AD methods ineffective in such cases. In this\npaper, we consider the problem of anomaly detection under distribution shift\nand establish performance benchmarks on three widely-used AD and\nout-of-distribution (OOD) generalization datasets. We demonstrate that simple\nadaptation of state-of-the-art OOD generalization methods to AD settings fails\nto work effectively due to the lack of labeled anomaly data. We further\nintroduce a novel robust AD approach to diverse distribution shifts by\nminimizing the distribution gap between in-distribution and OOD normal samples\nin both the training and inference stages in an unsupervised way. Our extensive\nempirical results on the three datasets show that our approach substantially\noutperforms state-of-the-art AD methods and OOD generalization methods on data\nwith various distribution shifts, while maintaining the detection accuracy on\nin-distribution data.\n","authors":["Tri Cao","Jiawen Zhu","Guansong Pang"],"pdf_url":"https://arxiv.org/pdf/2303.13845v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.13843v1","updated":"2023-03-24T07:37:09Z","published":"2023-03-24T07:37:09Z","title":"CompoNeRF: Text-guided Multi-object Compositional NeRF with Editable 3D\n  Scene Layout","summary":"  Recent research endeavors have shown that combining neural radiance fields\n(NeRFs) with pre-trained diffusion models holds great potential for text-to-3D\ngeneration.However, a hurdle is that they often encounter guidance collapse\nwhen rendering complex scenes from multi-object texts. Because the\ntext-to-image diffusion models are inherently unconstrained, making them less\ncompetent to accurately associate object semantics with specific 3D structures.\nTo address this issue, we propose a novel framework, dubbed CompoNeRF, that\nexplicitly incorporates an editable 3D scene layout to provide effective\nguidance at the single object (i.e., local) and whole scene (i.e., global)\nlevels. Firstly, we interpret the multi-object text as an editable 3D scene\nlayout containing multiple local NeRFs associated with the object-specific 3D\nbox coordinates and text prompt, which can be easily collected from users.\nThen, we introduce a global MLP to calibrate the compositional latent features\nfrom local NeRFs, which surprisingly improves the view consistency across\ndifferent local NeRFs. Lastly, we apply the text guidance on global and local\nlevels through their corresponding views to avoid guidance ambiguity. This way,\nour CompoNeRF allows for flexible scene editing and re-composition of trained\nlocal NeRFs into a new scene by manipulating the 3D layout or text prompt.\nLeveraging the open-source Stable Diffusion model, our CompoNeRF can generate\nfaithful and editable text-to-3D results while opening a potential direction\nfor text-guided multi-object composition via the editable 3D scene layout.\n","authors":["Yiqi Lin","Haotian Bai","Sijia Li","Haonan Lu","Xiaodong Lin","Hui Xiong","Lin Wang"],"pdf_url":"https://arxiv.org/pdf/2303.13843v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2212.14193v2","updated":"2023-03-24T07:35:15Z","published":"2022-12-29T06:42:51Z","title":"A Unified Object Counting Network with Object Occupation Prior","summary":"  The counting task, which plays a fundamental role in numerous applications\n(e.g., crowd counting, traffic statistics), aims to predict the number of\nobjects with various densities. Existing object counting tasks are designed for\na single object class. However, it is inevitable to encounter newly coming data\nwith new classes in our real world. We name this scenario as \\textit{evolving\nobject counting}. In this paper, we build the first evolving object counting\ndataset and propose a unified object counting network as the first attempt to\naddress this task. The proposed model consists of two key components: a\nclass-agnostic mask module and a class-incremental module. The class-agnostic\nmask module learns generic object occupation prior via predicting a\nclass-agnostic binary mask (e.g., 1 denotes there exists an object at the\nconsidering position in an image and 0 otherwise). The class-incremental module\nis used to handle new coming classes and provides discriminative class guidance\nfor density map prediction. The combined outputs of class-agnostic mask module\nand image feature extractor are used to predict the final density map. When new\nclasses come, we first add new neural nodes into the last regression and\nclassification layers of class-incremental module. Then, instead of retraining\nthe model from scratch, we utilize knowledge distillation to help the model\nremember what have already learned about previous object classes. We also\nemploy a support sample bank to store a small number of typical training\nsamples of each class, which are used to prevent the model from forgetting key\ninformation of old data. With this design, our model can efficiently and\neffectively adapt to new coming classes while keeping good performance on\nalready seen data without large-scale retraining. Extensive experiments on the\ncollected dataset demonstrate the favorable performance.\n","authors":["Shengqin Jiang","Qing Wang","Fengna Cheng","Yuankai Qi","Qingshan Liu"],"pdf_url":"https://arxiv.org/pdf/2212.14193v2.pdf","comment":"Under review; The dataset and code will be available at:\n  https://github.com/Tanyjiang/EOCO"},{"id":"http://arxiv.org/abs/2303.13842v1","updated":"2023-03-24T07:34:25Z","published":"2023-03-24T07:34:25Z","title":"FishDreamer: Towards Fisheye Semantic Completion via Unified Image\n  Outpainting and Segmentation","summary":"  This paper raises the new task of Fisheye Semantic Completion (FSC), where\ndense texture, structure, and semantics of a fisheye image are inferred even\nbeyond the sensor field-of-view (FoV). Fisheye cameras have larger FoV than\nordinary pinhole cameras, yet its unique special imaging model naturally leads\nto a blind area at the edge of the image plane. This is suboptimal for\nsafety-critical applications since important perception tasks, such as semantic\nsegmentation, become very challenging within the blind zone. Previous works\nconsidered the out-FoV outpainting and in-FoV segmentation separately. However,\nwe observe that these two tasks are actually closely coupled. To jointly\nestimate the tightly intertwined complete fisheye image and scene semantics, we\nintroduce the new FishDreamer which relies on successful ViTs enhanced with a\nnovel Polar-aware Cross Attention module (PCA) to leverage dense context and\nguide semantically-consistent content generation while considering different\npolar distributions. In addition to the contribution of the novel task and\narchitecture, we also derive Cityscapes-BF and KITTI360-BF datasets to\nfacilitate training and evaluation of this new track. Our experiments\ndemonstrate that the proposed FishDreamer outperforms methods solving each task\nin isolation and surpasses alternative approaches on the Fisheye Semantic\nCompletion. Code and datasets will be available at\nhttps://github.com/MasterHow/FishDreamer.\n","authors":["Hao Shi","Yu Li","Kailun Yang","Jiaming Zhang","Kunyu Peng","Alina Roitberg","Yaozu Ye","Huajian Ni","Kaiwei Wang","Rainer Stiefelhagen"],"pdf_url":"https://arxiv.org/pdf/2303.13842v1.pdf","comment":"Code and datasets will be available at\n  https://github.com/MasterHow/FishDreamer"},{"id":"http://arxiv.org/abs/2303.13839v1","updated":"2023-03-24T07:23:56Z","published":"2023-03-24T07:23:56Z","title":"HRDoc: Dataset and Baseline Method Toward Hierarchical Reconstruction of\n  Document Structures","summary":"  The problem of document structure reconstruction refers to converting digital\nor scanned documents into corresponding semantic structures. Most existing\nworks mainly focus on splitting the boundary of each element in a single\ndocument page, neglecting the reconstruction of semantic structure in\nmulti-page documents. This paper introduces hierarchical reconstruction of\ndocument structures as a novel task suitable for NLP and CV fields. To better\nevaluate the system performance on the new task, we built a large-scale dataset\nnamed HRDoc, which consists of 2,500 multi-page documents with nearly 2 million\nsemantic units. Every document in HRDoc has line-level annotations including\ncategories and relations obtained from rule-based extractors and human\nannotators. Moreover, we proposed an encoder-decoder-based hierarchical\ndocument structure parsing system (DSPS) to tackle this problem. By adopting a\nmulti-modal bidirectional encoder and a structure-aware GRU decoder with\nsoft-mask operation, the DSPS model surpass the baseline method by a large\nmargin. All scripts and datasets will be made publicly available at\nhttps://github.com/jfma-USTC/HRDoc.\n","authors":["Jiefeng Ma","Jun Du","Pengfei Hu","Zhenrong Zhang","Jianshu Zhang","Huihui Zhu","Cong Liu"],"pdf_url":"https://arxiv.org/pdf/2303.13839v1.pdf","comment":"8 pages, 6 figures. Accepted by AAAI-2023"},{"id":"http://arxiv.org/abs/2302.01047v3","updated":"2023-03-24T07:23:36Z","published":"2023-02-02T12:21:10Z","title":"Real-Time Evaluation in Online Continual Learning: A New Hope","summary":"  Current evaluations of Continual Learning (CL) methods typically assume that\nthere is no constraint on training time and computation. This is an unrealistic\nassumption for any real-world setting, which motivates us to propose: a\npractical real-time evaluation of continual learning, in which the stream does\nnot wait for the model to complete training before revealing the next data for\npredictions. To do this, we evaluate current CL methods with respect to their\ncomputational costs. We conduct extensive experiments on CLOC, a large-scale\ndataset containing 39 million time-stamped images with geolocation labels. We\nshow that a simple baseline outperforms state-of-the-art CL methods under this\nevaluation, questioning the applicability of existing methods in realistic\nsettings. In addition, we explore various CL components commonly used in the\nliterature, including memory sampling strategies and regularization approaches.\nWe find that all considered methods fail to be competitive against our simple\nbaseline. This surprisingly suggests that the majority of existing CL\nliterature is tailored to a specific class of streams that is not practical. We\nhope that the evaluation we provide will be the first step towards a paradigm\nshift to consider the computational cost in the development of online continual\nlearning methods.\n","authors":["Yasir Ghunaim","Adel Bibi","Kumail Alhamoud","Motasem Alfarra","Hasan Abed Al Kader Hammoud","Ameya Prabhu","Philip H. S. Torr","Bernard Ghanem"],"pdf_url":"https://arxiv.org/pdf/2302.01047v3.pdf","comment":"Accepted at CVPR'23 as Highlight (Top 2.5%)"},{"id":"http://arxiv.org/abs/2211.07381v2","updated":"2023-03-24T07:15:54Z","published":"2022-11-14T14:10:50Z","title":"FAPM: Fast Adaptive Patch Memory for Real-time Industrial Anomaly\n  Detection","summary":"  Feature embedding-based methods have shown exceptional performance in\ndetecting industrial anomalies by comparing features of target images with\nnormal images. However, some methods do not meet the speed requirements of\nreal-time inference, which is crucial for real-world applications. To address\nthis issue, we propose a new method called Fast Adaptive Patch Memory (FAPM)\nfor real-time industrial anomaly detection. FAPM utilizes patch-wise and\nlayer-wise memory banks that store the embedding features of images at the\npatch and layer level, respectively, which eliminates unnecessary repetitive\ncomputations. We also propose patch-wise adaptive coreset sampling for faster\nand more accurate detection. FAPM performs well in both accuracy and speed\ncompared to other state-of-the-art methods\n","authors":["Donghyeong Kim","Chaewon Park","Suhwan Cho","Sangyoun Lee"],"pdf_url":"https://arxiv.org/pdf/2211.07381v2.pdf","comment":"Accepted to 2023 IEEE International Conference on Acoustics, Speech\n  and Signal Processing (2023 ICASSP)"},{"id":"http://arxiv.org/abs/2202.10645v3","updated":"2023-03-24T07:14:27Z","published":"2022-02-22T03:21:51Z","title":"Combining the Silhouette and Skeleton Data for Gait Recognition","summary":"  Gait recognition, a long-distance biometric technology, has aroused intense\ninterest recently. Currently, the two dominant gait recognition works are\nappearance-based and model-based, which extract features from silhouettes and\nskeletons, respectively. However, appearance-based methods are greatly affected\nby clothes-changing and carrying conditions, while model-based methods are\nlimited by the accuracy of pose estimation. To tackle this challenge, a simple\nyet effective two-branch network is proposed in this paper, which contains a\nCNN-based branch taking silhouettes as input and a GCN-based branch taking\nskeletons as input. In addition, for better gait representation in the\nGCN-based branch, we present a fully connected graph convolution operator to\nintegrate multi-scale graph convolutions and alleviate the dependence on\nnatural joint connections. Also, we deploy a multi-dimension attention module\nnamed STC-Att to learn spatial, temporal and channel-wise attention\nsimultaneously. The experimental results on CASIA-B and OUMVLP show that our\nmethod achieves state-of-the-art performance in various conditions.\n","authors":["Likai Wang","Ruize Han","Wei Feng"],"pdf_url":"https://arxiv.org/pdf/2202.10645v3.pdf","comment":"Accepted by IEEE ICASSP 2023"},{"id":"http://arxiv.org/abs/2212.02499v2","updated":"2023-03-24T07:10:47Z","published":"2022-12-05T18:59:50Z","title":"Images Speak in Images: A Generalist Painter for In-Context Visual\n  Learning","summary":"  In-context learning, as a new paradigm in NLP, allows the model to rapidly\nadapt to various tasks with only a handful of prompts and examples. But in\ncomputer vision, the difficulties for in-context learning lie in that tasks\nvary significantly in the output representations, thus it is unclear how to\ndefine the general-purpose task prompts that the vision model can understand\nand transfer to out-of-domain tasks. In this work, we present Painter, a\ngeneralist model which addresses these obstacles with an \"image\"-centric\nsolution, that is, to redefine the output of core vision tasks as images, and\nspecify task prompts as also images. With this idea, our training process is\nextremely simple, which performs standard masked image modeling on the stitch\nof input and output image pairs. This makes the model capable of performing\ntasks conditioned on visible image patches. Thus, during inference, we can\nadopt a pair of input and output images from the same task as the input\ncondition, to indicate which task to perform. Without bells and whistles, our\ngeneralist Painter can achieve competitive performance compared to\nwell-established task-specific models, on seven representative vision tasks\nranging from high-level visual understanding to low-level image processing. In\naddition, Painter significantly outperforms recent generalist models on several\nchallenging tasks.\n","authors":["Xinlong Wang","Wen Wang","Yue Cao","Chunhua Shen","Tiejun Huang"],"pdf_url":"https://arxiv.org/pdf/2212.02499v2.pdf","comment":"Accepted to CVPR 2023. Code and model is available at:\n  https://github.com/baaivision/Painter"},{"id":"http://arxiv.org/abs/2303.12274v3","updated":"2023-03-24T07:05:08Z","published":"2023-03-22T02:47:42Z","title":"A Hierarchical Hybrid Learning Framework for Multi-agent Trajectory\n  Prediction","summary":"  Accurate and robust trajectory prediction of neighboring agents is critical\nfor autonomous vehicles traversing in complex scenes. Most methods proposed in\nrecent years are deep learning-based due to their strength in encoding complex\ninteractions. However, unplausible predictions are often generated since they\nrely heavily on past observations and cannot effectively capture the transient\nand contingency interactions from sparse samples. In this paper, we propose a\nhierarchical hybrid framework of deep learning (DL) and reinforcement learning\n(RL) for multi-agent trajectory prediction, to cope with the challenge of\npredicting motions shaped by multi-scale interactions. In the DL stage, the\ntraffic scene is divided into multiple intermediate-scale heterogenous graphs\nbased on which Transformer-style GNNs are adopted to encode heterogenous\ninteractions at intermediate and global levels. In the RL stage, we divide the\ntraffic scene into local sub-scenes utilizing the key future points predicted\nin the DL stage. To emulate the motion planning procedure so as to produce\ntrajectory predictions, a Transformer-based Proximal Policy Optimization (PPO)\nincorporated with a vehicle kinematics model is devised to plan motions under\nthe dominant influence of microscopic interactions. A multi-objective reward is\ndesigned to balance between agent-centric accuracy and scene-wise\ncompatibility. Experimental results show that our proposal matches the\nstate-of-the-arts on the Argoverse forecasting benchmark. It's also revealed by\nthe visualized results that the hierarchical learning framework captures the\nmulti-scale interactions and improves the feasibility and compliance of the\npredicted trajectories.\n","authors":["Yujun Jiao","Mingze Miao","Zhishuai Yin","Chunyuan Lei","Xu Zhu","Linzhen Nie","Bo Tao"],"pdf_url":"https://arxiv.org/pdf/2303.12274v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2212.04048v2","updated":"2023-03-24T06:59:39Z","published":"2022-12-08T03:07:00Z","title":"Executing your Commands via Motion Diffusion in Latent Space","summary":"  We study a challenging task, conditional human motion generation, which\nproduces plausible human motion sequences according to various conditional\ninputs, such as action classes or textual descriptors. Since human motions are\nhighly diverse and have a property of quite different distribution from\nconditional modalities, such as textual descriptors in natural languages, it is\nhard to learn a probabilistic mapping from the desired conditional modality to\nthe human motion sequences. Besides, the raw motion data from the motion\ncapture system might be redundant in sequences and contain noises; directly\nmodeling the joint distribution over the raw motion sequences and conditional\nmodalities would need a heavy computational overhead and might result in\nartifacts introduced by the captured noises. To learn a better representation\nof the various human motion sequences, we first design a powerful Variational\nAutoEncoder (VAE) and arrive at a representative and low-dimensional latent\ncode for a human motion sequence. Then, instead of using a diffusion model to\nestablish the connections between the raw motion sequences and the conditional\ninputs, we perform a diffusion process on the motion latent space. Our proposed\nMotion Latent-based Diffusion model (MLD) could produce vivid motion sequences\nconforming to the given conditional inputs and substantially reduce the\ncomputational overhead in both the training and inference stages. Extensive\nexperiments on various human motion generation tasks demonstrate that our MLD\nachieves significant improvements over the state-of-the-art methods among\nextensive human motion generation tasks, with two orders of magnitude faster\nthan previous diffusion models on raw motion sequences.\n","authors":["Xin Chen","Biao Jiang","Wen Liu","Zilong Huang","Bin Fu","Tao Chen","Jingyi Yu","Gang Yu"],"pdf_url":"https://arxiv.org/pdf/2212.04048v2.pdf","comment":"18 pages, 11 figures, conference"},{"id":"http://arxiv.org/abs/2303.10598v3","updated":"2023-03-24T06:57:50Z","published":"2023-03-19T08:26:06Z","title":"StyleRF: Zero-shot 3D Style Transfer of Neural Radiance Fields","summary":"  3D style transfer aims to render stylized novel views of a 3D scene with\nmulti-view consistency. However, most existing work suffers from a three-way\ndilemma over accurate geometry reconstruction, high-quality stylization, and\nbeing generalizable to arbitrary new styles. We propose StyleRF (Style Radiance\nFields), an innovative 3D style transfer technique that resolves the three-way\ndilemma by performing style transformation within the feature space of a\nradiance field. StyleRF employs an explicit grid of high-level features to\nrepresent 3D scenes, with which high-fidelity geometry can be reliably restored\nvia volume rendering. In addition, it transforms the grid features according to\nthe reference style which directly leads to high-quality zero-shot style\ntransfer. StyleRF consists of two innovative designs. The first is\nsampling-invariant content transformation that makes the transformation\ninvariant to the holistic statistics of the sampled 3D points and accordingly\nensures multi-view consistency. The second is deferred style transformation of\n2D feature maps which is equivalent to the transformation of 3D points but\ngreatly reduces memory footprint without degrading multi-view consistency.\nExtensive experiments show that StyleRF achieves superior 3D stylization\nquality with precise geometry reconstruction and it can generalize to various\nnew styles in a zero-shot manner.\n","authors":["Kunhao Liu","Fangneng Zhan","Yiwen Chen","Jiahui Zhang","Yingchen Yu","Abdulmotaleb El Saddik","Shijian Lu","Eric Xing"],"pdf_url":"https://arxiv.org/pdf/2303.10598v3.pdf","comment":"Accepted to CVPR 2023. Project website:\n  https://kunhao-liu.github.io/StyleRF/"},{"id":"http://arxiv.org/abs/2303.13827v1","updated":"2023-03-24T06:24:07Z","published":"2023-03-24T06:24:07Z","title":"Efficient Mixed-Type Wafer Defect Pattern Recognition Using Compact\n  Deformable Convolutional Transformers","summary":"  Manufacturing wafers is an intricate task involving thousands of steps.\nDefect Pattern Recognition (DPR) of wafer maps is crucial to find the root\ncause of the issue and further improving the yield in the wafer foundry.\nMixed-type DPR is much more complicated compared to single-type DPR due to\nvaried spatial features, the uncertainty of defects, and the number of defects\npresent. To accurately predict the number of defects as well as the types of\ndefects, we propose a novel compact deformable convolutional transformer (DC\nTransformer). Specifically, DC Transformer focuses on the global features\npresent in the wafer map by virtue of learnable deformable kernels and\nmulti-head attention to the global features. The proposed method succinctly\nmodels the internal relationship between the wafer maps and the defects. DC\nTransformer is evaluated on a real dataset containing 38 defect patterns.\nExperimental results show that DC Transformer performs exceptionally well in\nrecognizing both single and mixed-type defects. The proposed method outperforms\nthe current state of the models by a considerable margin\n","authors":["Nitish Shukla"],"pdf_url":"https://arxiv.org/pdf/2303.13827v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.13826v1","updated":"2023-03-24T06:22:57Z","published":"2023-03-24T06:22:57Z","title":"Hard Sample Matters a Lot in Zero-Shot Quantization","summary":"  Zero-shot quantization (ZSQ) is promising for compressing and accelerating\ndeep neural networks when the data for training full-precision models are\ninaccessible. In ZSQ, network quantization is performed using synthetic\nsamples, thus, the performance of quantized models depends heavily on the\nquality of synthetic samples. Nonetheless, we find that the synthetic samples\nconstructed in existing ZSQ methods can be easily fitted by models.\nAccordingly, quantized models obtained by these methods suffer from significant\nperformance degradation on hard samples. To address this issue, we propose HArd\nsample Synthesizing and Training (HAST). Specifically, HAST pays more attention\nto hard samples when synthesizing samples and makes synthetic samples hard to\nfit when training quantized models. HAST aligns features extracted by\nfull-precision and quantized models to ensure the similarity between features\nextracted by these two models. Extensive experiments show that HAST\nsignificantly outperforms existing ZSQ methods, achieving performance\ncomparable to models that are quantized with real data.\n","authors":["Huantong Li","Xiangmiao Wu","Fanbing Lv","Daihai Liao","Thomas H. Li","Yonggang Zhang","Bo Han","Mingkui Tan"],"pdf_url":"https://arxiv.org/pdf/2303.13826v1.pdf","comment":"12 pages, CVPR 2023"},{"id":"http://arxiv.org/abs/2204.09804v4","updated":"2023-03-24T06:22:03Z","published":"2022-04-20T22:48:05Z","title":"Weighted Bayesian Gaussian Mixture Model for Roadside LiDAR Object\n  Detection","summary":"  Background modeling is widely used for intelligent surveillance systems to\ndetect moving targets by subtracting the static background components. Most\nroadside LiDAR object detection methods filter out foreground points by\ncomparing new data points to pre-trained background references based on\ndescriptive statistics over many frames (e.g., voxel density, number of\nneighbors, maximum distance). However, these solutions are inefficient under\nheavy traffic, and parameter values are hard to transfer from one scenario to\nanother. In early studies, the probabilistic background modeling methods widely\nused for the video-based system were considered unsuitable for roadside LiDAR\nsurveillance systems due to the sparse and unstructured point cloud data. In\nthis paper, the raw LiDAR data were transformed into a structured\nrepresentation based on the elevation and azimuth value of each LiDAR point.\nWith this high-order tensor representation, we break the barrier to allow\nefficient high-dimensional multivariate analysis for roadside LiDAR background\nmodeling. The Bayesian Nonparametric (BNP) approach integrates the intensity\nvalue and 3D measurements to exploit the measurement data using 3D and\nintensity info entirely. The proposed method was compared against two\nstate-of-the-art roadside LiDAR background models, computer vision benchmark,\nand deep learning baselines, evaluated at point, object, and path levels under\nheavy traffic and challenging weather. This multimodal Weighted Bayesian\nGaussian Mixture Model (GMM) can handle dynamic backgrounds with noisy\nmeasurements and substantially enhances the infrastructure-based LiDAR object\ndetection, whereby various 3D modeling for smart city applications could be\ncreated.\n","authors":["Tianya Zhang","Yi Ge","Peter J. Jin"],"pdf_url":"https://arxiv.org/pdf/2204.09804v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.13396v2","updated":"2023-03-24T06:19:51Z","published":"2023-03-23T16:15:07Z","title":"Zero-guidance Segmentation Using Zero Segment Labels","summary":"  CLIP has enabled new and exciting joint vision-language applications, one of\nwhich is open-vocabulary segmentation, which can locate any segment given an\narbitrary text query. In our research, we ask whether it is possible to\ndiscover semantic segments without any user guidance in the form of text\nqueries or predefined classes, and label them using natural language\nautomatically? We propose a novel problem zero-guidance segmentation and the\nfirst baseline that leverages two pre-trained generalist models, DINO and CLIP,\nto solve this problem without any fine-tuning or segmentation dataset. The\ngeneral idea is to first segment an image into small over-segments, encode them\ninto CLIP's visual-language space, translate them into text labels, and merge\nsemantically similar segments together. The key challenge, however, is how to\nencode a visual segment into a segment-specific embedding that balances global\nand local context information, both useful for recognition. Our main\ncontribution is a novel attention-masking technique that balances the two\ncontexts by analyzing the attention layers inside CLIP. We also introduce\nseveral metrics for the evaluation of this new task. With CLIP's innate\nknowledge, our method can precisely locate the Mona Lisa painting among a\nmuseum crowd. Project page: https://zero-guide-seg.github.io/.\n","authors":["Pitchaporn Rewatbowornwong","Nattanat Chatthee","Ekapol Chuangsuwanich","Supasorn Suwajanakorn"],"pdf_url":"https://arxiv.org/pdf/2303.13396v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.13825v1","updated":"2023-03-24T06:19:19Z","published":"2023-03-24T06:19:19Z","title":"HandNeRF: Neural Radiance Fields for Animatable Interacting Hands","summary":"  We propose a novel framework to reconstruct accurate appearance and geometry\nwith neural radiance fields (NeRF) for interacting hands, enabling the\nrendering of photo-realistic images and videos for gesture animation from\narbitrary views. Given multi-view images of a single hand or interacting hands,\nan off-the-shelf skeleton estimator is first employed to parameterize the hand\nposes. Then we design a pose-driven deformation field to establish\ncorrespondence from those different poses to a shared canonical space, where a\npose-disentangled NeRF for one hand is optimized. Such unified modeling\nefficiently complements the geometry and texture cues in rarely-observed areas\nfor both hands. Meanwhile, we further leverage the pose priors to generate\npseudo depth maps as guidance for occlusion-aware density learning. Moreover, a\nneural feature distillation method is proposed to achieve cross-domain\nalignment for color optimization. We conduct extensive experiments to verify\nthe merits of our proposed HandNeRF and report a series of state-of-the-art\nresults both qualitatively and quantitatively on the large-scale InterHand2.6M\ndataset.\n","authors":["Zhiyang Guo","Wengang Zhou","Min Wang","Li Li","Houqiang Li"],"pdf_url":"https://arxiv.org/pdf/2303.13825v1.pdf","comment":"CVPR 2023"},{"id":"http://arxiv.org/abs/2211.15951v2","updated":"2023-03-24T06:05:59Z","published":"2022-11-29T06:24:14Z","title":"Feature-domain Adaptive Contrastive Distillation for Efficient Single\n  Image Super-Resolution","summary":"  Recently, CNN-based SISR has numerous parameters and high computational cost\nto achieve better performance, limiting its applicability to\nresource-constrained devices such as mobile. As one of the methods to make the\nnetwork efficient, Knowledge Distillation (KD), which transfers teacher's\nuseful knowledge to student, is currently being studied. More recently, KD for\nSISR utilizes Feature Distillation (FD) to minimize the Euclidean distance loss\nof feature maps between teacher and student networks, but it does not\nsufficiently consider how to effectively and meaningfully deliver knowledge\nfrom teacher to improve the student performance at given network capacity\nconstraints. In this paper, we propose a feature-domain adaptive contrastive\ndistillation (FACD) method for efficiently training lightweight student SISR\nnetworks. We show the limitations of the existing FD methods using Euclidean\ndistance loss, and propose a feature-domain contrastive loss that makes a\nstudent network learn richer information from the teacher's representation in\nthe feature domain. In addition, we propose an adaptive distillation that\nselectively applies distillation depending on the conditions of the training\npatches. The experimental results show that the student EDSR and RCAN networks\nwith the proposed FACD scheme improves not only the PSNR performance of the\nentire benchmark datasets and scales, but also the subjective image quality\ncompared to the conventional FD approaches.\n","authors":["HyeonCheol Moon","JinWoo Jeong","SungJei Kim"],"pdf_url":"https://arxiv.org/pdf/2211.15951v2.pdf","comment":"Under review"},{"id":"http://arxiv.org/abs/2211.04041v4","updated":"2023-03-24T05:57:41Z","published":"2022-11-08T06:50:10Z","title":"ParticleNeRF: A Particle-Based Encoding for Online Neural Radiance\n  Fields","summary":"  While existing Neural Radiance Fields (NeRFs) for dynamic scenes are offline\nmethods with an emphasis on visual fidelity, our paper addresses the online use\ncase that prioritises real-time adaptability. We present ParticleNeRF, a new\napproach that dynamically adapts to changes in the scene geometry by learning\nan up-to-date representation online, every 200ms. ParticleNeRF achieves this\nusing a novel particle-based parametric encoding. We couple features to\nparticles in space and backpropagate the photometric reconstruction loss into\nthe particles' position gradients, which are then interpreted as velocity\nvectors. Governed by a lightweight physics system to handle collisions, this\nlets the features move freely with the changing scene geometry. We demonstrate\nParticleNeRF on various dynamic scenes containing translating, rotating,\narticulated, and deformable objects. ParticleNeRF is the first online dynamic\nNeRF and achieves fast adaptability with better visual fidelity than\nbrute-force online InstantNGP and other baseline approaches on dynamic scenes\nwith online constraints. Videos of our system can be found at our project\nwebsite https://sites.google.com/view/particlenerf.\n","authors":["Jad Abou-Chakra","Feras Dayoub","Niko Sünderhauf"],"pdf_url":"https://arxiv.org/pdf/2211.04041v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.13818v1","updated":"2023-03-24T05:39:12Z","published":"2023-03-24T05:39:12Z","title":"Prior-RadGraphFormer: A Prior-Knowledge-Enhanced Transformer for\n  Generating Radiology Graphs from X-Rays","summary":"  The extraction of structured clinical information from free-text radiology\nreports in the form of radiology graphs has been demonstrated to be a valuable\napproach for evaluating the clinical correctness of report-generation methods.\nHowever, the direct generation of radiology graphs from chest X-ray (CXR)\nimages has not been attempted. To address this gap, we propose a novel approach\ncalled Prior-RadGraphFormer that utilizes a transformer model with prior\nknowledge in the form of a probabilistic knowledge graph (PKG) to generate\nradiology graphs directly from CXR images. The PKG models the statistical\nrelationship between radiology entities, including anatomical structures and\nmedical observations. This additional contextual information enhances the\naccuracy of entity and relation extraction. The generated radiology graphs can\nbe applied to various downstream tasks, such as free-text or structured reports\ngeneration and multi-label classification of pathologies. Our approach\nrepresents a promising method for generating radiology graphs directly from CXR\nimages, and has significant potential for improving medical image analysis and\nclinical decision-making.\n","authors":["Yiheng Xiong","Jingsong Liu","Kamilia Zaripova","Sahand Sharifzadeh","Matthias Keicher","Nassir Navab"],"pdf_url":"https://arxiv.org/pdf/2303.13818v1.pdf","comment":"12 pages, 4 figures, 4 tables"},{"id":"http://arxiv.org/abs/2303.02416v2","updated":"2023-03-24T05:37:41Z","published":"2023-03-04T13:38:51Z","title":"PixMIM: Rethinking Pixel Reconstruction in Masked Image Modeling","summary":"  Masked Image Modeling (MIM) has achieved promising progress with the advent\nof Masked Autoencoders (MAE) and BEiT. However, subsequent works have\ncomplicated the framework with new auxiliary tasks or extra pre-trained models,\ninevitably increasing computational overhead. This paper undertakes a\nfundamental analysis of MIM from the perspective of pixel reconstruction, which\nexamines the input image patches and reconstruction target, and highlights two\ncritical but previously overlooked bottlenecks. Based on this analysis, we\npropose a remarkably simple and effective method, {\\ourmethod}, that entails\ntwo strategies: 1) filtering the high-frequency components from the\nreconstruction target to de-emphasize the network's focus on texture-rich\ndetails and 2) adopting a conservative data transform strategy to alleviate the\nproblem of missing foreground in MIM training. {\\ourmethod} can be easily\nintegrated into most existing pixel-based MIM approaches (\\ie, using raw images\nas reconstruction target) with negligible additional computation. Without bells\nand whistles, our method consistently improves three MIM approaches, MAE,\nConvMAE, and LSMAE, across various downstream tasks. We believe this effective\nplug-and-play method will serve as a strong baseline for self-supervised\nlearning and provide insights for future improvements of the MIM framework.\nCode and models are available at\n\\url{https://github.com/open-mmlab/mmselfsup/tree/dev-1.x/configs/selfsup/pixmim}.\n","authors":["Yuan Liu","Songyang Zhang","Jiacheng Chen","Kai Chen","Dahua Lin"],"pdf_url":"https://arxiv.org/pdf/2303.02416v2.pdf","comment":"Update code link and add additional results"},{"id":"http://arxiv.org/abs/2303.13817v1","updated":"2023-03-24T05:34:39Z","published":"2023-03-24T05:34:39Z","title":"ABLE-NeRF: Attention-Based Rendering with Learnable Embeddings for\n  Neural Radiance Field","summary":"  Neural Radiance Field (NeRF) is a popular method in representing 3D scenes by\noptimising a continuous volumetric scene function. Its large success which lies\nin applying volumetric rendering (VR) is also its Achilles' heel in producing\nview-dependent effects. As a consequence, glossy and transparent surfaces often\nappear murky. A remedy to reduce these artefacts is to constrain this VR\nequation by excluding volumes with back-facing normal. While this approach has\nsome success in rendering glossy surfaces, translucent objects are still poorly\nrepresented. In this paper, we present an alternative to the physics-based VR\napproach by introducing a self-attention-based framework on volumes along a\nray. In addition, inspired by modern game engines which utilise Light Probes to\nstore local lighting passing through the scene, we incorporate Learnable\nEmbeddings to capture view dependent effects within the scene. Our method,\nwhich we call ABLE-NeRF, significantly reduces `blurry' glossy surfaces in\nrendering and produces realistic translucent surfaces which lack in prior art.\nIn the Blender dataset, ABLE-NeRF achieves SOTA results and surpasses Ref-NeRF\nin all 3 image quality metrics PSNR, SSIM, LPIPS.\n","authors":["Zhe Jun Tang","Tat-Jen Cham","Haiyu Zhao"],"pdf_url":"https://arxiv.org/pdf/2303.13817v1.pdf","comment":"IEEE / CVF Computer Vision and Pattern Recognition Conference (CVPR)\n  2023"},{"id":"http://arxiv.org/abs/2303.13814v1","updated":"2023-03-24T05:28:35Z","published":"2023-03-24T05:28:35Z","title":"Multimodal Adaptive Fusion of Face and Gait Features using Keyless\n  attention based Deep Neural Networks for Human Identification","summary":"  Biometrics plays a significant role in vision-based surveillance\napplications. Soft biometrics such as gait is widely used with face in\nsurveillance tasks like person recognition and re-identification. Nevertheless,\nin practical scenarios, classical fusion techniques respond poorly to changes\nin individual users and in the external environment. To this end, we propose a\nnovel adaptive multi-biometric fusion strategy for the dynamic incorporation of\ngait and face biometric cues by leveraging keyless attention deep neural\nnetworks. Various external factors such as viewpoint and distance to the\ncamera, are investigated in this study. Extensive experiments have shown\nsuperior performanceof the proposed model compared with the state-of-the-art\nmodel.\n","authors":["Ashwin Prakash","Thejaswin S","Athira Nambiar","Alexandre Bernardino"],"pdf_url":"https://arxiv.org/pdf/2303.13814v1.pdf","comment":"-"},{"id":"http://arxiv.org/abs/2303.13813v1","updated":"2023-03-24T05:24:23Z","published":"2023-03-24T05:24:23Z","title":"Generalist: Decoupling Natural and Robust Generalization","summary":"  Deep neural networks obtained by standard training have been constantly\nplagued by adversarial examples. Although adversarial training demonstrates its\ncapability to defend against adversarial examples, unfortunately, it leads to\nan inevitable drop in the natural generalization. To address the issue, we\ndecouple the natural generalization and the robust generalization from joint\ntraining and formulate different training strategies for each one.\nSpecifically, instead of minimizing a global loss on the expectation over these\ntwo generalization errors, we propose a bi-expert framework called\n\\emph{Generalist} where we simultaneously train base learners with task-aware\nstrategies so that they can specialize in their own fields. The parameters of\nbase learners are collected and combined to form a global learner at intervals\nduring the training process. The global learner is then distributed to the base\nlearners as initialized parameters for continued training. Theoretically, we\nprove that the risks of Generalist will get lower once the base learners are\nwell trained. Extensive experiments verify the applicability of Generalist to\nachieve high accuracy on natural examples while maintaining considerable\nrobustness to adversarial ones. Code is available at\nhttps://github.com/PKU-ML/Generalist.\n","authors":["Hongjun Wang","Yisen Wang"],"pdf_url":"https://arxiv.org/pdf/2303.13813v1.pdf","comment":"14 pages, 5 figures, CVPR 2023"},{"id":"http://arxiv.org/abs/2303.11101v2","updated":"2023-03-24T05:15:38Z","published":"2023-03-20T13:38:29Z","title":"Coreset Sampling from Open-Set for Fine-Grained Self-Supervised Learning","summary":"  Deep learning in general domains has constantly been extended to\ndomain-specific tasks requiring the recognition of fine-grained\ncharacteristics. However, real-world applications for fine-grained tasks suffer\nfrom two challenges: a high reliance on expert knowledge for annotation and\nnecessity of a versatile model for various downstream tasks in a specific\ndomain (e.g., prediction of categories, bounding boxes, or pixel-wise\nannotations). Fortunately, the recent self-supervised learning (SSL) is a\npromising approach to pretrain a model without annotations, serving as an\neffective initialization for any downstream tasks. Since SSL does not rely on\nthe presence of annotation, in general, it utilizes the large-scale unlabeled\ndataset, referred to as an open-set. In this sense, we introduce a novel\nOpen-Set Self-Supervised Learning problem under the assumption that a\nlarge-scale unlabeled open-set is available, as well as the fine-grained target\ndataset, during a pretraining phase. In our problem setup, it is crucial to\nconsider the distribution mismatch between the open-set and target dataset.\nHence, we propose SimCore algorithm to sample a coreset, the subset of an\nopen-set that has a minimum distance to the target dataset in the latent space.\nWe demonstrate that SimCore significantly improves representation learning\nperformance through extensive experimental settings, including eleven\nfine-grained datasets and seven open-sets in various downstream tasks.\n","authors":["Sungnyun Kim","Sangmin Bae","Se-Young Yun"],"pdf_url":"https://arxiv.org/pdf/2303.11101v2.pdf","comment":"CVPR 2023"},{"id":"http://arxiv.org/abs/2303.13810v1","updated":"2023-03-24T05:06:02Z","published":"2023-03-24T05:06:02Z","title":"Evidence-aware multi-modal data fusion and its application to total knee\n  replacement prediction","summary":"  Deep neural networks have been widely studied for predicting a medical\ncondition, such as total knee replacement (TKR). It has shown that data of\ndifferent modalities, such as imaging data, clinical variables and demographic\ninformation, provide complementary information and thus can improve the\nprediction accuracy together. However, the data sources of various modalities\nmay not always be of high quality, and each modality may have only partial\ninformation of medical condition. Thus, predictions from different modalities\ncan be opposite, and the final prediction may fail in the presence of such a\nconflict. Therefore, it is important to consider the reliability of each source\ndata and the prediction output when making a final decision. In this paper, we\npropose an evidence-aware multi-modal data fusion framework based on the\nDempster-Shafer theory (DST). The backbone models contain an image branch, a\nnon-image branch and a fusion branch. For each branch, there is an evidence\nnetwork that takes the extracted features as input and outputs an evidence\nscore, which is designed to represent the reliability of the output from the\ncurrent branch. The output probabilities along with the evidence scores from\nmultiple branches are combined with the Dempster's combination rule to make a\nfinal prediction. Experimental results on the public OA initiative (OAI)\ndataset for the TKR prediction task show the superiority of the proposed fusion\nstrategy on various backbone models.\n","authors":["Xinwen Liu","Jing Wang","S. Kevin Zhou","Craig Engstrom","Shekhar S. Chandra"],"pdf_url":"https://arxiv.org/pdf/2303.13810v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.13807v1","updated":"2023-03-24T05:04:52Z","published":"2023-03-24T05:04:52Z","title":"PFT-SSR: Parallax Fusion Transformer for Stereo Image Super-Resolution","summary":"  Stereo image super-resolution aims to boost the performance of image\nsuper-resolution by exploiting the supplementary information provided by\nbinocular systems. Although previous methods have achieved promising results,\nthey did not fully utilize the information of cross-view and intra-view. To\nfurther unleash the potential of binocular images, in this letter, we propose a\nnovel Transformerbased parallax fusion module called Parallax Fusion\nTransformer (PFT). PFT employs a Cross-view Fusion Transformer (CVFT) to\nutilize cross-view information and an Intra-view Refinement Transformer (IVRT)\nfor intra-view feature refinement. Meanwhile, we adopted the Swin Transformer\nas the backbone for feature extraction and SR reconstruction to form a pure\nTransformer architecture called PFT-SSR. Extensive experiments and ablation\nstudies show that PFT-SSR achieves competitive results and outperforms most\nSOTA methods. Source code is available at https://github.com/MIVRC/PFT-PyTorch.\n","authors":["Hansheng Guo","Juncheng Li","Guangwei Gao","Zhi Li","Tieyong Zeng"],"pdf_url":"https://arxiv.org/pdf/2303.13807v1.pdf","comment":"5 pages, 3 figures"},{"id":"http://arxiv.org/abs/2303.13805v1","updated":"2023-03-24T04:58:27Z","published":"2023-03-24T04:58:27Z","title":"Seeing Through the Glass: Neural 3D Reconstruction of Object Inside a\n  Transparent Container","summary":"  In this paper, we define a new problem of recovering the 3D geometry of an\nobject confined in a transparent enclosure. We also propose a novel method for\nsolving this challenging problem. Transparent enclosures pose challenges of\nmultiple light reflections and refractions at the interface between different\npropagation media e.g. air or glass. These multiple reflections and refractions\ncause serious image distortions which invalidate the single viewpoint\nassumption. Hence the 3D geometry of such objects cannot be reliably\nreconstructed using existing methods, such as traditional structure from motion\nor modern neural reconstruction methods. We solve this problem by explicitly\nmodeling the scene as two distinct sub-spaces, inside and outside the\ntransparent enclosure. We use an existing neural reconstruction method (NeuS)\nthat implicitly represents the geometry and appearance of the inner subspace.\nIn order to account for complex light interactions, we develop a hybrid\nrendering strategy that combines volume rendering with ray tracing. We then\nrecover the underlying geometry and appearance of the model by minimizing the\ndifference between the real and hybrid rendered images. We evaluate our method\non both synthetic and real data. Experiment results show that our method\noutperforms the state-of-the-art (SOTA) methods. Codes and data will be\navailable at https://github.com/hirotong/ReNeuS\n","authors":["Jinguang Tong","Sundaram Muthu","Fahira Afzal Maken","Chuong Nguyen","Hongdong Li"],"pdf_url":"https://arxiv.org/pdf/2303.13805v1.pdf","comment":"Accepted to CVPR2023"},{"id":"http://arxiv.org/abs/2303.13802v1","updated":"2023-03-24T04:54:44Z","published":"2023-03-24T04:54:44Z","title":"Decoupled Multimodal Distilling for Emotion Recognition","summary":"  Human multimodal emotion recognition (MER) aims to perceive human emotions\nvia language, visual and acoustic modalities. Despite the impressive\nperformance of previous MER approaches, the inherent multimodal heterogeneities\nstill haunt and the contribution of different modalities varies significantly.\nIn this work, we mitigate this issue by proposing a decoupled multimodal\ndistillation (DMD) approach that facilitates flexible and adaptive crossmodal\nknowledge distillation, aiming to enhance the discriminative features of each\nmodality. Specially, the representation of each modality is decoupled into two\nparts, i.e., modality-irrelevant/-exclusive spaces, in a self-regression\nmanner. DMD utilizes a graph distillation unit (GD-Unit) for each decoupled\npart so that each GD can be performed in a more specialized and effective\nmanner. A GD-Unit consists of a dynamic graph where each vertice represents a\nmodality and each edge indicates a dynamic knowledge distillation. Such GD\nparadigm provides a flexible knowledge transfer manner where the distillation\nweights can be automatically learned, thus enabling diverse crossmodal\nknowledge transfer patterns. Experimental results show DMD consistently obtains\nsuperior performance than state-of-the-art MER methods. Visualization results\nshow the graph edges in DMD exhibit meaningful distributional patterns w.r.t.\nthe modality-irrelevant/-exclusive feature spaces. Codes are released at\n\\url{https://github.com/mdswyz/DMD}.\n","authors":["Yong Li","Yuanzhi Wang","Zhen Cui"],"pdf_url":"https://arxiv.org/pdf/2303.13802v1.pdf","comment":"To appear at CVPR 2023, selected as a hightlight, 10% of accepted\n  papers, 2.5% of submissions"},{"id":"http://arxiv.org/abs/2303.13800v1","updated":"2023-03-24T04:45:45Z","published":"2023-03-24T04:45:45Z","title":"Aligning Step-by-Step Instructional Diagrams to Video Demonstrations","summary":"  Multimodal alignment facilitates the retrieval of instances from one modality\nwhen queried using another. In this paper, we consider a novel setting where\nsuch an alignment is between (i) instruction steps that are depicted as\nassembly diagrams (commonly seen in Ikea assembly manuals) and (ii) video\nsegments from in-the-wild videos; these videos comprising an enactment of the\nassembly actions in the real world. To learn this alignment, we introduce a\nnovel supervised contrastive learning method that learns to align videos with\nthe subtle details in the assembly diagrams, guided by a set of novel losses.\nTo study this problem and demonstrate the effectiveness of our method, we\nintroduce a novel dataset: IAW for Ikea assembly in the wild consisting of 183\nhours of videos from diverse furniture assembly collections and nearly 8,300\nillustrations from their associated instruction manuals and annotated for their\nground truth alignments. We define two tasks on this dataset: First, nearest\nneighbor retrieval between video segments and illustrations, and, second,\nalignment of instruction steps and the segments for each video. Extensive\nexperiments on IAW demonstrate superior performances of our approach against\nalternatives.\n","authors":["Jiahao Zhang","Anoop Cherian","Yanbin Liu","Yizhak Ben-Shabat","Cristian Rodriguez","Stephen Gould"],"pdf_url":"https://arxiv.org/pdf/2303.13800v1.pdf","comment":"Project website:\n  https://academic.davidz.cn/en/publication/zhang-cvpr-2023/"},{"id":"http://arxiv.org/abs/2303.13798v1","updated":"2023-03-24T04:39:50Z","published":"2023-03-24T04:39:50Z","title":"2D Floor Plan Segmentation Based on Down-sampling","summary":"  In recent years, floor plan segmentation has gained significant attention due\nto its wide range of applications in floor plan reconstruction and robotics. In\nthis paper, we propose a novel 2D floor plan segmentation technique based on a\ndown-sampling approach. Our method employs continuous down-sampling on a floor\nplan to maintain its structural information while reducing its complexity. We\ndemonstrate the effectiveness of our approach by presenting results obtained\nfrom both cluttered floor plans generated by a vacuum cleaning robot in unknown\nenvironments and a benchmark of floor plans. Our technique considerably reduces\nthe computational and implementation complexity of floor plan segmentation,\nmaking it more suitable for real-world applications. Additionally, we discuss\nthe appropriate metric for evaluating segmentation results. Overall, our\napproach yields promising results for 2D floor plan segmentation in cluttered\nenvironments.\n","authors":["Mohammadreza Sharif","Kiran Mohan","Sarath Suvarna"],"pdf_url":"https://arxiv.org/pdf/2303.13798v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.13223v2","updated":"2023-03-24T04:36:27Z","published":"2023-03-23T12:39:20Z","title":"Exploring Structured Semantic Prior for Multi Label Recognition with\n  Incomplete Labels","summary":"  Multi-label recognition (MLR) with incomplete labels is very challenging.\nRecent works strive to explore the image-to-label correspondence in the\nvision-language model, \\ie, CLIP, to compensate for insufficient annotations.\nIn spite of promising performance, they generally overlook the valuable prior\nabout the label-to-label correspondence. In this paper, we advocate remedying\nthe deficiency of label supervision for the MLR with incomplete labels by\nderiving a structured semantic prior about the label-to-label correspondence\nvia a semantic prior prompter. We then present a novel Semantic Correspondence\nPrompt Network (SCPNet), which can thoroughly explore the structured semantic\nprior. A Prior-Enhanced Self-Supervised Learning method is further introduced\nto enhance the use of the prior. Comprehensive experiments and analyses on\nseveral widely used benchmark datasets show that our method significantly\noutperforms existing methods on all datasets, well demonstrating the\neffectiveness and the superiority of our method. Our code will be available at\nhttps://github.com/jameslahm/SCPNet.\n","authors":["Zixuan Ding","Ao Wang","Hui Chen","Qiang Zhang","Pengzhang Liu","Yongjun Bao","Weipeng Yan","Jungong Han"],"pdf_url":"https://arxiv.org/pdf/2303.13223v2.pdf","comment":"Accepted by IEEE/CVF Conference on Computer Vision and Pattern\n  Recognition (CVPR) 2023"},{"id":"http://arxiv.org/abs/2303.13796v1","updated":"2023-03-24T04:22:41Z","published":"2023-03-24T04:22:41Z","title":"Zolly: Zoom Focal Length Correctly for Perspective-Distorted Human Mesh\n  Reconstruction","summary":"  As it is hard to calibrate single-view RGB images in the wild, existing 3D\nhuman mesh reconstruction (3DHMR) methods either use a constant large focal\nlength or estimate one based on the background environment context, which can\nnot tackle the problem of the torso, limb, hand or face distortion caused by\nperspective camera projection when the camera is close to the human body. The\nnaive focal length assumptions can harm this task with the incorrectly\nformulated projection matrices. To solve this, we propose Zolly, the first\n3DHMR method focusing on perspective-distorted images. Our approach begins with\nanalysing the reason for perspective distortion, which we find is mainly caused\nby the relative location of the human body to the camera center. We propose a\nnew camera model and a novel 2D representation, termed distortion image, which\ndescribes the 2D dense distortion scale of the human body. We then estimate the\ndistance from distortion scale features rather than environment context\nfeatures. Afterwards, we integrate the distortion feature with image features\nto reconstruct the body mesh. To formulate the correct projection matrix and\nlocate the human body position, we simultaneously use perspective and\nweak-perspective projection loss. Since existing datasets could not handle this\ntask, we propose the first synthetic dataset PDHuman and extend two real-world\ndatasets tailored for this task, all containing perspective-distorted human\nimages. Extensive experiments show that Zolly outperforms existing\nstate-of-the-art methods on both perspective-distorted datasets and the\nstandard benchmark (3DPW).\n","authors":["Wenjia Wang","Yongtao Ge","Haiyi Mei","Zhongang Cai","Qingping Sun","Yanjun Wang","Chunhua Shen","Lei Yang","Taku Komura"],"pdf_url":"https://arxiv.org/pdf/2303.13796v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.13794v1","updated":"2023-03-24T04:18:13Z","published":"2023-03-24T04:18:13Z","title":"Efficient and Accurate Co-Visible Region Localization with Matching\n  Key-Points Crop (MKPC): A Two-Stage Pipeline for Enhancing Image Matching\n  Performance","summary":"  Image matching is a classic and fundamental task in computer vision. In this\npaper, under the hypothesis that the areas outside the co-visible regions carry\nlittle information, we propose a matching key-points crop (MKPC) algorithm. The\nMKPC locates, proposes and crops the critical regions, which are the co-visible\nareas with great efficiency and accuracy. Furthermore, building upon MKPC, we\npropose a general two-stage pipeline for image matching, which is compatible to\nany image matching models or combinations. We experimented with plugging\nSuperPoint + SuperGlue into the two-stage pipeline, whose results show that our\nmethod enhances the performance for outdoor pose estimations. What's more, in a\nfair comparative condition, our method outperforms the SOTA on Image Matching\nChallenge 2022 Benchmark, which represents the hardest outdoor benchmark of\nimage matching currently.\n","authors":["Hongjian Song","Yuki Kashiwaba","Shuai Wu","Canming Wang"],"pdf_url":"https://arxiv.org/pdf/2303.13794v1.pdf","comment":"9 pages with 6 figures. Many experiments have not yet been conducted,\n  the theoretical sections are rather concise, and the references are not\n  adequately comprehensive. This version of the paper is being released to make\n  this work public, and code will also be published soon. We will continue to\n  conduct additional experiments and periodically update the paper"},{"id":"http://arxiv.org/abs/2303.13791v1","updated":"2023-03-24T04:03:55Z","published":"2023-03-24T04:03:55Z","title":"Progressively Optimized Local Radiance Fields for Robust View Synthesis","summary":"  We present an algorithm for reconstructing the radiance field of a\nlarge-scale scene from a single casually captured video. The task poses two\ncore challenges. First, most existing radiance field reconstruction approaches\nrely on accurate pre-estimated camera poses from Structure-from-Motion\nalgorithms, which frequently fail on in-the-wild videos. Second, using a\nsingle, global radiance field with finite representational capacity does not\nscale to longer trajectories in an unbounded scene. For handling unknown poses,\nwe jointly estimate the camera poses with radiance field in a progressive\nmanner. We show that progressive optimization significantly improves the\nrobustness of the reconstruction. For handling large unbounded scenes, we\ndynamically allocate new local radiance fields trained with frames within a\ntemporal window. This further improves robustness (e.g., performs well even\nunder moderate pose drifts) and allows us to scale to large scenes. Our\nextensive evaluation on the Tanks and Temples dataset and our collected outdoor\ndataset, Static Hikes, show that our approach compares favorably with the\nstate-of-the-art.\n","authors":["Andreas Meuleman","Yu-Lun Liu","Chen Gao","Jia-Bin Huang","Changil Kim","Min H. Kim","Johannes Kopf"],"pdf_url":"https://arxiv.org/pdf/2303.13791v1.pdf","comment":"Project page: https://localrf.github.io/"},{"id":"http://arxiv.org/abs/2303.13788v1","updated":"2023-03-24T03:57:21Z","published":"2023-03-24T03:57:21Z","title":"Application-Driven AI Paradigm for Person Counting in Various Scenarios","summary":"  Person counting is considered as a fundamental task in video surveillance.\nHowever, the scenario diversity in practical applications makes it difficult to\nexploit a single person counting model for general use. Consequently, engineers\nmust preview the video stream and manually specify an appropriate person\ncounting model based on the scenario of camera shot, which is time-consuming,\nespecially for large-scale deployments. In this paper, we propose a person\ncounting paradigm that utilizes a scenario classifier to automatically select a\nsuitable person counting model for each captured frame. First, the input image\nis passed through the scenario classifier to obtain a scenario label, which is\nthen used to allocate the frame to one of five fine-tuned models for person\ncounting. Additionally, we present five augmentation datasets collected from\ndifferent scenarios, including side-view, long-shot, top-view, customized and\ncrowd, which are also integrated to form a scenario classification dataset\ncontaining 26323 samples. In our comparative experiments, the proposed paradigm\nachieves better balance than any single model on the integrated dataset, thus\nits generalization in various scenarios has been proved.\n","authors":["Minjie Hua","Yibing Nan","Shiguo Lian"],"pdf_url":"https://arxiv.org/pdf/2303.13788v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.09875v2","updated":"2023-03-24T03:56:20Z","published":"2023-03-17T10:42:05Z","title":"A Dynamic Multi-Scale Voxel Flow Network for Video Prediction","summary":"  The performance of video prediction has been greatly boosted by advanced deep\nneural networks. However, most of the current methods suffer from large model\nsizes and require extra inputs, e.g., semantic/depth maps, for promising\nperformance. For efficiency consideration, in this paper, we propose a Dynamic\nMulti-scale Voxel Flow Network (DMVFN) to achieve better video prediction\nperformance at lower computational costs with only RGB images, than previous\nmethods. The core of our DMVFN is a differentiable routing module that can\neffectively perceive the motion scales of video frames. Once trained, our DMVFN\nselects adaptive sub-networks for different inputs at the inference stage.\nExperiments on several benchmarks demonstrate that our DMVFN is an order of\nmagnitude faster than Deep Voxel Flow and surpasses the state-of-the-art\niterative-based OPT on generated image quality. Our code and demo are available\nat https://huxiaotaostasy.github.io/DMVFN/.\n","authors":["Xiaotao Hu","Zhewei Huang","Ailin Huang","Jun Xu","Shuchang Zhou"],"pdf_url":"https://arxiv.org/pdf/2303.09875v2.pdf","comment":"CVPR 2023"},{"id":"http://arxiv.org/abs/2303.13110v2","updated":"2023-03-24T03:55:50Z","published":"2023-03-23T08:57:11Z","title":"OCELOT: Overlapped Cell on Tissue Dataset for Histopathology","summary":"  Cell detection is a fundamental task in computational pathology that can be\nused for extracting high-level medical information from whole-slide images. For\naccurate cell detection, pathologists often zoom out to understand the\ntissue-level structures and zoom in to classify cells based on their morphology\nand the surrounding context. However, there is a lack of efforts to reflect\nsuch behaviors by pathologists in the cell detection models, mainly due to the\nlack of datasets containing both cell and tissue annotations with overlapping\nregions. To overcome this limitation, we propose and publicly release OCELOT, a\ndataset purposely dedicated to the study of cell-tissue relationships for cell\ndetection in histopathology. OCELOT provides overlapping cell and tissue\nannotations on images acquired from multiple organs. Within this setting, we\nalso propose multi-task learning approaches that benefit from learning both\ncell and tissue tasks simultaneously. When compared against a model trained\nonly for the cell detection task, our proposed approaches improve cell\ndetection performance on 3 datasets: proposed OCELOT, public TIGER, and\ninternal CARP datasets. On the OCELOT test set in particular, we show up to\n6.79 improvement in F1-score. We believe the contributions of this paper,\nincluding the release of the OCELOT dataset at\nhttps://lunit-io.github.io/research/publications/ocelot are a crucial starting\npoint toward the important research direction of incorporating cell-tissue\nrelationships in computation pathology.\n","authors":["Jeongun Ryu","Aaron Valero Puche","JaeWoong Shin","Seonwook Park","Biagio Brattoli","Jinhee Lee","Wonkyung Jung","Soo Ick Cho","Kyunghyun Paeng","Chan-Young Ock","Donggeun Yoo","Sérgio Pereira"],"pdf_url":"https://arxiv.org/pdf/2303.13110v2.pdf","comment":"Accepted for publication at CVPR'23"},{"id":"http://arxiv.org/abs/2303.13779v1","updated":"2023-03-24T03:34:33Z","published":"2023-03-24T03:34:33Z","title":"Exploiting Unlabelled Photos for Stronger Fine-Grained SBIR","summary":"  This paper advances the fine-grained sketch-based image retrieval (FG-SBIR)\nliterature by putting forward a strong baseline that overshoots prior\nstate-of-the-arts by ~11%. This is not via complicated design though, but by\naddressing two critical issues facing the community (i) the gold standard\ntriplet loss does not enforce holistic latent space geometry, and (ii) there\nare never enough sketches to train a high accuracy model. For the former, we\npropose a simple modification to the standard triplet loss, that explicitly\nenforces separation amongst photos/sketch instances. For the latter, we put\nforward a novel knowledge distillation module can leverage photo data for model\ntraining. Both modules are then plugged into a novel plug-n-playable training\nparadigm that allows for more stable training. More specifically, for (i) we\nemploy an intra-modal triplet loss amongst sketches to bring sketches of the\nsame instance closer from others, and one more amongst photos to push away\ndifferent photo instances while bringing closer a structurally augmented\nversion of the same photo (offering a gain of ~4-6%). To tackle (ii), we first\npre-train a teacher on the large set of unlabelled photos over the\naforementioned intra-modal photo triplet loss. Then we distill the contextual\nsimilarity present amongst the instances in the teacher's embedding space to\nthat in the student's embedding space, by matching the distribution over\ninter-feature distances of respective samples in both embedding spaces\n(delivering a further gain of ~4-5%). Apart from outperforming prior arts\nsignificantly, our model also yields satisfactory results on generalising to\nnew classes. Project page: https://aneeshan95.github.io/Sketch_PVT/\n","authors":["Aneeshan Sain","Ayan Kumar Bhunia","Subhadeep Koley","Pinaki Nath Chowdhury","Soumitri Chattopadhyay","Tao Xiang","Yi-Zhe Song"],"pdf_url":"https://arxiv.org/pdf/2303.13779v1.pdf","comment":"Accepted in CVPR 2023. Project page available at\n  https://aneeshan95.github.io/Sketch_PVT/"},{"id":"http://arxiv.org/abs/2303.02700v2","updated":"2023-03-24T03:34:25Z","published":"2023-03-05T15:28:13Z","title":"HairStep: Transfer Synthetic to Real Using Strand and Depth Maps for\n  Single-View 3D Hair Modeling","summary":"  In this work, we tackle the challenging problem of learning-based single-view\n3D hair modeling. Due to the great difficulty of collecting paired real image\nand 3D hair data, using synthetic data to provide prior knowledge for real\ndomain becomes a leading solution. This unfortunately introduces the challenge\nof domain gap. Due to the inherent difficulty of realistic hair rendering,\nexisting methods typically use orientation maps instead of hair images as input\nto bridge the gap. We firmly think an intermediate representation is essential,\nbut we argue that orientation map using the dominant filtering-based methods is\nsensitive to uncertain noise and far from a competent representation. Thus, we\nfirst raise this issue up and propose a novel intermediate representation,\ntermed as HairStep, which consists of a strand map and a depth map. It is found\nthat HairStep not only provides sufficient information for accurate 3D hair\nmodeling, but also is feasible to be inferred from real images. Specifically,\nwe collect a dataset of 1,250 portrait images with two types of annotations. A\nlearning framework is further designed to transfer real images to the strand\nmap and depth map. It is noted that, an extra bonus of our new dataset is the\nfirst quantitative metric for 3D hair modeling. Our experiments show that\nHairStep narrows the domain gap between synthetic and real and achieves\nstate-of-the-art performance on single-view 3D hair reconstruction.\n","authors":["Yujian Zheng","Zirong Jin","Moran Li","Haibin Huang","Chongyang Ma","Shuguang Cui","Xiaoguang Han"],"pdf_url":"https://arxiv.org/pdf/2303.02700v2.pdf","comment":"CVPR 2023 Highlight, project page:\n  https://paulyzheng.github.io/research/hairstep/"},{"id":"http://arxiv.org/abs/2303.13777v1","updated":"2023-03-24T03:32:02Z","published":"2023-03-24T03:32:02Z","title":"GM-NeRF: Learning Generalizable Model-based Neural Radiance Fields from\n  Multi-view Images","summary":"  In this work, we focus on synthesizing high-fidelity novel view images for\narbitrary human performers, given a set of sparse multi-view images. It is a\nchallenging task due to the large variation among articulated body poses and\nheavy self-occlusions. To alleviate this, we introduce an effective\ngeneralizable framework Generalizable Model-based Neural Radiance Fields\n(GM-NeRF) to synthesize free-viewpoint images. Specifically, we propose a\ngeometry-guided attention mechanism to register the appearance code from\nmulti-view 2D images to a geometry proxy which can alleviate the misalignment\nbetween inaccurate geometry prior and pixel space. On top of that, we further\nconduct neural rendering and partial gradient backpropagation for efficient\nperceptual supervision and improvement of the perceptual quality of synthesis.\nTo evaluate our method, we conduct experiments on synthesized datasets\nTHuman2.0 and Multi-garment, and real-world datasets Genebody and ZJUMocap. The\nresults demonstrate that our approach outperforms state-of-the-art methods in\nterms of novel view synthesis and geometric reconstruction.\n","authors":["Jianchuan Chen","Wentao Yi","Liqian Ma","Xu Jia","Huchuan Lu"],"pdf_url":"https://arxiv.org/pdf/2303.13777v1.pdf","comment":"Accepted at CVPR 2023"},{"id":"http://arxiv.org/abs/2302.03298v3","updated":"2023-03-24T03:08:25Z","published":"2023-02-07T07:13:53Z","title":"Diversity is Definitely Needed: Improving Model-Agnostic Zero-shot\n  Classification via Stable Diffusion","summary":"  In this work, we investigate the problem of Model-Agnostic Zero-Shot\nClassification (MA-ZSC), which refers to training non-specific classification\narchitectures (downstream models) to classify real images without using any\nreal images during training. Recent research has demonstrated that generating\nsynthetic training images using diffusion models provides a potential solution\nto address MA-ZSC. However, the performance of this approach currently falls\nshort of that achieved by large-scale vision-language models. One possible\nexplanation is a potential significant domain gap between synthetic and real\nimages. Our work offers a fresh perspective on the problem by providing initial\ninsights that MA-ZSC performance can be improved by improving the diversity of\nimages in the generated dataset. We propose a set of modifications to the\ntext-to-image generation process using a pre-trained diffusion model to enhance\ndiversity, which we refer to as our $\\textbf{bag of tricks}$. Our approach\nshows notable improvements in various classification architectures, with\nresults comparable to state-of-the-art models such as CLIP. To validate our\napproach, we conduct experiments on CIFAR10, CIFAR100, and EuroSAT, which is\nparticularly difficult for zero-shot classification due to its satellite image\ndomain. We evaluate our approach with five classification architectures,\nincluding ResNet and ViT. Our findings provide initial insights into the\nproblem of MA-ZSC using diffusion models. All code will be available on GitHub.\n","authors":["Jordan Shipard","Arnold Wiliem","Kien Nguyen Thanh","Wei Xiang","Clinton Fookes"],"pdf_url":"https://arxiv.org/pdf/2302.03298v3.pdf","comment":"(10 pages, 6 figures, 3 tables, preprint)"},{"id":"http://arxiv.org/abs/2303.13440v2","updated":"2023-03-24T03:05:23Z","published":"2023-03-23T17:02:00Z","title":"CLIP for All Things Zero-Shot Sketch-Based Image Retrieval, Fine-Grained\n  or Not","summary":"  In this paper, we leverage CLIP for zero-shot sketch based image retrieval\n(ZS-SBIR). We are largely inspired by recent advances on foundation models and\nthe unparalleled generalisation ability they seem to offer, but for the first\ntime tailor it to benefit the sketch community. We put forward novel designs on\nhow best to achieve this synergy, for both the category setting and the\nfine-grained setting (\"all\"). At the very core of our solution is a prompt\nlearning setup. First we show just via factoring in sketch-specific prompts, we\nalready have a category-level ZS-SBIR system that overshoots all prior arts, by\na large margin (24.8%) - a great testimony on studying the CLIP and ZS-SBIR\nsynergy. Moving onto the fine-grained setup is however trickier, and requires a\ndeeper dive into this synergy. For that, we come up with two specific designs\nto tackle the fine-grained matching nature of the problem: (i) an additional\nregularisation loss to ensure the relative separation between sketches and\nphotos is uniform across categories, which is not the case for the gold\nstandard standalone triplet loss, and (ii) a clever patch shuffling technique\nto help establishing instance-level structural correspondences between\nsketch-photo pairs. With these designs, we again observe significant\nperformance gains in the region of 26.9% over previous state-of-the-art. The\ntake-home message, if any, is the proposed CLIP and prompt learning paradigm\ncarries great promise in tackling other sketch-related tasks (not limited to\nZS-SBIR) where data scarcity remains a great challenge. Project page:\nhttps://aneeshan95.github.io/Sketch_LVM/\n","authors":["Aneeshan Sain","Ayan Kumar Bhunia","Pinaki Nath Chowdhury","Subhadeep Koley","Tao Xiang","Yi-Zhe Song"],"pdf_url":"https://arxiv.org/pdf/2303.13440v2.pdf","comment":"Accepted in CVPR 2023. Project page available at\n  https://aneeshan95.github.io/Sketch_LVM/"},{"id":"http://arxiv.org/abs/2303.13769v1","updated":"2023-03-24T02:59:44Z","published":"2023-03-24T02:59:44Z","title":"Unknown Sniffer for Object Detection: Don't Turn a Blind Eye to Unknown\n  Objects","summary":"  The recently proposed open-world object and open-set detection achieve a\nbreakthrough in finding never-seen-before objects and distinguishing them from\nclass-known ones. However, their studies on knowledge transfer from known\nclasses to unknown ones need to be deeper, leading to the scanty capability for\ndetecting unknowns hidden in the background. In this paper, we propose the\nunknown sniffer (UnSniffer) to find both unknown and known objects. Firstly,\nthe generalized object confidence (GOC) score is introduced, which only uses\nclass-known samples for supervision and avoids improper suppression of unknowns\nin the background. Significantly, such confidence score learned from\nclass-known objects can be generalized to unknown ones. Additionally, we\npropose a negative energy suppression loss to further limit the non-object\nsamples in the background. Next, the best box of each unknown is hard to obtain\nduring inference due to lacking their semantic information in training. To\nsolve this issue, we introduce a graph-based determination scheme to replace\nhand-designed non-maximum suppression (NMS) post-processing. Finally, we\npresent the Unknown Object Detection Benchmark, the first publicly benchmark\nthat encompasses precision evaluation for unknown object detection to our\nknowledge. Experiments show that our method is far better than the existing\nstate-of-the-art methods. Code is available at:\nhttps://github.com/Went-Liang/UnSniffer.\n","authors":["Wenteng Liang","Feng Xue","Yihao Liu","Guofeng Zhong","Anlong Ming"],"pdf_url":"https://arxiv.org/pdf/2303.13769v1.pdf","comment":"CVPR 2023 camera-ready"},{"id":"http://arxiv.org/abs/2207.01971v5","updated":"2023-03-24T02:50:08Z","published":"2022-07-05T11:30:37Z","title":"DualAfford: Learning Collaborative Visual Affordance for Dual-gripper\n  Object Manipulation","summary":"  It is essential yet challenging for future home-assistant robots to\nunderstand and manipulate diverse 3D objects in daily human environments.\nTowards building scalable systems that can perform diverse manipulation tasks\nover various 3D shapes, recent works have advocated and demonstrated promising\nresults learning visual actionable affordance, which labels every point over\nthe input 3D geometry with an action likelihood of accomplishing the downstream\ntask (e.g., pushing or picking-up). However, these works only studied\nsingle-gripper manipulation tasks, yet many real-world tasks require two hands\nto achieve collaboratively. In this work, we propose a novel learning\nframework, DualAfford, to learn collaborative affordance for dual-gripper\nmanipulation tasks. The core design of the approach is to reduce the quadratic\nproblem for two grippers into two disentangled yet interconnected subtasks for\nefficient learning. Using the large-scale PartNet-Mobility and ShapeNet\ndatasets, we set up four benchmark tasks for dual-gripper manipulation.\nExperiments prove the effectiveness and superiority of our method over three\nbaselines.\n","authors":["Yan Zhao","Ruihai Wu","Zhehuan Chen","Yourong Zhang","Qingnan Fan","Kaichun Mo","Hao Dong"],"pdf_url":"https://arxiv.org/pdf/2207.01971v5.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.13767v1","updated":"2023-03-24T02:42:16Z","published":"2023-03-24T02:42:16Z","title":"Learning Spatial-Temporal Implicit Neural Representations for\n  Event-Guided Video Super-Resolution","summary":"  Event cameras sense the intensity changes asynchronously and produce event\nstreams with high dynamic range and low latency. This has inspired research\nendeavors utilizing events to guide the challenging video superresolution (VSR)\ntask. In this paper, we make the first attempt to address a novel problem of\nachieving VSR at random scales by taking advantages of the high temporal\nresolution property of events. This is hampered by the difficulties of\nrepresenting the spatial-temporal information of events when guiding VSR. To\nthis end, we propose a novel framework that incorporates the spatial-temporal\ninterpolation of events to VSR in a unified framework. Our key idea is to learn\nimplicit neural representations from queried spatial-temporal coordinates and\nfeatures from both RGB frames and events. Our method contains three parts.\nSpecifically, the Spatial-Temporal Fusion (STF) module first learns the 3D\nfeatures from events and RGB frames. Then, the Temporal Filter (TF) module\nunlocks more explicit motion information from the events near the queried\ntimestamp and generates the 2D features. Lastly, the SpatialTemporal Implicit\nRepresentation (STIR) module recovers the SR frame in arbitrary resolutions\nfrom the outputs of these two modules. In addition, we collect a real-world\ndataset with spatially aligned events and RGB frames. Extensive experiments\nshow that our method significantly surpasses the prior-arts and achieves VSR\nwith random scales, e.g., 6.5. Code and dataset are available at https:\n//vlis2022.github.io/cvpr23/egvsr.\n","authors":["Yunfan Lu","Zipeng Wang","Minjie Liu","Hongjian Wang","Lin Wang"],"pdf_url":"https://arxiv.org/pdf/2303.13767v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.13212v2","updated":"2023-03-24T02:40:47Z","published":"2023-03-23T12:13:29Z","title":"A Simple and Generic Framework for Feature Distillation via Channel-wise\n  Transformation","summary":"  Knowledge distillation is a popular technique for transferring the knowledge\nfrom a large teacher model to a smaller student model by mimicking. However,\ndistillation by directly aligning the feature maps between teacher and student\nmay enforce overly strict constraints on the student thus degrade the\nperformance of the student model. To alleviate the above feature misalignment\nissue, existing works mainly focus on spatially aligning the feature maps of\nthe teacher and the student, with pixel-wise transformation. In this paper, we\nnewly find that aligning the feature maps between teacher and student along the\nchannel-wise dimension is also effective for addressing the feature\nmisalignment issue. Specifically, we propose a learnable nonlinear channel-wise\ntransformation to align the features of the student and the teacher model.\nBased on it, we further propose a simple and generic framework for feature\ndistillation, with only one hyper-parameter to balance the distillation loss\nand the task specific loss. Extensive experimental results show that our method\nachieves significant performance improvements in various computer vision tasks\nincluding image classification (+3.28% top-1 accuracy for MobileNetV1 on\nImageNet-1K), object detection (+3.9% bbox mAP for ResNet50-based Faster-RCNN\non MS COCO), instance segmentation (+2.8% Mask mAP for ResNet50-based\nMask-RCNN), and semantic segmentation (+4.66% mIoU for ResNet18-based PSPNet in\nsemantic segmentation on Cityscapes), which demonstrates the effectiveness and\nthe versatility of the proposed method. The code will be made publicly\navailable.\n","authors":["Ziwei Liu","Yongtao Wang","Xiaojie Chu"],"pdf_url":"https://arxiv.org/pdf/2303.13212v2.pdf","comment":"13 pages"},{"id":"http://arxiv.org/abs/2303.13764v1","updated":"2023-03-24T02:33:45Z","published":"2023-03-24T02:33:45Z","title":"GQE-Net: A Graph-based Quality Enhancement Network for Point Cloud Color\n  Attribute","summary":"  In recent years, point clouds have become increasingly popular for\nrepresenting three-dimensional (3D) visual objects and scenes. To efficiently\nstore and transmit point clouds, compression methods have been developed, but\nthey often result in a degradation of quality. To reduce color distortion in\npoint clouds, we propose a graph-based quality enhancement network (GQE-Net)\nthat uses geometry information as an auxiliary input and graph convolution\nblocks to extract local features efficiently. Specifically, we use a\nparallel-serial graph attention module with a multi-head graph attention\nmechanism to focus on important points or features and help them fuse together.\nAdditionally, we design a feature refinement module that takes into account the\nnormals and geometry distance between points. To work within the limitations of\nGPU memory capacity, the distorted point cloud is divided into overlap-allowed\n3D patches, which are sent to GQE-Net for quality enhancement. To account for\ndifferences in data distribution among different color omponents, three models\nare trained for the three color components. Experimental results show that our\nmethod achieves state-of-the-art performance. For example, when implementing\nGQE-Net on the recent G-PCC coding standard test model, 0.43 dB, 0.25 dB, and\n0.36 dB Bjontegaard delta (BD)-peak-signal-to-noise ratio (PSNR), corresponding\nto 14.0%, 9.3%, and 14.5% BD-rate savings can be achieved on dense point clouds\nfor the Y, Cb, and Cr components, respectively.\n","authors":["Jinrui Xing","Hui Yuan","Raouf Hamzaoui","Hao Liu","Junhui Hou"],"pdf_url":"https://arxiv.org/pdf/2303.13764v1.pdf","comment":"13 pages, 11 figures, submitted to IEEE TIP"},{"id":"http://arxiv.org/abs/2303.13761v1","updated":"2023-03-24T02:17:51Z","published":"2023-03-24T02:17:51Z","title":"Unsupervised Hierarchical Domain Adaptation for Adverse Weather Optical\n  Flow","summary":"  Optical flow estimation has made great progress, but usually suffers from\ndegradation under adverse weather. Although semi/full-supervised methods have\nmade good attempts, the domain shift between the synthetic and real adverse\nweather images would deteriorate their performance. To alleviate this issue,\nour start point is to unsupervisedly transfer the knowledge from source clean\ndomain to target degraded domain. Our key insight is that adverse weather does\nnot change the intrinsic optical flow of the scene, but causes a significant\ndifference for the warp error between clean and degraded images. In this work,\nwe propose the first unsupervised framework for adverse weather optical flow\nvia hierarchical motion-boundary adaptation. Specifically, we first employ\nimage translation to construct the transformation relationship between clean\nand degraded domains. In motion adaptation, we utilize the flow consistency\nknowledge to align the cross-domain optical flows into a motion-invariance\ncommon space, where the optical flow from clean weather is used as the\nguidance-knowledge to obtain a preliminary optical flow for adverse weather.\nFurthermore, we leverage the warp error inconsistency which measures the motion\nmisalignment of the boundary between the clean and degraded domains, and\npropose a joint intra- and inter-scene boundary contrastive adaptation to\nrefine the motion boundary. The hierarchical motion and boundary adaptation\njointly promotes optical flow in a unified framework. Extensive quantitative\nand qualitative experiments have been performed to verify the superiority of\nthe proposed method.\n","authors":["Hanyu Zhou","Yi Chang","Gang Chen","Luxin Yan"],"pdf_url":"https://arxiv.org/pdf/2303.13761v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2112.05282v3","updated":"2023-03-24T02:16:07Z","published":"2021-12-10T01:25:24Z","title":"RamBoAttack: A Robust Query Efficient Deep Neural Network Decision\n  Exploit","summary":"  Machine learning models are critically susceptible to evasion attacks from\nadversarial examples. Generally, adversarial examples, modified inputs\ndeceptively similar to the original input, are constructed under whitebox\nsettings by adversaries with full access to the model. However, recent attacks\nhave shown a remarkable reduction in query numbers to craft adversarial\nexamples using blackbox attacks. Particularly, alarming is the ability to\nexploit the classification decision from the access interface of a trained\nmodel provided by a growing number of Machine Learning as a Service providers\nincluding Google, Microsoft, IBM and used by a plethora of applications\nincorporating these models. The ability of an adversary to exploit only the\npredicted label from a model to craft adversarial examples is distinguished as\na decision-based attack. In our study, we first deep dive into recent\nstate-of-the-art decision-based attacks in ICLR and SP to highlight the costly\nnature of discovering low distortion adversarial employing gradient estimation\nmethods. We develop a robust query efficient attack capable of avoiding\nentrapment in a local minimum and misdirection from noisy gradients seen in\ngradient estimation methods. The attack method we propose, RamBoAttack,\nexploits the notion of Randomized Block Coordinate Descent to explore the\nhidden classifier manifold, targeting perturbations to manipulate only\nlocalized input features to address the issues of gradient estimation methods.\nImportantly, the RamBoAttack is more robust to the different sample inputs\navailable to an adversary and the targeted class. Overall, for a given target\nclass, RamBoAttack is demonstrated to be more robust at achieving a lower\ndistortion within a given query budget. We curate our extensive results using\nthe large-scale high-resolution ImageNet dataset and open-source our attack,\ntest samples and artifacts on GitHub.\n","authors":["Viet Quoc Vo","Ehsan Abbasnejad","Damith C. Ranasinghe"],"pdf_url":"https://arxiv.org/pdf/2112.05282v3.pdf","comment":"Published in Network and Distributed System Security (NDSS) Symposium\n  2022. Code is available at https://ramboattack.github.io/"},{"id":"http://arxiv.org/abs/2303.13756v1","updated":"2023-03-24T02:12:29Z","published":"2023-03-24T02:12:29Z","title":"GP-VTON: Towards General Purpose Virtual Try-on via Collaborative\n  Local-Flow Global-Parsing Learning","summary":"  Image-based Virtual Try-ON aims to transfer an in-shop garment onto a\nspecific person. Existing methods employ a global warping module to model the\nanisotropic deformation for different garment parts, which fails to preserve\nthe semantic information of different parts when receiving challenging inputs\n(e.g, intricate human poses, difficult garments). Moreover, most of them\ndirectly warp the input garment to align with the boundary of the preserved\nregion, which usually requires texture squeezing to meet the boundary shape\nconstraint and thus leads to texture distortion. The above inferior performance\nhinders existing methods from real-world applications. To address these\nproblems and take a step towards real-world virtual try-on, we propose a\nGeneral-Purpose Virtual Try-ON framework, named GP-VTON, by developing an\ninnovative Local-Flow Global-Parsing (LFGP) warping module and a Dynamic\nGradient Truncation (DGT) training strategy. Specifically, compared with the\nprevious global warping mechanism, LFGP employs local flows to warp garments\nparts individually, and assembles the local warped results via the global\ngarment parsing, resulting in reasonable warped parts and a semantic-correct\nintact garment even with challenging inputs.On the other hand, our DGT training\nstrategy dynamically truncates the gradient in the overlap area and the warped\ngarment is no more required to meet the boundary constraint, which effectively\navoids the texture squeezing problem. Furthermore, our GP-VTON can be easily\nextended to multi-category scenario and jointly trained by using data from\ndifferent garment categories. Extensive experiments on two high-resolution\nbenchmarks demonstrate our superiority over the existing state-of-the-art\nmethods.\n","authors":["Zhenyu Xie","Zaiyu Huang","Xin Dong","Fuwei Zhao","Haoye Dong","Xijin Zhang","Feida Zhu","Xiaodan Liang"],"pdf_url":"https://arxiv.org/pdf/2303.13756v1.pdf","comment":"8 pages, 8 figures, The IEEE/CVF Computer Vision and Pattern\n  Recognition Conference (CVPR)"},{"id":"http://arxiv.org/abs/2303.13755v1","updated":"2023-03-24T02:12:28Z","published":"2023-03-24T02:12:28Z","title":"Sparsifiner: Learning Sparse Instance-Dependent Attention for Efficient\n  Vision Transformers","summary":"  Vision Transformers (ViT) have shown their competitive advantages\nperformance-wise compared to convolutional neural networks (CNNs) though they\noften come with high computational costs. To this end, previous methods explore\ndifferent attention patterns by limiting a fixed number of spatially nearby\ntokens to accelerate the ViT's multi-head self-attention (MHSA) operations.\nHowever, such structured attention patterns limit the token-to-token\nconnections to their spatial relevance, which disregards learned semantic\nconnections from a full attention mask. In this work, we propose a novel\napproach to learn instance-dependent attention patterns, by devising a\nlightweight connectivity predictor module to estimate the connectivity score of\neach pair of tokens. Intuitively, two tokens have high connectivity scores if\nthe features are considered relevant either spatially or semantically. As each\ntoken only attends to a small number of other tokens, the binarized\nconnectivity masks are often very sparse by nature and therefore provide the\nopportunity to accelerate the network via sparse computations. Equipped with\nthe learned unstructured attention pattern, sparse attention ViT (Sparsifiner)\nproduces a superior Pareto-optimal trade-off between FLOPs and top-1 accuracy\non ImageNet compared to token sparsity. Our method reduces 48% to 69% FLOPs of\nMHSA while the accuracy drop is within 0.4%. We also show that combining\nattention and token sparsity reduces ViT FLOPs by over 60%.\n","authors":["Cong Wei","Brendan Duke","Ruowei Jiang","Parham Aarabi","Graham W. Taylor","Florian Shkurti"],"pdf_url":"https://arxiv.org/pdf/2303.13755v1.pdf","comment":"Accepted at CVPR 2023"},{"id":"http://arxiv.org/abs/2202.00091v2","updated":"2023-03-24T02:12:06Z","published":"2022-01-31T21:10:47Z","title":"Query Efficient Decision Based Sparse Attacks Against Black-Box Deep\n  Learning Models","summary":"  Despite our best efforts, deep learning models remain highly vulnerable to\neven tiny adversarial perturbations applied to the inputs. The ability to\nextract information from solely the output of a machine learning model to craft\nadversarial perturbations to black-box models is a practical threat against\nreal-world systems, such as autonomous cars or machine learning models exposed\nas a service (MLaaS). Of particular interest are sparse attacks. The\nrealization of sparse attacks in black-box models demonstrates that machine\nlearning models are more vulnerable than we believe. Because these attacks aim\nto minimize the number of perturbed pixels measured by l_0 norm-required to\nmislead a model by solely observing the decision (the predicted label) returned\nto a model query; the so-called decision-based attack setting. But, such an\nattack leads to an NP-hard optimization problem. We develop an evolution-based\nalgorithm-SparseEvo-for the problem and evaluate against both convolutional\ndeep neural networks and vision transformers. Notably, vision transformers are\nyet to be investigated under a decision-based attack setting. SparseEvo\nrequires significantly fewer model queries than the state-of-the-art sparse\nattack Pointwise for both untargeted and targeted attacks. The attack\nalgorithm, although conceptually simple, is also competitive with only a\nlimited query budget against the state-of-the-art gradient-based whitebox\nattacks in standard computer vision tasks such as ImageNet. Importantly, the\nquery efficient SparseEvo, along with decision-based attacks, in general, raise\nnew questions regarding the safety of deployed systems and poses new directions\nto study and understand the robustness of machine learning models.\n","authors":["Viet Quoc Vo","Ehsan Abbasnejad","Damith C. Ranasinghe"],"pdf_url":"https://arxiv.org/pdf/2202.00091v2.pdf","comment":"Published as a conference paper at the International Conference on\n  Learning Representations (ICLR 2022). Code is available at\n  https://sparseevoattack.github.io/"}],"Information Retrieval":[{"id":"http://arxiv.org/abs/2303.13284v2","updated":"2023-03-24T14:59:15Z","published":"2023-03-23T14:06:26Z","title":"GETT-QA: Graph Embedding based T2T Transformer for Knowledge Graph\n  Question Answering","summary":"  In this work, we present an end-to-end Knowledge Graph Question Answering\n(KGQA) system named GETT-QA. GETT-QA uses T5, a popular text-to-text\npre-trained language model. The model takes a question in natural language as\ninput and produces a simpler form of the intended SPARQL query. In the simpler\nform, the model does not directly produce entity and relation IDs. Instead, it\nproduces corresponding entity and relation labels. The labels are grounded to\nKG entity and relation IDs in a subsequent step. To further improve the\nresults, we instruct the model to produce a truncated version of the KG\nembedding for each entity. The truncated KG embedding enables a finer search\nfor disambiguation purposes. We find that T5 is able to learn the truncated KG\nembeddings without any change of loss function, improving KGQA performance. As\na result, we report strong results for LC-QuAD 2.0 and SimpleQuestions-Wikidata\ndatasets on end-to-end KGQA over Wikidata.\n","authors":["Debayan Banerjee","Pranav Ajit Nair","Ricardo Usbeck","Chris Biemann"],"pdf_url":"https://arxiv.org/pdf/2303.13284v2.pdf","comment":"16 pages single column format accepted at ESWC 2023 research track"},{"id":"http://arxiv.org/abs/2212.02726v2","updated":"2023-03-24T14:26:38Z","published":"2022-12-06T03:28:09Z","title":"Dataset vs Reality: Understanding Model Performance from the Perspective\n  of Information Need","summary":"  Deep learning technologies have brought us many models that outperform human\nbeings on a few benchmarks. An interesting question is: can these models well\nsolve real-world problems with similar settings (e.g., identical input/output)\nto the benchmark datasets? We argue that a model is trained to answer the same\ninformation need for which the training dataset is created. Although some\ndatasets may share high structural similarities, e.g., question-answer pairs\nfor the question answering (QA) task and image-caption pairs for the image\ncaptioning (IC) task, they may represent different research tasks aiming for\nanswering different information needs. To support our argument, we use the QA\ntask and IC task as two case studies and compare their widely used benchmark\ndatasets. From the perspective of information need in the context of\ninformation retrieval, we show the differences in the dataset creation\nprocesses, and the differences in morphosyntactic properties between datasets.\nThe differences in these datasets can be attributed to the different\ninformation needs of the specific research tasks. We encourage all researchers\nto consider the information need the perspective of a research task before\nutilizing a dataset to train a model. Likewise, while creating a dataset,\nresearchers may also incorporate the information need perspective as a factor\nto determine the degree to which the dataset accurately reflects the research\ntask they intend to tackle.\n","authors":["Mengying Yu","Aixin Sun"],"pdf_url":"https://arxiv.org/pdf/2212.02726v2.pdf","comment":"19 pages, 5 figures"},{"id":"http://arxiv.org/abs/2301.10410v3","updated":"2023-03-24T10:51:05Z","published":"2023-01-25T05:16:43Z","title":"One Model for All Domains: Collaborative Domain-Prefix Tuning for\n  Cross-Domain NER","summary":"  Cross-domain NER is a challenging task to address the low-resource problem in\npractical scenarios. Previous typical solutions mainly obtain a NER model by\npre-trained language models (PLMs) with data from a rich-resource domain and\nadapt it to the target domain. Owing to the mismatch issue among entity types\nin different domains, previous approaches normally tune all parameters of PLMs,\nending up with an entirely new NER model for each domain. Moreover, current\nmodels only focus on leveraging knowledge in one general source domain while\nfailing to successfully transfer knowledge from multiple sources to the target.\nTo address these issues, we introduce Collaborative Domain-Prefix Tuning for\ncross-domain NER (CP-NER) based on text-to-text generative PLMs. Specifically,\nwe present text-to-text generation grounding domain-related instructors to\ntransfer knowledge to new domain NER tasks without structural modifications. We\nutilize frozen PLMs and conduct collaborative domain-prefix tuning to stimulate\nthe potential of PLMs to handle NER tasks across various domains. Experimental\nresults on the Cross-NER benchmark show that the proposed approach has flexible\ntransfer ability and performs better on both one-source and multiple-source\ncross-domain NER tasks. Codes will be available in\nhttps://github.com/zjunlp/DeepKE/tree/main/example/ner/cross.\n","authors":["Xiang Chen","Lei Li","Shuofei Qiao","Ningyu Zhang","Chuanqi Tan","Yong Jiang","Fei Huang","Huajun Chen"],"pdf_url":"https://arxiv.org/pdf/2301.10410v3.pdf","comment":"Work in progress"},{"id":"http://arxiv.org/abs/2303.13835v1","updated":"2023-03-24T06:50:22Z","published":"2023-03-24T06:50:22Z","title":"Where to Go Next for Recommender Systems? ID- vs. Modality-based\n  recommender models revisited","summary":"  Recommendation models that utilize unique identities (IDs) to represent\ndistinct users and items have been state-of-the-art (SOTA) and dominated the\nrecommender systems (RS) literature for over a decade. Meanwhile, the\npre-trained modality encoders, such as BERT and ViT, have become increasingly\npowerful in modeling the raw modality features of an item, such as text and\nimages. Given this, a natural question arises: can a purely modality-based\nrecommendation model (MoRec) outperforms or matches a pure ID-based model\n(IDRec) by replacing the itemID embedding with a SOTA modality encoder? In\nfact, this question was answered ten years ago when IDRec beats MoRec by a\nstrong margin in both recommendation accuracy and efficiency. We aim to revisit\nthis `old' question and systematically study MoRec from several aspects.\nSpecifically, we study several sub-questions: (i) which recommendation\nparadigm, MoRec or IDRec, performs better in practical scenarios, especially in\nthe general setting and warm item scenarios where IDRec has a strong advantage?\ndoes this hold for items with different modality features? (ii) can the latest\ntechnical advances from other communities (i.e., natural language processing\nand computer vision) translate into accuracy improvement for MoRec? (iii) how\nto effectively utilize item modality representation, can we use it directly or\ndo we have to adjust it with new data? (iv) are there some key challenges for\nMoRec to be solved in practical applications? To answer them, we conduct\nrigorous experiments for item recommendations with two popular modalities,\ni.e., text and vision. We provide the first empirical evidence that MoRec is\nalready comparable to its IDRec counterpart with an expensive end-to-end\ntraining method, even for warm item recommendation. Our results potentially\nimply that the dominance of IDRec in the RS field may be greatly challenged in\nthe future.\n","authors":["Zheng Yuan","Fajie Yuan","Yu Song","Youhua Li","Junchen Fu","Fei Yang","Yunzhu Pan","Yongxin Ni"],"pdf_url":"https://arxiv.org/pdf/2303.13835v1.pdf","comment":null}],"Machine Learning":[{"id":"http://arxiv.org/abs/2303.13511v2","updated":"2023-03-24T17:59:21Z","published":"2023-03-23T17:59:10Z","title":"Neural Preset for Color Style Transfer","summary":"  In this paper, we present a Neural Preset technique to address the\nlimitations of existing color style transfer methods, including visual\nartifacts, vast memory requirement, and slow style switching speed. Our method\nis based on two core designs. First, we propose Deterministic Neural Color\nMapping (DNCM) to consistently operate on each pixel via an image-adaptive\ncolor mapping matrix, avoiding artifacts and supporting high-resolution inputs\nwith a small memory footprint. Second, we develop a two-stage pipeline by\ndividing the task into color normalization and stylization, which allows\nefficient style switching by extracting color styles as presets and reusing\nthem on normalized input images. Due to the unavailability of pairwise\ndatasets, we describe how to train Neural Preset via a self-supervised\nstrategy. Various advantages of Neural Preset over existing methods are\ndemonstrated through comprehensive evaluations. Notably, Neural Preset enables\nstable 4K color style transfer in real-time without artifacts. Besides, we show\nthat our trained model can naturally support multiple applications without\nfine-tuning, including low-light image enhancement, underwater image\ncorrection, image dehazing, and image harmonization. Project page with demos:\nhttps://zhkkke.github.io/NeuralPreset .\n","authors":["Zhanghan Ke","Yuhao Liu","Lei Zhu","Nanxuan Zhao","Rynson W. H. Lau"],"pdf_url":"https://arxiv.org/pdf/2303.13511v2.pdf","comment":"Project page with demos: https://zhkkke.github.io/NeuralPreset .\n  Artifact-free real-time 4K color style transfer via AI-generated presets.\n  CVPR 2023"},{"id":"http://arxiv.org/abs/2210.14891v9","updated":"2023-03-24T17:56:23Z","published":"2022-10-26T17:45:01Z","title":"Broken Neural Scaling Laws","summary":"  We present a smoothly broken power law functional form (referred to by us as\na Broken Neural Scaling Law (BNSL)) that accurately models and extrapolates the\nscaling behaviors of deep neural networks (i.e. how the evaluation metric of\ninterest varies as the amount of compute used for training, number of model\nparameters, training dataset size, model input size, number of training steps,\nor upstream performance varies) for various architectures and for each of\nvarious tasks within a large and diverse set of upstream and downstream tasks,\nin zero-shot, prompted, and fine-tuned settings. This set includes large-scale\nvision, language, audio, video, diffusion, generative modeling, multimodal\nlearning, contrastive learning, AI alignment, robotics, out-of-distribution\n(OOD) generalization, continual learning, transfer learning, uncertainty\nestimation / calibration, out-of-distribution detection, adversarial\nrobustness, distillation, sparsity, retrieval, quantization, pruning,\nmolecules, computer programming/coding, math word problems, arithmetic,\nunsupervised/self-supervised learning, and reinforcement learning (single agent\nand multi-agent). When compared to other functional forms for neural scaling\nbehavior, this functional form yields extrapolations of scaling behavior that\nare considerably more accurate on this set. Moreover, this functional form\naccurately models and extrapolates scaling behavior that other functional forms\nare incapable of expressing such as the non-monotonic transitions present in\nthe scaling behavior of phenomena such as double descent and the delayed, sharp\ninflection points (often called \"emergent phase transitions\") present in the\nscaling behavior of tasks such as arithmetic. Lastly, we use this functional\nform to glean insights about the limit of the predictability of scaling\nbehavior. Code is available at\nhttps://github.com/ethancaballero/broken_neural_scaling_laws\n","authors":["Ethan Caballero","Kshitij Gupta","Irina Rish","David Krueger"],"pdf_url":"https://arxiv.org/pdf/2210.14891v9.pdf","comment":"Published as a conference paper at ICLR 2023"},{"id":"http://arxiv.org/abs/2303.14186v1","updated":"2023-03-24T17:56:22Z","published":"2023-03-24T17:56:22Z","title":"TRAK: Attributing Model Behavior at Scale","summary":"  The goal of data attribution is to trace model predictions back to training\ndata. Despite a long line of work towards this goal, existing approaches to\ndata attribution tend to force users to choose between computational\ntractability and efficacy. That is, computationally tractable methods can\nstruggle with accurately attributing model predictions in non-convex settings\n(e.g., in the context of deep neural networks), while methods that are\neffective in such regimes require training thousands of models, which makes\nthem impractical for large models or datasets.\n  In this work, we introduce TRAK (Tracing with the Randomly-projected After\nKernel), a data attribution method that is both effective and computationally\ntractable for large-scale, differentiable models. In particular, by leveraging\nonly a handful of trained models, TRAK can match the performance of attribution\nmethods that require training thousands of models. We demonstrate the utility\nof TRAK across various modalities and scales: image classifiers trained on\nImageNet, vision-language models (CLIP), and language models (BERT and mT5). We\nprovide code for using TRAK (and reproducing our work) at\nhttps://github.com/MadryLab/trak .\n","authors":["Sung Min Park","Kristian Georgiev","Andrew Ilyas","Guillaume Leclerc","Aleksander Madry"],"pdf_url":"https://arxiv.org/pdf/2303.14186v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2301.06907v2","updated":"2023-03-24T17:47:43Z","published":"2023-01-17T14:18:17Z","title":"Deep Conditional Measure Quantization","summary":"  Quantization of a probability measure means representing it with a finite set\nof Dirac masses that approximates the input distribution well enough (in some\nmetric space of probability measures). Various methods exists to do so, but the\nsituation of quantizing a conditional law has been less explored. We propose a\nmethod, called DCMQ, involving a Huber-energy kernel-based approach coupled\nwith a deep neural network architecture. The method is tested on several\nexamples and obtains promising results.\n","authors":["Gabriel Turinici"],"pdf_url":"https://arxiv.org/pdf/2301.06907v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2212.12380v2","updated":"2023-03-24T17:46:32Z","published":"2022-12-23T14:53:09Z","title":"Towards Scalable Physically Consistent Neural Networks: an Application\n  to Data-driven Multi-zone Thermal Building Models","summary":"  With more and more data being collected, data-driven modeling methods have\nbeen gaining in popularity in recent years. While physically sound, classical\ngray-box models are often cumbersome to identify and scale, and their accuracy\nmight be hindered by their limited expressiveness. On the other hand, classical\nblack-box methods, typically relying on Neural Networks (NNs) nowadays, often\nachieve impressive performance, even at scale, by deriving statistical patterns\nfrom data. However, they remain completely oblivious to the underlying physical\nlaws, which may lead to potentially catastrophic failures if decisions for\nreal-world physical systems are based on them. Physically Consistent Neural\nNetworks (PCNNs) were recently developed to address these aforementioned\nissues, ensuring physical consistency while still leveraging NNs to attain\nstate-of-the-art accuracy.\n  In this work, we scale PCNNs to model building temperature dynamics and\npropose a thorough comparison with classical gray-box and black-box methods.\nMore precisely, we design three distinct PCNN extensions, thereby exemplifying\nthe modularity and flexibility of the architecture, and formally prove their\nphysical consistency. In the presented case study, PCNNs are shown to achieve\nstate-of-the-art accuracy, even outperforming classical NN-based models despite\ntheir constrained structure. Our investigations furthermore provide a clear\nillustration of NNs achieving seemingly good performance while remaining\ncompletely physics-agnostic, which can be misleading in practice. While this\nperformance comes at the cost of computational complexity, PCNNs on the other\nhand show accuracy improvements of 17-35% compared to all other physically\nconsistent methods, paving the way for scalable physically consistent models\nwith state-of-the-art performance.\n","authors":["Loris Di Natale","Bratislav Svetozarevic","Philipp Heer","Colin Neil Jones"],"pdf_url":"https://arxiv.org/pdf/2212.12380v2.pdf","comment":"Submitted to Applied Energy"},{"id":"http://arxiv.org/abs/2303.14173v1","updated":"2023-03-24T17:36:15Z","published":"2023-03-24T17:36:15Z","title":"How many dimensions are required to find an adversarial example?","summary":"  Past work exploring adversarial vulnerability have focused on situations\nwhere an adversary can perturb all dimensions of model input. On the other\nhand, a range of recent works consider the case where either (i) an adversary\ncan perturb a limited number of input parameters or (ii) a subset of modalities\nin a multimodal problem. In both of these cases, adversarial examples are\neffectively constrained to a subspace $V$ in the ambient input space\n$\\mathcal{X}$. Motivated by this, in this work we investigate how adversarial\nvulnerability depends on $\\dim(V)$. In particular, we show that the adversarial\nsuccess of standard PGD attacks with $\\ell^p$ norm constraints behaves like a\nmonotonically increasing function of $\\epsilon (\\frac{\\dim(V)}{\\dim\n\\mathcal{X}})^{\\frac{1}{q}}$ where $\\epsilon$ is the perturbation budget and\n$\\frac{1}{p} + \\frac{1}{q} =1$, provided $p > 1$ (the case $p=1$ presents\nadditional subtleties which we analyze in some detail). This functional form\ncan be easily derived from a simple toy linear model, and as such our results\nland further credence to arguments that adversarial examples are endemic to\nlocally linear models on high dimensional spaces.\n","authors":["Charles Godfrey","Henry Kvinge","Elise Bishoff","Myles Mckay","Davis Brown","Tim Doster","Eleanor Byler"],"pdf_url":"https://arxiv.org/pdf/2303.14173v1.pdf","comment":"Comments welcome!"},{"id":"http://arxiv.org/abs/2205.14258v5","updated":"2023-03-24T17:25:53Z","published":"2022-05-27T22:29:08Z","title":"On the Symmetries of Deep Learning Models and their Internal\n  Representations","summary":"  Symmetry is a fundamental tool in the exploration of a broad range of complex\nsystems. In machine learning symmetry has been explored in both models and\ndata. In this paper we seek to connect the symmetries arising from the\narchitecture of a family of models with the symmetries of that family's\ninternal representation of data. We do this by calculating a set of fundamental\nsymmetry groups, which we call the intertwiner groups of the model. We connect\nintertwiner groups to a model's internal representations of data through a\nrange of experiments that probe similarities between hidden states across\nmodels with the same architecture. Our work suggests that the symmetries of a\nnetwork are propagated into the symmetries in that network's representation of\ndata, providing us with a better understanding of how architecture affects the\nlearning and prediction process. Finally, we speculate that for ReLU networks,\nthe intertwiner groups may provide a justification for the common practice of\nconcentrating model interpretability exploration on the activation basis in\nhidden layers rather than arbitrary linear combinations thereof.\n","authors":["Charles Godfrey","Davis Brown","Tegan Emerson","Henry Kvinge"],"pdf_url":"https://arxiv.org/pdf/2205.14258v5.pdf","comment":"CG and DB contributed equally. V2: clarified relationship between\n  $\\mu_{\\mathrm{CKA}}$ and existing instances of CKA. V3: more experiments,\n  alternative stitching capacity comparison, GeLU intertwiner group. V4: minor\n  typo corrections. V4: failure of PSD property for max kernel used in\n  $\\mu_{\\mathrm{CKA}}$ (thanks to Derek Lim)"},{"id":"http://arxiv.org/abs/2303.14157v1","updated":"2023-03-24T17:12:38Z","published":"2023-03-24T17:12:38Z","title":"Efficient Scale-Invariant Generator with Column-Row Entangled Pixel\n  Synthesis","summary":"  Any-scale image synthesis offers an efficient and scalable solution to\nsynthesize photo-realistic images at any scale, even going beyond 2K\nresolution. However, existing GAN-based solutions depend excessively on\nconvolutions and a hierarchical architecture, which introduce inconsistency and\nthe $``$texture sticking$\"$ issue when scaling the output resolution. From\nanother perspective, INR-based generators are scale-equivariant by design, but\ntheir huge memory footprint and slow inference hinder these networks from being\nadopted in large-scale or real-time systems. In this work, we propose\n$\\textbf{C}$olumn-$\\textbf{R}$ow $\\textbf{E}$ntangled $\\textbf{P}$ixel\n$\\textbf{S}$ynthesis ($\\textbf{CREPS}$), a new generative model that is both\nefficient and scale-equivariant without using any spatial convolutions or\ncoarse-to-fine design. To save memory footprint and make the system scalable,\nwe employ a novel bi-line representation that decomposes layer-wise feature\nmaps into separate $``$thick$\"$ column and row encodings. Experiments on\nvarious datasets, including FFHQ, LSUN-Church, MetFaces, and Flickr-Scenery,\nconfirm CREPS' ability to synthesize scale-consistent and alias-free images at\nany arbitrary resolution with proper training and inference speed. Code is\navailable at https://github.com/VinAIResearch/CREPS.\n","authors":["Thuan Hoang Nguyen","Thanh Van Le","Anh Tran"],"pdf_url":"https://arxiv.org/pdf/2303.14157v1.pdf","comment":"Accepted to CVPR 2023"},{"id":"http://arxiv.org/abs/2303.14153v1","updated":"2023-03-24T17:04:26Z","published":"2023-03-24T17:04:26Z","title":"Local Contrastive Learning for Medical Image Recognition","summary":"  The proliferation of Deep Learning (DL)-based methods for radiographic image\nanalysis has created a great demand for expert-labeled radiology data. Recent\nself-supervised frameworks have alleviated the need for expert labeling by\nobtaining supervision from associated radiology reports. These frameworks,\nhowever, struggle to distinguish the subtle differences between different\npathologies in medical images. Additionally, many of them do not provide\ninterpretation between image regions and text, making it difficult for\nradiologists to assess model predictions. In this work, we propose Local Region\nContrastive Learning (LRCLR), a flexible fine-tuning framework that adds layers\nfor significant image region selection as well as cross-modality interaction.\nOur results on an external validation set of chest x-rays suggest that LRCLR\nidentifies significant local image regions and provides meaningful\ninterpretation against radiology text while improving zero-shot performance on\nseveral chest x-ray medical findings.\n","authors":["S. A. Rizvi","R. Tang","X. Jiang","X. Ma","X. Hu"],"pdf_url":"https://arxiv.org/pdf/2303.14153v1.pdf","comment":"10 pages, 5 figures, 1 table, AMIA conference submission"},{"id":"http://arxiv.org/abs/2303.14151v1","updated":"2023-03-24T17:03:40Z","published":"2023-03-24T17:03:40Z","title":"Double Descent Demystified: Identifying, Interpreting & Ablating the\n  Sources of a Deep Learning Puzzle","summary":"  Double descent is a surprising phenomenon in machine learning, in which as\nthe number of model parameters grows relative to the number of data, test error\ndrops as models grow ever larger into the highly overparameterized (data\nundersampled) regime. This drop in test error flies against classical learning\ntheory on overfitting and has arguably underpinned the success of large models\nin machine learning. This non-monotonic behavior of test loss depends on the\nnumber of data, the dimensionality of the data and the number of model\nparameters. Here, we briefly describe double descent, then provide an\nexplanation of why double descent occurs in an informal and approachable\nmanner, requiring only familiarity with linear algebra and introductory\nprobability. We provide visual intuition using polynomial regression, then\nmathematically analyze double descent with ordinary linear regression and\nidentify three interpretable factors that, when simultaneously all present,\ntogether create double descent. We demonstrate that double descent occurs on\nreal data when using ordinary linear regression, then demonstrate that double\ndescent does not occur when any of the three factors are ablated. We use this\nunderstanding to shed light on recent observations in nonlinear models\nconcerning superposition and double descent. Code is publicly available.\n","authors":["Rylan Schaeffer","Mikail Khona","Zachary Robertson","Akhilan Boopathy","Kateryna Pistunova","Jason W. Rocks","Ila Rani Fiete","Oluwasanmi Koyejo"],"pdf_url":"https://arxiv.org/pdf/2303.14151v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2211.14306v2","updated":"2023-03-24T16:56:25Z","published":"2022-11-25T18:59:10Z","title":"RUST: Latent Neural Scene Representations from Unposed Imagery","summary":"  Inferring the structure of 3D scenes from 2D observations is a fundamental\nchallenge in computer vision. Recently popularized approaches based on neural\nscene representations have achieved tremendous impact and have been applied\nacross a variety of applications. One of the major remaining challenges in this\nspace is training a single model which can provide latent representations which\neffectively generalize beyond a single scene. Scene Representation Transformer\n(SRT) has shown promise in this direction, but scaling it to a larger set of\ndiverse scenes is challenging and necessitates accurately posed ground truth\ndata. To address this problem, we propose RUST (Really Unposed Scene\nrepresentation Transformer), a pose-free approach to novel view synthesis\ntrained on RGB images alone. Our main insight is that one can train a Pose\nEncoder that peeks at the target image and learns a latent pose embedding which\nis used by the decoder for view synthesis. We perform an empirical\ninvestigation into the learned latent pose structure and show that it allows\nmeaningful test-time camera transformations and accurate explicit pose\nreadouts. Perhaps surprisingly, RUST achieves similar quality as methods which\nhave access to perfect camera pose, thereby unlocking the potential for\nlarge-scale training of amortized neural scene representations.\n","authors":["Mehdi S. M. Sajjadi","Aravindh Mahendran","Thomas Kipf","Etienne Pot","Daniel Duckworth","Mario Lucic","Klaus Greff"],"pdf_url":"https://arxiv.org/pdf/2211.14306v2.pdf","comment":"CVPR 2023 Highlight. Project website: https://rust-paper.github.io/"},{"id":"http://arxiv.org/abs/2303.14126v1","updated":"2023-03-24T16:33:06Z","published":"2023-03-24T16:33:06Z","title":"CIFAKE: Image Classification and Explainable Identification of\n  AI-Generated Synthetic Images","summary":"  Recent technological advances in synthetic data have enabled the generation\nof images with such high quality that human beings cannot tell the difference\nbetween real-life photographs and Artificial Intelligence (AI) generated\nimages. Given the critical necessity of data reliability and authentication,\nthis article proposes to enhance our ability to recognise AI-generated images\nthrough computer vision. Initially, a synthetic dataset is generated that\nmirrors the ten classes of the already available CIFAR-10 dataset with latent\ndiffusion which provides a contrasting set of images for comparison to real\nphotographs. The model is capable of generating complex visual attributes, such\nas photorealistic reflections in water. The two sets of data present as a\nbinary classification problem with regard to whether the photograph is real or\ngenerated by AI. This study then proposes the use of a Convolutional Neural\nNetwork (CNN) to classify the images into two categories; Real or Fake.\nFollowing hyperparameter tuning and the training of 36 individual network\ntopologies, the optimal approach could correctly classify the images with\n92.98% accuracy. Finally, this study implements explainable AI via Gradient\nClass Activation Mapping to explore which features within the images are useful\nfor classification. Interpretation reveals interesting concepts within the\nimage, in particular, noting that the actual entity itself does not hold useful\ninformation for classification; instead, the model focuses on small visual\nimperfections in the background of the images. The complete dataset engineered\nfor this study, referred to as the CIFAKE dataset, is made publicly available\nto the research community for future work.\n","authors":["Jordan J. Bird","Ahmad Lotfi"],"pdf_url":"https://arxiv.org/pdf/2303.14126v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.14116v1","updated":"2023-03-24T16:24:08Z","published":"2023-03-24T16:24:08Z","title":"Improving Prediction Performance and Model Interpretability through\n  Attention Mechanisms from Basic and Applied Research Perspectives","summary":"  With the dramatic advances in deep learning technology, machine learning\nresearch is focusing on improving the interpretability of model predictions as\nwell as prediction performance in both basic and applied research. While deep\nlearning models have much higher prediction performance than traditional\nmachine learning models, the specific prediction process is still difficult to\ninterpret and/or explain. This is known as the black-boxing of machine learning\nmodels and is recognized as a particularly important problem in a wide range of\nresearch fields, including manufacturing, commerce, robotics, and other\nindustries where the use of such technology has become commonplace, as well as\nthe medical field, where mistakes are not tolerated. This bulletin is based on\nthe summary of the author's dissertation. The research summarized in the\ndissertation focuses on the attention mechanism, which has been the focus of\nmuch attention in recent years, and discusses its potential for both basic\nresearch in terms of improving prediction performance and interpretability, and\napplied research in terms of evaluating it for real-world applications using\nlarge data sets beyond the laboratory environment. The dissertation also\nconcludes with a summary of the implications of these findings for subsequent\nresearch and future prospects in the field.\n","authors":["Shunsuke Kitada"],"pdf_url":"https://arxiv.org/pdf/2303.14116v1.pdf","comment":"The bulletin of Graduate School of Science and Engineering, Hosei\n  University, Vol.64 (03/2023). This article draws heavily from\n  arxiv:2009.12064, arxiv:2104.08763, arxiv:1905.07289, and arxiv:2204.11588"},{"id":"http://arxiv.org/abs/2108.09805v2","updated":"2023-03-24T16:19:20Z","published":"2021-08-22T18:01:39Z","title":"Efficient Algorithms for Learning from Coarse Labels","summary":"  For many learning problems one may not have access to fine grained label\ninformation; e.g., an image can be labeled as husky, dog, or even animal\ndepending on the expertise of the annotator. In this work, we formalize these\nsettings and study the problem of learning from such coarse data. Instead of\nobserving the actual labels from a set $\\mathcal{Z}$, we observe coarse labels\ncorresponding to a partition of $\\mathcal{Z}$ (or a mixture of partitions).\n  Our main algorithmic result is that essentially any problem learnable from\nfine grained labels can also be learned efficiently when the coarse data are\nsufficiently informative. We obtain our result through a generic reduction for\nanswering Statistical Queries (SQ) over fine grained labels given only coarse\nlabels. The number of coarse labels required depends polynomially on the\ninformation distortion due to coarsening and the number of fine labels\n$|\\mathcal{Z}|$.\n  We also investigate the case of (infinitely many) real valued labels focusing\non a central problem in censored and truncated statistics: Gaussian mean\nestimation from coarse data. We provide an efficient algorithm when the sets in\nthe partition are convex and establish that the problem is NP-hard even for\nvery simple non-convex sets.\n","authors":["Dimitris Fotakis","Alkis Kalavasis","Vasilis Kontonis","Christos Tzamos"],"pdf_url":"https://arxiv.org/pdf/2108.09805v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.14111v1","updated":"2023-03-24T16:19:15Z","published":"2023-03-24T16:19:15Z","title":"Interpretable Anomaly Detection via Discrete Optimization","summary":"  Anomaly detection is essential in many application domains, such as cyber\nsecurity, law enforcement, medicine, and fraud protection. However, the\ndecision-making of current deep learning approaches is notoriously hard to\nunderstand, which often limits their practical applicability. To overcome this\nlimitation, we propose a framework for learning inherently interpretable\nanomaly detectors from sequential data. More specifically, we consider the task\nof learning a deterministic finite automaton (DFA) from a given multi-set of\nunlabeled sequences. We show that this problem is computationally hard and\ndevelop two learning algorithms based on constraint optimization. Moreover, we\nintroduce novel regularization schemes for our optimization problems that\nimprove the overall interpretability of our DFAs. Using a prototype\nimplementation, we demonstrate that our approach shows promising results in\nterms of accuracy and F1 score.\n","authors":["Simon Lutz","Florian Wittbold","Simon Dierl","Benedikt Böing","Falk Howar","Barbara König","Emmanuel Müller","Daniel Neider"],"pdf_url":"https://arxiv.org/pdf/2303.14111v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2212.14268v2","updated":"2023-03-24T16:10:35Z","published":"2022-12-29T11:42:46Z","title":"Detection of out-of-distribution samples using binary neuron activation\n  patterns","summary":"  Deep neural networks (DNN) have outstanding performance in various\napplications. Despite numerous efforts of the research community,\nout-of-distribution (OOD) samples remain a significant limitation of DNN\nclassifiers. The ability to identify previously unseen inputs as novel is\ncrucial in safety-critical applications such as self-driving cars, unmanned\naerial vehicles, and robots. Existing approaches to detect OOD samples treat a\nDNN as a black box and evaluate the confidence score of the output predictions.\nUnfortunately, this method frequently fails, because DNNs are not trained to\nreduce their confidence for OOD inputs. In this work, we introduce a novel\nmethod for OOD detection. Our method is motivated by theoretical analysis of\nneuron activation patterns (NAP) in ReLU-based architectures. The proposed\nmethod does not introduce a high computational overhead due to the binary\nrepresentation of the activation patterns extracted from convolutional layers.\nThe extensive empirical evaluation proves its high performance on various DNN\narchitectures and seven image datasets.\n","authors":["Bartlomiej Olber","Krystian Radlak","Adam Popowicz","Michal Szczepankiewicz","Krystian Chachuła"],"pdf_url":"https://arxiv.org/pdf/2212.14268v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.11602v2","updated":"2023-03-24T16:06:14Z","published":"2023-03-21T05:41:24Z","title":"Convergence of stochastic gradient descent on parameterized sphere with\n  applications to variational Monte Carlo simulation","summary":"  We analyze stochastic gradient descent (SGD) type algorithms on a\nhigh-dimensional sphere which is parameterized by a neural network up to a\nnormalization constant. We provide a new algorithm for the setting of\nsupervised learning and show its convergence both theoretically and\nnumerically. We also provide the first proof of convergence for the\nunsupervised setting, which corresponds to the widely used variational Monte\nCarlo (VMC) method in quantum physics.\n","authors":["Nilin Abrahamsen","Zhiyan Ding","Gil Goldshlager","Lin Lin"],"pdf_url":"https://arxiv.org/pdf/2303.11602v2.pdf","comment":"Added reference to recent (concurrent) proof of VMC convergence [Li\n  et al. arXiv:2303.10599]"},{"id":"http://arxiv.org/abs/2303.14096v1","updated":"2023-03-24T16:03:21Z","published":"2023-03-24T16:03:21Z","title":"Enhancing Multiple Reliability Measures via Nuisance-extended\n  Information Bottleneck","summary":"  In practical scenarios where training data is limited, many predictive\nsignals in the data can be rather from some biases in data acquisition (i.e.,\nless generalizable), so that one cannot prevent a model from co-adapting on\nsuch (so-called) \"shortcut\" signals: this makes the model fragile in various\ndistribution shifts. To bypass such failure modes, we consider an adversarial\nthreat model under a mutual information constraint to cover a wider class of\nperturbations in training. This motivates us to extend the standard information\nbottleneck to additionally model the nuisance information. We propose an\nautoencoder-based training to implement the objective, as well as practical\nencoder designs to facilitate the proposed hybrid discriminative-generative\ntraining concerning both convolutional- and Transformer-based architectures.\nOur experimental results show that the proposed scheme improves robustness of\nlearned representations (remarkably without using any domain-specific\nknowledge), with respect to multiple challenging reliability measures. For\nexample, our model could advance the state-of-the-art on a recent challenging\nOBJECTS benchmark in novelty detection by $78.4\\% \\rightarrow 87.2\\%$ in AUROC,\nwhile simultaneously enjoying improved corruption, background and (certified)\nadversarial robustness. Code is available at\nhttps://github.com/jh-jeong/nuisance_ib.\n","authors":["Jongheon Jeong","Sihyun Yu","Hankook Lee","Jinwoo Shin"],"pdf_url":"https://arxiv.org/pdf/2303.14096v1.pdf","comment":"25 pages; CVPR 2023"},{"id":"http://arxiv.org/abs/2303.14090v1","updated":"2023-03-24T15:55:09Z","published":"2023-03-24T15:55:09Z","title":"Physics-informed neural networks in the recreation of hydrodynamic\n  simulations from dark matter","summary":"  Physics-informed neural networks have emerged as a coherent framework for\nbuilding predictive models that combine statistical patterns with domain\nknowledge. The underlying notion is to enrich the optimization loss function\nwith known relationships to constrain the space of possible solutions.\nHydrodynamic simulations are a core constituent of modern cosmology, while the\nrequired computations are both expensive and time-consuming. At the same time,\nthe comparatively fast simulation of dark matter requires fewer resources,\nwhich has led to the emergence of machine learning algorithms for baryon\ninpainting as an active area of research; here, recreating the scatter found in\nhydrodynamic simulations is an ongoing challenge. This paper presents the first\napplication of physics-informed neural networks to baryon inpainting by\ncombining advances in neural network architectures with physical constraints,\ninjecting theory on baryon conversion efficiency into the model loss function.\nWe also introduce a punitive prediction comparison based on the\nKullback-Leibler divergence, which enforces scatter reproduction. By\nsimultaneously extracting the complete set of baryonic properties for the Simba\nsuite of cosmological simulations, our results demonstrate improved accuracy of\nbaryonic predictions based on dark matter halo properties, successful recovery\nof the fundamental metallicity relation, and retrieve scatter that traces the\ntarget simulation's distribution.\n","authors":["Zhenyu Dai","Ben Moews","Ricardo Vilalta","Romeel Dave"],"pdf_url":"https://arxiv.org/pdf/2303.14090v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.14084v1","updated":"2023-03-24T15:49:29Z","published":"2023-03-24T15:49:29Z","title":"Differentially Private Synthetic Control","summary":"  Synthetic control is a causal inference tool used to estimate the treatment\neffects of an intervention by creating synthetic counterfactual data. This\napproach combines measurements from other similar observations (i.e., donor\npool ) to predict a counterfactual time series of interest (i.e., target unit)\nby analyzing the relationship between the target and the donor pool before the\nintervention. As synthetic control tools are increasingly applied to sensitive\nor proprietary data, formal privacy protections are often required. In this\nwork, we provide the first algorithms for differentially private synthetic\ncontrol with explicit error bounds. Our approach builds upon tools from\nnon-private synthetic control and differentially private empirical risk\nminimization. We provide upper and lower bounds on the sensitivity of the\nsynthetic control query and provide explicit error bounds on the accuracy of\nour private synthetic control algorithms. We show that our algorithms produce\naccurate predictions for the target unit, and that the cost of privacy is\nsmall. Finally, we empirically evaluate the performance of our algorithm, and\nshow favorable performance in a variety of parameter regimes, as well as\nproviding guidance to practitioners for hyperparameter tuning.\n","authors":["Saeyoung Rho","Rachel Cummings","Vishal Misra"],"pdf_url":"https://arxiv.org/pdf/2303.14084v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.14083v1","updated":"2023-03-24T15:49:02Z","published":"2023-03-24T15:49:02Z","title":"Online Learning for the Random Feature Model in the Student-Teacher\n  Framework","summary":"  Deep neural networks are widely used prediction algorithms whose performance\noften improves as the number of weights increases, leading to\nover-parametrization. We consider a two-layered neural network whose first\nlayer is frozen while the last layer is trainable, known as the random feature\nmodel. We study over-parametrization in the context of a student-teacher\nframework by deriving a set of differential equations for the learning\ndynamics. For any finite ratio of hidden layer size and input dimension, the\nstudent cannot generalize perfectly, and we compute the non-zero asymptotic\ngeneralization error. Only when the student's hidden layer size is\nexponentially larger than the input dimension, an approach to perfect\ngeneralization is possible.\n","authors":["Roman Worschech","Bernd Rosenow"],"pdf_url":"https://arxiv.org/pdf/2303.14083v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.14077v1","updated":"2023-03-24T15:41:40Z","published":"2023-03-24T15:41:40Z","title":"Improved Adversarial Training Through Adaptive Instance-wise Loss\n  Smoothing","summary":"  Deep neural networks can be easily fooled into making incorrect predictions\nthrough corruption of the input by adversarial perturbations:\nhuman-imperceptible artificial noise. So far adversarial training has been the\nmost successful defense against such adversarial attacks. This work focuses on\nimproving adversarial training to boost adversarial robustness. We first\nanalyze, from an instance-wise perspective, how adversarial vulnerability\nevolves during adversarial training. We find that during training an overall\nreduction of adversarial loss is achieved by sacrificing a considerable\nproportion of training samples to be more vulnerable to adversarial attack,\nwhich results in an uneven distribution of adversarial vulnerability among\ndata. Such \"uneven vulnerability\", is prevalent across several popular robust\ntraining methods and, more importantly, relates to overfitting in adversarial\ntraining. Motivated by this observation, we propose a new adversarial training\nmethod: Instance-adaptive Smoothness Enhanced Adversarial Training (ISEAT). It\njointly smooths both input and weight loss landscapes in an adaptive,\ninstance-specific, way to enhance robustness more for those samples with higher\nadversarial vulnerability. Extensive experiments demonstrate the superiority of\nour method over existing defense methods. Noticeably, our method, when combined\nwith the latest data augmentation and semi-supervised learning techniques,\nachieves state-of-the-art robustness against $\\ell_{\\infty}$-norm constrained\nattacks on CIFAR10 of 59.32% for Wide ResNet34-10 without extra data, and\n61.55% for Wide ResNet28-10 with extra data. Code is available at\nhttps://github.com/TreeLLi/Instance-adaptive-Smoothness-Enhanced-AT.\n","authors":["Lin Li","Michael Spratling"],"pdf_url":"https://arxiv.org/pdf/2303.14077v1.pdf","comment":"12 pages, work in submission"},{"id":"http://arxiv.org/abs/2302.12002v2","updated":"2023-03-24T15:27:06Z","published":"2023-01-28T15:40:30Z","title":"Master's Thesis: Out-of-distribution Detection with Energy-based Models","summary":"  Today, deep learning is increasingly applied in security-critical situations\nsuch as autonomous driving and medical diagnosis. Despite its success, the\nbehavior and robustness of deep networks are not fully understood yet, posing a\nsignificant risk. In particular, researchers recently found that neural\nnetworks are overly confident in their predictions, even on data they have\nnever seen before. To tackle this issue, one can differentiate two approaches\nin the literature. One accounts for uncertainty in the predictions, while the\nsecond estimates the underlying density of the training data to decide whether\na given input is close to the training data, and thus the network is able to\nperform as expected.In this thesis, we investigate the capabilities of EBMs at\nthe task of fitting the training data distribution to perform detection of\nout-of-distribution (OOD) inputs. We find that on most datasets, EBMs do not\ninherently outperform other density estimators at detecting OOD data despite\ntheir flexibility. Thus, we additionally investigate the effects of\nsupervision, dimensionality reduction, and architectural modifications on the\nperformance of EBMs. Further, we propose Energy-Prior Network (EPN) which\nenables estimation of various uncertainties within an EBM for classification,\nbridging the gap between two approaches for tackling the OOD detection problem.\nWe identify a connection between the concentration parameters of the Dirichlet\ndistribution and the joint energy in an EBM. Additionally, this allows\noptimization without a held-out OOD dataset, which might not be available or\ncostly to collect in some applications. Finally, we empirically demonstrate\nthat Energy-Prior Network (EPN) is able to detect OOD inputs, datasets shifts,\nand adversarial examples. Theoretically, EPN offers favorable properties for\nthe asymptotic case when inputs are far from the training data.\n","authors":["Sven Elflein"],"pdf_url":"https://arxiv.org/pdf/2302.12002v2.pdf","comment":"Master's Thesis"},{"id":"http://arxiv.org/abs/2303.14068v1","updated":"2023-03-24T15:26:49Z","published":"2023-03-24T15:26:49Z","title":"A CNN-LSTM Architecture for Marine Vessel Track Association Using\n  Automatic Identification System (AIS) Data","summary":"  In marine surveillance, distinguishing between normal and anomalous vessel\nmovement patterns is critical for identifying potential threats in a timely\nmanner. Once detected, it is important to monitor and track these vessels until\na necessary intervention occurs. To achieve this, track association algorithms\nare used, which take sequential observations comprising geological and motion\nparameters of the vessels and associate them with respective vessels. The\nspatial and temporal variations inherent in these sequential observations make\nthe association task challenging for traditional multi-object tracking\nalgorithms. Additionally, the presence of overlapping tracks and missing data\ncan further complicate the trajectory tracking process. To address these\nchallenges, in this study, we approach this tracking task as a multivariate\ntime series problem and introduce a 1D CNN-LSTM architecture-based framework\nfor track association. This special neural network architecture can capture the\nspatial patterns as well as the long-term temporal relations that exist among\nthe sequential observations. During the training process, it learns and builds\nthe trajectory for each of these underlying vessels. Once trained, the proposed\nframework takes the marine vessel's location and motion data collected through\nthe Automatic Identification System (AIS) as input and returns the most likely\nvessel track as output in real-time. To evaluate the performance of our\napproach, we utilize an AIS dataset containing observations from 327 vessels\ntraveling in a specific geographic region. We measure the performance of our\nproposed framework using standard performance metrics such as accuracy,\nprecision, recall, and F1 score. When compared with other competitive neural\nnetwork architectures our approach demonstrates a superior tracking\nperformance.\n","authors":["Md Asif Bin Syed","Imtiaz Ahmed"],"pdf_url":"https://arxiv.org/pdf/2303.14068v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2203.09382v3","updated":"2023-03-24T15:18:21Z","published":"2022-03-17T15:18:34Z","title":"Euler State Networks: Non-dissipative Reservoir Computing","summary":"  Inspired by the numerical solution of ordinary differential equations, in\nthis paper we propose a novel Reservoir Computing (RC) model, called the Euler\nState Network (EuSN). The presented approach makes use of forward Euler\ndiscretization and antisymmetric recurrent matrices to design reservoir\ndynamics that are both stable and non-dissipative by construction.\n  Our mathematical analysis shows that the resulting model is biased towards a\nunitary effective spectral radius and zero local Lyapunov exponents,\nintrinsically operating near to the edge of stability. Experiments on long-term\nmemory tasks show the clear superiority of the proposed approach over standard\nRC models in problems requiring effective propagation of input information over\nmultiple time-steps. Furthermore, results on time-series classification\nbenchmarks indicate that EuSN is able to match (or even exceed) the accuracy of\ntrainable Recurrent Neural Networks, while retaining the training efficiency of\nthe RC family, resulting in up to $\\approx$ 490-fold savings in computation\ntime and $\\approx$ 1750-fold savings in energy consumption.\n","authors":["Claudio Gallicchio"],"pdf_url":"https://arxiv.org/pdf/2203.09382v3.pdf","comment":"paper submitted to journal"},{"id":"http://arxiv.org/abs/2103.00063v3","updated":"2023-03-24T15:05:00Z","published":"2021-02-26T21:59:29Z","title":"Local Clustering in Contextual Multi-Armed Bandits","summary":"  We study identifying user clusters in contextual multi-armed bandits (MAB).\nContextual MAB is an effective tool for many real applications, such as content\nrecommendation and online advertisement. In practice, user dependency plays an\nessential role in the user's actions, and thus the rewards. Clustering similar\nusers can improve the quality of reward estimation, which in turn leads to more\neffective content recommendation and targeted advertising. Different from\ntraditional clustering settings, we cluster users based on the unknown bandit\nparameters, which will be estimated incrementally. In particular, we define the\nproblem of cluster detection in contextual MAB, and propose a bandit algorithm,\nLOCB, embedded with local clustering procedure. And, we provide theoretical\nanalysis about LOCB in terms of the correctness and efficiency of clustering\nand its regret bound. Finally, we evaluate the proposed algorithm from various\naspects, which outperforms state-of-the-art baselines.\n","authors":["Yikun Ban","Jingrui He"],"pdf_url":"https://arxiv.org/pdf/2103.00063v3.pdf","comment":"13 pages"},{"id":"http://arxiv.org/abs/2303.11699v2","updated":"2023-03-24T15:01:24Z","published":"2023-03-21T09:37:29Z","title":"Neural networks trained on synthetically generated crystals can extract\n  structural information from ICSD powder X-ray diffractograms","summary":"  Machine learning techniques have successfully been used to extract structural\ninformation such as the crystal space group from powder X-ray diffractograms.\nHowever, training directly on simulated diffractograms from databases such as\nthe ICSD is challenging due to its limited size, class-inhomogeneity, and bias\ntoward certain structure types. We propose an alternative approach of\ngenerating synthetic crystals with random coordinates by using the symmetry\noperations of each space group. Based on this approach, we demonstrate online\ntraining of deep ResNet-like models on up to a few million unique on-the-fly\ngenerated synthetic diffractograms per hour. For our chosen task of space group\nclassification, we achieved a test accuracy of 79.9% on unseen ICSD structure\ntypes from most space groups. This surpasses the 56.1% accuracy of the current\nstate-of-the-art approach of training on ICSD crystals directly. Our results\ndemonstrate that synthetically generated crystals can be used to extract\nstructural information from ICSD powder diffractograms, which makes it possible\nto apply very large state-of-the-art machine learning models in the area of\npowder X-ray diffraction. We further show first steps toward applying our\nmethodology to experimental data, where automated XRD data analysis is crucial,\nespecially in high-throughput settings. While we focused on the prediction of\nthe space group, our approach has the potential to be extended to related tasks\nin the future.\n","authors":["Henrik Schopmans","Patrick Reiser","Pascal Friederich"],"pdf_url":"https://arxiv.org/pdf/2303.11699v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.14040v1","updated":"2023-03-24T14:51:06Z","published":"2023-03-24T14:51:06Z","title":"Euler Characteristic Tools For Topological Data Analysis","summary":"  In this article, we study Euler characteristic techniques in topological data\nanalysis. Pointwise computing the Euler characteristic of a family of\nsimplicial complexes built from data gives rise to the so-called Euler\ncharacteristic profile. We show that this simple descriptor achieve\nstate-of-the-art performance in supervised tasks at a very low computational\ncost. Inspired by signal analysis, we compute hybrid transforms of Euler\ncharacteristic profiles. These integral transforms mix Euler characteristic\ntechniques with Lebesgue integration to provide highly efficient compressors of\ntopological signals. As a consequence, they show remarkable performances in\nunsupervised settings. On the qualitative side, we provide numerous heuristics\non the topological and geometric information captured by Euler profiles and\ntheir hybrid transforms. Finally, we prove stability results for these\ndescriptors as well as asymptotic guarantees in random settings.\n","authors":["Olympio Hacquard","Vadim Lebovici"],"pdf_url":"https://arxiv.org/pdf/2303.14040v1.pdf","comment":"41 pages"},{"id":"http://arxiv.org/abs/2303.14029v1","updated":"2023-03-24T14:42:42Z","published":"2023-03-24T14:42:42Z","title":"PENTACET data -- 23 Million Contextual Code Comments and 500,000 SATD\n  comments","summary":"  Most Self-Admitted Technical Debt (SATD) research utilizes explicit SATD\nfeatures such as 'TODO' and 'FIXME' for SATD detection. A closer look reveals\nseveral SATD research uses simple SATD ('Easy to Find') code comments without\nthe contextual data (preceding and succeeding source code context). This work\naddresses this gap through PENTACET (or 5C dataset) data. PENTACET is a large\nCurated Contextual Code Comments per Contributor and the most extensive SATD\ndata. We mine 9,096 Open Source Software Java projects with a total of 435\nmillion LOC. The outcome is a dataset with 23 million code comments, preceding\nand succeeding source code context for each comment, and more than 500,000\ncomments labeled as SATD, including both 'Easy to Find' and 'Hard to Find'\nSATD. We believe PENTACET data will further SATD research using Artificial\nIntelligence techniques.\n","authors":["Murali Sridharan","Leevi Rantala","Mika Mäntylä"],"pdf_url":"https://arxiv.org/pdf/2303.14029v1.pdf","comment":"Accepted in MSR 2023 Tools and Data Showcase"},{"id":"http://arxiv.org/abs/2201.05158v2","updated":"2023-03-24T14:02:01Z","published":"2022-01-13T16:35:45Z","title":"Decompositional Quantum Graph Neural Network","summary":"  Quantum machine learning is a fast-emerging field that aims to tackle machine\nlearning using quantum algorithms and quantum computing. Due to the lack of\nphysical qubits and an effective means to map real-world data from Euclidean\nspace to Hilbert space, most of these methods focus on quantum analogies or\nprocess simulations rather than devising concrete architectures based on\nqubits. In this paper, we propose a novel hybrid quantum-classical algorithm\nfor graph-structured data, which we refer to as the Ego-graph based Quantum\nGraph Neural Network (egoQGNN). egoQGNN implements the GNN theoretical\nframework using the tensor product and unity matrix representation, which\ngreatly reduces the number of model parameters required. When controlled by a\nclassical computer, egoQGNN can accommodate arbitrarily sized graphs by\nprocessing ego-graphs from the input graph using a modestly-sized quantum\ndevice. The architecture is based on a novel mapping from real-world data to\nHilbert space. This mapping maintains the distance relations present in the\ndata and reduces information loss. Experimental results show that the proposed\nmethod outperforms competitive state-of-the-art models with only 1.68\\%\nparameters compared to those models.\n","authors":["Xing Ai","Zhihong Zhang","Luzhe Sun","Junchi Yan","Edwin Hancock"],"pdf_url":"https://arxiv.org/pdf/2201.05158v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.14006v1","updated":"2023-03-24T14:00:18Z","published":"2023-03-24T14:00:18Z","title":"ASTRA-sim2.0: Modeling Hierarchical Networks and Disaggregated Systems\n  for Large-model Training at Scale","summary":"  As deep learning models and input data are scaling at an unprecedented rate,\nit is inevitable to move towards distributed training platforms to fit the\nmodel and increase training throughput. State-of-the-art approaches and\ntechniques, such as wafer-scale nodes, multi-dimensional network topologies,\ndisaggregated memory systems, and parallelization strategies, have been\nactively adopted by emerging distributed training systems. This results in a\ncomplex SW/HW co-design stack of distributed training, necessitating a\nmodeling/simulation infrastructure for design-space exploration. In this paper,\nwe extend the open-source ASTRA-sim infrastructure and endow it with the\ncapabilities to model state-of-the-art and emerging distributed training models\nand platforms. More specifically, (i) we enable ASTRA-sim to support arbitrary\nmodel parallelization strategies via a graph-based training-loop\nimplementation, (ii) we implement a parameterizable multi-dimensional\nheterogeneous topology generation infrastructure with analytical performance\nestimates enabling simulating target systems at scale, and (iii) we enhance the\nmemory system modeling to support accurate modeling of in-network collective\ncommunication and disaggregated memory systems. With such capabilities, we run\ncomprehensive case studies targeting emerging distributed models and platforms.\nThis infrastructure lets system designers swiftly traverse the complex\nco-design stack and give meaningful insights when designing and deploying\ndistributed training platforms at scale.\n","authors":["William Won","Taekyung Heo","Saeed Rashidi","Srinivas Sridharan","Sudarshan Srinivasan","Tushar Krishna"],"pdf_url":"https://arxiv.org/pdf/2303.14006v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2302.11963v2","updated":"2023-03-24T13:40:27Z","published":"2023-02-23T12:23:35Z","title":"Investigating Catastrophic Overfitting in Fast Adversarial Training: A\n  Self-fitting Perspective","summary":"  Although fast adversarial training provides an efficient approach for\nbuilding robust networks, it may suffer from a serious problem known as\ncatastrophic overfitting (CO), where multi-step robust accuracy suddenly\ncollapses to zero. In this paper, we for the first time decouple single-step\nadversarial examples into data-information and self-information, which reveals\nan interesting phenomenon called \"self-fitting\". Self-fitting, i.e., the\nnetwork learns the self-information embedded in single-step perturbations,\nnaturally leads to the occurrence of CO. When self-fitting occurs, the network\nexperiences an obvious \"channel differentiation\" phenomenon that some\nconvolution channels accounting for recognizing self-information become\ndominant, while others for data-information are suppressed. In this way, the\nnetwork can only recognize images with sufficient self-information and loses\ngeneralization ability to other types of data. Based on self-fitting, we\nprovide new insights into the existing methods to mitigate CO and extend CO to\nmulti-step adversarial training. Our findings reveal a self-learning mechanism\nin adversarial training and open up new perspectives for suppressing different\nkinds of information to mitigate CO.\n","authors":["Zhengbao He","Tao Li","Sizhe Chen","Xiaolin Huang"],"pdf_url":"https://arxiv.org/pdf/2302.11963v2.pdf","comment":"Comment: The camera-ready version (accepted at CVPR Workshop of\n  Adversarial Machine Learning on Computer Vision: Art of Robustness, 2023)"},{"id":"http://arxiv.org/abs/2011.13704v2","updated":"2023-03-24T13:14:14Z","published":"2020-11-27T12:42:12Z","title":"Direct Evolutionary Optimization of Variational Autoencoders With Binary\n  Latents","summary":"  Discrete latent variables are considered important for real world data, which\nhas motivated research on Variational Autoencoders (VAEs) with discrete\nlatents. However, standard VAE training is not possible in this case, which has\nmotivated different strategies to manipulate discrete distributions in order to\ntrain discrete VAEs similarly to conventional ones. Here we ask if it is also\npossible to keep the discrete nature of the latents fully intact by applying a\ndirect discrete optimization for the encoding model. The approach is\nconsequently strongly diverting from standard VAE-training by sidestepping\nsampling approximation, reparameterization trick and amortization. Discrete\noptimization is realized in a variational setting using truncated posteriors in\nconjunction with evolutionary algorithms. For VAEs with binary latents, we (A)\nshow how such a discrete variational method ties into gradient ascent for\nnetwork weights, and (B) how the decoder is used to select latent states for\ntraining. Conventional amortized training is more efficient and applicable to\nlarge neural networks. However, using smaller networks, we here find direct\ndiscrete optimization to be efficiently scalable to hundreds of latents. More\nimportantly, we find the effectiveness of direct optimization to be highly\ncompetitive in `zero-shot' learning. In contrast to large supervised networks,\nthe here investigated VAEs can, e.g., denoise a single image without previous\ntraining on clean data and/or training on large image datasets. More generally,\nthe studied approach shows that training of VAEs is indeed possible without\nsampling-based approximation and reparameterization, which may be interesting\nfor the analysis of VAE-training in general. For `zero-shot' settings a direct\noptimization, furthermore, makes VAEs competitive where they have previously\nbeen outperformed by non-generative approaches.\n","authors":["Enrico Guiraud","Jakob Drefs","Jörg Lücke"],"pdf_url":"https://arxiv.org/pdf/2011.13704v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.13974v1","updated":"2023-03-24T12:52:43Z","published":"2023-03-24T12:52:43Z","title":"Mixed-Type Wafer Classification For Low Memory Devices Using Knowledge\n  Distillation","summary":"  Manufacturing wafers is an intricate task involving thousands of steps.\nDefect Pattern Recognition (DPR) of wafer maps is crucial for determining the\nroot cause of production defects, which may further provide insight for yield\nimprovement in wafer foundry. During manufacturing, various defects may appear\nstandalone in the wafer or may appear as different combinations. Identifying\nmultiple defects in a wafer is generally harder compared to identifying a\nsingle defect. Recently, deep learning methods have gained significant traction\nin mixed-type DPR. However, the complexity of defects requires complex and\nlarge models making them very difficult to operate on low-memory embedded\ndevices typically used in fabrication labs. Another common issue is the\nunavailability of labeled data to train complex networks. In this work, we\npropose an unsupervised training routine to distill the knowledge of complex\npre-trained models to lightweight deployment-ready models. We empirically show\nthat this type of training compresses the model without sacrificing accuracy\ndespite being up to 10 times smaller than the teacher model. The compressed\nmodel also manages to outperform contemporary state-of-the-art models.\n","authors":["Nitish Shukla","Anurima Dey","Srivatsan K"],"pdf_url":"https://arxiv.org/pdf/2303.13974v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.13972v1","updated":"2023-03-24T12:48:21Z","published":"2023-03-24T12:48:21Z","title":"Uncovering Energy-Efficient Practices in Deep Learning Training:\n  Preliminary Steps Towards Green AI","summary":"  Modern AI practices all strive towards the same goal: better results. In the\ncontext of deep learning, the term \"results\" often refers to the achieved\naccuracy on a competitive problem set. In this paper, we adopt an idea from the\nemerging field of Green AI to consider energy consumption as a metric of equal\nimportance to accuracy and to reduce any irrelevant tasks or energy usage. We\nexamine the training stage of the deep learning pipeline from a sustainability\nperspective, through the study of hyperparameter tuning strategies and the\nmodel complexity, two factors vastly impacting the overall pipeline's energy\nconsumption. First, we investigate the effectiveness of grid search, random\nsearch and Bayesian optimisation during hyperparameter tuning, and we find that\nBayesian optimisation significantly dominates the other strategies.\nFurthermore, we analyse the architecture of convolutional neural networks with\nthe energy consumption of three prominent layer types: convolutional, linear\nand ReLU layers. The results show that convolutional layers are the most\ncomputationally expensive by a strong margin. Additionally, we observe\ndiminishing returns in accuracy for more energy-hungry models. The overall\nenergy consumption of training can be halved by reducing the network\ncomplexity. In conclusion, we highlight innovative and promising\nenergy-efficient practices for training deep learning models. To expand the\napplication of Green AI, we advocate for a shift in the design of deep learning\nmodels, by considering the trade-off between energy efficiency and accuracy.\n","authors":["Tim Yarally","Luís Cruz","Daniel Feitosa","June Sallou","Arie van Deursen"],"pdf_url":"https://arxiv.org/pdf/2303.13972v1.pdf","comment":"12 pages, 9 figures, 5 tables. Accepted to CAIN23, 2nd International\n  Conference on AI Engineering - Software Engineering for AI"},{"id":"http://arxiv.org/abs/2303.13971v1","updated":"2023-03-24T12:45:42Z","published":"2023-03-24T12:45:42Z","title":"Optimal Transport for Offline Imitation Learning","summary":"  With the advent of large datasets, offline reinforcement learning (RL) is a\npromising framework for learning good decision-making policies without the need\nto interact with the real environment. However, offline RL requires the dataset\nto be reward-annotated, which presents practical challenges when reward\nengineering is difficult or when obtaining reward annotations is\nlabor-intensive. In this paper, we introduce Optimal Transport Reward labeling\n(OTR), an algorithm that assigns rewards to offline trajectories, with a few\nhigh-quality demonstrations. OTR's key idea is to use optimal transport to\ncompute an optimal alignment between an unlabeled trajectory in the dataset and\nan expert demonstration to obtain a similarity measure that can be interpreted\nas a reward, which can then be used by an offline RL algorithm to learn the\npolicy. OTR is easy to implement and computationally efficient. On D4RL\nbenchmarks, we show that OTR with a single demonstration can consistently match\nthe performance of offline RL with ground-truth rewards.\n","authors":["Yicheng Luo","Zhengyao Jiang","Samuel Cohen","Edward Grefenstette","Marc Peter Deisenroth"],"pdf_url":"https://arxiv.org/pdf/2303.13971v1.pdf","comment":"Published in ICLR 2023"},{"id":"http://arxiv.org/abs/2303.13964v1","updated":"2023-03-24T12:37:43Z","published":"2023-03-24T12:37:43Z","title":"Gradient scarcity with Bilevel Optimization for Graph Learning","summary":"  A common issue in graph learning under the semi-supervised setting is\nreferred to as gradient scarcity. That is, learning graphs by minimizing a loss\non a subset of nodes causes edges between unlabelled nodes that are far from\nlabelled ones to receive zero gradients. The phenomenon was first described\nwhen optimizing the graph and the weights of a Graph Neural Network (GCN) with\na joint optimization algorithm. In this work, we give a precise mathematical\ncharacterization of this phenomenon, and prove that it also emerges in bilevel\noptimization, where additional dependency exists between the parameters of the\nproblem. While for GCNs gradient scarcity occurs due to their finite receptive\nfield, we show that it also occurs with the Laplacian regularization model, in\nthe sense that gradients amplitude decreases exponentially with distance to\nlabelled nodes. To alleviate this issue, we study several solutions: we propose\nto resort to latent graph learning using a Graph-to-Graph model (G2G), graph\nregularization to impose a prior structure on the graph, or optimizing on a\nlarger graph than the original one with a reduced diameter. Our experiments on\nsynthetic and real datasets validate our analysis and prove the efficiency of\nthe proposed solutions.\n","authors":["Hashem Ghanem","Samuel Vaiter","Nicolas Keriven"],"pdf_url":"https://arxiv.org/pdf/2303.13964v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.13957v1","updated":"2023-03-24T12:31:48Z","published":"2023-03-24T12:31:48Z","title":"Factorizers for Distributed Sparse Block Codes","summary":"  Distributed sparse block codes (SBCs) exhibit compact representations for\nencoding and manipulating symbolic data structures using fixed-with vectors.\nOne major challenge however is to disentangle, or factorize, such data\nstructures into their constituent elements without having to search through all\npossible combinations. This factorization becomes more challenging when queried\nby noisy SBCs wherein symbol representations are relaxed due to perceptual\nuncertainty and approximations made when modern neural networks are used to\ngenerate the query vectors. To address these challenges, we first propose a\nfast and highly accurate method for factorizing a more flexible and hence\ngeneralized form of SBCs, dubbed GSBCs. Our iterative factorizer introduces a\nthreshold-based nonlinear activation, a conditional random sampling, and an\n$\\ell_\\infty$-based similarity metric. Its random sampling mechanism in\ncombination with the search in superposition allows to analytically determine\nthe expected number of decoding iterations, which matches the empirical\nobservations up to the GSBC's bundling capacity. Secondly, the proposed\nfactorizer maintains its high accuracy when queried by noisy product vectors\ngenerated using deep convolutional neural networks (CNNs). This facilitates its\napplication in replacing the large fully connected layer (FCL) in CNNs, whereby\nC trainable class vectors, or attribute combinations, can be implicitly\nrepresented by our factorizer having F-factor codebooks, each with\n$\\sqrt[\\leftroot{-2}\\uproot{2}F]{C}$ fixed codevectors. We provide a\nmethodology to flexibly integrate our factorizer in the classification layer of\nCNNs with a novel loss function. We demonstrate the feasibility of our method\non four deep CNN architectures over CIFAR-100, ImageNet-1K, and RAVEN datasets.\nIn all use cases, the number of parameters and operations are significantly\nreduced compared to the FCL.\n","authors":["Michael Hersche","Aleksandar Terzic","Geethan Karunaratne","Jovin Langenegger","Angéline Pouget","Giovanni Cherubini","Luca Benini","Abu Sebastian","Abbas Rahimi"],"pdf_url":"https://arxiv.org/pdf/2303.13957v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2212.03201v2","updated":"2023-03-24T12:04:32Z","published":"2022-12-06T18:21:47Z","title":"Misspecification in Inverse Reinforcement Learning","summary":"  The aim of Inverse Reinforcement Learning (IRL) is to infer a reward function\n$R$ from a policy $\\pi$. To do this, we need a model of how $\\pi$ relates to\n$R$. In the current literature, the most common models are optimality,\nBoltzmann rationality, and causal entropy maximisation. One of the primary\nmotivations behind IRL is to infer human preferences from human behaviour.\nHowever, the true relationship between human preferences and human behaviour is\nmuch more complex than any of the models currently used in IRL. This means that\nthey are misspecified, which raises the worry that they might lead to unsound\ninferences if applied to real-world data. In this paper, we provide a\nmathematical analysis of how robust different IRL models are to\nmisspecification, and answer precisely how the demonstrator policy may differ\nfrom each of the standard models before that model leads to faulty inferences\nabout the reward function $R$. We also introduce a framework for reasoning\nabout misspecification in IRL, together with formal tools that can be used to\neasily derive the misspecification robustness of new IRL models.\n","authors":["Joar Skalse","Alessandro Abate"],"pdf_url":"https://arxiv.org/pdf/2212.03201v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2209.10584v3","updated":"2023-03-24T11:58:07Z","published":"2022-09-21T18:18:32Z","title":"Continuous Mixtures of Tractable Probabilistic Models","summary":"  Probabilistic models based on continuous latent spaces, such as variational\nautoencoders, can be understood as uncountable mixture models where components\ndepend continuously on the latent code. They have proven to be expressive tools\nfor generative and probabilistic modelling, but are at odds with tractable\nprobabilistic inference, that is, computing marginals and conditionals of the\nrepresented probability distribution. Meanwhile, tractable probabilistic models\nsuch as probabilistic circuits (PCs) can be understood as hierarchical discrete\nmixture models, and thus are capable of performing exact inference efficiently\nbut often show subpar performance in comparison to continuous latent-space\nmodels. In this paper, we investigate a hybrid approach, namely continuous\nmixtures of tractable models with a small latent dimension. While these models\nare analytically intractable, they are well amenable to numerical integration\nschemes based on a finite set of integration points. With a large enough number\nof integration points the approximation becomes de-facto exact. Moreover, for a\nfinite set of integration points, the integration method effectively compiles\nthe continuous mixture into a standard PC. In experiments, we show that this\nsimple scheme proves remarkably effective, as PCs learnt this way set new state\nof the art for tractable models on many standard density estimation benchmarks.\n","authors":["Alvaro H. C. Correia","Gennaro Gala","Erik Quaeghebeur","Cassio de Campos","Robert Peharz"],"pdf_url":"https://arxiv.org/pdf/2209.10584v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.13937v1","updated":"2023-03-24T11:50:08Z","published":"2023-03-24T11:50:08Z","title":"Topological Reconstruction of Particle Physics Processes using Graph\n  Neural Networks","summary":"  We present a new approach, the Topograph, which reconstructs underlying\nphysics processes, including the intermediary particles, by leveraging\nunderlying priors from the nature of particle physics decays and the\nflexibility of message passing graph neural networks. The Topograph not only\nsolves the combinatoric assignment of observed final state objects, associating\nthem to their original mother particles, but directly predicts the properties\nof intermediate particles in hard scatter processes and their subsequent\ndecays. In comparison to standard combinatoric approaches or modern approaches\nusing graph neural networks, which scale exponentially or quadratically, the\ncomplexity of Topographs scales linearly with the number of reconstructed\nobjects.\n  We apply Topographs to top quark pair production in the all hadronic decay\nchannel, where we outperform the standard approach and match the performance of\nthe state-of-the-art machine learning technique.\n","authors":["Lukas Ehrke","John Andrew Raine","Knut Zoch","Manuel Guth","Tobias Golling"],"pdf_url":"https://arxiv.org/pdf/2303.13937v1.pdf","comment":"24 pages, 25 figures, 7 tables"},{"id":"http://arxiv.org/abs/2202.09357v2","updated":"2023-03-24T11:39:47Z","published":"2022-02-18T18:56:05Z","title":"ProxSkip: Yes! Local Gradient Steps Provably Lead to Communication\n  Acceleration! Finally!","summary":"  We introduce ProxSkip -- a surprisingly simple and provably efficient method\nfor minimizing the sum of a smooth ($f$) and an expensive nonsmooth proximable\n($\\psi$) function. The canonical approach to solving such problems is via the\nproximal gradient descent (ProxGD) algorithm, which is based on the evaluation\nof the gradient of $f$ and the prox operator of $\\psi$ in each iteration. In\nthis work we are specifically interested in the regime in which the evaluation\nof prox is costly relative to the evaluation of the gradient, which is the case\nin many applications. ProxSkip allows for the expensive prox operator to be\nskipped in most iterations: while its iteration complexity is\n$\\mathcal{O}\\left(\\kappa \\log \\frac{1}{\\varepsilon}\\right)$, where $\\kappa$ is\nthe condition number of $f$, the number of prox evaluations is\n$\\mathcal{O}\\left(\\sqrt{\\kappa} \\log \\frac{1}{\\varepsilon}\\right)$ only. Our\nmain motivation comes from federated learning, where evaluation of the gradient\noperator corresponds to taking a local GD step independently on all devices,\nand evaluation of prox corresponds to (expensive) communication in the form of\ngradient averaging. In this context, ProxSkip offers an effective acceleration\nof communication complexity. Unlike other local gradient-type methods, such as\nFedAvg, SCAFFOLD, S-Local-GD and FedLin, whose theoretical communication\ncomplexity is worse than, or at best matching, that of vanilla GD in the\nheterogeneous data regime, we obtain a provable and large improvement without\nany heterogeneity-bounding assumptions.\n","authors":["Konstantin Mishchenko","Grigory Malinovsky","Sebastian Stich","Peter Richtárik"],"pdf_url":"https://arxiv.org/pdf/2202.09357v2.pdf","comment":"20 pages, 5 algorithms, 2 figures"},{"id":"http://arxiv.org/abs/2303.13930v1","updated":"2023-03-24T11:38:35Z","published":"2023-03-24T11:38:35Z","title":"Particle Mean Field Variational Bayes","summary":"  The Mean Field Variational Bayes (MFVB) method is one of the most\ncomputationally efficient techniques for Bayesian inference. However, its use\nhas been restricted to models with conjugate priors or those that require\nanalytical calculations. This paper proposes a novel particle-based MFVB\napproach that greatly expands the applicability of the MFVB method. We\nestablish the theoretical basis of the new method by leveraging the connection\nbetween Wasserstein gradient flows and Langevin diffusion dynamics, and\ndemonstrate the effectiveness of this approach using Bayesian logistic\nregression, stochastic volatility, and deep neural networks.\n","authors":["Minh-Ngoc Tran","Paco Tseng","Robert Kohn"],"pdf_url":"https://arxiv.org/pdf/2303.13930v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.13918v1","updated":"2023-03-24T11:13:33Z","published":"2023-03-24T11:13:33Z","title":"Removing confounding information from fetal ultrasound images","summary":"  Confounding information in the form of text or markings embedded in medical\nimages can severely affect the training of diagnostic deep learning algorithms.\nHowever, data collected for clinical purposes often have such markings embedded\nin them. In dermatology, known examples include drawings or rulers that are\noverrepresented in images of malignant lesions. In this paper, we encounter\ntext and calipers placed on the images found in national databases containing\nfetal screening ultrasound scans, which correlate with standard planes to be\npredicted. In order to utilize the vast amounts of data available in these\ndatabases, we develop and validate a series of methods for minimizing the\nconfounding effects of embedded text and calipers on deep learning algorithms\ndesigned for ultrasound, using standard plane classification as a test case.\n","authors":["Kamil Mikolaj","Manxi Lin","Zahra Bashir","Morten Bo Søndergaard Svendsen","Martin Tolsgaard","Anders Nymark","Aasa Feragen"],"pdf_url":"https://arxiv.org/pdf/2303.13918v1.pdf","comment":"Fetal ultrasound, confounders, shortcut learning"},{"id":"http://arxiv.org/abs/2303.13917v1","updated":"2023-03-24T11:12:37Z","published":"2023-03-24T11:12:37Z","title":"Convolutional Neural Networks for the classification of glitches in\n  gravitational-wave data streams","summary":"  We investigate the use of Convolutional Neural Networks (including the modern\nConvNeXt network family) to classify transient noise signals (i.e.~glitches)\nand gravitational waves in data from the Advanced LIGO detectors. First, we use\nmodels with a supervised learning approach, both trained from scratch using the\nGravity Spy dataset and employing transfer learning by fine-tuning pre-trained\nmodels in this dataset. Second, we also explore a self-supervised approach,\npre-training models with automatically generated pseudo-labels. Our findings\nare very close to existing results for the same dataset, reaching values for\nthe F1 score of 97.18% (94.15%) for the best supervised (self-supervised)\nmodel. We further test the models using actual gravitational-wave signals from\nLIGO-Virgo's O3 run. Although trained using data from previous runs (O1 and\nO2), the models show good performance, in particular when using transfer\nlearning. We find that transfer learning improves the scores without the need\nfor any training on real signals apart from the less than 50 chirp examples\nfrom hardware injections present in the Gravity Spy dataset. This motivates the\nuse of transfer learning not only for glitch classification but also for signal\nclassification.\n","authors":["Tiago S. Fernandes","Samuel J. Vieira","Antonio Onofre","Juan Calderón Bustillo","Alejandro Torres-Forné","José A. Font"],"pdf_url":"https://arxiv.org/pdf/2303.13917v1.pdf","comment":"15 pages, 14 figures"},{"id":"http://arxiv.org/abs/2303.13915v1","updated":"2023-03-24T11:04:16Z","published":"2023-03-24T11:04:16Z","title":"Benchmarking the Impact of Noise on Deep Learning-based Classification\n  of Atrial Fibrillation in 12-Lead ECG","summary":"  Electrocardiography analysis is widely used in various clinical applications\nand Deep Learning models for classification tasks are currently in the focus of\nresearch. Due to their data-driven character, they bear the potential to handle\nsignal noise efficiently, but its influence on the accuracy of these methods is\nstill unclear. Therefore, we benchmark the influence of four types of noise on\nthe accuracy of a Deep Learning-based method for atrial fibrillation detection\nin 12-lead electrocardiograms. We use a subset of a publicly available dataset\n(PTBXL) and use the metadata provided by human experts regarding noise for\nassigning a signal quality to each electrocardiogram. Furthermore, we compute a\nquantitative signal-to-noise ratio for each electrocardiogram. We analyze the\naccuracy of the Deep Learning model with respect to both metrics and observe\nthat the method can robustly identify atrial fibrillation, even in cases\nsignals are labelled by human experts as being noisy on multiple leads. False\npositive and false negative rates are slightly worse for data being labelled as\nnoisy. Interestingly, data annotated as showing baseline drift noise results in\nan accuracy very similar to data without. We conclude that the issue of\nprocessing noisy electrocardiography data can be addressed successfully by Deep\nLearning methods that might not need preprocessing as many conventional methods\ndo.\n","authors":["Theresa Bender","Philip Gemke","Ennio Idrobo-Avila","Henning Dathe","Dagmar Krefting","Nicolai Spicher"],"pdf_url":"https://arxiv.org/pdf/2303.13915v1.pdf","comment":"to be published in proceedings of Medical Informatics Europe 2023"},{"id":"http://arxiv.org/abs/2301.10410v3","updated":"2023-03-24T10:51:05Z","published":"2023-01-25T05:16:43Z","title":"One Model for All Domains: Collaborative Domain-Prefix Tuning for\n  Cross-Domain NER","summary":"  Cross-domain NER is a challenging task to address the low-resource problem in\npractical scenarios. Previous typical solutions mainly obtain a NER model by\npre-trained language models (PLMs) with data from a rich-resource domain and\nadapt it to the target domain. Owing to the mismatch issue among entity types\nin different domains, previous approaches normally tune all parameters of PLMs,\nending up with an entirely new NER model for each domain. Moreover, current\nmodels only focus on leveraging knowledge in one general source domain while\nfailing to successfully transfer knowledge from multiple sources to the target.\nTo address these issues, we introduce Collaborative Domain-Prefix Tuning for\ncross-domain NER (CP-NER) based on text-to-text generative PLMs. Specifically,\nwe present text-to-text generation grounding domain-related instructors to\ntransfer knowledge to new domain NER tasks without structural modifications. We\nutilize frozen PLMs and conduct collaborative domain-prefix tuning to stimulate\nthe potential of PLMs to handle NER tasks across various domains. Experimental\nresults on the Cross-NER benchmark show that the proposed approach has flexible\ntransfer ability and performs better on both one-source and multiple-source\ncross-domain NER tasks. Codes will be available in\nhttps://github.com/zjunlp/DeepKE/tree/main/example/ner/cross.\n","authors":["Xiang Chen","Lei Li","Shuofei Qiao","Ningyu Zhang","Chuanqi Tan","Yong Jiang","Fei Huang","Huajun Chen"],"pdf_url":"https://arxiv.org/pdf/2301.10410v3.pdf","comment":"Work in progress"},{"id":"http://arxiv.org/abs/2303.13909v1","updated":"2023-03-24T10:46:40Z","published":"2023-03-24T10:46:40Z","title":"Wave-U-Net Discriminator: Fast and Lightweight Discriminator for\n  Generative Adversarial Network-Based Speech Synthesis","summary":"  In speech synthesis, a generative adversarial network (GAN), training a\ngenerator (speech synthesizer) and a discriminator in a min-max game, is widely\nused to improve speech quality. An ensemble of discriminators is commonly used\nin recent neural vocoders (e.g., HiFi-GAN) and end-to-end text-to-speech (TTS)\nsystems (e.g., VITS) to scrutinize waveforms from multiple perspectives. Such\ndiscriminators allow synthesized speech to adequately approach real speech;\nhowever, they require an increase in the model size and computation time\naccording to the increase in the number of discriminators. Alternatively, this\nstudy proposes a Wave-U-Net discriminator, which is a single but expressive\ndiscriminator with Wave-U-Net architecture. This discriminator is unique; it\ncan assess a waveform in a sample-wise manner with the same resolution as the\ninput signal, while extracting multilevel features via an encoder and decoder\nwith skip connections. This architecture provides a generator with sufficiently\nrich information for the synthesized speech to be closely matched to the real\nspeech. During the experiments, the proposed ideas were applied to a\nrepresentative neural vocoder (HiFi-GAN) and an end-to-end TTS system (VITS).\nThe results demonstrate that the proposed models can achieve comparable speech\nquality with a 2.31 times faster and 14.5 times more lightweight discriminator\nwhen used in HiFi-GAN and a 1.90 times faster and 9.62 times more lightweight\ndiscriminator when used in VITS. Audio samples are available at\nhttps://www.kecl.ntt.co.jp/people/kaneko.takuhiro/projects/waveunetd/.\n","authors":["Takuhiro Kaneko","Hirokazu Kameoka","Kou Tanaka","Shogo Seki"],"pdf_url":"https://arxiv.org/pdf/2303.13909v1.pdf","comment":"Accepted to ICASSP 2023. Project page:\n  https://www.kecl.ntt.co.jp/people/kaneko.takuhiro/projects/waveunetd/"},{"id":"http://arxiv.org/abs/2212.01386v2","updated":"2023-03-24T10:41:48Z","published":"2022-12-01T13:10:56Z","title":"Convolution, aggregation and attention based deep neural networks for\n  accelerating simulations in mechanics","summary":"  Deep learning surrogate models are being increasingly used in accelerating\nscientific simulations as a replacement for costly conventional numerical\ntechniques. However, their use remains a significant challenge when dealing\nwith real-world complex examples. In this work, we demonstrate three types of\nneural network architectures for efficient learning of highly non-linear\ndeformations of solid bodies. The first two architectures are based on the\nrecently proposed CNN U-NET and MAgNET (graph U-NET) frameworks which have\nshown promising performance for learning on mesh-based data. The third\narchitecture is Perceiver IO, a very recent architecture that belongs to the\nfamily of attention-based neural networks--a class that has revolutionised\ndiverse engineering fields and is still unexplored in computational mechanics.\nWe study and compare the performance of all three networks on two benchmark\nexamples, and show their capabilities to accurately predict the non-linear\nmechanical responses of soft bodies.\n","authors":["Saurabh Deshpande","Raúl I. Sosa","Stéphane P. A. Bordas","Jakub Lengiewicz"],"pdf_url":"https://arxiv.org/pdf/2212.01386v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.13898v1","updated":"2023-03-24T10:18:28Z","published":"2023-03-24T10:18:28Z","title":"Remind of the Past: Incremental Learning with Analogical Prompts","summary":"  Although data-free incremental learning methods are memory-friendly,\naccurately estimating and counteracting representation shifts is challenging in\nthe absence of historical data. This paper addresses this thorny problem by\nproposing a novel incremental learning method inspired by human analogy\ncapabilities. Specifically, we design an analogy-making mechanism to remap the\nnew data into the old class by prompt tuning. It mimics the feature\ndistribution of the target old class on the old model using only samples of new\nclasses. The learnt prompts are further used to estimate and counteract the\nrepresentation shift caused by fine-tuning for the historical prototypes. The\nproposed method sets up new state-of-the-art performance on four incremental\nlearning benchmarks under both the class and domain incremental learning\nsettings. It consistently outperforms data-replay methods by only saving\nfeature prototypes for each class. It has almost hit the empirical upper bound\nby joint training on the Core50 benchmark. The code will be released at\n\\url{https://github.com/ZhihengCV/A-Prompts}.\n","authors":["Zhiheng Ma","Xiaopeng Hong","Beinan Liu","Yabin Wang","Pinyue Guo","Huiyun Li"],"pdf_url":"https://arxiv.org/pdf/2303.13898v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2210.12685v2","updated":"2023-03-24T10:09:01Z","published":"2022-10-23T10:15:32Z","title":"Less Emphasis on Difficult Layer Regions: Curriculum Learning for\n  Singularly Perturbed Convection-Diffusion-Reaction Problems","summary":"  Although Physics-Informed Neural Networks (PINNs) have been successfully\napplied in a wide variety of science and engineering fields, they can fail to\naccurately predict the underlying solution in slightly challenging\nconvection-diffusion-reaction problems. In this paper, we investigate the\nreason of this failure from a domain distribution perspective, and identify\nthat learning multi-scale fields simultaneously makes the network unable to\nadvance its training and easily get stuck in poor local minima. We show that\nthe widespread experience of sampling more collocation points in high-loss\nlayer regions hardly help optimize and may even worsen the results. These\nfindings motivate the development of a novel curriculum learning method that\nencourages neural networks to prioritize learning on easier non-layer regions\nwhile downplaying learning on harder layer regions. The proposed method helps\nPINNs automatically adjust the learning emphasis and thereby facilitate the\noptimization procedure. Numerical results on typical benchmark equations show\nthat the proposed curriculum learning approach mitigates the failure modes of\nPINNs and can produce accurate results for very sharp boundary and interior\nlayers. Our work reveals that for equations whose solutions have large scale\ndifferences, paying less attention to high-loss regions can be an effective\nstrategy for learning them accurately.\n","authors":["Yufeng Wang","Cong Xu","Min Yang","Jin Zhang"],"pdf_url":"https://arxiv.org/pdf/2210.12685v2.pdf","comment":"22 pages"},{"id":"http://arxiv.org/abs/2303.13896v1","updated":"2023-03-24T10:05:22Z","published":"2023-03-24T10:05:22Z","title":"Regularization of polynomial networks for image recognition","summary":"  Deep Neural Networks (DNNs) have obtained impressive performance across\ntasks, however they still remain as black boxes, e.g., hard to theoretically\nanalyze. At the same time, Polynomial Networks (PNs) have emerged as an\nalternative method with a promising performance and improved interpretability\nbut have yet to reach the performance of the powerful DNN baselines. In this\nwork, we aim to close this performance gap. We introduce a class of PNs, which\nare able to reach the performance of ResNet across a range of six benchmarks.\nWe demonstrate that strong regularization is critical and conduct an extensive\nstudy of the exact regularization schemes required to match performance. To\nfurther motivate the regularization schemes, we introduce D-PolyNets that\nachieve a higher-degree of expansion than previously proposed polynomial\nnetworks. D-PolyNets are more parameter-efficient while achieving a similar\nperformance as other polynomial networks. We expect that our new models can\nlead to an understanding of the role of elementwise activation functions (which\nare no longer required for training PNs). The source code is available at\nhttps://github.com/grigorisg9gr/regularized_polynomials.\n","authors":["Grigorios G Chrysos","Bohan Wang","Jiankang Deng","Volkan Cevher"],"pdf_url":"https://arxiv.org/pdf/2303.13896v1.pdf","comment":"Accepted at CVPR'23"},{"id":"http://arxiv.org/abs/2303.07768v3","updated":"2023-03-24T09:57:25Z","published":"2023-03-14T10:18:31Z","title":"DBSCAN of Multi-Slice Clustering for Third-Order Tensors","summary":"  Several methods for triclustering three-dimensional data require the cluster\nsize or the number of clusters in each dimension to be specified. To address\nthis issue, the Multi-Slice Clustering (MSC) for 3-order tensor finds signal\nslices that lie in a low dimensional subspace for a rank-one tensor dataset in\norder to find a cluster based on the threshold similarity. We propose an\nextension algorithm called MSC-DBSCAN to extract the different clusters of\nslices that lie in the different subspaces from the data if the dataset is a\nsum of r rank-one tensor (r > 1). Our algorithm uses the same input as the MSC\nalgorithm and can find the same solution for rank-one tensor data as MSC.\n","authors":["Dina Faneva Andriantsiory","Joseph Ben Geloun","Mustapha Lebbah"],"pdf_url":"https://arxiv.org/pdf/2303.07768v3.pdf","comment":"13 pages, improved version, typos removed, text restructured, same\n  results"},{"id":"http://arxiv.org/abs/2303.13113v2","updated":"2023-03-24T09:23:37Z","published":"2023-03-23T09:00:38Z","title":"Adaptive Regularization for Class-Incremental Learning","summary":"  Class-Incremental Learning updates a deep classifier with new categories\nwhile maintaining the previously observed class accuracy. Regularizing the\nneural network weights is a common method to prevent forgetting previously\nlearned classes while learning novel ones. However, existing regularizers use a\nconstant magnitude throughout the learning sessions, which may not reflect the\nvarying levels of difficulty of the tasks encountered during incremental\nlearning. This study investigates the necessity of adaptive regularization in\nClass-Incremental Learning, which dynamically adjusts the regularization\nstrength according to the complexity of the task at hand. We propose a Bayesian\nOptimization-based approach to automatically determine the optimal\nregularization magnitude for each learning task. Our experiments on two\ndatasets via two regularizers demonstrate the importance of adaptive\nregularization for achieving accurate and less forgetful visual incremental\nlearning.\n","authors":["Elif Ceren Gok Yildirim","Murat Onur Yildirim","Mert Kilickaya","Joaquin Vanschoren"],"pdf_url":"https://arxiv.org/pdf/2303.13113v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2111.09543v4","updated":"2023-03-24T09:17:17Z","published":"2021-11-18T06:48:00Z","title":"DeBERTaV3: Improving DeBERTa using ELECTRA-Style Pre-Training with\n  Gradient-Disentangled Embedding Sharing","summary":"  This paper presents a new pre-trained language model, DeBERTaV3, which\nimproves the original DeBERTa model by replacing mask language modeling (MLM)\nwith replaced token detection (RTD), a more sample-efficient pre-training task.\nOur analysis shows that vanilla embedding sharing in ELECTRA hurts training\nefficiency and model performance. This is because the training losses of the\ndiscriminator and the generator pull token embeddings in different directions,\ncreating the \"tug-of-war\" dynamics. We thus propose a new gradient-disentangled\nembedding sharing method that avoids the tug-of-war dynamics, improving both\ntraining efficiency and the quality of the pre-trained model. We have\npre-trained DeBERTaV3 using the same settings as DeBERTa to demonstrate its\nexceptional performance on a wide range of downstream natural language\nunderstanding (NLU) tasks. Taking the GLUE benchmark with eight tasks as an\nexample, the DeBERTaV3 Large model achieves a 91.37% average score, which is\n1.37% over DeBERTa and 1.91% over ELECTRA, setting a new state-of-the-art\n(SOTA) among the models with a similar structure. Furthermore, we have\npre-trained a multi-lingual model mDeBERTa and observed a larger improvement\nover strong baselines compared to English models. For example, the mDeBERTa\nBase achieves a 79.8% zero-shot cross-lingual accuracy on XNLI and a 3.6%\nimprovement over XLM-R Base, creating a new SOTA on this benchmark. We have\nmade our pre-trained models and inference code publicly available at\nhttps://github.com/microsoft/DeBERTa.\n","authors":["Pengcheng He","Jianfeng Gao","Weizhu Chen"],"pdf_url":"https://arxiv.org/pdf/2111.09543v4.pdf","comment":"16 pages, 10 tables, 2 Figures. The DeBERTaV3 model significantly\n  improves performance of the downstream NLU tasks over models with a similar\n  structure, e.g. DeBERTaV3 large achieves 91.37% average GLUE score which is\n  1.37% over DeBERTa large. XSmall has only 22M backbone parameters, but\n  significantly outperforms RoBERTa/XLNet-base. Paper is published as a\n  conference paper at ICLR 2023"},{"id":"http://arxiv.org/abs/2210.05674v3","updated":"2023-03-24T08:56:11Z","published":"2022-10-11T07:39:08Z","title":"Semi-supervised detection of structural damage using Variational\n  Autoencoder and a One-Class Support Vector Machine","summary":"  In recent years, Artificial Neural Networks (ANNs) have been introduced in\nStructural Health Monitoring (SHM) systems. A semi-supervised method with a\ndata-driven approach allows the ANN training on data acquired from an undamaged\nstructural condition to detect structural damages. In standard approaches,\nafter the training stage, a decision rule is manually defined to detect\nanomalous data. However, this process could be made automatic using machine\nlearning methods, whom performances are maximised using hyperparameter\noptimization techniques. The paper proposes a semi-supervised method with a\ndata-driven approach to detect structural anomalies. The methodology consists\nof: (i) a Variational Autoencoder (VAE) to approximate undamaged data\ndistribution and (ii) a One-Class Support Vector Machine (OC-SVM) to\ndiscriminate different health conditions using damage sensitive features\nextracted from VAE's signal reconstruction. The method is applied to a scale\nsteel structure that was tested in nine damage's scenarios by IASC-ASCE\nStructural Health Monitoring Task Group.\n","authors":["Andrea Pollastro","Giusiana Testa","Antonio Bilotta","Roberto Prevete"],"pdf_url":"https://arxiv.org/pdf/2210.05674v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2203.11009v2","updated":"2023-03-24T08:32:24Z","published":"2022-03-21T14:23:18Z","title":"Continual Spatio-Temporal Graph Convolutional Networks","summary":"  Graph-based reasoning over skeleton data has emerged as a promising approach\nfor human action recognition. However, the application of prior graph-based\nmethods, which predominantly employ whole temporal sequences as their input, to\nthe setting of online inference entails considerable computational redundancy.\nIn this paper, we tackle this issue by reformulating the Spatio-Temporal Graph\nConvolutional Neural Network as a Continual Inference Network, which can\nperform step-by-step predictions in time without repeat frame processing. To\nevaluate our method, we create a continual version of ST-GCN, CoST-GCN,\nalongside two derived methods with different self-attention mechanisms, CoAGCN\nand CoS-TR. We investigate weight transfer strategies and architectural\nmodifications for inference acceleration, and perform experiments on the NTU\nRGB+D 60, NTU RGB+D 120, and Kinetics Skeleton 400 datasets. Retaining similar\npredictive accuracy, we observe up to 109x reduction in time complexity,\non-hardware accelerations of 26x, and reductions in maximum allocated memory of\n52% during online inference.\n","authors":["Lukas Hedegaard","Negar Heidari","Alexandros Iosifidis"],"pdf_url":"https://arxiv.org/pdf/2203.11009v2.pdf","comment":"12 pages, 6 figures, 5 tables"},{"id":"http://arxiv.org/abs/2303.13850v1","updated":"2023-03-24T08:17:31Z","published":"2023-03-24T08:17:31Z","title":"Learning Causal Attributions in Neural Networks: Beyond Direct Effects","summary":"  There has been a growing interest in capturing and maintaining causal\nrelationships in Neural Network (NN) models in recent years. We study causal\napproaches to estimate and maintain input-output attributions in NN models in\nthis work. In particular, existing efforts in this direction assume\nindependence among input variables (by virtue of the NN architecture), and\nhence study only direct causal effects. Viewing an NN as a structural causal\nmodel (SCM), we instead focus on going beyond direct effects, introduce edges\namong input features, and provide a simple yet effective methodology to capture\nand maintain direct and indirect causal effects while training an NN model. We\nalso propose effective approximation strategies to quantify causal attributions\nin high dimensional data. Our wide range of experiments on synthetic and\nreal-world datasets show that the proposed ante-hoc method learns causal\nattributions for both direct and indirect causal effects close to the ground\ntruth effects.\n","authors":["Abbaavaram Gowtham Reddy","Saketh Bachu","Harsharaj Pathak","Benin L Godfrey","Vineeth N. Balasubramanian","Varshaneya V","Satya Narayanan Kar"],"pdf_url":"https://arxiv.org/pdf/2303.13850v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.12497v3","updated":"2023-03-24T07:58:13Z","published":"2023-03-22T12:09:12Z","title":"Lower Bounds on the Bayesian Risk via Information Measures","summary":"  This paper focuses on parameter estimation and introduces a new method for\nlower bounding the Bayesian risk. The method allows for the use of virtually\n\\emph{any} information measure, including R\\'enyi's $\\alpha$,\n$\\varphi$-Divergences, and Sibson's $\\alpha$-Mutual Information. The approach\nconsiders divergences as functionals of measures and exploits the duality\nbetween spaces of measures and spaces of functions. In particular, we show that\none can lower bound the risk with any information measure by upper bounding its\ndual via Markov's inequality. We are thus able to provide estimator-independent\nimpossibility results thanks to the Data-Processing Inequalities that\ndivergences satisfy. The results are then applied to settings of interest\ninvolving both discrete and continuous parameters, including the\n``Hide-and-Seek'' problem, and compared to the state-of-the-art techniques. An\nimportant observation is that the behaviour of the lower bound in the number of\nsamples is influenced by the choice of the information measure. We leverage\nthis by introducing a new divergence inspired by the ``Hockey-Stick''\nDivergence, which is demonstrated empirically to provide the largest\nlower-bound across all considered settings. If the observations are subject to\nprivatisation, stronger impossibility results can be obtained via Strong\nData-Processing Inequalities. The paper also discusses some generalisations and\nalternative directions.\n","authors":["Amedeo Roberto Esposito","Adrien Vandenbroucque","Michael Gastpar"],"pdf_url":"https://arxiv.org/pdf/2303.12497v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2206.10210v4","updated":"2023-03-24T07:45:50Z","published":"2022-06-21T09:26:25Z","title":"The Integration of Machine Learning into Automated Test Generation: A\n  Systematic Mapping Study","summary":"  Context: Machine learning (ML) may enable effective automated test\ngeneration.\n  Objective: We characterize emerging research, examining testing practices,\nresearcher goals, ML techniques applied, evaluation, and challenges.\n  Methods: We perform a systematic mapping on a sample of 102 publications.\n  Results: ML generates input for system, GUI, unit, performance, and\ncombinatorial testing or improves the performance of existing generation\nmethods. ML is also used to generate test verdicts, property-based, and\nexpected output oracles. Supervised learning - often based on neural networks -\nand reinforcement learning - often based on Q-learning - are common, and some\npublications also employ unsupervised or semi-supervised learning.\n(Semi-/Un-)Supervised approaches are evaluated using both traditional testing\nmetrics and ML-related metrics (e.g., accuracy), while reinforcement learning\nis often evaluated using testing metrics tied to the reward function.\n  Conclusion: Work-to-date shows great promise, but there are open challenges\nregarding training data, retraining, scalability, evaluation complexity, ML\nalgorithms employed - and how they are applied - benchmarks, and replicability.\nOur findings can serve as a roadmap and inspiration for researchers in this\nfield.\n","authors":["Afonso Fontes","Gregory Gay"],"pdf_url":"https://arxiv.org/pdf/2206.10210v4.pdf","comment":"Under submission to Software Testing, Verification, and Reliability\n  journal. (arXiv admin note: text overlap with arXiv:2107.00906 - This is an\n  earlier study that this study extends)"},{"id":"http://arxiv.org/abs/2302.01047v3","updated":"2023-03-24T07:23:36Z","published":"2023-02-02T12:21:10Z","title":"Real-Time Evaluation in Online Continual Learning: A New Hope","summary":"  Current evaluations of Continual Learning (CL) methods typically assume that\nthere is no constraint on training time and computation. This is an unrealistic\nassumption for any real-world setting, which motivates us to propose: a\npractical real-time evaluation of continual learning, in which the stream does\nnot wait for the model to complete training before revealing the next data for\npredictions. To do this, we evaluate current CL methods with respect to their\ncomputational costs. We conduct extensive experiments on CLOC, a large-scale\ndataset containing 39 million time-stamped images with geolocation labels. We\nshow that a simple baseline outperforms state-of-the-art CL methods under this\nevaluation, questioning the applicability of existing methods in realistic\nsettings. In addition, we explore various CL components commonly used in the\nliterature, including memory sampling strategies and regularization approaches.\nWe find that all considered methods fail to be competitive against our simple\nbaseline. This surprisingly suggests that the majority of existing CL\nliterature is tailored to a specific class of streams that is not practical. We\nhope that the evaluation we provide will be the first step towards a paradigm\nshift to consider the computational cost in the development of online continual\nlearning methods.\n","authors":["Yasir Ghunaim","Adel Bibi","Kumail Alhamoud","Motasem Alfarra","Hasan Abed Al Kader Hammoud","Ameya Prabhu","Philip H. S. Torr","Bernard Ghanem"],"pdf_url":"https://arxiv.org/pdf/2302.01047v3.pdf","comment":"Accepted at CVPR'23 as Highlight (Top 2.5%)"},{"id":"http://arxiv.org/abs/2303.11562v2","updated":"2023-03-24T07:00:10Z","published":"2023-03-21T03:05:21Z","title":"Dynamics-Aware Loss for Learning with Label Noise","summary":"  Label noise poses a serious threat to deep neural networks (DNNs). Employing\nrobust loss function which reconciles fitting ability with robustness is a\nsimple but effective strategy to handle this problem. However, the widely-used\nstatic trade-off between these two factors contradicts the dynamic nature of\nDNNs learning with label noise, leading to inferior performance. Therefore, we\npropose a dynamics-aware loss (DAL) to solve this problem. Considering that\nDNNs tend to first learn generalized patterns, then gradually overfit label\nnoise, DAL strengthens the fitting ability initially, then gradually increases\nthe weight of robustness. Moreover, at the later stage, we let DNNs put more\nemphasis on easy examples which are more likely to be correctly labeled than\nhard ones and introduce a bootstrapping term to further reduce the negative\nimpact of label noise. Both the detailed theoretical analyses and extensive\nexperimental results demonstrate the superiority of our method.\n","authors":["Xiu-Chuan Li","Xiaobo Xia","Fei Zhu","Tongliang Liu","Xu-Yao Zhang","Cheng-Lin Liu"],"pdf_url":"https://arxiv.org/pdf/2303.11562v2.pdf","comment":"submitted to a journal"},{"id":"http://arxiv.org/abs/2303.13830v1","updated":"2023-03-24T06:38:42Z","published":"2023-03-24T06:38:42Z","title":"Editing Driver Character: Socially-Controllable Behavior Generation for\n  Interactive Traffic Simulation","summary":"  Traffic simulation plays a crucial role in evaluating and improving\nautonomous driving planning systems. After being deployed on public roads,\nautonomous vehicles need to interact with human road participants with\ndifferent social preferences (e.g., selfish or courteous human drivers). To\nensure that autonomous vehicles take safe and efficient maneuvers in different\ninteractive traffic scenarios, we should be able to evaluate autonomous\nvehicles against reactive agents with different social characteristics in the\nsimulation environment. We propose a socially-controllable behavior generation\n(SCBG) model for this purpose, which allows the users to specify the level of\ncourtesy of the generated trajectory while ensuring realistic and human-like\ntrajectory generation through learning from real-world driving data.\nSpecifically, we define a novel and differentiable measure to quantify the\nlevel of courtesy of driving behavior, leveraging marginal and conditional\nbehavior prediction models trained from real-world driving data. The proposed\ncourtesy measure allows us to auto-label the courtesy levels of trajectories\nfrom real-world driving data and conveniently train an SCBG model generating\ntrajectories based on the input courtesy values. We examined the SCBG model on\nthe Waymo Open Motion Dataset (WOMD) and showed that we were able to control\nthe SCBG model to generate realistic driving behaviors with desired courtesy\nlevels. Interestingly, we found that the SCBG model was able to identify\ndifferent motion patterns of courteous behaviors according to the scenarios.\n","authors":["Wei-Jer Chang","Chen Tang","Chenran Li","Yeping Hu","Masayoshi Tomizuka","Wei Zhan"],"pdf_url":"https://arxiv.org/pdf/2303.13830v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.13827v1","updated":"2023-03-24T06:24:07Z","published":"2023-03-24T06:24:07Z","title":"Efficient Mixed-Type Wafer Defect Pattern Recognition Using Compact\n  Deformable Convolutional Transformers","summary":"  Manufacturing wafers is an intricate task involving thousands of steps.\nDefect Pattern Recognition (DPR) of wafer maps is crucial to find the root\ncause of the issue and further improving the yield in the wafer foundry.\nMixed-type DPR is much more complicated compared to single-type DPR due to\nvaried spatial features, the uncertainty of defects, and the number of defects\npresent. To accurately predict the number of defects as well as the types of\ndefects, we propose a novel compact deformable convolutional transformer (DC\nTransformer). Specifically, DC Transformer focuses on the global features\npresent in the wafer map by virtue of learnable deformable kernels and\nmulti-head attention to the global features. The proposed method succinctly\nmodels the internal relationship between the wafer maps and the defects. DC\nTransformer is evaluated on a real dataset containing 38 defect patterns.\nExperimental results show that DC Transformer performs exceptionally well in\nrecognizing both single and mixed-type defects. The proposed method outperforms\nthe current state of the models by a considerable margin\n","authors":["Nitish Shukla"],"pdf_url":"https://arxiv.org/pdf/2303.13827v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2008.09312v3","updated":"2023-03-24T06:22:34Z","published":"2020-08-21T05:23:47Z","title":"Near Optimal Adversarial Attack on UCB Bandits","summary":"  We consider a stochastic multi-arm bandit problem where rewards are subject\nto adversarial corruption. We propose a novel attack strategy that manipulates\na UCB principle into pulling some non-optimal target arm $T - o(T)$ times with\na cumulative cost that scales as $\\sqrt{\\log T}$, where $T$ is the number of\nrounds. We also prove the first lower bound on the cumulative attack cost. Our\nlower bound matches our upper bound up to $\\log \\log T$ factors, showing our\nattack to be near optimal.\n","authors":["Shiliang Zuo"],"pdf_url":"https://arxiv.org/pdf/2008.09312v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.13821v1","updated":"2023-03-24T05:57:53Z","published":"2023-03-24T05:57:53Z","title":"Factor Decomposed Generative Adversarial Networks for Text-to-Image\n  Synthesis","summary":"  Prior works about text-to-image synthesis typically concatenated the sentence\nembedding with the noise vector, while the sentence embedding and the noise\nvector are two different factors, which control the different aspects of the\ngeneration. Simply concatenating them will entangle the latent factors and\nencumber the generative model.\n  In this paper, we attempt to decompose these two factors and propose Factor\nDecomposed Generative Adversarial Networks~(FDGAN). To achieve this, we firstly\ngenerate images from the noise vector and then apply the sentence embedding in\nthe normalization layer for both generator and discriminators. We also design\nan additive norm layer to align and fuse the text-image features. The\nexperimental results show that decomposing the noise and the sentence embedding\ncan disentangle latent factors in text-to-image synthesis, and make the\ngenerative model more efficient. Compared with the baseline, FDGAN can achieve\nbetter performance, while fewer parameters are used.\n","authors":["Jiguo Li","Xiaobin Liu","Lirong Zheng"],"pdf_url":"https://arxiv.org/pdf/2303.13821v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2211.16596v3","updated":"2023-03-24T05:50:31Z","published":"2022-11-29T21:15:51Z","title":"Towards Dynamic Causal Discovery with Rare Events: A Nonparametric\n  Conditional Independence Test","summary":"  Causal phenomena associated with rare events occur across a wide range of\nengineering problems, such as risk-sensitive safety analysis, accident analysis\nand prevention, and extreme value theory. However, current methods for causal\ndiscovery are often unable to uncover causal links, between random variables in\na dynamic setting, that manifest only when the variables first experience\nlow-probability realizations. To address this issue, we introduce a novel\nstatistical independence test on data collected from time-invariant dynamical\nsystems in which rare but consequential events occur. In particular, we exploit\nthe time-invariance of the underlying data to construct a superimposed dataset\nof the system state before rare events happen at different timesteps. We then\ndesign a conditional independence test on the reorganized data. We provide\nnon-asymptotic sample complexity bounds for the consistency of our method, and\nvalidate its performance across various simulated and real-world datasets,\nincluding incident data collected from the Caltrans Performance Measurement\nSystem (PeMS). Code containing the datasets and experiments is publicly\navailable.\n","authors":["Chih-Yuan Chiu","Kshitij Kulkarni","Shankar Sastry"],"pdf_url":"https://arxiv.org/pdf/2211.16596v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.10727v2","updated":"2023-03-24T05:45:38Z","published":"2023-03-19T18:08:18Z","title":"ERSAM: Neural Architecture Search For Energy-Efficient and Real-Time\n  Social Ambiance Measurement","summary":"  Social ambiance describes the context in which social interactions happen,\nand can be measured using speech audio by counting the number of concurrent\nspeakers. This measurement has enabled various mental health tracking and\nhuman-centric IoT applications. While on-device Socal Ambiance Measure (SAM) is\nhighly desirable to ensure user privacy and thus facilitate wide adoption of\nthe aforementioned applications, the required computational complexity of\nstate-of-the-art deep neural networks (DNNs) powered SAM solutions stands at\nodds with the often constrained resources on mobile devices. Furthermore, only\nlimited labeled data is available or practical when it comes to SAM under\nclinical settings due to various privacy constraints and the required human\neffort, further challenging the achievable accuracy of on-device SAM solutions.\nTo this end, we propose a dedicated neural architecture search framework for\nEnergy-efficient and Real-time SAM (ERSAM). Specifically, our ERSAM framework\ncan automatically search for DNNs that push forward the achievable accuracy vs.\nhardware efficiency frontier of mobile SAM solutions. For example,\nERSAM-delivered DNNs only consume 40 mW x 12 h energy and 0.05 seconds\nprocessing latency for a 5 seconds audio segment on a Pixel 3 phone, while only\nachieving an error rate of 14.3% on a social ambiance dataset generated by\nLibriSpeech. We can expect that our ERSAM framework can pave the way for\nubiquitous on-device SAM solutions which are in growing demand.\n","authors":["Chaojian Li","Wenwan Chen","Jiayi Yuan","Yingyan Lin","Ashutosh Sabharwal"],"pdf_url":"https://arxiv.org/pdf/2303.10727v2.pdf","comment":"Accepted by ICASSP'23"},{"id":"http://arxiv.org/abs/2201.12691v3","updated":"2023-03-24T05:30:59Z","published":"2022-01-30T00:47:04Z","title":"Coordinate Descent Methods for Fractional Minimization","summary":"  We consider a class of structured fractional minimization problems, in which\nthe numerator part of the objective is the sum of a differentiable convex\nfunction and a convex non-smooth function, while the denominator part is a\nconvex or concave function. This problem is difficult to solve since it is\nnon-convex. By exploiting the structure of the problem, we propose two\nCoordinate Descent (CD) methods for solving this problem. The proposed methods\niteratively solve a one-dimensional subproblem \\textit{globally}, and they are\nguaranteed to converge to coordinate-wise stationary points. In the case of a\nconvex denominator, under a weak \\textit{locally bounded non-convexity\ncondition}, we prove that the optimality of coordinate-wise stationary point is\nstronger than that of the standard critical point and directional point. Under\nadditional suitable conditions, CD methods converge Q-linearly to\ncoordinate-wise stationary points. In the case of a concave denominator, we\nshow that any critical point is a global minimum, and CD methods converge to\nthe global minimum with a sublinear convergence rate. We demonstrate the\napplicability of the proposed methods to some machine learning and signal\nprocessing models. Our experiments on real-world data have shown that our\nmethod significantly and consistently outperforms existing methods in terms of\naccuracy.\n","authors":["Ganzhao Yuan"],"pdf_url":"https://arxiv.org/pdf/2201.12691v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.13813v1","updated":"2023-03-24T05:24:23Z","published":"2023-03-24T05:24:23Z","title":"Generalist: Decoupling Natural and Robust Generalization","summary":"  Deep neural networks obtained by standard training have been constantly\nplagued by adversarial examples. Although adversarial training demonstrates its\ncapability to defend against adversarial examples, unfortunately, it leads to\nan inevitable drop in the natural generalization. To address the issue, we\ndecouple the natural generalization and the robust generalization from joint\ntraining and formulate different training strategies for each one.\nSpecifically, instead of minimizing a global loss on the expectation over these\ntwo generalization errors, we propose a bi-expert framework called\n\\emph{Generalist} where we simultaneously train base learners with task-aware\nstrategies so that they can specialize in their own fields. The parameters of\nbase learners are collected and combined to form a global learner at intervals\nduring the training process. The global learner is then distributed to the base\nlearners as initialized parameters for continued training. Theoretically, we\nprove that the risks of Generalist will get lower once the base learners are\nwell trained. Extensive experiments verify the applicability of Generalist to\nachieve high accuracy on natural examples while maintaining considerable\nrobustness to adversarial ones. Code is available at\nhttps://github.com/PKU-ML/Generalist.\n","authors":["Hongjun Wang","Yisen Wang"],"pdf_url":"https://arxiv.org/pdf/2303.13813v1.pdf","comment":"14 pages, 5 figures, CVPR 2023"},{"id":"http://arxiv.org/abs/2201.10542v2","updated":"2023-03-24T05:18:18Z","published":"2022-01-25T18:52:28Z","title":"Augmented RBMLE-UCB Approach for Adaptive Control of Linear Quadratic\n  Systems","summary":"  We consider the problem of controlling an unknown stochastic linear system\nwith quadratic costs - called the adaptive LQ control problem. We re-examine an\napproach called ''Reward Biased Maximum Likelihood Estimate'' (RBMLE) that was\nproposed more than forty years ago, and which predates the ''Upper Confidence\nBound'' (UCB) method as well as the definition of ''regret'' for bandit\nproblems. It simply added a term favoring parameters with larger rewards to the\ncriterion for parameter estimation. We show how the RBMLE and UCB methods can\nbe reconciled, and thereby propose an Augmented RBMLE-UCB algorithm that\ncombines the penalty of the RBMLE method with the constraints of the UCB\nmethod, uniting the two approaches to optimism in the face of uncertainty. We\nestablish that theoretically, this method retains\n$\\Tilde{\\mathcal{O}}(\\sqrt{T})$ regret, the best-known so far. We further\ncompare the empirical performance of the proposed Augmented RBMLE-UCB and the\nstandard RBMLE (without the augmentation) with UCB, Thompson Sampling, Input\nPerturbation, Randomized Certainty Equivalence and StabL on many real-world\nexamples including flight control of Boeing 747 and Unmanned Aerial Vehicle. We\nperform extensive simulation studies showing that the Augmented RBMLE\nconsistently outperforms UCB, Thompson Sampling and StabL by a huge margin,\nwhile it is marginally better than Input Perturbation and moderately better\nthan Randomized Certainty Equivalence.\n","authors":["Akshay Mete","Rahul Singh","P. R. Kumar"],"pdf_url":"https://arxiv.org/pdf/2201.10542v2.pdf","comment":"36th Conference on Neural Information Processing Systems (NeurIPS\n  2022). https://openreview.net/forum?id=7pNV4PCjbQy"},{"id":"http://arxiv.org/abs/2303.13808v1","updated":"2023-03-24T05:05:01Z","published":"2023-03-24T05:05:01Z","title":"marl-jax: Multi-agent Reinforcement Leaning framework for Social\n  Generalization","summary":"  Recent advances in Reinforcement Learning (RL) have led to many exciting\napplications. These advancements have been driven by improvements in both\nalgorithms and engineering, which have resulted in faster training of RL\nagents. We present marl-jax, a multi-agent reinforcement learning software\npackage for training and evaluating social generalization of the agents. The\npackage is designed for training a population of agents in multi-agent\nenvironments and evaluating their ability to generalize to diverse background\nagents. It is built on top of DeepMind's JAX ecosystem~\\cite{deepmind2020jax}\nand leverages the RL ecosystem developed by DeepMind. Our framework marl-jax is\ncapable of working in cooperative and competitive, simultaneous-acting\nenvironments with multiple agents. The package offers an intuitive and\nuser-friendly command-line interface for training a population and evaluating\nits generalization capabilities. In conclusion, marl-jax provides a valuable\nresource for researchers interested in exploring social generalization in the\ncontext of MARL. The open-source code for marl-jax is available at:\n\\href{https://github.com/kinalmehta/marl-jax}{https://github.com/kinalmehta/marl-jax}\n","authors":["Kinal Mehta","Anuj Mahajan","Pawan Kumar"],"pdf_url":"https://arxiv.org/pdf/2303.13808v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.13804v1","updated":"2023-03-24T04:57:17Z","published":"2023-03-24T04:57:17Z","title":"UniTS: A Universal Time Series Analysis Framework with Self-supervised\n  Representation Learning","summary":"  Machine learning has emerged as a powerful tool for time series analysis.\nExisting methods are usually customized for different analysis tasks and face\nchallenges in tackling practical problems such as partial labeling and domain\nshift. To achieve universal analysis and address the aforementioned problems,\nwe develop UniTS, a novel framework that incorporates self-supervised\nrepresentation learning (or pre-training). The components of UniTS are designed\nusing sklearn-like APIs to allow flexible extensions. We demonstrate how users\ncan easily perform an analysis task using the user-friendly GUIs, and show the\nsuperior performance of UniTS over the traditional task-specific methods\nwithout self-supervised pre-training on five mainstream tasks and two practical\nsettings.\n","authors":["Zhiyu Liang","Chen Liang","Zheng Liang","Hongzhi Wang"],"pdf_url":"https://arxiv.org/pdf/2303.13804v1.pdf","comment":"4 pages"},{"id":"http://arxiv.org/abs/2303.13801v1","updated":"2023-03-24T04:51:22Z","published":"2023-03-24T04:51:22Z","title":"Toward Open-domain Slot Filling via Self-supervised Co-training","summary":"  Slot filling is one of the critical tasks in modern conversational systems.\nThe majority of existing literature employs supervised learning methods, which\nrequire labeled training data for each new domain. Zero-shot learning and weak\nsupervision approaches, among others, have shown promise as alternatives to\nmanual labeling. Nonetheless, these learning paradigms are significantly\ninferior to supervised learning approaches in terms of performance. To minimize\nthis performance gap and demonstrate the possibility of open-domain slot\nfilling, we propose a Self-supervised Co-training framework, called SCot, that\nrequires zero in-domain manually labeled training examples and works in three\nphases. Phase one acquires two sets of complementary pseudo labels\nautomatically. Phase two leverages the power of the pre-trained language model\nBERT, by adapting it for the slot filling task using these sets of pseudo\nlabels. In phase three, we introduce a self-supervised cotraining mechanism,\nwhere both models automatically select highconfidence soft labels to further\nimprove the performance of the other in an iterative fashion. Our thorough\nevaluations show that SCot outperforms state-of-the-art models by 45.57% and\n37.56% on SGD and MultiWoZ datasets, respectively. Moreover, our proposed\nframework SCot achieves comparable performance when compared to\nstate-of-the-art fully supervised models.\n","authors":["Adib Mosharrof","Moghis Fereidouni","A. B. Siddique"],"pdf_url":"https://arxiv.org/pdf/2303.13801v1.pdf","comment":"10 pages, 6 tables, 2 figures, ACM Web Conference 2023 (WWW'23)"},{"id":"http://arxiv.org/abs/2303.13797v1","updated":"2023-03-24T04:33:40Z","published":"2023-03-24T04:33:40Z","title":"Personalizing Task-oriented Dialog Systems via Zero-shot Generalizable\n  Reward Function","summary":"  Task-oriented dialog systems enable users to accomplish tasks using natural\nlanguage. State-of-the-art systems respond to users in the same way regardless\nof their personalities, although personalizing dialogues can lead to higher\nlevels of adoption and better user experiences. Building personalized dialog\nsystems is an important, yet challenging endeavor and only a handful of works\ntook on the challenge. Most existing works rely on supervised learning\napproaches and require laborious and expensive labeled training data for each\nuser profile. Additionally, collecting and labeling data for each user profile\nis virtually impossible. In this work, we propose a novel framework, P-ToD, to\npersonalize task-oriented dialog systems capable of adapting to a wide range of\nuser profiles in an unsupervised fashion using a zero-shot generalizable reward\nfunction. P-ToD uses a pre-trained GPT-2 as a backbone model and works in three\nphases. Phase one performs task-specific training. Phase two kicks off\nunsupervised personalization by leveraging the proximal policy optimization\nalgorithm that performs policy gradients guided by the zero-shot generalizable\nreward function. Our novel reward function can quantify the quality of the\ngenerated responses even for unseen profiles. The optional final phase\nfine-tunes the personalized model using a few labeled training examples. We\nconduct extensive experimental analysis using the personalized bAbI dialogue\nbenchmark for five tasks and up to 180 diverse user profiles. The experimental\nresults demonstrate that P-ToD, even when it had access to zero labeled\nexamples, outperforms state-of-the-art supervised personalization models and\nachieves competitive performance on BLEU and ROUGE metrics when compared to a\nstrong fully-supervised GPT-2 baseline\n","authors":["A. B. Siddique","M. H. Maqbool","Kshitija Taywade","Hassan Foroosh"],"pdf_url":"https://arxiv.org/pdf/2303.13797v1.pdf","comment":"11 pages, 4 tables, 31st ACM International Conference on Information\n  and Knowledge Management (CIKM'22)"},{"id":"http://arxiv.org/abs/2303.13794v1","updated":"2023-03-24T04:18:13Z","published":"2023-03-24T04:18:13Z","title":"Efficient and Accurate Co-Visible Region Localization with Matching\n  Key-Points Crop (MKPC): A Two-Stage Pipeline for Enhancing Image Matching\n  Performance","summary":"  Image matching is a classic and fundamental task in computer vision. In this\npaper, under the hypothesis that the areas outside the co-visible regions carry\nlittle information, we propose a matching key-points crop (MKPC) algorithm. The\nMKPC locates, proposes and crops the critical regions, which are the co-visible\nareas with great efficiency and accuracy. Furthermore, building upon MKPC, we\npropose a general two-stage pipeline for image matching, which is compatible to\nany image matching models or combinations. We experimented with plugging\nSuperPoint + SuperGlue into the two-stage pipeline, whose results show that our\nmethod enhances the performance for outdoor pose estimations. What's more, in a\nfair comparative condition, our method outperforms the SOTA on Image Matching\nChallenge 2022 Benchmark, which represents the hardest outdoor benchmark of\nimage matching currently.\n","authors":["Hongjian Song","Yuki Kashiwaba","Shuai Wu","Canming Wang"],"pdf_url":"https://arxiv.org/pdf/2303.13794v1.pdf","comment":"9 pages with 6 figures. Many experiments have not yet been conducted,\n  the theoretical sections are rather concise, and the references are not\n  adequately comprehensive. This version of the paper is being released to make\n  this work public, and code will also be published soon. We will continue to\n  conduct additional experiments and periodically update the paper"},{"id":"http://arxiv.org/abs/2303.13793v1","updated":"2023-03-24T04:15:03Z","published":"2023-03-24T04:15:03Z","title":"Forecasting Competitions with Correlated Events","summary":"  Beginning with Witkowski et al. [2022], recent work on forecasting\ncompetitions has addressed incentive problems with the common winner-take-all\nmechanism. Frongillo et al. [2021] propose a competition mechanism based on\nfollow-the-regularized-leader (FTRL), an online learning framework. They show\nthat their mechanism selects an $\\epsilon$-optimal forecaster with high\nprobability using only $O(\\log(n)/\\epsilon^2)$ events. These works, together\nwith all prior work on this problem thus far, assume that events are\nindependent. We initiate the study of forecasting competitions for correlated\nevents. To quantify correlation, we introduce a notion of block correlation,\nwhich allows each event to be strongly correlated with up to $b$ others. We\nshow that under distributions with this correlation, the FTRL mechanism retains\nits $\\epsilon$-optimal guarantee using $O(b^2 \\log(n)/\\epsilon^2)$ events. Our\nproof involves a novel concentration bound for correlated random variables\nwhich may be of broader interest.\n","authors":["Rafael Frongillo","Manuel Lladser","Anish Thilagar","Bo Waggoner"],"pdf_url":"https://arxiv.org/pdf/2303.13793v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.13790v1","updated":"2023-03-24T03:59:19Z","published":"2023-03-24T03:59:19Z","title":"Towards Fair Patient-Trial Matching via Patient-Criterion Level Fairness\n  Constraint","summary":"  Clinical trials are indispensable in developing new treatments, but they face\nobstacles in patient recruitment and retention, hindering the enrollment of\nnecessary participants. To tackle these challenges, deep learning frameworks\nhave been created to match patients to trials. These frameworks calculate the\nsimilarity between patients and clinical trial eligibility criteria,\nconsidering the discrepancy between inclusion and exclusion criteria. Recent\nstudies have shown that these frameworks outperform earlier approaches.\nHowever, deep learning models may raise fairness issues in patient-trial\nmatching when certain sensitive groups of individuals are underrepresented in\nclinical trials, leading to incomplete or inaccurate data and potential harm.\nTo tackle the issue of fairness, this work proposes a fair patient-trial\nmatching framework by generating a patient-criterion level fairness constraint.\nThe proposed framework considers the inconsistency between the embedding of\ninclusion and exclusion criteria among patients of different sensitive groups.\nThe experimental results on real-world patient-trial and patient-criterion\nmatching tasks demonstrate that the proposed framework can successfully\nalleviate the predictions that tend to be biased.\n","authors":["Chia-Yuan Chang","Jiayi Yuan","Sirui Ding","Qiaoyu Tan","Kai Zhang","Xiaoqian Jiang","Xia Hu","Na Zou"],"pdf_url":"https://arxiv.org/pdf/2303.13790v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2212.01141v3","updated":"2023-03-24T03:41:05Z","published":"2022-12-02T12:42:53Z","title":"MHCCL: Masked Hierarchical Cluster-Wise Contrastive Learning for\n  Multivariate Time Series","summary":"  Learning semantic-rich representations from raw unlabeled time series data is\ncritical for downstream tasks such as classification and forecasting.\nContrastive learning has recently shown its promising representation learning\ncapability in the absence of expert annotations. However, existing contrastive\napproaches generally treat each instance independently, which leads to false\nnegative pairs that share the same semantics. To tackle this problem, we\npropose MHCCL, a Masked Hierarchical Cluster-wise Contrastive Learning model,\nwhich exploits semantic information obtained from the hierarchical structure\nconsisting of multiple latent partitions for multivariate time series.\nMotivated by the observation that fine-grained clustering preserves higher\npurity while coarse-grained one reflects higher-level semantics, we propose a\nnovel downward masking strategy to filter out fake negatives and supplement\npositives by incorporating the multi-granularity information from the\nclustering hierarchy. In addition, a novel upward masking strategy is designed\nin MHCCL to remove outliers of clusters at each partition to refine prototypes,\nwhich helps speed up the hierarchical clustering process and improves the\nclustering quality. We conduct experimental evaluations on seven widely-used\nmultivariate time series datasets. The results demonstrate the superiority of\nMHCCL over the state-of-the-art approaches for unsupervised time series\nrepresentation learning.\n","authors":["Qianwen Meng","Hangwei Qian","Yong Liu","Lizhen Cui","Yonghui Xu","Zhiqi Shen"],"pdf_url":"https://arxiv.org/pdf/2212.01141v3.pdf","comment":"accepted by AAAI 2023"},{"id":"http://arxiv.org/abs/2303.13775v1","updated":"2023-03-24T03:28:05Z","published":"2023-03-24T03:28:05Z","title":"GSplit: Scaling Graph Neural Network Training on Large Graphs via\n  Split-Parallelism","summary":"  Large-scale graphs with billions of edges are ubiquitous in many industries,\nscience, and engineering fields such as recommendation systems, social graph\nanalysis, knowledge base, material science, and biology. Graph neural networks\n(GNN), an emerging class of machine learning models, are increasingly adopted\nto learn on these graphs due to their superior performance in various graph\nanalytics tasks. Mini-batch training is commonly adopted to train on large\ngraphs, and data parallelism is the standard approach to scale mini-batch\ntraining to multiple GPUs. In this paper, we argue that several fundamental\nperformance bottlenecks of GNN training systems have to do with inherent\nlimitations of the data parallel approach. We then propose split parallelism, a\nnovel parallel mini-batch training paradigm. We implement split parallelism in\na novel system called gsplit and show that it outperforms state-of-the-art\nsystems such as DGL, Quiver, and PaGraph.\n","authors":["Sandeep Polisetty","Juelin Liu","Kobi Falus","Yi Ren Fung","Seung-Hwan Lim","Hui Guan","Marco Serafini"],"pdf_url":"https://arxiv.org/pdf/2303.13775v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.13773v1","updated":"2023-03-24T03:17:28Z","published":"2023-03-24T03:17:28Z","title":"A Graph Neural Network Approach to Nanosatellite Task Scheduling:\n  Insights into Learning Mixed-Integer Models","summary":"  This study investigates how to schedule nanosatellite tasks more efficiently\nusing Graph Neural Networks (GNN). In the Offline Nanosatellite Task Scheduling\n(ONTS) problem, the goal is to find the optimal schedule for tasks to be\ncarried out in orbit while taking into account Quality-of-Service (QoS)\nconsiderations such as priority, minimum and maximum activation events,\nexecution time-frames, periods, and execution windows, as well as constraints\non the satellite's power resources and the complexity of energy harvesting and\nmanagement. The ONTS problem has been approached using conventional\nmathematical formulations and precise methods, but their applicability to\nchallenging cases of the problem is limited. This study examines the use of\nGNNs in this context, which has been effectively applied to many optimization\nproblems, including traveling salesman problems, scheduling problems, and\nfacility placement problems. Here, we fully represent MILP instances of the\nONTS problem in bipartite graphs. We apply a feature aggregation and\nmessage-passing methodology allied to a ReLU activation function to learn using\na classic deep learning model, obtaining an optimal set of parameters.\nFurthermore, we apply Explainable AI (XAI), another emerging field of research,\nto determine which features -- nodes, constraints -- had the most significant\nimpact on learning performance, shedding light on the inner workings and\ndecision process of such models. We also explored an early fixing approach by\nobtaining an accuracy above 80\\% both in predicting the feasibility of a\nsolution and the probability of a decision variable value being in the optimal\nsolution. Our results point to GNNs as a potentially effective method for\nscheduling nanosatellite tasks and shed light on the advantages of explainable\nmachine learning models for challenging combinatorial optimization problems.\n","authors":["Bruno Machado Pacheco","Laio Oriel Seman","Cezar Antônio Rigo","Eduardo Camponogara","Eduardo Augusto Bezerra","Leandro dos Santos Coelho"],"pdf_url":"https://arxiv.org/pdf/2303.13773v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.13769v1","updated":"2023-03-24T02:59:44Z","published":"2023-03-24T02:59:44Z","title":"Unknown Sniffer for Object Detection: Don't Turn a Blind Eye to Unknown\n  Objects","summary":"  The recently proposed open-world object and open-set detection achieve a\nbreakthrough in finding never-seen-before objects and distinguishing them from\nclass-known ones. However, their studies on knowledge transfer from known\nclasses to unknown ones need to be deeper, leading to the scanty capability for\ndetecting unknowns hidden in the background. In this paper, we propose the\nunknown sniffer (UnSniffer) to find both unknown and known objects. Firstly,\nthe generalized object confidence (GOC) score is introduced, which only uses\nclass-known samples for supervision and avoids improper suppression of unknowns\nin the background. Significantly, such confidence score learned from\nclass-known objects can be generalized to unknown ones. Additionally, we\npropose a negative energy suppression loss to further limit the non-object\nsamples in the background. Next, the best box of each unknown is hard to obtain\nduring inference due to lacking their semantic information in training. To\nsolve this issue, we introduce a graph-based determination scheme to replace\nhand-designed non-maximum suppression (NMS) post-processing. Finally, we\npresent the Unknown Object Detection Benchmark, the first publicly benchmark\nthat encompasses precision evaluation for unknown object detection to our\nknowledge. Experiments show that our method is far better than the existing\nstate-of-the-art methods. Code is available at:\nhttps://github.com/Went-Liang/UnSniffer.\n","authors":["Wenteng Liang","Feng Xue","Yihao Liu","Guofeng Zhong","Anlong Ming"],"pdf_url":"https://arxiv.org/pdf/2303.13769v1.pdf","comment":"CVPR 2023 camera-ready"},{"id":"http://arxiv.org/abs/2303.13763v1","updated":"2023-03-24T02:28:55Z","published":"2023-03-24T02:28:55Z","title":"Edge-free but Structure-aware: Prototype-Guided Knowledge Distillation\n  from GNNs to MLPs","summary":"  Distilling high-accuracy Graph Neural Networks~(GNNs) to low-latency\nmultilayer perceptrons~(MLPs) on graph tasks has become a hot research topic.\nHowever, MLPs rely exclusively on the node features and fail to capture the\ngraph structural information. Previous methods address this issue by processing\ngraph edges into extra inputs for MLPs, but such graph structures may be\nunavailable for various scenarios. To this end, we propose a Prototype-Guided\nKnowledge Distillation~(PGKD) method, which does not require graph\nedges~(edge-free) yet learns structure-aware MLPs. Specifically, we analyze the\ngraph structural information in GNN teachers, and distill such information from\nGNNs to MLPs via prototypes in an edge-free setting. Experimental results on\npopular graph benchmarks demonstrate the effectiveness and robustness of the\nproposed PGKD.\n","authors":["Taiqiang Wu","Zhe Zhao","Jiahao Wang","Xingyu Bai","Lei Wang","Ngai Wong","Yujiu Yang"],"pdf_url":"https://arxiv.org/pdf/2303.13763v1.pdf","comment":"8 pages, 4 figures, 9 tables"},{"id":"http://arxiv.org/abs/2112.05282v3","updated":"2023-03-24T02:16:07Z","published":"2021-12-10T01:25:24Z","title":"RamBoAttack: A Robust Query Efficient Deep Neural Network Decision\n  Exploit","summary":"  Machine learning models are critically susceptible to evasion attacks from\nadversarial examples. Generally, adversarial examples, modified inputs\ndeceptively similar to the original input, are constructed under whitebox\nsettings by adversaries with full access to the model. However, recent attacks\nhave shown a remarkable reduction in query numbers to craft adversarial\nexamples using blackbox attacks. Particularly, alarming is the ability to\nexploit the classification decision from the access interface of a trained\nmodel provided by a growing number of Machine Learning as a Service providers\nincluding Google, Microsoft, IBM and used by a plethora of applications\nincorporating these models. The ability of an adversary to exploit only the\npredicted label from a model to craft adversarial examples is distinguished as\na decision-based attack. In our study, we first deep dive into recent\nstate-of-the-art decision-based attacks in ICLR and SP to highlight the costly\nnature of discovering low distortion adversarial employing gradient estimation\nmethods. We develop a robust query efficient attack capable of avoiding\nentrapment in a local minimum and misdirection from noisy gradients seen in\ngradient estimation methods. The attack method we propose, RamBoAttack,\nexploits the notion of Randomized Block Coordinate Descent to explore the\nhidden classifier manifold, targeting perturbations to manipulate only\nlocalized input features to address the issues of gradient estimation methods.\nImportantly, the RamBoAttack is more robust to the different sample inputs\navailable to an adversary and the targeted class. Overall, for a given target\nclass, RamBoAttack is demonstrated to be more robust at achieving a lower\ndistortion within a given query budget. We curate our extensive results using\nthe large-scale high-resolution ImageNet dataset and open-source our attack,\ntest samples and artifacts on GitHub.\n","authors":["Viet Quoc Vo","Ehsan Abbasnejad","Damith C. Ranasinghe"],"pdf_url":"https://arxiv.org/pdf/2112.05282v3.pdf","comment":"Published in Network and Distributed System Security (NDSS) Symposium\n  2022. Code is available at https://ramboattack.github.io/"},{"id":"http://arxiv.org/abs/2303.13757v1","updated":"2023-03-24T02:13:32Z","published":"2023-03-24T02:13:32Z","title":"Structural Imbalance Aware Graph Augmentation Learning","summary":"  Graph machine learning (GML) has made great progress in node classification,\nlink prediction, graph classification and so on. However, graphs in reality are\noften structurally imbalanced, that is, only a few hub nodes have a denser\nlocal structure and higher influence. The imbalance may compromise the\nrobustness of existing GML models, especially in learning tail nodes. This\npaper proposes a selective graph augmentation method (SAug) to solve this\nproblem. Firstly, a Pagerank-based sampling strategy is designed to identify\nhub nodes and tail nodes in the graph. Secondly, a selective augmentation\nstrategy is proposed, which drops the noisy neighbors of hub nodes on one side,\nand discovers the latent neighbors and generates pseudo neighbors for tail\nnodes on the other side. It can also alleviate the structural imbalance between\ntwo types of nodes. Finally, a GNN model will be retrained on the augmented\ngraph. Extensive experiments demonstrate that SAug can significantly improve\nthe backbone GNNs and achieve superior performance to its competitors of graph\naugmentation methods and hub/tail aware methods.\n","authors":["Zulong Liu"," Kejia-Chen","Zheng Liu"],"pdf_url":"https://arxiv.org/pdf/2303.13757v1.pdf","comment":"13 pages, 11 figures, 7 tables"},{"id":"http://arxiv.org/abs/2303.13755v1","updated":"2023-03-24T02:12:28Z","published":"2023-03-24T02:12:28Z","title":"Sparsifiner: Learning Sparse Instance-Dependent Attention for Efficient\n  Vision Transformers","summary":"  Vision Transformers (ViT) have shown their competitive advantages\nperformance-wise compared to convolutional neural networks (CNNs) though they\noften come with high computational costs. To this end, previous methods explore\ndifferent attention patterns by limiting a fixed number of spatially nearby\ntokens to accelerate the ViT's multi-head self-attention (MHSA) operations.\nHowever, such structured attention patterns limit the token-to-token\nconnections to their spatial relevance, which disregards learned semantic\nconnections from a full attention mask. In this work, we propose a novel\napproach to learn instance-dependent attention patterns, by devising a\nlightweight connectivity predictor module to estimate the connectivity score of\neach pair of tokens. Intuitively, two tokens have high connectivity scores if\nthe features are considered relevant either spatially or semantically. As each\ntoken only attends to a small number of other tokens, the binarized\nconnectivity masks are often very sparse by nature and therefore provide the\nopportunity to accelerate the network via sparse computations. Equipped with\nthe learned unstructured attention pattern, sparse attention ViT (Sparsifiner)\nproduces a superior Pareto-optimal trade-off between FLOPs and top-1 accuracy\non ImageNet compared to token sparsity. Our method reduces 48% to 69% FLOPs of\nMHSA while the accuracy drop is within 0.4%. We also show that combining\nattention and token sparsity reduces ViT FLOPs by over 60%.\n","authors":["Cong Wei","Brendan Duke","Ruowei Jiang","Parham Aarabi","Graham W. Taylor","Florian Shkurti"],"pdf_url":"https://arxiv.org/pdf/2303.13755v1.pdf","comment":"Accepted at CVPR 2023"},{"id":"http://arxiv.org/abs/2202.00091v2","updated":"2023-03-24T02:12:06Z","published":"2022-01-31T21:10:47Z","title":"Query Efficient Decision Based Sparse Attacks Against Black-Box Deep\n  Learning Models","summary":"  Despite our best efforts, deep learning models remain highly vulnerable to\neven tiny adversarial perturbations applied to the inputs. The ability to\nextract information from solely the output of a machine learning model to craft\nadversarial perturbations to black-box models is a practical threat against\nreal-world systems, such as autonomous cars or machine learning models exposed\nas a service (MLaaS). Of particular interest are sparse attacks. The\nrealization of sparse attacks in black-box models demonstrates that machine\nlearning models are more vulnerable than we believe. Because these attacks aim\nto minimize the number of perturbed pixels measured by l_0 norm-required to\nmislead a model by solely observing the decision (the predicted label) returned\nto a model query; the so-called decision-based attack setting. But, such an\nattack leads to an NP-hard optimization problem. We develop an evolution-based\nalgorithm-SparseEvo-for the problem and evaluate against both convolutional\ndeep neural networks and vision transformers. Notably, vision transformers are\nyet to be investigated under a decision-based attack setting. SparseEvo\nrequires significantly fewer model queries than the state-of-the-art sparse\nattack Pointwise for both untargeted and targeted attacks. The attack\nalgorithm, although conceptually simple, is also competitive with only a\nlimited query budget against the state-of-the-art gradient-based whitebox\nattacks in standard computer vision tasks such as ImageNet. Importantly, the\nquery efficient SparseEvo, along with decision-based attacks, in general, raise\nnew questions regarding the safety of deployed systems and poses new directions\nto study and understand the robustness of machine learning models.\n","authors":["Viet Quoc Vo","Ehsan Abbasnejad","Damith C. Ranasinghe"],"pdf_url":"https://arxiv.org/pdf/2202.00091v2.pdf","comment":"Published as a conference paper at the International Conference on\n  Learning Representations (ICLR 2022). Code is available at\n  https://sparseevoattack.github.io/"},{"id":"http://arxiv.org/abs/2303.13752v1","updated":"2023-03-24T02:10:53Z","published":"2023-03-24T02:10:53Z","title":"Leveraging Old Knowledge to Continually Learn New Classes in Medical\n  Images","summary":"  Class-incremental continual learning is a core step towards developing\nartificial intelligence systems that can continuously adapt to changes in the\nenvironment by learning new concepts without forgetting those previously\nlearned. This is especially needed in the medical domain where continually\nlearning from new incoming data is required to classify an expanded set of\ndiseases. In this work, we focus on how old knowledge can be leveraged to learn\nnew classes without catastrophic forgetting. We propose a framework that\ncomprises of two main components: (1) a dynamic architecture with expanding\nrepresentations to preserve previously learned features and accommodate new\nfeatures; and (2) a training procedure alternating between two objectives to\nbalance the learning of new features while maintaining the model's performance\non old classes. Experiment results on multiple medical datasets show that our\nsolution is able to achieve superior performance over state-of-the-art\nbaselines in terms of class accuracy and forgetting.\n","authors":["Evelyn Chee","Mong Li Lee","Wynne Hsu"],"pdf_url":"https://arxiv.org/pdf/2303.13752v1.pdf","comment":"Accepted to AAAI23"},{"id":"http://arxiv.org/abs/2303.13750v1","updated":"2023-03-24T02:07:46Z","published":"2023-03-24T02:07:46Z","title":"LONGNN: Spectral GNNs with Learnable Orthonormal Basis","summary":"  In recent years, a plethora of spectral graph neural networks (GNN) methods\nhave utilized polynomial basis with learnable coefficients to achieve top-tier\nperformances on many node-level tasks. Although various kinds of polynomial\nbases have been explored, each such method adopts a fixed polynomial basis\nwhich might not be the optimal choice for the given graph. Besides, we identify\nthe so-called over-passing issue of these methods and show that it is somewhat\nrooted in their less-principled regularization strategy and unnormalized basis.\nIn this paper, we make the first attempts to address these two issues.\nLeveraging Jacobi polynomials, we design a novel spectral GNN, LON-GNN, with\nLearnable OrthoNormal bases and prove that regularizing coefficients becomes\nequivalent to regularizing the norm of learned filter function now. We conduct\nextensive experiments on diverse graph datasets to evaluate the fitting and\ngeneralization capability of LON-GNN, where the results imply its superiority.\n","authors":["Qian Tao","Zhen Wang","Wenyuan Yu","Yaliang Li","Zhewei Wei"],"pdf_url":"https://arxiv.org/pdf/2303.13750v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2208.11356v2","updated":"2023-03-24T02:06:36Z","published":"2022-08-24T08:09:25Z","title":"Towards Efficient Use of Multi-Scale Features in Transformer-Based\n  Object Detectors","summary":"  Multi-scale features have been proven highly effective for object detection\nbut often come with huge and even prohibitive extra computation costs,\nespecially for the recent Transformer-based detectors. In this paper, we\npropose Iterative Multi-scale Feature Aggregation (IMFA) -- a generic paradigm\nthat enables efficient use of multi-scale features in Transformer-based object\ndetectors. The core idea is to exploit sparse multi-scale features from just a\nfew crucial locations, and it is achieved with two novel designs. First, IMFA\nrearranges the Transformer encoder-decoder pipeline so that the encoded\nfeatures can be iteratively updated based on the detection predictions. Second,\nIMFA sparsely samples scale-adaptive features for refined detection from just a\nfew keypoint locations under the guidance of prior detection predictions. As a\nresult, the sampled multi-scale features are sparse yet still highly beneficial\nfor object detection. Extensive experiments show that the proposed IMFA boosts\nthe performance of multiple Transformer-based object detectors significantly\nyet with only slight computational overhead.\n","authors":["Gongjie Zhang","Zhipeng Luo","Zichen Tian","Jingyi Zhang","Xiaoqin Zhang","Shijian Lu"],"pdf_url":"https://arxiv.org/pdf/2208.11356v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.13746v1","updated":"2023-03-24T01:58:58Z","published":"2023-03-24T01:58:58Z","title":"FixFit: using parameter-compression to solve the inverse problem in\n  overdetermined models","summary":"  All fields of science depend on mathematical models. One of the fundamental\nproblems with using complex nonlinear models is that data-driven parameter\nestimation often fails because interactions between model parameters lead to\nmultiple parameter sets fitting the data equally well. Here, we develop a new\nmethod to address this problem, FixFit, which compresses a given mathematical\nmodel's parameters into a latent representation unique to model outputs. We\nacquire this representation by training a neural network with a bottleneck\nlayer on data pairs of model parameters and model outputs. The bottleneck layer\nnodes correspond to the unique latent parameters, and their dimensionality\nindicates the information content of the model. The trained neural network can\nbe split at the bottleneck layer into an encoder to characterize the\nredundancies and a decoder to uniquely infer latent parameters from\nmeasurements. We demonstrate FixFit in two use cases drawn from classical\nphysics and neuroscience.\n","authors":["Botond B Antal","Anthony G Chesebro","Helmut H Strey","Lilianne R Mujica-Parodi","Corey Weistuch"],"pdf_url":"https://arxiv.org/pdf/2303.13746v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.13745v1","updated":"2023-03-24T01:56:21Z","published":"2023-03-24T01:56:21Z","title":"EdgeTran: Co-designing Transformers for Efficient Inference on Mobile\n  Edge Platforms","summary":"  Automated design of efficient transformer models has recently attracted\nsignificant attention from industry and academia. However, most works only\nfocus on certain metrics while searching for the best-performing transformer\narchitecture. Furthermore, running traditional, complex, and large transformer\nmodels on low-compute edge platforms is a challenging problem. In this work, we\npropose a framework, called ProTran, to profile the hardware performance\nmeasures for a design space of transformer architectures and a diverse set of\nedge devices. We use this profiler in conjunction with the proposed co-design\ntechnique to obtain the best-performing models that have high accuracy on the\ngiven task and minimize latency, energy consumption, and peak power draw to\nenable edge deployment. We refer to our framework for co-optimizing accuracy\nand hardware performance measures as EdgeTran. It searches for the best\ntransformer model and edge device pair. Finally, we propose GPTran, a\nmulti-stage block-level grow-and-prune post-processing step that further\nimproves accuracy in a hardware-aware manner. The obtained transformer model is\n2.8$\\times$ smaller and has a 0.8% higher GLUE score than the baseline\n(BERT-Base). Inference with it on the selected edge device enables 15.0% lower\nlatency, 10.0$\\times$ lower energy, and 10.8$\\times$ lower peak power draw\ncompared to an off-the-shelf GPU.\n","authors":["Shikhar Tuli","Niraj K. Jha"],"pdf_url":"https://arxiv.org/pdf/2303.13745v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.13735v1","updated":"2023-03-24T01:39:56Z","published":"2023-03-24T01:39:56Z","title":"An investigation of licensing of datasets for machine learning based on\n  the GQM model","summary":"  Dataset licensing is currently an issue in the development of machine\nlearning systems. And in the development of machine learning systems, the most\nwidely used are publicly available datasets. However, since the images in the\npublicly available dataset are mainly obtained from the Internet, some images\nare not commercially available. Furthermore, developers of machine learning\nsystems do not often care about the license of the dataset when training\nmachine learning models with it. In summary, the licensing of datasets for\nmachine learning systems is in a state of incompleteness in all aspects at this\nstage.\n  Our investigation of two collection datasets revealed that most of the\ncurrent datasets lacked licenses, and the lack of licenses made it impossible\nto determine the commercial availability of the datasets. Therefore, we decided\nto take a more scientific and systematic approach to investigate the licensing\nof datasets and the licensing of machine learning systems that use the dataset\nto make it easier and more compliant for future developers of machine learning\nsystems.\n","authors":["Junyu Chen","Norihiro Yoshida","Hiroaki Takada"],"pdf_url":"https://arxiv.org/pdf/2303.13735v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.13731v1","updated":"2023-03-24T01:02:59Z","published":"2023-03-24T01:02:59Z","title":"How Does Attention Work in Vision Transformers? A Visual Analytics\n  Attempt","summary":"  Vision transformer (ViT) expands the success of transformer models from\nsequential data to images. The model decomposes an image into many smaller\npatches and arranges them into a sequence. Multi-head self-attentions are then\napplied to the sequence to learn the attention between patches. Despite many\nsuccessful interpretations of transformers on sequential data, little effort\nhas been devoted to the interpretation of ViTs, and many questions remain\nunanswered. For example, among the numerous attention heads, which one is more\nimportant? How strong are individual patches attending to their spatial\nneighbors in different heads? What attention patterns have individual heads\nlearned? In this work, we answer these questions through a visual analytics\napproach. Specifically, we first identify what heads are more important in ViTs\nby introducing multiple pruning-based metrics. Then, we profile the spatial\ndistribution of attention strengths between patches inside individual heads, as\nwell as the trend of attention strengths across attention layers. Third, using\nan autoencoder-based learning solution, we summarize all possible attention\npatterns that individual heads could learn. Examining the attention strengths\nand patterns of the important heads, we answer why they are important. Through\nconcrete case studies with experienced deep learning experts on multiple ViTs,\nwe validate the effectiveness of our solution that deepens the understanding of\nViTs from head importance, head attention strength, and head attention pattern.\n","authors":["Yiran Li","Junpeng Wang","Xin Dai","Liang Wang","Chin-Chia Michael Yeh","Yan Zheng","Wei Zhang","Kwan-Liu Ma"],"pdf_url":"https://arxiv.org/pdf/2303.13731v1.pdf","comment":"Accepted by PacificVis 2023 and selected to be published in TVCG"},{"id":"http://arxiv.org/abs/2303.13727v1","updated":"2023-03-24T00:40:08Z","published":"2023-03-24T00:40:08Z","title":"A Survey on Secure and Private Federated Learning Using Blockchain:\n  Theory and Application in Resource-constrained Computing","summary":"  Federated Learning (FL) has gained widespread popularity in recent years due\nto the fast booming of advanced machine learning and artificial intelligence\nalong with emerging security and privacy threats. FL enables efficient model\ngeneration from local data storage of the edge devices without revealing the\nsensitive data to any entities. While this paradigm partly mitigates the\nprivacy issues of users' sensitive data, the performance of the FL process can\nbe threatened and reached a bottleneck due to the growing cyber threats and\nprivacy violation techniques. To expedite the proliferation of FL process, the\nintegration of blockchain for FL environments has drawn prolific attention from\nthe people of academia and industry. Blockchain has the potential to prevent\nsecurity and privacy threats with its decentralization, immutability,\nconsensus, and transparency characteristic. However, if the blockchain\nmechanism requires costly computational resources, then the\nresource-constrained FL clients cannot be involved in the training. Considering\nthat, this survey focuses on reviewing the challenges, solutions, and future\ndirections for the successful deployment of blockchain in resource-constrained\nFL environments. We comprehensively review variant blockchain mechanisms that\nare suitable for FL process and discuss their trade-offs for a limited resource\nbudget. Further, we extensively analyze the cyber threats that could be\nobserved in a resource-constrained FL environment, and how blockchain can play\na key role to block those cyber attacks. To this end, we highlight some\npotential solutions towards the coupling of blockchain and federated learning\nthat can offer high levels of reliability, data privacy, and distributed\ncomputing performance.\n","authors":["Ervin Moore","Ahmed Imteaj","Shabnam Rezapour","M. Hadi Amini"],"pdf_url":"https://arxiv.org/pdf/2303.13727v1.pdf","comment":null}],"Multimedia":[{"id":"http://arxiv.org/abs/2303.14116v1","updated":"2023-03-24T16:24:08Z","published":"2023-03-24T16:24:08Z","title":"Improving Prediction Performance and Model Interpretability through\n  Attention Mechanisms from Basic and Applied Research Perspectives","summary":"  With the dramatic advances in deep learning technology, machine learning\nresearch is focusing on improving the interpretability of model predictions as\nwell as prediction performance in both basic and applied research. While deep\nlearning models have much higher prediction performance than traditional\nmachine learning models, the specific prediction process is still difficult to\ninterpret and/or explain. This is known as the black-boxing of machine learning\nmodels and is recognized as a particularly important problem in a wide range of\nresearch fields, including manufacturing, commerce, robotics, and other\nindustries where the use of such technology has become commonplace, as well as\nthe medical field, where mistakes are not tolerated. This bulletin is based on\nthe summary of the author's dissertation. The research summarized in the\ndissertation focuses on the attention mechanism, which has been the focus of\nmuch attention in recent years, and discusses its potential for both basic\nresearch in terms of improving prediction performance and interpretability, and\napplied research in terms of evaluating it for real-world applications using\nlarge data sets beyond the laboratory environment. The dissertation also\nconcludes with a summary of the implications of these findings for subsequent\nresearch and future prospects in the field.\n","authors":["Shunsuke Kitada"],"pdf_url":"https://arxiv.org/pdf/2303.14116v1.pdf","comment":"The bulletin of Graduate School of Science and Engineering, Hosei\n  University, Vol.64 (03/2023). This article draws heavily from\n  arxiv:2009.12064, arxiv:2104.08763, arxiv:1905.07289, and arxiv:2204.11588"},{"id":"http://arxiv.org/abs/2303.12930v2","updated":"2023-03-24T11:14:02Z","published":"2023-03-22T22:00:17Z","title":"Dense-Localizing Audio-Visual Events in Untrimmed Videos: A Large-Scale\n  Benchmark and Baseline","summary":"  Existing audio-visual event localization (AVE) handles manually trimmed\nvideos with only a single instance in each of them. However, this setting is\nunrealistic as natural videos often contain numerous audio-visual events with\ndifferent categories. To better adapt to real-life applications, in this paper\nwe focus on the task of dense-localizing audio-visual events, which aims to\njointly localize and recognize all audio-visual events occurring in an\nuntrimmed video. The problem is challenging as it requires fine-grained\naudio-visual scene and context understanding. To tackle this problem, we\nintroduce the first Untrimmed Audio-Visual (UnAV-100) dataset, which contains\n10K untrimmed videos with over 30K audio-visual events. Each video has 2.8\naudio-visual events on average, and the events are usually related to each\nother and might co-occur as in real-life scenes. Next, we formulate the task\nusing a new learning-based framework, which is capable of fully integrating\naudio and visual modalities to localize audio-visual events with various\nlengths and capture dependencies between them in a single pass. Extensive\nexperiments demonstrate the effectiveness of our method as well as the\nsignificance of multi-scale cross-modal perception and dependency modeling for\nthis task.\n","authors":["Tiantian Geng","Teng Wang","Jinming Duan","Runmin Cong","Feng Zheng"],"pdf_url":"https://arxiv.org/pdf/2303.12930v2.pdf","comment":"Accepted by CVPR2023"},{"id":"http://arxiv.org/abs/2303.13859v1","updated":"2023-03-24T08:47:02Z","published":"2023-03-24T08:47:02Z","title":"XGC-VQA: A unified video quality assessment model for User,\n  Professionally, and Occupationally-Generated Content","summary":"  With the rapid growth of Internet video data amounts and types, a unified\nVideo Quality Assessment (VQA) is needed to inspire video communication with\nperceptual quality. To meet the real-time and universal requirements in\nproviding such inspiration, this study proposes a VQA model from a\nclassification of User Generated Content (UGC), Professionally Generated\nContent (PGC), and Occupationally Generated Content (OGC). In the time domain,\nthis study utilizes non-uniform sampling, as each content type has varying\ntemporal importance based on its perceptual quality. In the spatial domain,\ncentralized downsampling is performed before the VQA process by utilizing a\npatch splicing/sampling mechanism to lower complexity for real-time assessment.\nThe experimental results demonstrate that the proposed method achieves a median\ncorrelation of $0.7$ while limiting the computation time below 5s for three\ncontent types, which ensures that the communication experience of UGC, PGC, and\nOGC can be optimized altogether.\n","authors":["Xinhui Huang","Chunyi Li","Abdelhak Bentaleb","Roger Zimmermann","Guangtao Zhai"],"pdf_url":"https://arxiv.org/pdf/2303.13859v1.pdf","comment":"6 pages, 4 figures"},{"id":"http://arxiv.org/abs/2303.13821v1","updated":"2023-03-24T05:57:53Z","published":"2023-03-24T05:57:53Z","title":"Factor Decomposed Generative Adversarial Networks for Text-to-Image\n  Synthesis","summary":"  Prior works about text-to-image synthesis typically concatenated the sentence\nembedding with the noise vector, while the sentence embedding and the noise\nvector are two different factors, which control the different aspects of the\ngeneration. Simply concatenating them will entangle the latent factors and\nencumber the generative model.\n  In this paper, we attempt to decompose these two factors and propose Factor\nDecomposed Generative Adversarial Networks~(FDGAN). To achieve this, we firstly\ngenerate images from the noise vector and then apply the sentence embedding in\nthe normalization layer for both generator and discriminators. We also design\nan additive norm layer to align and fuse the text-image features. The\nexperimental results show that decomposing the noise and the sentence embedding\ncan disentangle latent factors in text-to-image synthesis, and make the\ngenerative model more efficient. Compared with the baseline, FDGAN can achieve\nbetter performance, while fewer parameters are used.\n","authors":["Jiguo Li","Xiaobin Liu","Lirong Zheng"],"pdf_url":"https://arxiv.org/pdf/2303.13821v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2208.11356v2","updated":"2023-03-24T02:06:36Z","published":"2022-08-24T08:09:25Z","title":"Towards Efficient Use of Multi-Scale Features in Transformer-Based\n  Object Detectors","summary":"  Multi-scale features have been proven highly effective for object detection\nbut often come with huge and even prohibitive extra computation costs,\nespecially for the recent Transformer-based detectors. In this paper, we\npropose Iterative Multi-scale Feature Aggregation (IMFA) -- a generic paradigm\nthat enables efficient use of multi-scale features in Transformer-based object\ndetectors. The core idea is to exploit sparse multi-scale features from just a\nfew crucial locations, and it is achieved with two novel designs. First, IMFA\nrearranges the Transformer encoder-decoder pipeline so that the encoded\nfeatures can be iteratively updated based on the detection predictions. Second,\nIMFA sparsely samples scale-adaptive features for refined detection from just a\nfew keypoint locations under the guidance of prior detection predictions. As a\nresult, the sampled multi-scale features are sparse yet still highly beneficial\nfor object detection. Extensive experiments show that the proposed IMFA boosts\nthe performance of multiple Transformer-based object detectors significantly\nyet with only slight computational overhead.\n","authors":["Gongjie Zhang","Zhipeng Luo","Zichen Tian","Jingyi Zhang","Xiaoqin Zhang","Shijian Lu"],"pdf_url":"https://arxiv.org/pdf/2208.11356v2.pdf","comment":null}]},"2023-03-27T00:00:00Z":{"Computation and Language":[{"id":"http://arxiv.org/abs/2303.15445v1","updated":"2023-03-27T17:59:55Z","published":"2023-03-27T17:59:55Z","title":"IRFL: Image Recognition of Figurative Language","summary":"  Figures of speech such as metaphors, similes, and idioms allow language to be\nexpressive, invoke emotion, and communicate abstract ideas that might otherwise\nbe difficult to visualize. These figurative forms are often conveyed through\nmultiple modes, such as text and images, and frequently appear in advertising,\nnews, social media, etc. Understanding multimodal figurative language is an\nessential component of human communication, and it plays a significant role in\nour daily interactions. While humans can intuitively understand multimodal\nfigurative language, this poses a challenging task for machines that requires\nthe cognitive ability to map between domains, abstraction, commonsense, and\nprofound language and cultural knowledge. In this work, we propose the Image\nRecognition of Figurative Language dataset to examine vision and language\nmodels' understanding of figurative language. We leverage human annotation and\nan automatic pipeline we created to generate a multimodal dataset and introduce\ntwo novel tasks as a benchmark for multimodal figurative understanding. We\nexperiment with several baseline models and find that all perform substantially\nworse than humans. We hope our dataset and benchmark will drive the development\nof models that will better understand figurative language.\n","authors":["Ron Yosef","Yonatan Bitton","Dafna Shahaf"],"pdf_url":"https://arxiv.org/pdf/2303.15445v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.15441v1","updated":"2023-03-27T17:59:33Z","published":"2023-03-27T17:59:33Z","title":"Zero-shot Model Diagnosis","summary":"  When it comes to deploying deep vision models, the behavior of these systems\nmust be explicable to ensure confidence in their reliability and fairness. A\ncommon approach to evaluate deep learning models is to build a labeled test set\nwith attributes of interest and assess how well it performs. However, creating\na balanced test set (i.e., one that is uniformly sampled over all the important\ntraits) is often time-consuming, expensive, and prone to mistakes. The question\nwe try to address is: can we evaluate the sensitivity of deep learning models\nto arbitrary visual attributes without an annotated test set? This paper argues\nthe case that Zero-shot Model Diagnosis (ZOOM) is possible without the need for\na test set nor labeling. To avoid the need for test sets, our system relies on\na generative model and CLIP. The key idea is enabling the user to select a set\nof prompts (relevant to the problem) and our system will automatically search\nfor semantic counterfactual images (i.e., synthesized images that flip the\nprediction in the case of a binary classifier) using the generative model. We\nevaluate several visual tasks (classification, key-point detection, and\nsegmentation) in multiple visual domains to demonstrate the viability of our\nmethodology. Extensive experiments demonstrate that our method is capable of\nproducing counterfactual images and offering sensitivity analysis for model\ndiagnosis without the need for a test set.\n","authors":["Jinqi Luo","Zhaoning Wang","Chen Henry Wu","Dong Huang","Fernando De la Torre"],"pdf_url":"https://arxiv.org/pdf/2303.15441v1.pdf","comment":"Accepted in CVPR 2023"},{"id":"http://arxiv.org/abs/2303.15430v1","updated":"2023-03-27T17:54:32Z","published":"2023-03-27T17:54:32Z","title":"TextMI: Textualize Multimodal Information for Integrating Non-verbal\n  Cues in Pre-trained Language Models","summary":"  Pre-trained large language models have recently achieved ground-breaking\nperformance in a wide variety of language understanding tasks. However, the\nsame model can not be applied to multimodal behavior understanding tasks (e.g.,\nvideo sentiment/humor detection) unless non-verbal features (e.g., acoustic and\nvisual) can be integrated with language. Jointly modeling multiple modalities\nsignificantly increases the model complexity, and makes the training process\ndata-hungry. While an enormous amount of text data is available via the web,\ncollecting large-scale multimodal behavioral video datasets is extremely\nexpensive, both in terms of time and money. In this paper, we investigate\nwhether large language models alone can successfully incorporate non-verbal\ninformation when they are presented in textual form. We present a way to\nconvert the acoustic and visual information into corresponding textual\ndescriptions and concatenate them with the spoken text. We feed this augmented\ninput to a pre-trained BERT model and fine-tune it on three downstream\nmultimodal tasks: sentiment, humor, and sarcasm detection. Our approach,\nTextMI, significantly reduces model complexity, adds interpretability to the\nmodel's decision, and can be applied for a diverse set of tasks while achieving\nsuperior (multimodal sarcasm detection) or near SOTA (multimodal sentiment\nanalysis and multimodal humor detection) performance. We propose TextMI as a\ngeneral, competitive baseline for multimodal behavioral analysis tasks,\nparticularly in a low-resource setting.\n","authors":["Md Kamrul Hasan","Md Saiful Islam","Sangwu Lee","Wasifur Rahman","Iftekhar Naim","Mohammed Ibrahim Khan","Ehsan Hoque"],"pdf_url":"https://arxiv.org/pdf/2303.15430v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.08774v3","updated":"2023-03-27T17:46:54Z","published":"2023-03-15T17:15:04Z","title":"GPT-4 Technical Report","summary":"  We report the development of GPT-4, a large-scale, multimodal model which can\naccept image and text inputs and produce text outputs. While less capable than\nhumans in many real-world scenarios, GPT-4 exhibits human-level performance on\nvarious professional and academic benchmarks, including passing a simulated bar\nexam with a score around the top 10% of test takers. GPT-4 is a\nTransformer-based model pre-trained to predict the next token in a document.\nThe post-training alignment process results in improved performance on measures\nof factuality and adherence to desired behavior. A core component of this\nproject was developing infrastructure and optimization methods that behave\npredictably across a wide range of scales. This allowed us to accurately\npredict some aspects of GPT-4's performance based on models trained with no\nmore than 1/1,000th the compute of GPT-4.\n","authors":[" OpenAI"],"pdf_url":"https://arxiv.org/pdf/2303.08774v3.pdf","comment":"100 pages"},{"id":"http://arxiv.org/abs/2303.15422v1","updated":"2023-03-27T17:45:38Z","published":"2023-03-27T17:45:38Z","title":"KPEval: Towards Fine-grained Semantic-based Evaluation of Keyphrase\n  Extraction and Generation Systems","summary":"  Despite the significant advancements in keyphrase extraction and keyphrase\ngeneration methods, the predominant approach for evaluation only relies on\nexact matching with human references and disregards reference-free attributes.\nThis scheme fails to recognize systems that generate keyphrases that are\nsemantically equivalent to the references or keyphrases that have practical\nutility. To better understand the strengths and weaknesses of different\nkeyphrase systems, we propose a comprehensive evaluation framework consisting\nof six critical dimensions: naturalness, faithfulness, saliency, coverage,\ndiversity, and utility. For each dimension, we discuss the desiderata and\ndesign semantic-based metrics that align with the evaluation objectives.\nRigorous meta-evaluation studies demonstrate that our evaluation strategy\ncorrelates better with human preferences compared to a range of previously used\nmetrics. Using this framework, we re-evaluate 18 keyphrase systems and further\ndiscover that (1) the best model differs in different dimensions, with\npre-trained language models achieving the best in most dimensions; (2) the\nutility in downstream tasks does not always correlate well with reference-based\nmetrics; and (3) large language models exhibit a strong performance in\nreference-free evaluation.\n","authors":["Di Wu","Da Yin","Kai-Wei Chang"],"pdf_url":"https://arxiv.org/pdf/2303.15422v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2209.02821v3","updated":"2023-03-27T17:41:21Z","published":"2022-09-06T21:20:41Z","title":"Multilingual Bidirectional Unsupervised Translation Through Multilingual\n  Finetuning and Back-Translation","summary":"  We propose a two-stage approach for training a single NMT model to translate\nunseen languages both to and from English. For the first stage, we initialize\nan encoder-decoder model to pretrained XLM-R and RoBERTa weights, then perform\nmultilingual fine-tuning on parallel data in 40 languages to English. We find\nthis model can generalize to zero-shot translations on unseen languages. For\nthe second stage, we leverage this generalization ability to generate synthetic\nparallel data from monolingual datasets, then train with successive rounds of\nbidirectional back-translation.\n  We term our approach EcXTra ({E}nglish-{c}entric Crosslingual ({X})\n{Tra}nsfer). Our approach is conceptually simple, only using a standard\ncross-entropy objective throughout, and also is data-driven, sequentially\nleveraging auxiliary parallel data and monolingual data. We evaluate our\nunsupervised NMT results on 7 low-resource languages, and find that each round\nof back-translation training further refines bidirectional performance. Our\nfinal single EcXTra-trained model achieves competitive translation performance\nin all translation directions, notably establishing a new state-of-the-art for\nEnglish-to-Kazakh (22.9 > 10.4 BLEU).\n","authors":["Bryan Li","Mohammad Sadegh Rasooli","Ajay Patel","Chris Callison-Burch"],"pdf_url":"https://arxiv.org/pdf/2209.02821v3.pdf","comment":"LoResMT @ EACL 2023"},{"id":"http://arxiv.org/abs/2303.15413v1","updated":"2023-03-27T17:31:13Z","published":"2023-03-27T17:31:13Z","title":"Debiasing Scores and Prompts of 2D Diffusion for Robust Text-to-3D\n  Generation","summary":"  The view inconsistency problem in score-distilling text-to-3D generation,\nalso known as the Janus problem, arises from the intrinsic bias of 2D diffusion\nmodels, which leads to the unrealistic generation of 3D objects. In this work,\nwe explore score-distilling text-to-3D generation and identify the main causes\nof the Janus problem. Based on these findings, we propose two approaches to\ndebias the score-distillation frameworks for robust text-to-3D generation. Our\nfirst approach, called score debiasing, involves gradually increasing the\ntruncation value for the score estimated by 2D diffusion models throughout the\noptimization process. Our second approach, called prompt debiasing, identifies\nconflicting words between user prompts and view prompts utilizing a language\nmodel and adjusts the discrepancy between view prompts and object-space camera\nposes. Our experimental results show that our methods improve realism by\nsignificantly reducing artifacts and achieve a good trade-off between\nfaithfulness to the 2D diffusion models and 3D consistency with little\noverhead.\n","authors":["Susung Hong","Donghoon Ahn","Seungryong Kim"],"pdf_url":"https://arxiv.org/pdf/2303.15413v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.13035v2","updated":"2023-03-27T16:56:26Z","published":"2023-03-23T04:47:46Z","title":"SPeC: A Soft Prompt-Based Calibration on Mitigating Performance\n  Variability in Clinical Notes Summarization","summary":"  Electronic health records (EHRs) store an extensive array of patient\ninformation, encompassing medical histories, diagnoses, treatments, and test\noutcomes. These records are crucial for enabling healthcare providers to make\nwell-informed decisions regarding patient care. Summarizing clinical notes\nfurther assists healthcare professionals in pinpointing potential health risks\nand making better-informed decisions. This process contributes to reducing\nerrors and enhancing patient outcomes by ensuring providers have access to the\nmost pertinent and current patient data. Recent research has shown that\nincorporating prompts with large language models (LLMs) substantially boosts\nthe efficacy of summarization tasks. However, we show that this approach also\nleads to increased output variance, resulting in notably divergent outputs even\nwhen prompts share similar meanings. To tackle this challenge, we introduce a\nmodel-agnostic Soft Prompt-Based Calibration (SPeC) pipeline that employs soft\nprompts to diminish variance while preserving the advantages of prompt-based\nsummarization. Experimental findings on multiple clinical note tasks and LLMs\nindicate that our method not only bolsters performance but also effectively\ncurbs variance for various LLMs, providing a more uniform and dependable\nsolution for summarizing vital medical information.\n","authors":["Yu-Neng Chuang","Ruixiang Tang","Xiaoqian Jiang","Xia Hu"],"pdf_url":"https://arxiv.org/pdf/2303.13035v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.15381v1","updated":"2023-03-27T16:55:49Z","published":"2023-03-27T16:55:49Z","title":"Causal schema induction for knowledge discovery","summary":"  Making sense of familiar yet new situations typically involves making\ngeneralizations about causal schemas, stories that help humans reason about\nevent sequences. Reasoning about events includes identifying cause and effect\nrelations shared across event instances, a process we refer to as causal schema\ninduction. Statistical schema induction systems may leverage structural\nknowledge encoded in discourse or the causal graphs associated with event\nmeaning, however resources to study such causal structure are few in number and\nlimited in size. In this work, we investigate how to apply schema induction\nmodels to the task of knowledge discovery for enhanced search of\nEnglish-language news texts. To tackle the problem of data scarcity, we present\nTorquestra, a manually curated dataset of text-graph-schema units integrating\ntemporal, event, and causal structures. We benchmark our dataset on three\nknowledge discovery tasks, building and evaluating models for each. Results\nshow that systems that harness causal structure are effective at identifying\ntexts sharing similar causal meaning components rather than relying on lexical\ncues alone. We make our dataset and models available for research purposes.\n","authors":["Michael Regan","Jena D. Hwang","Keisuke Sakaguchi","James Pustejovsky"],"pdf_url":"https://arxiv.org/pdf/2303.15381v1.pdf","comment":"8 pages, appendix"},{"id":"http://arxiv.org/abs/2303.15350v1","updated":"2023-03-27T16:07:44Z","published":"2023-03-27T16:07:44Z","title":"Improving Neural Topic Models with Wasserstein Knowledge Distillation","summary":"  Topic modeling is a dominant method for exploring document collections on the\nweb and in digital libraries. Recent approaches to topic modeling use\npretrained contextualized language models and variational autoencoders.\nHowever, large neural topic models have a considerable memory footprint. In\nthis paper, we propose a knowledge distillation framework to compress a\ncontextualized topic model without loss in topic quality. In particular, the\nproposed distillation objective is to minimize the cross-entropy of the soft\nlabels produced by the teacher and the student models, as well as to minimize\nthe squared 2-Wasserstein distance between the latent distributions learned by\nthe two models. Experiments on two publicly available datasets show that the\nstudent trained with knowledge distillation achieves topic coherence much\nhigher than that of the original student model, and even surpasses the teacher\nwhile containing far fewer parameters than the teacher's. The distilled model\nalso outperforms several other competitive topic models on topic coherence.\n","authors":["Suman Adhya","Debarshi Kumar Sanyal"],"pdf_url":"https://arxiv.org/pdf/2303.15350v1.pdf","comment":"Accepted at ECIR 2023"},{"id":"http://arxiv.org/abs/2201.12501v2","updated":"2023-03-27T15:38:19Z","published":"2022-01-29T05:48:42Z","title":"Does Transliteration Help Multilingual Language Modeling?","summary":"  As there is a scarcity of large representative corpora for most languages, it\nis important for Multilingual Language Models (MLLM) to extract the most out of\nexisting corpora. In this regard, script diversity presents a challenge to\nMLLMs by reducing lexical overlap among closely related languages. Therefore,\ntransliterating closely related languages that use different writing scripts to\na common script may improve the downstream task performance of MLLMs. In this\npaper, we pretrain two ALBERT models to empirically measure the effect of\ntransliteration on MLLMs. We specifically focus on the Indo-Aryan language\nfamily, which has the highest script diversity in the world. Afterward, we\nevaluate our models on the IndicGLUE benchmark. We perform Mann-Whitney U test\nto rigorously verify whether the effect of transliteration is significant or\nnot. We find that transliteration benefits the low-resource languages without\nnegatively affecting the comparatively high-resource languages. We also measure\nthe cross-lingual representation similarity (CLRS) of the models using centered\nkernel alignment (CKA) on parallel sentences of eight languages from the\nFLORES-101 dataset. We find that the hidden representations of the\ntransliteration-based model have higher and more stable CLRS scores. Our code\nis available at Github (github.com/ibraheem-moosa/XLM-Indic) and Hugging Face\nHub (huggingface.co/ibraheemmoosa/xlmindic-base-multiscript and\nhuggingface.co/ibraheemmoosa/xlmindic-base-uniscript).\n","authors":["Ibraheem Muhammad Moosa","Mahmud Elahi Akhter","Ashfia Binte Habib"],"pdf_url":"https://arxiv.org/pdf/2201.12501v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.07522v2","updated":"2023-03-27T15:10:51Z","published":"2023-03-13T23:17:51Z","title":"Audio Visual Language Maps for Robot Navigation","summary":"  While interacting in the world is a multi-sensory experience, many robots\ncontinue to predominantly rely on visual perception to map and navigate in\ntheir environments. In this work, we propose Audio-Visual-Language Maps\n(AVLMaps), a unified 3D spatial map representation for storing cross-modal\ninformation from audio, visual, and language cues. AVLMaps integrate the\nopen-vocabulary capabilities of multimodal foundation models pre-trained on\nInternet-scale data by fusing their features into a centralized 3D voxel grid.\nIn the context of navigation, we show that AVLMaps enable robot systems to\nindex goals in the map based on multimodal queries, e.g., textual descriptions,\nimages, or audio snippets of landmarks. In particular, the addition of audio\ninformation enables robots to more reliably disambiguate goal locations.\nExtensive experiments in simulation show that AVLMaps enable zero-shot\nmultimodal goal navigation from multimodal prompts and provide 50% better\nrecall in ambiguous scenarios. These capabilities extend to mobile robots in\nthe real world - navigating to landmarks referring to visual, audio, and\nspatial concepts. Videos and code are available at: https://avlmaps.github.io.\n","authors":["Chenguang Huang","Oier Mees","Andy Zeng","Wolfram Burgard"],"pdf_url":"https://arxiv.org/pdf/2303.07522v2.pdf","comment":"Project page: https://avlmaps.github.io/"},{"id":"http://arxiv.org/abs/2303.15265v1","updated":"2023-03-27T14:54:43Z","published":"2023-03-27T14:54:43Z","title":"Bilex Rx: Lexical Data Augmentation for Massively Multilingual Machine\n  Translation","summary":"  Neural machine translation (NMT) has progressed rapidly over the past several\nyears, and modern models are able to achieve relatively high quality using only\nmonolingual text data, an approach dubbed Unsupervised Machine Translation\n(UNMT). However, these models still struggle in a variety of ways, including\naspects of translation that for a human are the easiest - for instance,\ncorrectly translating common nouns. This work explores a cheap and abundant\nresource to combat this problem: bilingual lexica. We test the efficacy of\nbilingual lexica in a real-world set-up, on 200-language translation models\ntrained on web-crawled text. We present several findings: (1) using lexical\ndata augmentation, we demonstrate sizable performance gains for unsupervised\ntranslation; (2) we compare several families of data augmentation,\ndemonstrating that they yield similar improvements, and can be combined for\neven greater improvements; (3) we demonstrate the importance of carefully\ncurated lexica over larger, noisier ones, especially with larger models; and\n(4) we compare the efficacy of multilingual lexicon data versus\nhuman-translated parallel data. Finally, we open-source GATITOS (available at\nhttps://github.com/google-research/url-nlp/tree/main/gatitos), a new\nmultilingual lexicon for 26 low-resource languages, which had the highest\nperformance among lexica in our experiments.\n","authors":["Alex Jones","Isaac Caswell","Ishank Saxena","Orhan Firat"],"pdf_url":"https://arxiv.org/pdf/2303.15265v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.15247v1","updated":"2023-03-27T14:31:25Z","published":"2023-03-27T14:31:25Z","title":"Zero-Shot Composed Image Retrieval with Textual Inversion","summary":"  Composed Image Retrieval (CIR) aims to retrieve a target image based on a\nquery composed of a reference image and a relative caption that describes the\ndifference between the two images. The high effort and cost required for\nlabeling datasets for CIR hamper the widespread usage of existing methods, as\nthey rely on supervised learning. In this work, we propose a new task,\nZero-Shot CIR (ZS-CIR), that aims to address CIR without requiring a labeled\ntraining dataset. Our approach, named zero-Shot composEd imAge Retrieval with\ntextuaL invErsion (SEARLE), maps the visual features of the reference image\ninto a pseudo-word token in CLIP token embedding space and integrates it with\nthe relative caption. To support research on ZS-CIR, we introduce an\nopen-domain benchmarking dataset named Composed Image Retrieval on Common\nObjects in context (CIRCO), which is the first dataset for CIR containing\nmultiple ground truths for each query. The experiments show that SEARLE\nexhibits better performance than the baselines on the two main datasets for CIR\ntasks, FashionIQ and CIRR, and on the proposed CIRCO. The dataset, the code and\nthe model are publicly available at https://github.com/miccunifi/SEARLE .\n","authors":["Alberto Baldrati","Lorenzo Agnolucci","Marco Bertini","Alberto Del Bimbo"],"pdf_url":"https://arxiv.org/pdf/2303.15247v1.pdf","comment":"SEARLE repository: https://github.com/miccunifi/SEARLE . CIRCO\n  repository: https://github.com/miccunifi/CIRCO"},{"id":"http://arxiv.org/abs/2303.09522v2","updated":"2023-03-27T14:22:28Z","published":"2023-03-16T17:38:15Z","title":"$P+$: Extended Textual Conditioning in Text-to-Image Generation","summary":"  We introduce an Extended Textual Conditioning space in text-to-image models,\nreferred to as $P+$. This space consists of multiple textual conditions,\nderived from per-layer prompts, each corresponding to a layer of the denoising\nU-net of the diffusion model.\n  We show that the extended space provides greater disentangling and control\nover image synthesis. We further introduce Extended Textual Inversion (XTI),\nwhere the images are inverted into $P+$, and represented by per-layer tokens.\n  We show that XTI is more expressive and precise, and converges faster than\nthe original Textual Inversion (TI) space. The extended inversion method does\nnot involve any noticeable trade-off between reconstruction and editability and\ninduces more regular inversions.\n  We conduct a series of extensive experiments to analyze and understand the\nproperties of the new space, and to showcase the effectiveness of our method\nfor personalizing text-to-image models. Furthermore, we utilize the unique\nproperties of this space to achieve previously unattainable results in\nobject-style mixing using text-to-image models. Project page:\nhttps://prompt-plus.github.io\n","authors":["Andrey Voynov","Qinghao Chu","Daniel Cohen-Or","Kfir Aberman"],"pdf_url":"https://arxiv.org/pdf/2303.09522v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2210.06254v2","updated":"2023-03-27T14:11:29Z","published":"2022-10-12T14:37:00Z","title":"Zero-Shot On-the-Fly Event Schema Induction","summary":"  What are the events involved in a pandemic outbreak? What steps should be\ntaken when planning a wedding? The answers to these questions can be found by\ncollecting many documents on the complex event of interest, extracting relevant\ninformation, and analyzing it. We present a new approach in which large\nlanguage models are utilized to generate source documents that allow\npredicting, given a high-level event definition, the specific events,\narguments, and relations between them to construct a schema that describes the\ncomplex event in its entirety. Using our model, complete schemas on any topic\ncan be generated on-the-fly without any manual data collection, i.e., in a\nzero-shot manner. Moreover, we develop efficient methods to extract pertinent\ninformation from texts and demonstrate in a series of experiments that these\nschemas are considered to be more complete than human-curated ones in the\nmajority of examined scenarios. Finally, we show that this framework is\ncomparable in performance with previous supervised schema induction methods\nthat rely on collecting real texts while being more general and flexible\nwithout the need for a predefined ontology.\n","authors":["Rotem Dror","Haoyu Wang","Dan Roth"],"pdf_url":"https://arxiv.org/pdf/2210.06254v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.15230v1","updated":"2023-03-27T14:10:26Z","published":"2023-03-27T14:10:26Z","title":"Troika: Multi-Path Cross-Modal Traction for Compositional Zero-Shot\n  Learning","summary":"  Recent compositional zero-shot learning (CZSL) methods adapt pre-trained\nvision-language models (VLMs) by constructing trainable prompts only for\ncomposed state-object pairs. Relying on learning the joint representation of\nseen compositions, these methods ignore the explicit modeling of the state and\nobject, thus limiting the exploitation of pre-trained knowledge and\ngeneralization to unseen compositions. With a particular focus on the\nuniversality of the solution, in this work, we propose a novel paradigm for\nCZSL models that establishes three identification branches (i.e., Multi-Path)\nto jointly model the state, object, and composition. The presented Troika is\nour implementation that aligns the branch-specific prompt representations with\ndecomposed visual features. To calibrate the bias between semantically similar\nmulti-modal representations, we further devise a Cross-Modal Traction module\ninto Troika that shifts the prompt representation towards the current visual\ncontent. We conduct extensive experiments on three popular benchmarks, where\nour method significantly outperforms existing methods in both closed-world and\nopen-world settings.\n","authors":["Siteng Huang","Biao Gong","Yutong Feng","Yiliang Lv","Donglin Wang"],"pdf_url":"https://arxiv.org/pdf/2303.15230v1.pdf","comment":"14 pages"},{"id":"http://arxiv.org/abs/2303.15193v1","updated":"2023-03-27T13:29:09Z","published":"2023-03-27T13:29:09Z","title":"CoCon: A Data Set on Combined Contextualized Research Artifact Use","summary":"  In the wake of information overload in academia, methodologies and systems\nfor search, recommendation, and prediction to aid researchers in identifying\nrelevant research are actively studied and developed. Existing work, however,\nis limited in terms of granularity, focusing only on the level of papers or a\nsingle type of artifact, such as data sets. To enable more holistic analyses\nand systems dealing with academic publications and their content, we propose\nCoCon, a large scholarly data set reflecting the combined use of research\nartifacts, contextualized in academic publications' full-text. Our data set\ncomprises 35 k artifacts (data sets, methods, models, and tasks) and 340 k\npublications. We additionally formalize a link prediction task for \"combined\nresearch artifact use prediction\" and provide code to utilize analyses of and\nthe development of ML applications on our data. All data and code is publicly\navailable at https://github.com/IllDepence/contextgraph.\n","authors":["Tarek Saier","Youxiang Dong","Michael Färber"],"pdf_url":"https://arxiv.org/pdf/2303.15193v1.pdf","comment":"submitted to JCDL2023"},{"id":"http://arxiv.org/abs/2303.15190v1","updated":"2023-03-27T13:26:02Z","published":"2023-03-27T13:26:02Z","title":"Evaluating self-attention interpretability through human-grounded\n  experimental protocol","summary":"  Attention mechanisms have played a crucial role in the development of complex\narchitectures such as Transformers in natural language processing. However,\nTransformers remain hard to interpret and are considered as black-boxes. This\npaper aims to assess how attention coefficients from Transformers can help in\nproviding interpretability. A new attention-based interpretability method\ncalled CLaSsification-Attention (CLS-A) is proposed. CLS-A computes an\ninterpretability score for each word based on the attention coefficient\ndistribution related to the part specific to the classification task within the\nTransformer architecture. A human-grounded experiment is conducted to evaluate\nand compare CLS-A to other interpretability methods. The experimental protocol\nrelies on the capacity of an interpretability method to provide explanation in\nline with human reasoning. Experiment design includes measuring reaction times\nand correct response rates by human subjects. CLS-A performs comparably to\nusual interpretability methods regarding average participant reaction time and\naccuracy. The lower computational cost of CLS-A compared to other\ninterpretability methods and its availability by design within the classifier\nmake it particularly interesting. Data analysis also highlights the link\nbetween the probability score of a classifier prediction and adequate\nexplanations. Finally, our work confirms the relevancy of the use of CLS-A and\nshows to which extent self-attention contains rich information to explain\nTransformer classifiers.\n","authors":["Milan Bhan","Nina Achache","Victor Legrand","Annabelle Blangero","Nicolas Chesneau"],"pdf_url":"https://arxiv.org/pdf/2303.15190v1.pdf","comment":"11 pages, 7 figures"},{"id":"http://arxiv.org/abs/2303.15132v1","updated":"2023-03-27T12:08:05Z","published":"2023-03-27T12:08:05Z","title":"Cross-utterance ASR Rescoring with Graph-based Label Propagation","summary":"  We propose a novel approach for ASR N-best hypothesis rescoring with\ngraph-based label propagation by leveraging cross-utterance acoustic\nsimilarity. In contrast to conventional neural language model (LM) based ASR\nrescoring/reranking models, our approach focuses on acoustic information and\nconducts the rescoring collaboratively among utterances, instead of\nindividually. Experiments on the VCTK dataset demonstrate that our approach\nconsistently improves ASR performance, as well as fairness across speaker\ngroups with different accents. Our approach provides a low-cost solution for\nmitigating the majoritarian bias of ASR systems, without the need to train new\ndomain- or accent-specific models.\n","authors":["Srinath Tankasala","Long Chen","Andreas Stolcke","Anirudh Raju","Qianli Deng","Chander Chandak","Aparna Khare","Roland Maas","Venkatesh Ravichandran"],"pdf_url":"https://arxiv.org/pdf/2303.15132v1.pdf","comment":"To appear in IEEE ICASSP 2023"},{"id":"http://arxiv.org/abs/2303.15125v1","updated":"2023-03-27T11:56:26Z","published":"2023-03-27T11:56:26Z","title":"LMCanvas: Object-Oriented Interaction to Personalize Large Language\n  Model-Powered Writing Environments","summary":"  Large language models (LLMs) can enhance writing by automating or supporting\nspecific tasks in writers' workflows (e.g., paraphrasing, creating analogies).\nLeveraging this capability, a collection of interfaces have been developed that\nprovide LLM-powered tools for specific writing tasks. However, these interfaces\nprovide limited support for writers to create personal tools for their own\nunique tasks, and may not comprehensively fulfill a writer's needs -- requiring\nthem to continuously switch between interfaces during writing. In this work, we\nenvision LMCanvas, an interface that enables writers to create their own\nLLM-powered writing tools and arrange their personal writing environment by\ninteracting with \"blocks\" in a canvas. In this interface, users can create text\nblocks to encapsulate writing and LLM prompts, model blocks for model parameter\nconfigurations, and connect these to create pipeline blocks that output\ngenerations. In this workshop paper, we discuss the design for LMCanvas and our\nplans to develop this concept.\n","authors":["Tae Soo Kim","Arghya Sarkar","Yoonjoo Lee","Minsuk Chang","Juho Kim"],"pdf_url":"https://arxiv.org/pdf/2303.15125v1.pdf","comment":"Accepted to CHI 2023 Workshop on Generative AI and HCI"},{"id":"http://arxiv.org/abs/2303.15116v1","updated":"2023-03-27T11:35:40Z","published":"2023-03-27T11:35:40Z","title":"An ontology-aided, natural language-based approach for multi-constraint\n  BIM model querying","summary":"  Being able to efficiently retrieve the required building information is\ncritical for construction project stakeholders to carry out their engineering\nand management activities. Natural language interface (NLI) systems are\nemerging as a time and cost-effective way to query Building Information Models\n(BIMs). However, the existing methods cannot logically combine different\nconstraints to perform fine-grained queries, dampening the usability of natural\nlanguage (NL)-based BIM queries. This paper presents a novel ontology-aided\nsemantic parser to automatically map natural language queries (NLQs) that\ncontain different attribute and relational constraints into computer-readable\ncodes for querying complex BIM models. First, a modular ontology was developed\nto represent NL expressions of Industry Foundation Classes (IFC) concepts and\nrelationships, and was then populated with entities from target BIM models to\nassimilate project-specific information. Hereafter, the ontology-aided semantic\nparser progressively extracts concepts, relationships, and value restrictions\nfrom NLQs to fully identify constraint conditions, resulting in standard SPARQL\nqueries with reasoning rules to successfully retrieve IFC-based BIM models. The\napproach was evaluated based on 225 NLQs collected from BIM users, with a 91%\naccuracy rate. Finally, a case study about the design-checking of a real-world\nresidential building demonstrates the practical value of the proposed approach\nin the construction industry.\n","authors":["Mengtian Yin","Llewellyn Tang","Chris Webster","Shen Xu","Xiongyi Li","Huaquan Ying"],"pdf_url":"https://arxiv.org/pdf/2303.15116v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.15110v1","updated":"2023-03-27T11:29:09Z","published":"2023-03-27T11:29:09Z","title":"Beyond Toxic: Toxicity Detection Datasets are Not Enough for Brand\n  Safety","summary":"  The rapid growth in user generated content on social media has resulted in a\nsignificant rise in demand for automated content moderation. Various methods\nand frameworks have been proposed for the tasks of hate speech detection and\ntoxic comment classification. In this work, we combine common datasets to\nextend these tasks to brand safety. Brand safety aims to protect commercial\nbranding by identifying contexts where advertisements should not appear and\ncovers not only toxicity, but also other potentially harmful content. As these\ndatasets contain different label sets, we approach the overall problem as a\nbinary classification task. We demonstrate the need for building brand safety\nspecific datasets via the application of common toxicity detection datasets to\na subset of brand safety and empirically analyze the effects of weighted\nsampling strategies in text classification.\n","authors":["Elizaveta Korotkova","Isaac Kwan Yin Chung"],"pdf_url":"https://arxiv.org/pdf/2303.15110v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.15100v1","updated":"2023-03-27T11:08:35Z","published":"2023-03-27T11:08:35Z","title":"An Information Extraction Study: Take In Mind the Tokenization!","summary":"  Current research on the advantages and trade-offs of using characters,\ninstead of tokenized text, as input for deep learning models, has evolved\nsubstantially. New token-free models remove the traditional tokenization step;\nhowever, their efficiency remains unclear. Moreover, the effect of tokenization\nis relatively unexplored in sequence tagging tasks. To this end, we investigate\nthe impact of tokenization when extracting information from documents and\npresent a comparative study and analysis of subword-based and character-based\nmodels. Specifically, we study Information Extraction (IE) from biomedical\ntexts. The main outcome is twofold: tokenization patterns can introduce\ninductive bias that results in state-of-the-art performance, and the\ncharacter-based models produce promising results; thus, transitioning to\ntoken-free IE models is feasible.\n","authors":["Christos Theodoropoulos","Marie-Francine Moens"],"pdf_url":"https://arxiv.org/pdf/2303.15100v1.pdf","comment":"presented at EUSFLAT 2023"},{"id":"http://arxiv.org/abs/2303.15078v1","updated":"2023-03-27T10:40:59Z","published":"2023-03-27T10:40:59Z","title":"Large Language Models are Diverse Role-Players for Summarization\n  Evaluation","summary":"  Text summarization has a wide range of applications in many scenarios. The\nevaluation of the quality of the generated text is a complex problem. A big\nchallenge to language evaluation is that there is a clear divergence between\nexisting metrics and human evaluation. For example, the quality of a document\nsummary can be measured by human annotators from both objective aspects, such\nas grammatical and semantic correctness, as well as subjective dimensions, such\nas comprehensiveness, succinctness, and interestingness. Most of the automatic\nevaluation methods like BLUE/ROUGE may be not able to capture the above\ndimensions well. In this paper, we propose a new evaluation framework based on\nLLMs, which provides a comprehensive evaluation framework by comparing\ngenerated text and reference text from both objective and subjective aspects.\nFirst, we propose to model objective and subjective dimensions of generated\ntext based on roleplayers prompting mechanism. Furthermore, we introduce a\ncontext-based prompting mechanism that is able to generate dynamic roleplayer\nprofiles based on input context. Finally, we design a multi-roleplayer\nprompting technology based on batch prompting to integrate multiple evaluation\nresults into evaluation results. Experimental results on two real datasets for\nsummarization show that our model is highly competitive and has a very high\nconsistency with human annotators.\n","authors":["Ning Wu","Ming Gong","Linjun Shou","Shining Liang","Daxin Jiang"],"pdf_url":"https://arxiv.org/pdf/2303.15078v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.15056v1","updated":"2023-03-27T09:59:48Z","published":"2023-03-27T09:59:48Z","title":"ChatGPT Outperforms Crowd-Workers for Text-Annotation Tasks","summary":"  Many NLP applications require manual data annotations for a variety of tasks,\nnotably to train classifiers or evaluate the performance of unsupervised\nmodels. Depending on the size and degree of complexity, the tasks may be\nconducted by crowd-workers on platforms such as MTurk as well as trained\nannotators, such as research assistants. Using a sample of 2,382 tweets, we\ndemonstrate that ChatGPT outperforms crowd-workers for several annotation\ntasks, including relevance, stance, topics, and frames detection. Specifically,\nthe zero-shot accuracy of ChatGPT exceeds that of crowd-workers for four out of\nfive tasks, while ChatGPT's intercoder agreement exceeds that of both\ncrowd-workers and trained annotators for all tasks. Moreover, the\nper-annotation cost of ChatGPT is less than $0.003 -- about twenty times\ncheaper than MTurk. These results show the potential of large language models\nto drastically increase the efficiency of text classification.\n","authors":["Fabrizio Gilardi","Meysam Alizadeh","Maël Kubli"],"pdf_url":"https://arxiv.org/pdf/2303.15056v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.15049v1","updated":"2023-03-27T09:46:56Z","published":"2023-03-27T09:46:56Z","title":"InterviewBot: Real-Time End-to-End Dialogue System to Interview Students\n  for College Admission","summary":"  We present the InterviewBot that dynamically integrates conversation history\nand customized topics into a coherent embedding space to conduct 10 mins\nhybrid-domain (open and closed) conversations with foreign students applying to\nU.S. colleges for assessing their academic and cultural readiness. To build a\nneural-based end-to-end dialogue model, 7,361 audio recordings of\nhuman-to-human interviews are automatically transcribed, where 440 are manually\ncorrected for finetuning and evaluation. To overcome the input/output size\nlimit of a transformer-based encoder-decoder model, two new methods are\nproposed, context attention and topic storing, allowing the model to make\nrelevant and consistent interactions. Our final model is tested both\nstatistically by comparing its responses to the interview data and dynamically\nby inviting professional interviewers and various students to interact with it\nin real-time, finding it highly satisfactory in fluency and context awareness.\n","authors":["Zihao Wang","Jinho Choi"],"pdf_url":"https://arxiv.org/pdf/2303.15049v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.15016v1","updated":"2023-03-27T08:59:55Z","published":"2023-03-27T08:59:55Z","title":"Borrowing Human Senses: Comment-Aware Self-Training for Social Media\n  Multimodal Classification","summary":"  Social media is daily creating massive multimedia content with paired image\nand text, presenting the pressing need to automate the vision and language\nunderstanding for various multimodal classification tasks. Compared to the\ncommonly researched visual-lingual data, social media posts tend to exhibit\nmore implicit image-text relations. To better glue the cross-modal semantics\ntherein, we capture hinting features from user comments, which are retrieved\nvia jointly leveraging visual and lingual similarity. Afterwards, the\nclassification tasks are explored via self-training in a teacher-student\nframework, motivated by the usually limited labeled data scales in existing\nbenchmarks. Substantial experiments are conducted on four multimodal social\nmedia benchmarks for image text relation classification, sarcasm detection,\nsentiment classification, and hate speech detection. The results show that our\nmethod further advances the performance of previous state-of-the-art models,\nwhich do not employ comment modeling or self-training.\n","authors":["Chunpu Xu","Jing Li"],"pdf_url":"https://arxiv.org/pdf/2303.15016v1.pdf","comment":"accepted to EMNLP 2022"},{"id":"http://arxiv.org/abs/2303.15006v1","updated":"2023-03-27T08:47:18Z","published":"2023-03-27T08:47:18Z","title":"Curriculum Learning for Compositional Visual Reasoning","summary":"  Visual Question Answering (VQA) is a complex task requiring large datasets\nand expensive training. Neural Module Networks (NMN) first translate the\nquestion to a reasoning path, then follow that path to analyze the image and\nprovide an answer. We propose an NMN method that relies on predefined\ncross-modal embeddings to ``warm start'' learning on the GQA dataset, then\nfocus on Curriculum Learning (CL) as a way to improve training and make a\nbetter use of the data. Several difficulty criteria are employed for defining\nCL methods. We show that by an appropriate selection of the CL method the cost\nof training and the amount of training data can be greatly reduced, with a\nlimited impact on the final VQA accuracy. Furthermore, we introduce\nintermediate losses during training and find that this allows to simplify the\nCL strategy.\n","authors":["Wafa Aissa","Marin Ferecatu","Michel Crucianu"],"pdf_url":"https://arxiv.org/pdf/2303.15006v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2211.14905v2","updated":"2023-03-27T08:39:13Z","published":"2022-11-27T18:13:05Z","title":"Multi-Modal Few-Shot Temporal Action Detection","summary":"  Few-shot (FS) and zero-shot (ZS) learning are two different approaches for\nscaling temporal action detection (TAD) to new classes. The former adapts a\npretrained vision model to a new task represented by as few as a single video\nper class, whilst the latter requires no training examples by exploiting a\nsemantic description of the new class. In this work, we introduce a new\nmulti-modality few-shot (MMFS) TAD problem, which can be considered as a\nmarriage of FS-TAD and ZS-TAD by leveraging few-shot support videos and new\nclass names jointly. To tackle this problem, we further introduce a novel\nMUlti-modality PromPt mETa-learning (MUPPET) method. This is enabled by\nefficiently bridging pretrained vision and language models whilst maximally\nreusing already learned capacity. Concretely, we construct multi-modal prompts\nby mapping support videos into the textual token space of a vision-language\nmodel using a meta-learned adapter-equipped visual semantics tokenizer. To\ntackle large intra-class variation, we further design a query feature\nregulation scheme. Extensive experiments on ActivityNetv1.3 and THUMOS14\ndemonstrate that our MUPPET outperforms state-of-the-art alternative methods,\noften by a large margin. We also show that our MUPPET can be easily extended to\ntackle the few-shot object detection problem and again achieves the\nstate-of-the-art performance on MS-COCO dataset. The code will be available in\nhttps://github.com/sauradip/MUPPET\n","authors":["Sauradip Nag","Mengmeng Xu","Xiatian Zhu","Juan-Manuel Perez-Rua","Bernard Ghanem","Yi-Zhe Song","Tao Xiang"],"pdf_url":"https://arxiv.org/pdf/2211.14905v2.pdf","comment":"Technical Report"},{"id":"http://arxiv.org/abs/2303.14963v1","updated":"2023-03-27T07:53:23Z","published":"2023-03-27T07:53:23Z","title":"Variation and Instability in Dialect-Based Embedding Spaces","summary":"  This paper measures variation in embedding spaces which have been trained on\ndifferent regional varieties of English while controlling for instability in\nthe embeddings. While previous work has shown that it is possible to\ndistinguish between similar varieties of a language, this paper experiments\nwith two follow-up questions: First, does the variety represented in the\ntraining data systematically influence the resulting embedding space after\ntraining? This paper shows that differences in embeddings across varieties are\nsignificantly higher than baseline instability. Second, is such dialect-based\nvariation spread equally throughout the lexicon? This paper shows that specific\nparts of the lexicon are particularly subject to variation. Taken together,\nthese experiments confirm that embedding spaces are significantly influenced by\nthe dialect represented in the training data. This finding implies that there\nis semantic variation across dialects, in addition to previously-studied\nlexical and syntactic variation.\n","authors":["Jonathan Dunn"],"pdf_url":"https://arxiv.org/pdf/2303.14963v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.14957v1","updated":"2023-03-27T07:40:59Z","published":"2023-03-27T07:40:59Z","title":"unarXive 2022: All arXiv Publications Pre-Processed for NLP, Including\n  Structured Full-Text and Citation Network","summary":"  Large-scale data sets on scholarly publications are the basis for a variety\nof bibliometric analyses and natural language processing (NLP) applications.\nEspecially data sets derived from publication's full-text have recently gained\nattention. While several such data sets already exist, we see key shortcomings\nin terms of their domain and time coverage, citation network completeness, and\nrepresentation of full-text content. To address these points, we propose a new\nversion of the data set unarXive. We base our data processing pipeline and\noutput format on two existing data sets, and improve on each of them. Our\nresulting data set comprises 1.9 M publications spanning multiple disciplines\nand 32 years. It furthermore has a more complete citation network than its\npredecessors and retains a richer representation of document structure as well\nas non-textual publication content such as mathematical notation. In addition\nto the data set, we provide ready-to-use training/test data for citation\nrecommendation and IMRaD classification. All data and source code is publicly\navailable at https://github.com/IllDepence/unarXive.\n","authors":["Tarek Saier","Johan Krause","Michael Färber"],"pdf_url":"https://arxiv.org/pdf/2303.14957v1.pdf","comment":"submitted to JCDL2023"},{"id":"http://arxiv.org/abs/2303.14956v1","updated":"2023-03-27T07:39:05Z","published":"2023-03-27T07:39:05Z","title":"Unified Text Structuralization with Instruction-tuned Language Models","summary":"  Text structuralization is one of the important fields of natural language\nprocessing (NLP) consists of information extraction (IE) and structure\nformalization. However, current studies of text structuralization suffer from a\nshortage of manually annotated high-quality datasets from different domains and\nlanguages, which require specialized professional knowledge. In addition, most\nIE methods are designed for a specific type of structured data, e.g., entities,\nrelations, and events, making them hard to generalize to others. In this work,\nwe propose a simple and efficient approach to instruct large language model\n(LLM) to extract a variety of structures from texts. More concretely, we add a\nprefix and a suffix instruction to indicate the desired IE task and structure\ntype, respectively, before feeding the text into a LLM. Experiments on two LLMs\nshow that this approach can enable language models to perform comparable with\nother state-of-the-art methods on datasets of a variety of languages and\nknowledge, and can generalize to other IE sub-tasks via changing the content of\ninstruction. Another benefit of our approach is that it can help researchers to\nbuild datasets in low-source and domain-specific scenarios, e.g., fields in\nfinance and law, with low cost.\n","authors":["Xuanfan Ni","Piji Li"],"pdf_url":"https://arxiv.org/pdf/2303.14956v1.pdf","comment":"13 pages, 5 figures"},{"id":"http://arxiv.org/abs/2210.16433v3","updated":"2023-03-27T07:33:14Z","published":"2022-10-28T23:18:43Z","title":"Knowledge-in-Context: Towards Knowledgeable Semi-Parametric Language\n  Models","summary":"  Fully-parametric language models generally require a huge number of model\nparameters to store the necessary knowledge for solving multiple natural\nlanguage tasks in zero/few-shot settings. In addition, it is hard to adapt to\nthe evolving world knowledge without the costly model re-training. In this\npaper, we develop a novel semi-parametric language model architecture,\nKnowledge-in-Context (KiC), which empowers a parametric text-to-text language\nmodel with a knowledge-rich external memory. Specifically, the external memory\ncontains six different types of knowledge: entity, dictionary, commonsense,\nevent, script, and causality knowledge. For each input instance, the KiC model\nadaptively selects a knowledge type and retrieves the most helpful pieces of\nknowledge. The input instance along with its knowledge augmentation is fed into\na text-to-text model (e.g., T5) to generate the output answer, where both the\ninput and the output are in natural language forms after prompting.\nInterestingly, we find that KiC can be identified as a special\nmixture-of-experts (MoE) model, where the knowledge selector plays the role of\na router that is used to determine the sequence-to-expert assignment in MoE.\nThis key observation inspires us to develop a novel algorithm for training KiC\nwith an instance-adaptive knowledge selector. As a knowledge-rich\nsemi-parametric language model, KiC only needs a much smaller parametric part\nto achieve superior zero-shot performance on unseen tasks. By evaluating on 40+\ndifferent tasks, we show that KiC_Large with 770M parameters easily outperforms\nlarge language models (LMs) that are 4-39x larger by a large margin. We also\ndemonstrate that KiC exhibits emergent abilities at a much smaller model scale\ncompared to the fully-parametric models.\n","authors":["Xiaoman Pan","Wenlin Yao","Hongming Zhang","Dian Yu","Dong Yu","Jianshu Chen"],"pdf_url":"https://arxiv.org/pdf/2210.16433v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.14951v1","updated":"2023-03-27T07:28:46Z","published":"2023-03-27T07:28:46Z","title":"Improving Contextualized Topic Models with Negative Sampling","summary":"  Topic modeling has emerged as a dominant method for exploring large document\ncollections. Recent approaches to topic modeling use large contextualized\nlanguage models and variational autoencoders. In this paper, we propose a\nnegative sampling mechanism for a contextualized topic model to improve the\nquality of the generated topics. In particular, during model training, we\nperturb the generated document-topic vector and use a triplet loss to encourage\nthe document reconstructed from the correct document-topic vector to be similar\nto the input document and dissimilar to the document reconstructed from the\nperturbed vector. Experiments for different topic counts on three publicly\navailable benchmark datasets show that in most cases, our approach leads to an\nincrease in topic coherence over that of the baselines. Our model also achieves\nvery high topic diversity.\n","authors":["Suman Adhya","Avishek Lahiri","Debarshi Kumar Sanyal","Partha Pratim Das"],"pdf_url":"https://arxiv.org/pdf/2303.14951v1.pdf","comment":"Accepted at 19th International Conference on Natural Language\n  Processing (ICON 2022)"},{"id":"http://arxiv.org/abs/2303.14935v1","updated":"2023-03-27T06:31:21Z","published":"2023-03-27T06:31:21Z","title":"TabIQA: Table Questions Answering on Business Document Images","summary":"  Table answering questions from business documents has many challenges that\nrequire understanding tabular structures, cross-document referencing, and\nadditional numeric computations beyond simple search queries. This paper\nintroduces a novel pipeline, named TabIQA, to answer questions about business\ndocument images. TabIQA combines state-of-the-art deep learning techniques 1)\nto extract table content and structural information from images and 2) to\nanswer various questions related to numerical data, text-based information, and\ncomplex queries from structured tables. The evaluation results on VQAonBD 2023\ndataset demonstrate the effectiveness of TabIQA in achieving promising\nperformance in answering table-related questions. The TabIQA repository is\navailable at https://github.com/phucty/itabqa.\n","authors":["Phuc Nguyen","Nam Tuan Ly","Hideaki Takeda","Atsuhiro Takasu"],"pdf_url":"https://arxiv.org/pdf/2303.14935v1.pdf","comment":"First two authors contributed equally"},{"id":"http://arxiv.org/abs/2303.14920v1","updated":"2023-03-27T05:34:19Z","published":"2023-03-27T05:34:19Z","title":"Adapting Pretrained Language Models for Solving Tabular Prediction\n  Problems in the Electronic Health Record","summary":"  We propose an approach for adapting the DeBERTa model for electronic health\nrecord (EHR) tasks using domain adaptation. We pretrain a small DeBERTa model\non a dataset consisting of MIMIC-III discharge summaries, clinical notes,\nradiology reports, and PubMed abstracts. We compare this model's performance\nwith a DeBERTa model pre-trained on clinical texts from our institutional EHR\n(MeDeBERTa) and an XGBoost model. We evaluate performance on three benchmark\ntasks for emergency department outcomes using the MIMIC-IV-ED dataset. We\npreprocess the data to convert it into text format and generate four versions\nof the original datasets to compare data processing and data inclusion. The\nresults show that our proposed approach outperforms the alternative models on\ntwo of three tasks (p<0.001) and matches performance on the third task, with\nthe use of descriptive columns improving performance over the original column\nnames.\n","authors":["Christopher McMaster","David FL Liew","Douglas EV Pires"],"pdf_url":"https://arxiv.org/pdf/2303.14920v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.13939v2","updated":"2023-03-27T03:51:52Z","published":"2023-03-24T11:52:25Z","title":"MUG: A General Meeting Understanding and Generation Benchmark","summary":"  Listening to long video/audio recordings from video conferencing and online\ncourses for acquiring information is extremely inefficient. Even after ASR\nsystems transcribe recordings into long-form spoken language documents, reading\nASR transcripts only partly speeds up seeking information. It has been observed\nthat a range of NLP applications, such as keyphrase extraction, topic\nsegmentation, and summarization, significantly improve users' efficiency in\ngrasping important information. The meeting scenario is among the most valuable\nscenarios for deploying these spoken language processing (SLP) capabilities.\nHowever, the lack of large-scale public meeting datasets annotated for these\nSLP tasks severely hinders their advancement. To prompt SLP advancement, we\nestablish a large-scale general Meeting Understanding and Generation Benchmark\n(MUG) to benchmark the performance of a wide range of SLP tasks, including\ntopic segmentation, topic-level and session-level extractive summarization and\ntopic title generation, keyphrase extraction, and action item detection. To\nfacilitate the MUG benchmark, we construct and release a large-scale meeting\ndataset for comprehensive long-form SLP development, the AliMeeting4MUG Corpus,\nwhich consists of 654 recorded Mandarin meeting sessions with diverse topic\ncoverage, with manual annotations for SLP tasks on manual transcripts of\nmeeting recordings. To the best of our knowledge, the AliMeeting4MUG Corpus is\nso far the largest meeting corpus in scale and facilitates most SLP tasks. In\nthis paper, we provide a detailed introduction of this corpus, SLP tasks and\nevaluation methods, baseline systems and their performance.\n","authors":["Qinglin Zhang","Chong Deng","Jiaqing Liu","Hai Yu","Qian Chen","Wen Wang","Zhijie Yan","Jinglin Liu","Yi Ren","Zhou Zhao"],"pdf_url":"https://arxiv.org/pdf/2303.13939v2.pdf","comment":"Paper accepted to the 2023 IEEE International Conference on\n  Acoustics, Speech, and Signal Processing (ICASSP 2023), Rhodes, Greece"},{"id":"http://arxiv.org/abs/2303.14898v1","updated":"2023-03-27T03:15:27Z","published":"2023-03-27T03:15:27Z","title":"Mutually-paced Knowledge Distillation for Cross-lingual Temporal\n  Knowledge Graph Reasoning","summary":"  This paper investigates cross-lingual temporal knowledge graph reasoning\nproblem, which aims to facilitate reasoning on Temporal Knowledge Graphs (TKGs)\nin low-resource languages by transfering knowledge from TKGs in high-resource\nones. The cross-lingual distillation ability across TKGs becomes increasingly\ncrucial, in light of the unsatisfying performance of existing reasoning methods\non those severely incomplete TKGs, especially in low-resource languages.\nHowever, it poses tremendous challenges in two aspects. First, the\ncross-lingual alignments, which serve as bridges for knowledge transfer, are\nusually too scarce to transfer sufficient knowledge between two TKGs. Second,\ntemporal knowledge discrepancy of the aligned entities, especially when\nalignments are unreliable, can mislead the knowledge distillation process. We\ncorrespondingly propose a mutually-paced knowledge distillation model MP-KD,\nwhere a teacher network trained on a source TKG can guide the training of a\nstudent network on target TKGs with an alignment module. Concretely, to deal\nwith the scarcity issue, MP-KD generates pseudo alignments between TKGs based\non the temporal information extracted by our representation module. To maximize\nthe efficacy of knowledge transfer and control the noise caused by the temporal\nknowledge discrepancy, we enhance MP-KD with a temporal cross-lingual attention\nmechanism to dynamically estimate the alignment strength. The two procedures\nare mutually paced along with model training. Extensive experiments on twelve\ncross-lingual TKG transfer tasks in the EventKG benchmark demonstrate the\neffectiveness of the proposed MP-KD method.\n","authors":["Ruijie Wang","Zheng Li","Jingfeng Yang","Tianyu Cao","Chao Zhang","Bing Yin","Tarek Abdelzaher"],"pdf_url":"https://arxiv.org/pdf/2303.14898v1.pdf","comment":"This paper is accepted by The Web Conference 2023"},{"id":"http://arxiv.org/abs/2303.14871v1","updated":"2023-03-27T01:41:48Z","published":"2023-03-27T01:41:48Z","title":"Coupling Artificial Neurons in BERT and Biological Neurons in the Human\n  Brain","summary":"  Linking computational natural language processing (NLP) models and neural\nresponses to language in the human brain on the one hand facilitates the effort\ntowards disentangling the neural representations underpinning language\nperception, on the other hand provides neurolinguistics evidence to evaluate\nand improve NLP models. Mappings of an NLP model's representations of and the\nbrain activities evoked by linguistic input are typically deployed to reveal\nthis symbiosis. However, two critical problems limit its advancement: 1) The\nmodel's representations (artificial neurons, ANs) rely on layer-level\nembeddings and thus lack fine-granularity; 2) The brain activities (biological\nneurons, BNs) are limited to neural recordings of isolated cortical unit (i.e.,\nvoxel/region) and thus lack integrations and interactions among brain\nfunctions. To address those problems, in this study, we 1) define ANs with\nfine-granularity in transformer-based NLP models (BERT in this study) and\nmeasure their temporal activations to input text sequences; 2) define BNs as\nfunctional brain networks (FBNs) extracted from functional magnetic resonance\nimaging (fMRI) data to capture functional interactions in the brain; 3) couple\nANs and BNs by maximizing the synchronization of their temporal activations.\nOur experimental results demonstrate 1) The activations of ANs and BNs are\nsignificantly synchronized; 2) the ANs carry meaningful linguistic/semantic\ninformation and anchor to their BN signatures; 3) the anchored BNs are\ninterpretable in a neurolinguistic context. Overall, our study introduces a\nnovel, general, and effective framework to link transformer-based NLP models\nand neural activities in response to language and may provide novel insights\nfor future studies such as brain-inspired evaluation and development of NLP\nmodels.\n","authors":["Xu Liu","Mengyue Zhou","Gaosheng Shi","Yu Du","Lin Zhao","Zihao Wu","David Liu","Tianming Liu","Xintao Hu"],"pdf_url":"https://arxiv.org/pdf/2303.14871v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.08233v2","updated":"2023-03-27T01:10:12Z","published":"2023-03-14T20:59:04Z","title":"NL4Opt Competition: Formulating Optimization Problems Based on Their\n  Natural Language Descriptions","summary":"  The Natural Language for Optimization (NL4Opt) Competition was created to\ninvestigate methods of extracting the meaning and formulation of an\noptimization problem based on its text description. Specifically, the goal of\nthe competition is to increase the accessibility and usability of optimization\nsolvers by allowing non-experts to interface with them using natural language.\nWe separate this challenging goal into two sub-tasks: (1) recognize and label\nthe semantic entities that correspond to the components of the optimization\nproblem; (2) generate a meaning representation (i.e., a logical form) of the\nproblem from its detected problem entities. The first task aims to reduce\nambiguity by detecting and tagging the entities of the optimization problems.\nThe second task creates an intermediate representation of the linear\nprogramming (LP) problem that is converted into a format that can be used by\ncommercial solvers. In this report, we present the LP word problem dataset and\nshared tasks for the NeurIPS 2022 competition. Furthermore, we investigate and\ncompare the performance of the ChatGPT large language model against the winning\nsolutions. Through this competition, we hope to bring interest towards the\ndevelopment of novel machine learning applications and datasets for\noptimization modeling.\n","authors":["Rindranirina Ramamonjison","Timothy T. Yu","Raymond Li","Haley Li","Giuseppe Carenini","Bissan Ghaddar","Shiqi He","Mahdi Mostajabdaveh","Amin Banitalebi-Dehkordi","Zirui Zhou","Yong Zhang"],"pdf_url":"https://arxiv.org/pdf/2303.08233v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2302.14383v2","updated":"2023-03-27T01:02:17Z","published":"2023-02-28T08:11:56Z","title":"Linear Spaces of Meanings: Compositional Structures in Vision-Language\n  Models","summary":"  We investigate compositional structures in data embeddings from pre-trained\nvision-language models (VLMs). Traditionally, compositionality has been\nassociated with algebraic operations on embeddings of words from a pre-existing\nvocabulary. In contrast, we seek to approximate representations from an encoder\nas combinations of a smaller set of vectors in the embedding space. These\nvectors can be seen as \"ideal words\" for generating concepts directly within\nthe embedding space of the model. We first present a framework for\nunderstanding compositional structures from a geometric perspective. We then\nexplain what these compositional structures entail probabilistically in the\ncase of VLM embeddings, providing intuitions for why they arise in practice.\nFinally, we empirically explore these structures in CLIP's embeddings and we\nevaluate their usefulness for solving different vision-language tasks such as\nclassification, debiasing, and retrieval. Our results show that simple linear\nalgebraic operations on embedding vectors can be used as compositional and\ninterpretable methods for regulating the behavior of VLMs.\n","authors":["Matthew Trager","Pramuditha Perera","Luca Zancato","Alessandro Achille","Parminder Bhatia","Stefano Soatto"],"pdf_url":"https://arxiv.org/pdf/2302.14383v2.pdf","comment":"18 pages, 9 figures, 7 tables"},{"id":"http://arxiv.org/abs/2303.12712v3","updated":"2023-03-27T22:36:40Z","published":"2023-03-22T16:51:28Z","title":"Sparks of Artificial General Intelligence: Early experiments with GPT-4","summary":"  Artificial intelligence (AI) researchers have been developing and refining\nlarge language models (LLMs) that exhibit remarkable capabilities across a\nvariety of domains and tasks, challenging our understanding of learning and\ncognition. The latest model developed by OpenAI, GPT-4, was trained using an\nunprecedented scale of compute and data. In this paper, we report on our\ninvestigation of an early version of GPT-4, when it was still in active\ndevelopment by OpenAI. We contend that (this early version of) GPT-4 is part of\na new cohort of LLMs (along with ChatGPT and Google's PaLM for example) that\nexhibit more general intelligence than previous AI models. We discuss the\nrising capabilities and implications of these models. We demonstrate that,\nbeyond its mastery of language, GPT-4 can solve novel and difficult tasks that\nspan mathematics, coding, vision, medicine, law, psychology and more, without\nneeding any special prompting. Moreover, in all of these tasks, GPT-4's\nperformance is strikingly close to human-level performance, and often vastly\nsurpasses prior models such as ChatGPT. Given the breadth and depth of GPT-4's\ncapabilities, we believe that it could reasonably be viewed as an early (yet\nstill incomplete) version of an artificial general intelligence (AGI) system.\nIn our exploration of GPT-4, we put special emphasis on discovering its\nlimitations, and we discuss the challenges ahead for advancing towards deeper\nand more comprehensive versions of AGI, including the possible need for\npursuing a new paradigm that moves beyond next-word prediction. We conclude\nwith reflections on societal influences of the recent technological leap and\nfuture research directions.\n","authors":["Sébastien Bubeck","Varun Chandrasekaran","Ronen Eldan","Johannes Gehrke","Eric Horvitz","Ece Kamar","Peter Lee","Yin Tat Lee","Yuanzhi Li","Scott Lundberg","Harsha Nori","Hamid Palangi","Marco Tulio Ribeiro","Yi Zhang"],"pdf_url":"https://arxiv.org/pdf/2303.12712v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.15621v1","updated":"2023-03-27T22:30:39Z","published":"2023-03-27T22:30:39Z","title":"ChatGPT as a Factual Inconsistency Evaluator for Abstractive Text\n  Summarization","summary":"  The performance of abstractive text summarization has been greatly boosted by\npre-trained language models recently. The main concern of existing abstractive\nsummarization methods is the factual inconsistency problem of their generated\nsummary. To alleviate the problem, many efforts have focused on developing\neffective factuality evaluation metrics based on natural language inference and\nquestion answering et al. However, they have limitations of high computational\ncomplexity and relying on annotated data. Most recently, large language models\nsuch as ChatGPT have shown strong ability in not only natural language\nunderstanding but also natural language inference. In this paper, we study the\nfactual inconsistency evaluation ability of ChatGPT under the zero-shot setting\nby evaluating it on the coarse-grained and fine-grained factuality evaluation\ntasks including binary natural language inference (NLI), summary ranking, and\nconsistency rating. Experimental results show that ChatGPT outperforms previous\nSOTA evaluation metrics on 6/9 datasets across three tasks, demonstrating its\ngreat potential for assessing factual inconsistency in the zero-shot setting.\nThe results also highlight the importance of prompt design and the need for\nfuture efforts to address ChatGPT's limitations on evaluation bias, wrong\nreasoning, and hallucination.\n","authors":["Zheheng Luo","Qianqian Xie","Sophia Ananiadou"],"pdf_url":"https://arxiv.org/pdf/2303.15621v1.pdf","comment":"ongoing work, 12 pages, 4 figures"},{"id":"http://arxiv.org/abs/2303.15619v1","updated":"2023-03-27T22:27:23Z","published":"2023-03-27T22:27:23Z","title":"Typhoon: Towards an Effective Task-Specific Masking Strategy for\n  Pre-trained Language Models","summary":"  Through exploiting a high level of parallelism enabled by graphics processing\nunits, transformer architectures have enabled tremendous strides forward in the\nfield of natural language processing. In a traditional masked language model,\nspecial MASK tokens are used to prompt our model to gather contextual\ninformation from surrounding words to restore originally hidden information. In\nthis paper, we explore a task-specific masking framework for pre-trained large\nlanguage models that enables superior performance on particular downstream\ntasks on the datasets in the GLUE benchmark. We develop our own masking\nalgorithm, Typhoon, based on token input gradients, and compare this with other\nstandard baselines. We find that Typhoon offers performance competitive with\nwhole-word masking on the MRPC dataset. Our implementation can be found in a\npublic Github Repository.\n","authors":["Muhammed Shahir Abdurrahman","Hashem Elezabi","Bruce Changlong Xu"],"pdf_url":"https://arxiv.org/pdf/2303.15619v1.pdf","comment":"9 pages, 2 figures"},{"id":"http://arxiv.org/abs/2210.06207v2","updated":"2023-03-27T22:00:44Z","published":"2022-10-12T13:48:59Z","title":"SilverAlign: MT-Based Silver Data Algorithm For Evaluating Word\n  Alignment","summary":"  Word alignments are essential for a variety of NLP tasks. Therefore, choosing\nthe best approaches for their creation is crucial. However, the scarce\navailability of gold evaluation data makes the choice difficult. We propose\nSilverAlign, a new method to automatically create silver data for the\nevaluation of word aligners by exploiting machine translation and minimal\npairs. We show that performance on our silver data correlates well with gold\nbenchmarks for 9 language pairs, making our approach a valid resource for\nevaluation of different domains and languages when gold data are not available.\nThis addresses the important scenario of missing gold data alignments for\nlow-resource languages.\n","authors":["Abdullatif Köksal","Silvia Severini","Hinrich Schütze"],"pdf_url":"https://arxiv.org/pdf/2210.06207v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.14070v2","updated":"2023-03-27T20:41:46Z","published":"2023-03-24T15:29:16Z","title":"ChatDoctor: A Medical Chat Model Fine-tuned on LLaMA Model using Medical\n  Domain Knowledge","summary":"  Recent large language models (LLMs) in the general domain, such as ChatGPT,\nhave shown remarkable success in following instructions and producing\nhuman-like responses. However, such language models have not been tailored to\nthe medical domain, resulting in poor answer accuracy and inability to give\nplausible recommendations for medical diagnosis, medications, etc. To address\nthis issue, we collected more than 700 diseases and their corresponding\nsymptoms, required medical tests, and recommended medications, from which we\ngenerated 5K doctor-patient conversations. By fine-tuning LLMs using these\ntailored doctor-patient conversations, the resulting models emerge with great\npotential to understand patients' needs, provide informed advice, and offer\nvaluable assistance in a variety of medical-related fields. The integration of\nthese advanced language models into healthcare can revolutionize the way\nhealthcare professionals and patients communicate, ultimately improving the\noverall efficiency and quality of patient care and outcomes. In addition, we\nmade public all the source codes, datasets, and model weights to facilitate the\nfurther development of dialogue models in the medical field. The training data,\ncodes, and weights of this project are available at:\nhttps://github.com/Kent0n-Li/ChatDoctor.\n","authors":["Li Yunxiang","Li Zihan","Zhang Kai","Dan Ruilong","Zhang You"],"pdf_url":"https://arxiv.org/pdf/2303.14070v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.15587v1","updated":"2023-03-27T20:33:40Z","published":"2023-03-27T20:33:40Z","title":"Linguistically Informed ChatGPT Prompts to Enhance Japanese-Chinese\n  Machine Translation: A Case Study on Attributive Clauses","summary":"  In the field of Japanese-Chinese translation linguistics, the issue of\ncorrectly translating attributive clauses has persistently proven to be\nchallenging. Present-day machine translation tools often fail to accurately\ntranslate attributive clauses from Japanese to Chinese. In light of this, this\npaper investigates the linguistic problem underlying such difficulties, namely\nhow does the semantic role of the modified noun affect the selection of\ntranslation patterns for attributive clauses, from a linguistic perspective. To\nad-dress these difficulties, a pre-edit scheme is proposed, which aims to\nenhance the accuracy of translation. Furthermore, we propose a novel two-step\nprompt strategy, which combines this pre-edit scheme with ChatGPT, currently\nthe most widely used large language model. This prompt strategy is capable of\noptimizing translation input in zero-shot scenarios and has been demonstrated\nto improve the average translation accuracy score by over 35%.\n","authors":["Wenshi Gu"],"pdf_url":"https://arxiv.org/pdf/2303.15587v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2301.09919v2","updated":"2023-03-27T19:13:35Z","published":"2023-01-24T11:00:17Z","title":"Opportunities and Challenges in Neural Dialog Tutoring","summary":"  Designing dialog tutors has been challenging as it involves modeling the\ndiverse and complex pedagogical strategies employed by human tutors. Although\nthere have been significant recent advances in neural conversational systems\nusing large language models (LLMs) and growth in available dialog corpora,\ndialog tutoring has largely remained unaffected by these advances. In this\npaper, we rigorously analyze various generative language models on two dialog\ntutoring datasets for language learning using automatic and human evaluations\nto understand the new opportunities brought by these advances as well as the\nchallenges we must overcome to build models that would be usable in real\neducational settings. We find that although current approaches can model\ntutoring in constrained learning scenarios when the number of concepts to be\ntaught and possible teacher strategies are small, they perform poorly in less\nconstrained scenarios. Our human quality evaluation shows that both models and\nground-truth annotations exhibit low performance in terms of equitable\ntutoring, which measures learning opportunities for students and how engaging\nthe dialog is. To understand the behavior of our models in a real tutoring\nsetting, we conduct a user study using expert annotators and find a\nsignificantly large number of model reasoning errors in 45% of conversations.\nFinally, we connect our findings to outline future work.\n","authors":["Jakub Macina","Nico Daheim","Lingzhi Wang","Tanmay Sinha","Manu Kapur","Iryna Gurevych","Mrinmaya Sachan"],"pdf_url":"https://arxiv.org/pdf/2301.09919v2.pdf","comment":"EACL 2023 (main conference, camera-ready)"}],"Computer Vision and Pattern Recognition":[{"id":"http://arxiv.org/abs/2303.15446v1","updated":"2023-03-27T17:59:58Z","published":"2023-03-27T17:59:58Z","title":"SwiftFormer: Efficient Additive Attention for Transformer-based\n  Real-time Mobile Vision Applications","summary":"  Self-attention has become a defacto choice for capturing global context in\nvarious vision applications. However, its quadratic computational complexity\nwith respect to image resolution limits its use in real-time applications,\nespecially for deployment on resource-constrained mobile devices. Although\nhybrid approaches have been proposed to combine the advantages of convolutions\nand self-attention for a better speed-accuracy trade-off, the expensive matrix\nmultiplication operations in self-attention remain a bottleneck. In this work,\nwe introduce a novel efficient additive attention mechanism that effectively\nreplaces the quadratic matrix multiplication operations with linear\nelement-wise multiplications. Our design shows that the key-value interaction\ncan be replaced with a linear layer without sacrificing any accuracy. Unlike\nprevious state-of-the-art methods, our efficient formulation of self-attention\nenables its usage at all stages of the network. Using our proposed efficient\nadditive attention, we build a series of models called \"SwiftFormer\" which\nachieves state-of-the-art performance in terms of both accuracy and mobile\ninference speed. Our small variant achieves 78.5% top-1 ImageNet-1K accuracy\nwith only 0.8 ms latency on iPhone 14, which is more accurate and 2x faster\ncompared to MobileViT-v2. Code: https://github.com/Amshaker/SwiftFormer\n","authors":["Abdelrahman Shaker","Muhammad Maaz","Hanoona Rasheed","Salman Khan","Ming-Hsuan Yang","Fahad Shahbaz Khan"],"pdf_url":"https://arxiv.org/pdf/2303.15446v1.pdf","comment":"Technical report"},{"id":"http://arxiv.org/abs/2303.15445v1","updated":"2023-03-27T17:59:55Z","published":"2023-03-27T17:59:55Z","title":"IRFL: Image Recognition of Figurative Language","summary":"  Figures of speech such as metaphors, similes, and idioms allow language to be\nexpressive, invoke emotion, and communicate abstract ideas that might otherwise\nbe difficult to visualize. These figurative forms are often conveyed through\nmultiple modes, such as text and images, and frequently appear in advertising,\nnews, social media, etc. Understanding multimodal figurative language is an\nessential component of human communication, and it plays a significant role in\nour daily interactions. While humans can intuitively understand multimodal\nfigurative language, this poses a challenging task for machines that requires\nthe cognitive ability to map between domains, abstraction, commonsense, and\nprofound language and cultural knowledge. In this work, we propose the Image\nRecognition of Figurative Language dataset to examine vision and language\nmodels' understanding of figurative language. We leverage human annotation and\nan automatic pipeline we created to generate a multimodal dataset and introduce\ntwo novel tasks as a benchmark for multimodal figurative understanding. We\nexperiment with several baseline models and find that all perform substantially\nworse than humans. We hope our dataset and benchmark will drive the development\nof models that will better understand figurative language.\n","authors":["Ron Yosef","Yonatan Bitton","Dafna Shahaf"],"pdf_url":"https://arxiv.org/pdf/2303.15445v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.15444v1","updated":"2023-03-27T17:59:54Z","published":"2023-03-27T17:59:54Z","title":"Quantum Multi-Model Fitting","summary":"  Geometric model fitting is a challenging but fundamental computer vision\nproblem. Recently, quantum optimization has been shown to enhance robust\nfitting for the case of a single model, while leaving the question of\nmulti-model fitting open. In response to this challenge, this paper shows that\nthe latter case can significantly benefit from quantum hardware and proposes\nthe first quantum approach to multi-model fitting (MMF). We formulate MMF as a\nproblem that can be efficiently sampled by modern adiabatic quantum computers\nwithout the relaxation of the objective function. We also propose an iterative\nand decomposed version of our method, which supports real-world-sized problems.\nThe experimental evaluation demonstrates promising results on a variety of\ndatasets. The source code is available at:\nhttps://github.com/FarinaMatteo/qmmf.\n","authors":["Matteo Farina","Luca Magri","Willi Menapace","Elisa Ricci","Vladislav Golyanik","Federica Arrigoni"],"pdf_url":"https://arxiv.org/pdf/2303.15444v1.pdf","comment":"In Computer Vision and Pattern Recognition (CVPR) 2023; Highlight"},{"id":"http://arxiv.org/abs/2303.15443v1","updated":"2023-03-27T17:59:34Z","published":"2023-03-27T17:59:34Z","title":"GeoNet: Benchmarking Unsupervised Adaptation across Geographies","summary":"  In recent years, several efforts have been aimed at improving the robustness\nof vision models to domains and environments unseen during training. An\nimportant practical problem pertains to models deployed in a new geography that\nis under-represented in the training dataset, posing a direct challenge to fair\nand inclusive computer vision. In this paper, we study the problem of\ngeographic robustness and make three main contributions. First, we introduce a\nlarge-scale dataset GeoNet for geographic adaptation containing benchmarks\nacross diverse tasks like scene recognition (GeoPlaces), image classification\n(GeoImNet) and universal adaptation (GeoUniDA). Second, we investigate the\nnature of distribution shifts typical to the problem of geographic adaptation\nand hypothesize that the major source of domain shifts arise from significant\nvariations in scene context (context shift), object design (design shift) and\nlabel distribution (prior shift) across geographies. Third, we conduct an\nextensive evaluation of several state-of-the-art unsupervised domain adaptation\nalgorithms and architectures on GeoNet, showing that they do not suffice for\ngeographical adaptation, and that large-scale pre-training using large vision\nmodels also does not lead to geographic robustness. Our dataset is publicly\navailable at https://tarun005.github.io/GeoNet.\n","authors":["Tarun Kalluri","Wangdong Xu","Manmohan Chandraker"],"pdf_url":"https://arxiv.org/pdf/2303.15443v1.pdf","comment":"CVPR 2023 Camera Ready. Project Page:\n  https://tarun005.github.io/GeoNet"},{"id":"http://arxiv.org/abs/2303.15441v1","updated":"2023-03-27T17:59:33Z","published":"2023-03-27T17:59:33Z","title":"Zero-shot Model Diagnosis","summary":"  When it comes to deploying deep vision models, the behavior of these systems\nmust be explicable to ensure confidence in their reliability and fairness. A\ncommon approach to evaluate deep learning models is to build a labeled test set\nwith attributes of interest and assess how well it performs. However, creating\na balanced test set (i.e., one that is uniformly sampled over all the important\ntraits) is often time-consuming, expensive, and prone to mistakes. The question\nwe try to address is: can we evaluate the sensitivity of deep learning models\nto arbitrary visual attributes without an annotated test set? This paper argues\nthe case that Zero-shot Model Diagnosis (ZOOM) is possible without the need for\na test set nor labeling. To avoid the need for test sets, our system relies on\na generative model and CLIP. The key idea is enabling the user to select a set\nof prompts (relevant to the problem) and our system will automatically search\nfor semantic counterfactual images (i.e., synthesized images that flip the\nprediction in the case of a binary classifier) using the generative model. We\nevaluate several visual tasks (classification, key-point detection, and\nsegmentation) in multiple visual domains to demonstrate the viability of our\nmethodology. Extensive experiments demonstrate that our method is capable of\nproducing counterfactual images and offering sensitivity analysis for model\ndiagnosis without the need for a test set.\n","authors":["Jinqi Luo","Zhaoning Wang","Chen Henry Wu","Dong Huang","Fernando De la Torre"],"pdf_url":"https://arxiv.org/pdf/2303.15441v1.pdf","comment":"Accepted in CVPR 2023"},{"id":"http://arxiv.org/abs/2303.15440v1","updated":"2023-03-27T17:59:29Z","published":"2023-03-27T17:59:29Z","title":"EFEM: Equivariant Neural Field Expectation Maximization for 3D Object\n  Segmentation Without Scene Supervision","summary":"  We introduce Equivariant Neural Field Expectation Maximization (EFEM), a\nsimple, effective, and robust geometric algorithm that can segment objects in\n3D scenes without annotations or training on scenes. We achieve such\nunsupervised segmentation by exploiting single object shape priors. We make two\nnovel steps in that direction. First, we introduce equivariant shape\nrepresentations to this problem to eliminate the complexity induced by the\nvariation in object configuration. Second, we propose a novel EM algorithm that\ncan iteratively refine segmentation masks using the equivariant shape prior. We\ncollect a novel real dataset Chairs and Mugs that contains various object\nconfigurations and novel scenes in order to verify the effectiveness and\nrobustness of our method. Experimental results demonstrate that our method\nachieves consistent and robust performance across different scenes where the\n(weakly) supervised methods may fail. Code and data available at\nhttps://www.cis.upenn.edu/~leijh/projects/efem\n","authors":["Jiahui Lei","Congyue Deng","Karl Schmeckpeper","Leonidas Guibas","Kostas Daniilidis"],"pdf_url":"https://arxiv.org/pdf/2303.15440v1.pdf","comment":"Accepted by CVPR2023, project page\n  https://www.cis.upenn.edu/~leijh/projects/efem"},{"id":"http://arxiv.org/abs/2303.15437v1","updated":"2023-03-27T17:59:10Z","published":"2023-03-27T17:59:10Z","title":"FaceLit: Neural 3D Relightable Faces","summary":"  We propose a generative framework, FaceLit, capable of generating a 3D face\nthat can be rendered at various user-defined lighting conditions and views,\nlearned purely from 2D images in-the-wild without any manual annotation. Unlike\nexisting works that require careful capture setup or human labor, we rely on\noff-the-shelf pose and illumination estimators. With these estimates, we\nincorporate the Phong reflectance model in the neural volume rendering\nframework. Our model learns to generate shape and material properties of a face\nsuch that, when rendered according to the natural statistics of pose and\nillumination, produces photorealistic face images with multiview 3D and\nillumination consistency. Our method enables photorealistic generation of faces\nwith explicit illumination and view controls on multiple datasets - FFHQ,\nMetFaces and CelebA-HQ. We show state-of-the-art photorealism among 3D aware\nGANs on FFHQ dataset achieving an FID score of 3.5.\n","authors":["Anurag Ranjan","Kwang Moo Yi","Jen-Hao Rick Chang","Oncel Tuzel"],"pdf_url":"https://arxiv.org/pdf/2303.15437v1.pdf","comment":"CVPR 2023"},{"id":"http://arxiv.org/abs/2303.15435v1","updated":"2023-03-27T17:57:33Z","published":"2023-03-27T17:57:33Z","title":"The Stable Signature: Rooting Watermarks in Latent Diffusion Models","summary":"  Generative image modeling enables a wide range of applications but raises\nethical concerns about responsible deployment. This paper introduces an active\nstrategy combining image watermarking and Latent Diffusion Models. The goal is\nfor all generated images to conceal an invisible watermark allowing for future\ndetection and/or identification. The method quickly fine-tunes the latent\ndecoder of the image generator, conditioned on a binary signature. A\npre-trained watermark extractor recovers the hidden signature from any\ngenerated image and a statistical test then determines whether it comes from\nthe generative model. We evaluate the invisibility and robustness of the\nwatermarks on a variety of generation tasks, showing that Stable Signature\nworks even after the images are modified. For instance, it detects the origin\nof an image generated from a text prompt, then cropped to keep $10\\%$ of the\ncontent, with $90$+$\\%$ accuracy at a false positive rate below 10$^{-6}$.\n","authors":["Pierre Fernandez","Guillaume Couairon","Hervé Jégou","Matthijs Douze","Teddy Furon"],"pdf_url":"https://arxiv.org/pdf/2303.15435v1.pdf","comment":"Website at https://pierrefdz.github.io/publications/stablesignature"},{"id":"http://arxiv.org/abs/2303.15433v1","updated":"2023-03-27T17:55:44Z","published":"2023-03-27T17:55:44Z","title":"Anti-DreamBooth: Protecting users from personalized text-to-image\n  synthesis","summary":"  Text-to-image diffusion models are nothing but a revolution, allowing anyone,\neven without design skills, to create realistic images from simple text inputs.\nWith powerful personalization tools like DreamBooth, they can generate images\nof a specific person just by learning from his/her few reference images.\nHowever, when misused, such a powerful and convenient tool can produce fake\nnews or disturbing content targeting any individual victim, posing a severe\nnegative social impact. In this paper, we explore a defense system called\nAnti-DreamBooth against such malicious use of DreamBooth. The system aims to\nadd subtle noise perturbation to each user's image before publishing in order\nto disrupt the generation quality of any DreamBooth model trained on these\nperturbed images. We investigate a wide range of algorithms for perturbation\noptimization and extensively evaluate them on two facial datasets over various\ntext-to-image model versions. Despite the complicated formulation of DreamBooth\nand Diffusion-based text-to-image models, our methods effectively defend users\nfrom the malicious use of those models. Their effectiveness withstands even\nadverse conditions, such as model or prompt/term mismatching between training\nand testing. Our code will be available at\n\\href{https://github.com/VinAIResearch/Anti-DreamBooth.git}{https://github.com/VinAIResearch/Anti-DreamBooth.git}.\n","authors":["Thanh Van Le","Hao Phung","Thuan Hoang Nguyen","Quan Dao","Ngoc Tran","Anh Tran"],"pdf_url":"https://arxiv.org/pdf/2303.15433v1.pdf","comment":"Project page: https://anti-dreambooth.github.io/"},{"id":"http://arxiv.org/abs/2303.15427v1","updated":"2023-03-27T17:52:21Z","published":"2023-03-27T17:52:21Z","title":"JAWS: Just A Wild Shot for Cinematic Transfer in Neural Radiance Fields","summary":"  This paper presents JAWS, an optimization-driven approach that achieves the\nrobust transfer of visual cinematic features from a reference in-the-wild video\nclip to a newly generated clip. To this end, we rely on an\nimplicit-neural-representation (INR) in a way to compute a clip that shares the\nsame cinematic features as the reference clip. We propose a general formulation\nof a camera optimization problem in an INR that computes extrinsic and\nintrinsic camera parameters as well as timing. By leveraging the\ndifferentiability of neural representations, we can back-propagate our designed\ncinematic losses measured on proxy estimators through a NeRF network to the\nproposed cinematic parameters directly. We also introduce specific enhancements\nsuch as guidance maps to improve the overall quality and efficiency. Results\ndisplay the capacity of our system to replicate well known camera sequences\nfrom movies, adapting the framing, camera parameters and timing of the\ngenerated video clip to maximize the similarity with the reference clip.\n","authors":["Xi Wang","Robin Courant","Jinglei Shi","Eric Marchand","Marc Christie"],"pdf_url":"https://arxiv.org/pdf/2303.15427v1.pdf","comment":"CVPR 2023. Project page with videos and code:\n  http://www.lix.polytechnique.fr/vista/projects/2023_cvpr_wang"},{"id":"http://arxiv.org/abs/2303.15421v1","updated":"2023-03-27T17:43:57Z","published":"2023-03-27T17:43:57Z","title":"ACAT: Adversarial Counterfactual Attention for Classification and\n  Detection in Medical Imaging","summary":"  In some medical imaging tasks and other settings where only small parts of\nthe image are informative for the classification task, traditional CNNs can\nsometimes struggle to generalise. Manually annotated Regions of Interest (ROI)\nare sometimes used to isolate the most informative parts of the image. However,\nthese are expensive to collect and may vary significantly across annotators. To\novercome these issues, we propose a framework that employs saliency maps to\nobtain soft spatial attention masks that modulate the image features at\ndifferent scales. We refer to our method as Adversarial Counterfactual\nAttention (ACAT). ACAT increases the baseline classification accuracy of\nlesions in brain CT scans from 71.39% to 72.55% and of COVID-19 related\nfindings in lung CT scans from 67.71% to 70.84% and exceeds the performance of\ncompeting methods. We investigate the best way to generate the saliency maps\nemployed in our architecture and propose a way to obtain them from\nadversarially generated counterfactual images. They are able to isolate the\narea of interest in brain and lung CT scans without using any manual\nannotations. In the task of localising the lesion location out of 6 possible\nregions, they obtain a score of 65.05% on brain CT scans, improving the score\nof 61.29% obtained with the best competing method.\n","authors":["Alessandro Fontanella","Antreas Antoniou","Wenwen Li","Joanna Wardlaw","Grant Mair","Emanuele Trucco","Amos Storkey"],"pdf_url":"https://arxiv.org/pdf/2303.15421v1.pdf","comment":"17 pages, 7 figures"},{"id":"http://arxiv.org/abs/2211.09107v2","updated":"2023-03-27T17:43:18Z","published":"2022-11-16T18:50:11Z","title":"Interpretable Few-shot Learning with Online Attribute Selection","summary":"  Few-shot learning (FSL) is a challenging learning problem in which only a few\nsamples are available for each class. Decision interpretation is more important\nin few-shot classification since there is a greater chance of error than in\ntraditional classification. However, most of the previous FSL methods are\nblack-box models. In this paper, we propose an inherently interpretable model\nfor FSL based on human-friendly attributes. Moreover, we propose an online\nattribute selection mechanism that can effectively filter out irrelevant\nattributes in each episode. The attribute selection mechanism improves the\naccuracy and helps with interpretability by reducing the number of participated\nattributes in each episode. We propose a mechanism that automatically detects\nthe episodes where the pool of human-friendly attributes are not adequate, and\ncompensates by engaging learned unknown attributes. We demonstrate that the\nproposed method achieves results on par with black-box few-shot-learning models\non four widely used datasets.\n","authors":["Mohammad Reza Zarei","Majid Komeili"],"pdf_url":"https://arxiv.org/pdf/2211.09107v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.15417v1","updated":"2023-03-27T17:40:29Z","published":"2023-03-27T17:40:29Z","title":"Recovering 3D Hand Mesh Sequence from a Single Blurry Image: A New\n  Dataset and Temporal Unfolding","summary":"  Hands, one of the most dynamic parts of our body, suffer from blur due to\ntheir active movements. However, previous 3D hand mesh recovery methods have\nmainly focused on sharp hand images rather than considering blur due to the\nabsence of datasets providing blurry hand images. We first present a novel\ndataset BlurHand, which contains blurry hand images with 3D groundtruths. The\nBlurHand is constructed by synthesizing motion blur from sequential sharp hand\nimages, imitating realistic and natural motion blurs. In addition to the new\ndataset, we propose BlurHandNet, a baseline network for accurate 3D hand mesh\nrecovery from a blurry hand image. Our BlurHandNet unfolds a blurry input image\nto a 3D hand mesh sequence to utilize temporal information in the blurry input\nimage, while previous works output a static single hand mesh. We demonstrate\nthe usefulness of BlurHand for the 3D hand mesh recovery from blurry images in\nour experiments. The proposed BlurHandNet produces much more robust results on\nblurry images while generalizing well to in-the-wild images. The training codes\nand BlurHand dataset are available at\nhttps://github.com/JaehaKim97/BlurHand_RELEASE.\n","authors":["Yeonguk Oh","JoonKyu Park","Jaeha Kim","Gyeongsik Moon","Kyoung Mu Lee"],"pdf_url":"https://arxiv.org/pdf/2303.15417v1.pdf","comment":"Accepted at CVPR 2023"},{"id":"http://arxiv.org/abs/2303.15416v1","updated":"2023-03-27T17:39:39Z","published":"2023-03-27T17:39:39Z","title":"3D Video Object Detection with Learnable Object-Centric Global\n  Optimization","summary":"  We explore long-term temporal visual correspondence-based optimization for 3D\nvideo object detection in this work. Visual correspondence refers to one-to-one\nmappings for pixels across multiple images. Correspondence-based optimization\nis the cornerstone for 3D scene reconstruction but is less studied in 3D video\nobject detection, because moving objects violate multi-view geometry\nconstraints and are treated as outliers during scene reconstruction. We address\nthis issue by treating objects as first-class citizens during\ncorrespondence-based optimization. In this work, we propose BA-Det, an\nend-to-end optimizable object detector with object-centric temporal\ncorrespondence learning and featuremetric object bundle adjustment.\nEmpirically, we verify the effectiveness and efficiency of BA-Det for multiple\nbaseline 3D detectors under various setups. Our BA-Det achieves SOTA\nperformance on the large-scale Waymo Open Dataset (WOD) with only marginal\ncomputation cost. Our code is available at\nhttps://github.com/jiaweihe1996/BA-Det.\n","authors":["Jiawei He","Yuntao Chen","Naiyan Wang","Zhaoxiang Zhang"],"pdf_url":"https://arxiv.org/pdf/2303.15416v1.pdf","comment":"CVPR2023"},{"id":"http://arxiv.org/abs/2303.06464v2","updated":"2023-03-27T17:39:05Z","published":"2023-03-11T17:30:36Z","title":"PARASOL: Parametric Style Control for Diffusion Image Synthesis","summary":"  We propose PARASOL, a multi-modal synthesis model that enables disentangled,\nparametric control of the visual style of the image by jointly conditioning\nsynthesis on both content and a fine-grained visual style embedding. We train a\nlatent diffusion model (LDM) using specific losses for each modality and adapt\nthe classifier-free guidance for encouraging disentangled control over\nindependent content and style modalities at inference time. We leverage\nauxiliary semantic and style-based search to create training triplets for\nsupervision of the LDM, ensuring complementarity of content and style cues.\nPARASOL shows promise for enabling nuanced control over visual style in\ndiffusion models for image creation and stylization, as well as generative\nsearch where text-based search results may be adapted to more closely match\nuser intent by interpolating both content and style descriptors.\n","authors":["Gemma Canet Tarrés","Dan Ruta","Tu Bui","John Collomosse"],"pdf_url":"https://arxiv.org/pdf/2303.06464v2.pdf","comment":"Added Appendix"},{"id":"http://arxiv.org/abs/2303.15414v1","updated":"2023-03-27T17:39:00Z","published":"2023-03-27T17:39:00Z","title":"Learnable Graph Matching: A Practical Paradigm for Data Association","summary":"  Data association is at the core of many computer vision tasks, e.g., multiple\nobject tracking, image matching, and point cloud registration. Existing methods\nusually solve the data association problem by network flow optimization,\nbipartite matching, or end-to-end learning directly. Despite their popularity,\nwe find some defects of the current solutions: they mostly ignore the\nintra-view context information; besides, they either train deep association\nmodels in an end-to-end way and hardly utilize the advantage of\noptimization-based assignment methods, or only use an off-the-shelf neural\nnetwork to extract features. In this paper, we propose a general learnable\ngraph matching method to address these issues. Especially, we model the\nintra-view relationships as an undirected graph. Then data association turns\ninto a general graph matching problem between graphs. Furthermore, to make\noptimization end-to-end differentiable, we relax the original graph matching\nproblem into continuous quadratic programming and then incorporate training\ninto a deep graph neural network with KKT conditions and implicit function\ntheorem. In MOT task, our method achieves state-of-the-art performance on\nseveral MOT datasets. For image matching, our method outperforms\nstate-of-the-art methods with half training data and iterations on a popular\nindoor dataset, ScanNet. Code will be available at\nhttps://github.com/jiaweihe1996/GMTracker.\n","authors":["Jiawei He","Zehao Huang","Naiyan Wang","Zhaoxiang Zhang"],"pdf_url":"https://arxiv.org/pdf/2303.15414v1.pdf","comment":"Submitted to TPAMI on Mar 21, 2022. arXiv admin note: substantial\n  text overlap with arXiv:2103.16178"},{"id":"http://arxiv.org/abs/2303.15413v1","updated":"2023-03-27T17:31:13Z","published":"2023-03-27T17:31:13Z","title":"Debiasing Scores and Prompts of 2D Diffusion for Robust Text-to-3D\n  Generation","summary":"  The view inconsistency problem in score-distilling text-to-3D generation,\nalso known as the Janus problem, arises from the intrinsic bias of 2D diffusion\nmodels, which leads to the unrealistic generation of 3D objects. In this work,\nwe explore score-distilling text-to-3D generation and identify the main causes\nof the Janus problem. Based on these findings, we propose two approaches to\ndebias the score-distillation frameworks for robust text-to-3D generation. Our\nfirst approach, called score debiasing, involves gradually increasing the\ntruncation value for the score estimated by 2D diffusion models throughout the\noptimization process. Our second approach, called prompt debiasing, identifies\nconflicting words between user prompts and view prompts utilizing a language\nmodel and adjusts the discrepancy between view prompts and object-space camera\nposes. Our experimental results show that our methods improve realism by\nsignificantly reducing artifacts and achieve a good trade-off between\nfaithfulness to the 2D diffusion models and 3D consistency with little\noverhead.\n","authors":["Susung Hong","Donghoon Ahn","Seungryong Kim"],"pdf_url":"https://arxiv.org/pdf/2303.15413v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.15410v1","updated":"2023-03-27T17:28:25Z","published":"2023-03-27T17:28:25Z","title":"Human Pose Estimation in Extremely Low-Light Conditions","summary":"  We study human pose estimation in extremely low-light images. This task is\nchallenging due to the difficulty of collecting real low-light images with\naccurate labels, and severely corrupted inputs that degrade prediction quality\nsignificantly. To address the first issue, we develop a dedicated camera system\nand build a new dataset of real low-light images with accurate pose labels.\nThanks to our camera system, each low-light image in our dataset is coupled\nwith an aligned well-lit image, which enables accurate pose labeling and is\nused as privileged information during training. We also propose a new model and\na new training strategy that fully exploit the privileged information to learn\nrepresentation insensitive to lighting conditions. Our method demonstrates\noutstanding performance on real extremely low light images, and extensive\nanalyses validate that both of our model and dataset contribute to the success.\n","authors":["Sohyun Lee","Jaesung Rim","Boseung Jeong","Geonu Kim","Byungju Woo","Haechan Lee","Sunghyun Cho","Suha Kwak"],"pdf_url":"https://arxiv.org/pdf/2303.15410v1.pdf","comment":"Accepted to CVPR 2023"},{"id":"http://arxiv.org/abs/2303.15409v1","updated":"2023-03-27T17:28:20Z","published":"2023-03-27T17:28:20Z","title":"Classifier Robustness Enhancement Via Test-Time Transformation","summary":"  It has been recently discovered that adversarially trained classifiers\nexhibit an intriguing property, referred to as perceptually aligned gradients\n(PAG). PAG implies that the gradients of such classifiers possess a meaningful\nstructure, aligned with human perception. Adversarial training is currently the\nbest-known way to achieve classification robustness under adversarial attacks.\nThe PAG property, however, has yet to be leveraged for further improving\nclassifier robustness. In this work, we introduce Classifier Robustness\nEnhancement Via Test-Time Transformation (TETRA) -- a novel defense method that\nutilizes PAG, enhancing the performance of trained robust classifiers. Our\nmethod operates in two phases. First, it modifies the input image via a\ndesignated targeted adversarial attack into each of the dataset's classes.\nThen, it classifies the input image based on the distance to each of the\nmodified instances, with the assumption that the shortest distance relates to\nthe true class. We show that the proposed method achieves state-of-the-art\nresults and validate our claim through extensive experiments on a variety of\ndefense methods, classifier architectures, and datasets. We also empirically\ndemonstrate that TETRA can boost the accuracy of any differentiable adversarial\ntraining classifier across a variety of attacks, including ones unseen at\ntraining. Specifically, applying TETRA leads to substantial improvement of up\nto $+23\\%$, $+20\\%$, and $+26\\%$ on CIFAR10, CIFAR100, and ImageNet,\nrespectively.\n","authors":["Tsachi Blau","Roy Ganz","Chaim Baskin","Michael Elad","Alex Bronstein"],"pdf_url":"https://arxiv.org/pdf/2303.15409v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.15403v1","updated":"2023-03-27T17:19:50Z","published":"2023-03-27T17:19:50Z","title":"Training-free Style Transfer Emerges from h-space in Diffusion models","summary":"  Diffusion models (DMs) synthesize high-quality images in various domains.\nHowever, controlling their generative process is still hazy because the\nintermediate variables in the process are not rigorously studied. Recently,\nStyleCLIP-like editing of DMs is found in the bottleneck of the U-Net, named\n$h$-space. In this paper, we discover that DMs inherently have disentangled\nrepresentations for content and style of the resulting images: $h$-space\ncontains the content and the skip connections convey the style. Furthermore, we\nintroduce a principled way to inject content of one image to another\nconsidering progressive nature of the generative process. Briefly, given the\noriginal generative process, 1) the feature of the source content should be\ngradually blended, 2) the blended feature should be normalized to preserve the\ndistribution, 3) the change of skip connections due to content injection should\nbe calibrated. Then, the resulting image has the source content with the style\nof the original image just like image-to-image translation. Interestingly,\ninjecting contents to styles of unseen domains produces harmonization-like\nstyle transfer. To the best of our knowledge, our method introduces the first\ntraining-free feed-forward style transfer only with an unconditional pretrained\nfrozen generative network. The code is available at\nhttps://curryjung.github.io/DiffStyle/.\n","authors":["Jaeseok Jeong","Mingi Kwon","Youngjung Uh"],"pdf_url":"https://arxiv.org/pdf/2303.15403v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2211.15069v2","updated":"2023-03-27T17:18:59Z","published":"2022-11-28T05:06:03Z","title":"FeatureBooster: Boosting Feature Descriptors with a Lightweight Neural\n  Network","summary":"  We introduce a lightweight network to improve descriptors of keypoints within\nthe same image. The network takes the original descriptors and the geometric\nproperties of keypoints as the input, and uses an MLP-based self-boosting stage\nand a Transformer-based cross-boosting stage to enhance the descriptors. The\nboosted descriptors can be either real-valued or binary ones. We use the\nproposed network to boost both hand-crafted (ORB, SIFT) and the\nstate-of-the-art learning-based descriptors (SuperPoint, ALIKE) and evaluate\nthem on image matching, visual localization, and structure-from-motion tasks.\nThe results show that our method significantly improves the performance of each\ntask, particularly in challenging cases such as large illumination changes or\nrepetitive patterns. Our method requires only 3.2ms on desktop GPU and 27ms on\nembedded GPU to process 2000 features, which is fast enough to be applied to a\npractical system. The code and trained weights are publicly available at\ngithub.com/SJTU-ViSYS/FeatureBooster.\n","authors":["Xinjiang Wang","Zeyu Liu","Yu Hu","Wei Xi","Wenxian Yu","Danping Zou"],"pdf_url":"https://arxiv.org/pdf/2211.15069v2.pdf","comment":"Accept by CVPR2023; 15 pages, 8 figures, 7 tables"},{"id":"http://arxiv.org/abs/2302.14348v3","updated":"2023-03-27T17:08:27Z","published":"2023-02-28T06:38:25Z","title":"Im2Hands: Learning Attentive Implicit Representation of Interacting\n  Two-Hand Shapes","summary":"  We present Implicit Two Hands (Im2Hands), the first neural implicit\nrepresentation of two interacting hands. Unlike existing methods on two-hand\nreconstruction that rely on a parametric hand model and/or low-resolution\nmeshes, Im2Hands can produce fine-grained geometry of two hands with high\nhand-to-hand and hand-to-image coherency. To handle the shape complexity and\ninteraction context between two hands, Im2Hands models the occupancy volume of\ntwo hands - conditioned on an RGB image and coarse 3D keypoints - by two novel\nattention-based modules responsible for (1) initial occupancy estimation and\n(2) context-aware occupancy refinement, respectively. Im2Hands first learns\nper-hand neural articulated occupancy in the canonical space designed for each\nhand using query-image attention. It then refines the initial two-hand\noccupancy in the posed space to enhance the coherency between the two hand\nshapes using query-anchor attention. In addition, we introduce an optional\nkeypoint refinement module to enable robust two-hand shape estimation from\npredicted hand keypoints in a single-image reconstruction scenario. We\nexperimentally demonstrate the effectiveness of Im2Hands on two-hand\nreconstruction in comparison to related methods, where ours achieves\nstate-of-the-art results. Our code is publicly available at\nhttps://github.com/jyunlee/Im2Hands.\n","authors":["Jihyun Lee","Minhyuk Sung","Honggyu Choi","Tae-Kyun Kim"],"pdf_url":"https://arxiv.org/pdf/2302.14348v3.pdf","comment":"6 figures, 14 pages, accepted to CVPR 2023, project page:\n  https://jyunlee.github.io/projects/implicit-two-hands/"},{"id":"http://arxiv.org/abs/2303.15390v1","updated":"2023-03-27T17:03:30Z","published":"2023-03-27T17:03:30Z","title":"Learning to Zoom and Unzoom","summary":"  Many perception systems in mobile computing, autonomous navigation, and AR/VR\nface strict compute constraints that are particularly challenging for\nhigh-resolution input images. Previous works propose nonuniform downsamplers\nthat \"learn to zoom\" on salient image regions, reducing compute while retaining\ntask-relevant image information. However, for tasks with spatial labels (such\nas 2D/3D object detection and semantic segmentation), such distortions may harm\nperformance. In this work (LZU), we \"learn to zoom\" in on the input image,\ncompute spatial features, and then \"unzoom\" to revert any deformations. To\nenable efficient and differentiable unzooming, we approximate the zooming warp\nwith a piecewise bilinear mapping that is invertible. LZU can be applied to any\ntask with 2D spatial input and any model with 2D spatial features, and we\ndemonstrate this versatility by evaluating on a variety of tasks and datasets:\nobject detection on Argoverse-HD, semantic segmentation on Cityscapes, and\nmonocular 3D object detection on nuScenes. Interestingly, we observe boosts in\nperformance even when high-resolution sensor data is unavailable, implying that\nLZU can be used to \"learn to upsample\" as well.\n","authors":["Chittesh Thavamani","Mengtian Li","Francesco Ferroni","Deva Ramanan"],"pdf_url":"https://arxiv.org/pdf/2303.15390v1.pdf","comment":"CVPR 2023. Code and additional visuals available at\n  https://tchittesh.github.io/lzu/"},{"id":"http://arxiv.org/abs/2303.15389v1","updated":"2023-03-27T17:02:21Z","published":"2023-03-27T17:02:21Z","title":"EVA-CLIP: Improved Training Techniques for CLIP at Scale","summary":"  Contrastive language-image pre-training, CLIP for short, has gained\nincreasing attention for its potential in various scenarios. In this paper, we\npropose EVA-CLIP, a series of models that significantly improve the efficiency\nand effectiveness of CLIP training. Our approach incorporates new techniques\nfor representation learning, optimization, and augmentation, enabling EVA-CLIP\nto achieve superior performance compared to previous CLIP models with the same\nnumber of parameters but significantly smaller training costs. Notably, our\nlargest 5.0B-parameter EVA-02-CLIP-E/14+ with only 9 billion seen samples\nachieves 82.0 zero-shot top-1 accuracy on ImageNet-1K val. A smaller\nEVA-02-CLIP-L/14+ with only 430 million parameters and 6 billion seen samples\nachieves 80.4 zero-shot top-1 accuracy on ImageNet-1K val. To facilitate open\naccess and open research, we release the complete suite of EVA-CLIP to the\ncommunity at https://github.com/baaivision/EVA/tree/master/EVA-CLIP.\n","authors":["Quan Sun","Yuxin Fang","Ledell Wu","Xinlong Wang","Yue Cao"],"pdf_url":"https://arxiv.org/pdf/2303.15389v1.pdf","comment":"To Rei and the moon. Code & Models:\n  https://github.com/baaivision/EVA/tree/master/EVA-CLIP"},{"id":"http://arxiv.org/abs/2303.15387v1","updated":"2023-03-27T17:00:31Z","published":"2023-03-27T17:00:31Z","title":"Generalizable Neural Voxels for Fast Human Radiance Fields","summary":"  Rendering moving human bodies at free viewpoints only from a monocular video\nis quite a challenging problem. The information is too sparse to model\ncomplicated human body structures and motions from both view and pose\ndimensions. Neural radiance fields (NeRF) have shown great power in novel view\nsynthesis and have been applied to human body rendering. However, most current\nNeRF-based methods bear huge costs for both training and rendering, which\nimpedes the wide applications in real-life scenarios. In this paper, we propose\na rendering framework that can learn moving human body structures extremely\nquickly from a monocular video. The framework is built by integrating both\nneural fields and neural voxels. Especially, a set of generalizable neural\nvoxels are constructed. With pretrained on various human bodies, these general\nvoxels represent a basic skeleton and can provide strong geometric priors. For\nthe fine-tuning process, individual voxels are constructed for learning\ndifferential textures, complementary to general voxels. Thus learning a novel\nbody can be further accelerated, taking only a few minutes. Our method shows\nsignificantly higher training efficiency compared with previous methods, while\nmaintaining similar rendering quality. The project page is at\nhttps://taoranyi.com/gneuvox .\n","authors":["Taoran Yi","Jiemin Fang","Xinggang Wang","Wenyu Liu"],"pdf_url":"https://arxiv.org/pdf/2303.15387v1.pdf","comment":"Project page: http://taoranyi.com/gneuvox"},{"id":"http://arxiv.org/abs/2303.15385v1","updated":"2023-03-27T16:58:39Z","published":"2023-03-27T16:58:39Z","title":"Recognizing Rigid Patterns of Unlabeled Point Clouds by Complete and\n  Continuous Isometry Invariants with no False Negatives and no False Positives","summary":"  Rigid structures such as cars or any other solid objects are often\nrepresented by finite clouds of unlabeled points. The most natural equivalence\non these point clouds is rigid motion or isometry maintaining all inter-point\ndistances. Rigid patterns of point clouds can be reliably compared only by\ncomplete isometry invariants that can also be called equivariant descriptors\nwithout false negatives (isometric clouds having different descriptions) and\nwithout false positives (non-isometric clouds with the same description). Noise\nand motion in data motivate a search for invariants that are continuous under\nperturbations of points in a suitable metric. We propose the first continuous\nand complete invariant of unlabeled clouds in any Euclidean space. For a fixed\ndimension, the new metric for this invariant is computable in a polynomial time\nin the number of points.\n","authors":["Daniel Widdowson","Vitaliy Kurlin"],"pdf_url":"https://arxiv.org/pdf/2303.15385v1.pdf","comment":"This conference version is for CVPR (Computer Vision and Pattern\n  Recognition), https://cvpr2023.thecvf.com. The latest file is\n  http://kurlin.org/projects/cloud-isometry-spaces/distance-based-invariants.pdf.\n  The extended versions of sections 3-4 with all proofs and big examples are at\n  arXiv:2303.14161 for metric spaces, arXiv:2303.13486 for Euclidean spaces.\n  arXiv admin note: substantial text overlap with arXiv:2303.13486,\n  arXiv:2303.14161"},{"id":"http://arxiv.org/abs/2303.11969v2","updated":"2023-03-27T16:56:38Z","published":"2023-03-21T16:01:55Z","title":"Explain To Me: Salience-Based Explainability for Synthetic Face\n  Detection Models","summary":"  The performance of convolutional neural networks has continued to improve\nover the last decade. At the same time, as model complexity grows, it becomes\nincreasingly more difficult to explain model decisions. Such explanations may\nbe of critical importance for reliable operation of human-machine pairing\nsetups, or for model selection when the \"best\" model among many\nequally-accurate models must be established. Saliency maps represent one\npopular way of explaining model decisions by highlighting image regions models\ndeem important when making a prediction. However, examining salience maps at\nscale is not practical. In this paper, we propose five novel methods of\nleveraging model salience to explain a model behavior at scale. These methods\nask: (a) what is the average entropy for a model's salience maps, (b) how does\nmodel salience change when fed out-of-set samples, (c) how closely does model\nsalience follow geometrical transformations, (d) what is the stability of model\nsalience across independent training runs, and (e) how does model salience\nreact to salience-guided image degradations. To assess the proposed measures on\na concrete and topical problem, we conducted a series of experiments for the\ntask of synthetic face detection with two types of models: those trained\ntraditionally with cross-entropy loss, and those guided by human salience when\ntraining to increase model generalizability. These two types of models are\ncharacterized by different, interpretable properties of their salience maps,\nwhich allows for the evaluation of the correctness of the proposed measures. We\noffer source codes for each measure along with this paper.\n","authors":["Colton Crum","Patrick Tinsley","Aidan Boyd","Jacob Piland","Christopher Sweet","Timothy Kelley","Kevin Bowyer","Adam Czajka"],"pdf_url":"https://arxiv.org/pdf/2303.11969v2.pdf","comment":"13 pages, 10 figures"},{"id":"http://arxiv.org/abs/2303.15380v1","updated":"2023-03-27T16:53:09Z","published":"2023-03-27T16:53:09Z","title":"Hi4D: 4D Instance Segmentation of Close Human Interaction","summary":"  We propose Hi4D, a method and dataset for the automatic analysis of\nphysically close human-human interaction under prolonged contact. Robustly\ndisentangling several in-contact subjects is a challenging task due to\nocclusions and complex shapes. Hence, existing multi-view systems typically\nfuse 3D surfaces of close subjects into a single, connected mesh. To address\nthis issue we leverage i) individually fitted neural implicit avatars; ii) an\nalternating optimization scheme that refines pose and surface through periods\nof close proximity; and iii) thus segment the fused raw scans into individual\ninstances. From these instances we compile Hi4D dataset of 4D textured scans of\n20 subject pairs, 100 sequences, and a total of more than 11K frames. Hi4D\ncontains rich interaction-centric annotations in 2D and 3D alongside accurately\nregistered parametric body models. We define varied human pose and shape\nestimation tasks on this dataset and provide results from state-of-the-art\nmethods on these benchmarks.\n","authors":["Yifei Yin","Chen Guo","Manuel Kaufmann","Juan Jose Zarate","Jie Song","Otmar Hilliges"],"pdf_url":"https://arxiv.org/pdf/2303.15380v1.pdf","comment":"Project page: https://yifeiyin04.github.io/Hi4D/"},{"id":"http://arxiv.org/abs/2303.15377v1","updated":"2023-03-27T16:51:51Z","published":"2023-03-27T16:51:51Z","title":"AIR-DA: Adversarial Image Reconstruction for Unsupervised Domain\n  Adaptive Object Detection","summary":"  Unsupervised domain adaptive object detection is a challenging vision task\nwhere object detectors are adapted from a label-rich source domain to an\nunlabeled target domain. Recent advances prove the efficacy of the adversarial\nbased domain alignment where the adversarial training between the feature\nextractor and domain discriminator results in domain-invariance in the feature\nspace. However, due to the domain shift, domain discrimination, especially on\nlow-level features, is an easy task. This results in an imbalance of the\nadversarial training between the domain discriminator and the feature\nextractor. In this work, we achieve a better domain alignment by introducing an\nauxiliary regularization task to improve the training balance. Specifically, we\npropose Adversarial Image Reconstruction (AIR) as the regularizer to facilitate\nthe adversarial training of the feature extractor. We further design a\nmulti-level feature alignment module to enhance the adaptation performance. Our\nevaluations across several datasets of challenging domain shifts demonstrate\nthat the proposed method outperforms all previous methods, of both one- and\ntwo-stage, in most settings.\n","authors":["Kunyang Sun","Wei Lin","Haoqin Shi","Zhengming Zhang","Yongming Huang","Horst Bischof"],"pdf_url":"https://arxiv.org/pdf/2303.15377v1.pdf","comment":"Accepted at IEEE Robotics and Automation Letters 2023"},{"id":"http://arxiv.org/abs/2211.11646v3","updated":"2023-03-27T16:40:30Z","published":"2022-11-21T17:02:01Z","title":"NeRF-RPN: A general framework for object detection in NeRFs","summary":"  This paper presents the first significant object detection framework,\nNeRF-RPN, which directly operates on NeRF. Given a pre-trained NeRF model,\nNeRF-RPN aims to detect all bounding boxes of objects in a scene. By exploiting\na novel voxel representation that incorporates multi-scale 3D neural volumetric\nfeatures, we demonstrate it is possible to regress the 3D bounding boxes of\nobjects in NeRF directly without rendering the NeRF at any viewpoint. NeRF-RPN\nis a general framework and can be applied to detect objects without class\nlabels. We experimented NeRF-RPN with various backbone architectures, RPN head\ndesigns and loss functions. All of them can be trained in an end-to-end manner\nto estimate high quality 3D bounding boxes. To facilitate future research in\nobject detection for NeRF, we built a new benchmark dataset which consists of\nboth synthetic and real-world data with careful labeling and clean up. Code and\ndataset are available at https://github.com/lyclyc52/NeRF_RPN.\n","authors":["Benran Hu","Junkai Huang","Yichen Liu","Yu-Wing Tai","Chi-Keung Tang"],"pdf_url":"https://arxiv.org/pdf/2211.11646v3.pdf","comment":"Accepted by CVPR 2023"},{"id":"http://arxiv.org/abs/2301.09632v2","updated":"2023-03-27T16:39:58Z","published":"2023-01-23T18:59:25Z","title":"HexPlane: A Fast Representation for Dynamic Scenes","summary":"  Modeling and re-rendering dynamic 3D scenes is a challenging task in 3D\nvision. Prior approaches build on NeRF and rely on implicit representations.\nThis is slow since it requires many MLP evaluations, constraining real-world\napplications. We show that dynamic 3D scenes can be explicitly represented by\nsix planes of learned features, leading to an elegant solution we call\nHexPlane. A HexPlane computes features for points in spacetime by fusing\nvectors extracted from each plane, which is highly efficient. Pairing a\nHexPlane with a tiny MLP to regress output colors and training via volume\nrendering gives impressive results for novel view synthesis on dynamic scenes,\nmatching the image quality of prior work but reducing training time by more\nthan $100\\times$. Extensive ablations confirm our HexPlane design and show that\nit is robust to different feature fusion mechanisms, coordinate systems, and\ndecoding mechanisms. HexPlane is a simple and effective solution for\nrepresenting 4D volumes, and we hope they can broadly contribute to modeling\nspacetime for dynamic 3D scenes.\n","authors":["Ang Cao","Justin Johnson"],"pdf_url":"https://arxiv.org/pdf/2301.09632v2.pdf","comment":"CVPR 2023, Camera Ready Project page:\n  https://caoang327.github.io/HexPlane"},{"id":"http://arxiv.org/abs/2303.15368v1","updated":"2023-03-27T16:35:28Z","published":"2023-03-27T16:35:28Z","title":"NeUDF: Learning Unsigned Distance Fields from Multi-view Images for\n  Reconstructing Non-watertight Models","summary":"  Volume rendering-based 3D reconstruction from multi-view images has gained\npopularity in recent years, largely due to the success of neural radiance\nfields (NeRF). A number of methods have been developed that build upon NeRF and\nuse neural volume rendering to learn signed distance fields (SDFs) for\nreconstructing 3D models. However, SDF-based methods cannot represent\nnon-watertight models and, therefore, cannot capture open boundaries. This\npaper proposes a new algorithm for learning an accurate unsigned distance field\n(UDF) from multi-view images, which is specifically designed for reconstructing\nnon-watertight, textureless models. The proposed method, called NeUDF,\naddresses the limitations of existing UDF-based methods by introducing a simple\nand approximately unbiased and occlusion-aware density function. In addition, a\nsmooth and differentiable UDF representation is presented to make the learning\nprocess easier and more efficient. Experiments on both texture-rich and\ntextureless models demonstrate the robustness and effectiveness of the proposed\napproach, making it a promising solution for reconstructing challenging 3D\nmodels from multi-view images.\n","authors":["Fei Hou","Jukai Deng","Xuhui Chen","Wencheng Wang","Ying He"],"pdf_url":"https://arxiv.org/pdf/2303.15368v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.15361v1","updated":"2023-03-27T16:32:21Z","published":"2023-03-27T16:32:21Z","title":"A Comprehensive Survey on Test-Time Adaptation under Distribution Shifts","summary":"  Machine learning methods strive to acquire a robust model during training\nthat can generalize well to test samples, even under distribution shifts.\nHowever, these methods often suffer from a performance drop due to unknown test\ndistributions. Test-time adaptation (TTA), an emerging paradigm, has the\npotential to adapt a pre-trained model to unlabeled data during testing, before\nmaking predictions. Recent progress in this paradigm highlights the significant\nbenefits of utilizing unlabeled data for training self-adapted models prior to\ninference. In this survey, we divide TTA into several distinct categories,\nnamely, test-time (source-free) domain adaptation, test-time batch adaptation,\nonline test-time adaptation, and test-time prior adaptation. For each category,\nwe provide a comprehensive taxonomy of advanced algorithms, followed by a\ndiscussion of different learning scenarios. Furthermore, we analyze relevant\napplications of TTA and discuss open challenges and promising areas for future\nresearch. A comprehensive list of TTA methods can be found at\n\\url{https://github.com/tim-learn/awesome-test-time-adaptation}.\n","authors":["Jian Liang","Ran He","Tieniu Tan"],"pdf_url":"https://arxiv.org/pdf/2303.15361v1.pdf","comment":"Discussions, comments, and questions are all welcomed in\n  \\url{https://github.com/tim-learn/awesome-test-time-adaptation}"},{"id":"http://arxiv.org/abs/2303.15343v1","updated":"2023-03-27T15:53:01Z","published":"2023-03-27T15:53:01Z","title":"Sigmoid Loss for Language Image Pre-Training","summary":"  We propose a simple pairwise sigmoid loss for image-text pre-training. Unlike\nstandard contrastive learning with softmax normalization, the sigmoid loss\noperates solely on image-text pairs and does not require a global view of the\npairwise similarities for normalization. The sigmoid loss simultaneously allows\nfurther scaling up the batch size, while also performing better at smaller\nbatch sizes. With only four TPUv4 chips, we can train a Base CLIP model at 4k\nbatch size and a Large LiT model at 20k batch size, the latter achieves 84.5%\nImageNet zero-shot accuracy in two days. This disentanglement of the batch size\nfrom the loss further allows us to study the impact of examples vs pairs and\nnegative to positive ratio. Finally, we push the batch size to the extreme, up\nto one million, and find that the benefits of growing batch size quickly\ndiminish, with a more reasonable batch size of 32k being sufficient. We hope\nour research motivates further explorations in improving the quality and\nefficiency of language-image pre-training.\n","authors":["Xiaohua Zhai","Basil Mustafa","Alexander Kolesnikov","Lucas Beyer"],"pdf_url":"https://arxiv.org/pdf/2303.15343v1.pdf","comment":"Xiaohua and Lucas contributed equally"},{"id":"http://arxiv.org/abs/2303.15342v1","updated":"2023-03-27T15:52:14Z","published":"2023-03-27T15:52:14Z","title":"Exploring Continual Learning of Diffusion Models","summary":"  Diffusion models have achieved remarkable success in generating high-quality\nimages thanks to their novel training procedures applied to unprecedented\namounts of data. However, training a diffusion model from scratch is\ncomputationally expensive. This highlights the need to investigate the\npossibility of training these models iteratively, reusing computation while the\ndata distribution changes. In this study, we take the first step in this\ndirection and evaluate the continual learning (CL) properties of diffusion\nmodels. We begin by benchmarking the most common CL methods applied to\nDenoising Diffusion Probabilistic Models (DDPMs), where we note the strong\nperformance of the experience replay with the reduced rehearsal coefficient.\nFurthermore, we provide insights into the dynamics of forgetting, which exhibit\ndiverse behavior across diffusion timesteps. We also uncover certain pitfalls\nof using the bits-per-dimension metric for evaluating CL.\n","authors":["Michał Zając","Kamil Deja","Anna Kuzina","Jakub M. Tomczak","Tomasz Trzciński","Florian Shkurti","Piotr Miłoś"],"pdf_url":"https://arxiv.org/pdf/2303.15342v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2204.07841v3","updated":"2023-03-27T15:40:57Z","published":"2022-04-16T16:45:06Z","title":"Multi-Modal Few-Shot Object Detection with Meta-Learning-Based\n  Cross-Modal Prompting","summary":"  We study multi-modal few-shot object detection (FSOD) in this paper, using\nboth few-shot visual examples and class semantic information for detection,\nwhich are complementary to each other by definition. Most of the previous works\non multi-modal FSOD are fine-tuning-based which are inefficient for online\napplications. Moreover, these methods usually require expertise like class\nnames to extract class semantic embedding, which are hard to get for rare\nclasses. Our approach is motivated by the high-level conceptual similarity of\n(metric-based) meta-learning and prompt-based learning to learn generalizable\nfew-shot and zero-shot object detection models respectively without\nfine-tuning. Specifically, we combine the few-shot visual classifier and text\nclassifier learned via meta-learning and prompt-based learning respectively to\nbuild the multi-modal classifier and detection models. In addition, to fully\nexploit the pre-trained language models, we propose meta-learning-based\ncross-modal prompting to generate soft prompts for novel classes present in\nfew-shot visual examples, which are then used to learn the text classifier.\nKnowledge distillation is introduced to learn the soft prompt generator without\nusing human prior knowledge of class names, which may not be available for rare\nclasses. Our insight is that the few-shot support images naturally include\nrelated context information and semantics of the class. We comprehensively\nevaluate the proposed multi-modal FSOD models on multiple few-shot object\ndetection benchmarks, achieving promising results.\n","authors":["Guangxing Han","Long Chen","Jiawei Ma","Shiyuan Huang","Rama Chellappa","Shih-Fu Chang"],"pdf_url":"https://arxiv.org/pdf/2204.07841v3.pdf","comment":"17 pages"},{"id":"http://arxiv.org/abs/2303.15334v1","updated":"2023-03-27T15:35:21Z","published":"2023-03-27T15:35:21Z","title":"ByteTrackV2: 2D and 3D Multi-Object Tracking by Associating Every\n  Detection Box","summary":"  Multi-object tracking (MOT) aims at estimating bounding boxes and identities\nof objects across video frames. Detection boxes serve as the basis of both 2D\nand 3D MOT. The inevitable changing of detection scores leads to object missing\nafter tracking. We propose a hierarchical data association strategy to mine the\ntrue objects in low-score detection boxes, which alleviates the problems of\nobject missing and fragmented trajectories. The simple and generic data\nassociation strategy shows effectiveness under both 2D and 3D settings. In 3D\nscenarios, it is much easier for the tracker to predict object velocities in\nthe world coordinate. We propose a complementary motion prediction strategy\nthat incorporates the detected velocities with a Kalman filter to address the\nproblem of abrupt motion and short-term disappearing. ByteTrackV2 leads the\nnuScenes 3D MOT leaderboard in both camera (56.4% AMOTA) and LiDAR (70.1%\nAMOTA) modalities. Furthermore, it is nonparametric and can be integrated with\nvarious detectors, making it appealing in real applications. The source code is\nreleased at https://github.com/ifzhang/ByteTrack-V2.\n","authors":["Yifu Zhang","Xinggang Wang","Xiaoqing Ye","Wei Zhang","Jincheng Lu","Xiao Tan","Errui Ding","Peize Sun","Jingdong Wang"],"pdf_url":"https://arxiv.org/pdf/2303.15334v1.pdf","comment":"Code is available at https://github.com/ifzhang/ByteTrack-V2. arXiv\n  admin note: text overlap with arXiv:2110.06864; substantial text overlap with\n  arXiv:2203.06424 by other authors"},{"id":"http://arxiv.org/abs/2303.13508v2","updated":"2023-03-27T15:34:19Z","published":"2023-03-23T17:59:00Z","title":"DreamBooth3D: Subject-Driven Text-to-3D Generation","summary":"  We present DreamBooth3D, an approach to personalize text-to-3D generative\nmodels from as few as 3-6 casually captured images of a subject. Our approach\ncombines recent advances in personalizing text-to-image models (DreamBooth)\nwith text-to-3D generation (DreamFusion). We find that naively combining these\nmethods fails to yield satisfactory subject-specific 3D assets due to\npersonalized text-to-image models overfitting to the input viewpoints of the\nsubject. We overcome this through a 3-stage optimization strategy where we\njointly leverage the 3D consistency of neural radiance fields together with the\npersonalization capability of text-to-image models. Our method can produce\nhigh-quality, subject-specific 3D assets with text-driven modifications such as\nnovel poses, colors and attributes that are not seen in any of the input images\nof the subject.\n","authors":["Amit Raj","Srinivas Kaza","Ben Poole","Michael Niemeyer","Nataniel Ruiz","Ben Mildenhall","Shiran Zada","Kfir Aberman","Michael Rubinstein","Jonathan Barron","Yuanzhen Li","Varun Jampani"],"pdf_url":"https://arxiv.org/pdf/2303.13508v2.pdf","comment":"Project page at https://dreambooth3d.github.io/ Video Summary at\n  https://youtu.be/kKVDrbfvOoA"},{"id":"http://arxiv.org/abs/2302.04774v3","updated":"2023-03-27T15:33:35Z","published":"2023-02-09T17:08:43Z","title":"3D Human Pose and Shape Estimation via HybrIK-Transformer","summary":"  HybrIK relies on a combination of analytical inverse kinematics and deep\nlearning to produce more accurate 3D pose estimation from 2D monocular images.\nHybrIK has three major components: (1) pretrained convolution backbone, (2)\ndeconvolution to lift 3D pose from 2D convolution features, (3) analytical\ninverse kinematics pass correcting deep learning prediction using learned\ndistribution of plausible twist and swing angles. In this paper we propose an\nenhancement of the 2D to 3D lifting module, replacing deconvolution with\nTransformer, resulting in accuracy and computational efficiency improvement\nrelative to the original HybrIK method. We demonstrate our results on commonly\nused H36M, PW3D, COCO and HP3D datasets. Our code is publicly available\nhttps://github.com/boreshkinai/hybrik-transformer.\n","authors":["Boris N. Oreshkin"],"pdf_url":"https://arxiv.org/pdf/2302.04774v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.15322v1","updated":"2023-03-27T15:21:43Z","published":"2023-03-27T15:21:43Z","title":"Progressive Semantic-Visual Mutual Adaption for Generalized Zero-Shot\n  Learning","summary":"  Generalized Zero-Shot Learning (GZSL) identifies unseen categories by\nknowledge transferred from the seen domain, relying on the intrinsic\ninteractions between visual and semantic information. Prior works mainly\nlocalize regions corresponding to the sharing attributes. When various visual\nappearances correspond to the same attribute, the sharing attributes inevitably\nintroduce semantic ambiguity, hampering the exploration of accurate\nsemantic-visual interactions. In this paper, we deploy the dual semantic-visual\ntransformer module (DSVTM) to progressively model the correspondences between\nattribute prototypes and visual features, constituting a progressive\nsemantic-visual mutual adaption (PSVMA) network for semantic disambiguation and\nknowledge transferability improvement. Specifically, DSVTM devises an\ninstance-motivated semantic encoder that learns instance-centric prototypes to\nadapt to different images, enabling the recast of the unmatched semantic-visual\npair into the matched one. Then, a semantic-motivated instance decoder\nstrengthens accurate cross-domain interactions between the matched pair for\nsemantic-related instance adaption, encouraging the generation of unambiguous\nvisual representations. Moreover, to mitigate the bias towards seen classes in\nGZSL, a debiasing loss is proposed to pursue response consistency between seen\nand unseen predictions. The PSVMA consistently yields superior performances\nagainst other state-of-the-art methods. Code will be available at:\nhttps://github.com/ManLiuCoder/PSVMA.\n","authors":["Man Liu","Feng Li","Chunjie Zhang","Yunchao Wei","Huihui Bai","Yao Zhao"],"pdf_url":"https://arxiv.org/pdf/2303.15322v1.pdf","comment":"Accepted by CVPR2023"},{"id":"http://arxiv.org/abs/2303.07522v2","updated":"2023-03-27T15:10:51Z","published":"2023-03-13T23:17:51Z","title":"Audio Visual Language Maps for Robot Navigation","summary":"  While interacting in the world is a multi-sensory experience, many robots\ncontinue to predominantly rely on visual perception to map and navigate in\ntheir environments. In this work, we propose Audio-Visual-Language Maps\n(AVLMaps), a unified 3D spatial map representation for storing cross-modal\ninformation from audio, visual, and language cues. AVLMaps integrate the\nopen-vocabulary capabilities of multimodal foundation models pre-trained on\nInternet-scale data by fusing their features into a centralized 3D voxel grid.\nIn the context of navigation, we show that AVLMaps enable robot systems to\nindex goals in the map based on multimodal queries, e.g., textual descriptions,\nimages, or audio snippets of landmarks. In particular, the addition of audio\ninformation enables robots to more reliably disambiguate goal locations.\nExtensive experiments in simulation show that AVLMaps enable zero-shot\nmultimodal goal navigation from multimodal prompts and provide 50% better\nrecall in ambiguous scenarios. These capabilities extend to mobile robots in\nthe real world - navigating to landmarks referring to visual, audio, and\nspatial concepts. Videos and code are available at: https://avlmaps.github.io.\n","authors":["Chenguang Huang","Oier Mees","Andy Zeng","Wolfram Burgard"],"pdf_url":"https://arxiv.org/pdf/2303.07522v2.pdf","comment":"Project page: https://avlmaps.github.io/"},{"id":"http://arxiv.org/abs/2303.15288v1","updated":"2023-03-27T15:10:19Z","published":"2023-03-27T15:10:19Z","title":"Diffusion Models for Memory-efficient Processing of 3D Medical Images","summary":"  Denoising diffusion models have recently achieved state-of-the-art\nperformance in many image-generation tasks. They do, however, require a large\namount of computational resources. This limits their application to medical\ntasks, where we often deal with large 3D volumes, like high-resolution\nthree-dimensional data. In this work, we present a number of different ways to\nreduce the resource consumption for 3D diffusion models and apply them to a\ndataset of 3D images. The main contribution of this paper is the\nmemory-efficient patch-based diffusion model \\textit{PatchDDM}, which can be\napplied to the total volume during inference while the training is performed\nonly on patches. While the proposed diffusion model can be applied to any image\ngeneration tasks, we evaluate the method on the tumor segmentation task of the\nBraTS2020 dataset and demonstrate that we can generate meaningful\nthree-dimensional segmentations.\n","authors":["Florentin Bieder","Julia Wolleb","Alicia Durrer","Robin Sandkühler","Philippe C. Cattin"],"pdf_url":"https://arxiv.org/pdf/2303.15288v1.pdf","comment":"Accepted at MIDL 2023"},{"id":"http://arxiv.org/abs/2303.15286v1","updated":"2023-03-27T15:07:55Z","published":"2023-03-27T15:07:55Z","title":"Unsupervised Adaptation from Repeated Traversals for Autonomous Driving","summary":"  For a self-driving car to operate reliably, its perceptual system must\ngeneralize to the end-user's environment -- ideally without additional\nannotation efforts. One potential solution is to leverage unlabeled data (e.g.,\nunlabeled LiDAR point clouds) collected from the end-users' environments (i.e.\ntarget domain) to adapt the system to the difference between training and\ntesting environments. While extensive research has been done on such an\nunsupervised domain adaptation problem, one fundamental problem lingers: there\nis no reliable signal in the target domain to supervise the adaptation process.\nTo overcome this issue we observe that it is easy to collect unsupervised data\nfrom multiple traversals of repeated routes. While different from conventional\nunsupervised domain adaptation, this assumption is extremely realistic since\nmany drivers share the same roads. We show that this simple additional\nassumption is sufficient to obtain a potent signal that allows us to perform\niterative self-training of 3D object detectors on the target domain.\nConcretely, we generate pseudo-labels with the out-of-domain detector but\nreduce false positives by removing detections of supposedly mobile objects that\nare persistent across traversals. Further, we reduce false negatives by\nencouraging predictions in regions that are not persistent. We experiment with\nour approach on two large-scale driving datasets and show remarkable\nimprovement in 3D object detection of cars, pedestrians, and cyclists, bringing\nus a step closer to generalizable autonomous driving.\n","authors":["Yurong You","Cheng Perng Phoo","Katie Z Luo","Travis Zhang","Wei-Lun Chao","Bharath Hariharan","Mark Campbell","Kilian Q. Weinberger"],"pdf_url":"https://arxiv.org/pdf/2303.15286v1.pdf","comment":"Accepted by NeurIPS 2022. Code is available at\n  https://github.com/YurongYou/Rote-DA"},{"id":"http://arxiv.org/abs/2303.15274v1","updated":"2023-03-27T15:02:48Z","published":"2023-03-27T15:02:48Z","title":"Gazeformer: Scalable, Effective and Fast Prediction of Goal-Directed\n  Human Attention","summary":"  Predicting human gaze is important in Human-Computer Interaction (HCI).\nHowever, to practically serve HCI applications, gaze prediction models must be\nscalable, fast, and accurate in their spatial and temporal gaze predictions.\nRecent scanpath prediction models focus on goal-directed attention (search).\nSuch models are limited in their application due to a common approach relying\non trained target detectors for all possible objects, and the availability of\nhuman gaze data for their training (both not scalable). In response, we pose a\nnew task called ZeroGaze, a new variant of zero-shot learning where gaze is\npredicted for never-before-searched objects, and we develop a novel model,\nGazeformer, to solve the ZeroGaze problem. In contrast to existing methods\nusing object detector modules, Gazeformer encodes the target using a natural\nlanguage model, thus leveraging semantic similarities in scanpath prediction.\nWe use a transformer-based encoder-decoder architecture because transformers\nare particularly useful for generating contextual representations. Gazeformer\nsurpasses other models by a large margin on the ZeroGaze setting. It also\noutperforms existing target-detection models on standard gaze prediction for\nboth target-present and target-absent search tasks. In addition to its improved\nperformance, Gazeformer is more than five times faster than the\nstate-of-the-art target-present visual search model.\n","authors":["Sounak Mondal","Zhibo Yang","Seoyoung Ahn","Dimitris Samaras","Gregory Zelinsky","Minh Hoai"],"pdf_url":"https://arxiv.org/pdf/2303.15274v1.pdf","comment":"IEEE Conference on Computer Vision and Pattern Recognition (CVPR),\n  2023"},{"id":"http://arxiv.org/abs/2302.07661v2","updated":"2023-03-27T15:02:36Z","published":"2023-02-15T13:48:10Z","title":"Depth- and Semantics-aware Multi-modal Domain Translation: Generating 3D\n  Panoramic Color Images from LiDAR Point Clouds","summary":"  This work presents a new depth- and semantics-aware conditional generative\nmodel, named TITAN-Next, for cross-domain image-to-image translation in a\nmulti-modal setup between LiDAR and camera sensors. The proposed model\nleverages scene semantics as a mid-level representation and is able to\ntranslate raw LiDAR point clouds to RGB-D camera images by solely relying on\nsemantic scene segments. We claim that this is the first framework of its kind\nand it has practical applications in autonomous vehicles such as providing a\nfail-safe mechanism and augmenting available data in the target image domain.\nThe proposed model is evaluated on the large-scale and challenging\nSemantic-KITTI dataset, and experimental findings show that it considerably\noutperforms the original TITAN-Net and other strong baselines by 23.7$\\%$\nmargin in terms of IoU.\n","authors":["Tiago Cortinhal","Eren Erdal Aksoy"],"pdf_url":"https://arxiv.org/pdf/2302.07661v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.15270v1","updated":"2023-03-27T14:59:08Z","published":"2023-03-27T14:59:08Z","title":"Unified Keypoint-based Action Recognition Framework via Structured\n  Keypoint Pooling","summary":"  This paper simultaneously addresses three limitations associated with\nconventional skeleton-based action recognition; skeleton detection and tracking\nerrors, poor variety of the targeted actions, as well as person-wise and\nframe-wise action recognition. A point cloud deep-learning paradigm is\nintroduced to the action recognition, and a unified framework along with a\nnovel deep neural network architecture called Structured Keypoint Pooling is\nproposed. The proposed method sparsely aggregates keypoint features in a\ncascaded manner based on prior knowledge of the data structure (which is\ninherent in skeletons), such as the instances and frames to which each keypoint\nbelongs, and achieves robustness against input errors. Its less constrained and\ntracking-free architecture enables time-series keypoints consisting of human\nskeletons and nonhuman object contours to be efficiently treated as an input 3D\npoint cloud and extends the variety of the targeted action. Furthermore, we\npropose a Pooling-Switching Trick inspired by Structured Keypoint Pooling. This\ntrick switches the pooling kernels between the training and inference phases to\ndetect person-wise and frame-wise actions in a weakly supervised manner using\nonly video-level action labels. This trick enables our training scheme to\nnaturally introduce novel data augmentation, which mixes multiple point clouds\nextracted from different videos. In the experiments, we comprehensively verify\nthe effectiveness of the proposed method against the limitations, and the\nmethod outperforms state-of-the-art skeleton-based action recognition and\nspatio-temporal action localization methods.\n","authors":["Ryo Hachiuma","Fumiaki Sato","Taiki Sekii"],"pdf_url":"https://arxiv.org/pdf/2303.15270v1.pdf","comment":"CVPR 2023"},{"id":"http://arxiv.org/abs/2303.15269v1","updated":"2023-03-27T14:58:20Z","published":"2023-03-27T14:58:20Z","title":"Handwritten Text Generation from Visual Archetypes","summary":"  Generating synthetic images of handwritten text in a writer-specific style is\na challenging task, especially in the case of unseen styles and new words, and\neven more when these latter contain characters that are rarely encountered\nduring training. While emulating a writer's style has been recently addressed\nby generative models, the generalization towards rare characters has been\ndisregarded. In this work, we devise a Transformer-based model for Few-Shot\nstyled handwritten text generation and focus on obtaining a robust and\ninformative representation of both the text and the style. In particular, we\npropose a novel representation of the textual content as a sequence of dense\nvectors obtained from images of symbols written as standard GNU Unifont glyphs,\nwhich can be considered their visual archetypes. This strategy is more suitable\nfor generating characters that, despite having been seen rarely during\ntraining, possibly share visual details with the frequently observed ones. As\nfor the style, we obtain a robust representation of unseen writers' calligraphy\nby exploiting specific pre-training on a large synthetic dataset. Quantitative\nand qualitative results demonstrate the effectiveness of our proposal in\ngenerating words in unseen styles and with rare characters more faithfully than\nexisting approaches relying on independent one-hot encodings of the characters.\n","authors":["Vittorio Pippi","Silvia Cascianelli","Rita Cucchiara"],"pdf_url":"https://arxiv.org/pdf/2303.15269v1.pdf","comment":"Accepted at CVPR2023"},{"id":"http://arxiv.org/abs/2303.15266v1","updated":"2023-03-27T14:54:50Z","published":"2023-03-27T14:54:50Z","title":"Multi-Granularity Archaeological Dating of Chinese Bronze Dings Based on\n  a Knowledge-Guided Relation Graph","summary":"  The archaeological dating of bronze dings has played a critical role in the\nstudy of ancient Chinese history. Current archaeology depends on trained\nexperts to carry out bronze dating, which is time-consuming and\nlabor-intensive. For such dating, in this study, we propose a learning-based\napproach to integrate advanced deep learning techniques and archaeological\nknowledge. To achieve this, we first collect a large-scale image dataset of\nbronze dings, which contains richer attribute information than other existing\nfine-grained datasets. Second, we introduce a multihead classifier and a\nknowledge-guided relation graph to mine the relationship between attributes and\nthe ding era. Third, we conduct comparison experiments with various existing\nmethods, the results of which show that our dating method achieves a\nstate-of-the-art performance. We hope that our data and applied networks will\nenrich fine-grained classification research relevant to other interdisciplinary\nareas of expertise. The dataset and source code used are included in our\nsupplementary materials, and will be open after submission owing to the\nanonymity policy. Source codes and data are available at:\nhttps://github.com/zhourixin/bronze-Ding.\n","authors":["Rixin Zhou","Jiafu Wei","Qian Zhang","Ruihua Qi","Xi Yang","Chuntao Li"],"pdf_url":"https://arxiv.org/pdf/2303.15266v1.pdf","comment":"CVPR2023 accepted"},{"id":"http://arxiv.org/abs/2303.15263v1","updated":"2023-03-27T14:52:08Z","published":"2023-03-27T14:52:08Z","title":"Joint Person Identity, Gender and Age Estimation from Hand Images using\n  Deep Multi-Task Representation Learning","summary":"  In this paper, we propose a multi-task representation learning framework to\njointly estimate the identity, gender and age of individuals from their hand\nimages for the purpose of criminal investigations since the hand images are\noften the only available information in cases of serious crime such as sexual\nabuse. We investigate different up-to-date deep learning architectures and\ncompare their performance for joint estimation of identity, gender and age from\nhand images of perpetrators of serious crime. To overcome the data imbalance\nand simplify the age prediction, we create age groups for the age estimation.\nWe make extensive evaluations and comparisons of both convolution-based and\ntransformer-based deep learning architectures on a publicly available 11k hands\ndataset. Our experimental analysis shows that it is possible to efficiently\nestimate not only identity but also other attributes such as gender and age of\nsuspects jointly from hand images for criminal investigations, which is crucial\nin assisting international police forces in the court to identify and convict\nabusers.\n","authors":["Nathanael L. Baisa"],"pdf_url":"https://arxiv.org/pdf/2303.15263v1.pdf","comment":"arXiv admin note: text overlap with arXiv:2209.04821"},{"id":"http://arxiv.org/abs/2212.01206v2","updated":"2023-03-27T14:51:07Z","published":"2022-12-02T14:37:20Z","title":"DiffRF: Rendering-Guided 3D Radiance Field Diffusion","summary":"  We introduce DiffRF, a novel approach for 3D radiance field synthesis based\non denoising diffusion probabilistic models. While existing diffusion-based\nmethods operate on images, latent codes, or point cloud data, we are the first\nto directly generate volumetric radiance fields. To this end, we propose a 3D\ndenoising model which directly operates on an explicit voxel grid\nrepresentation. However, as radiance fields generated from a set of posed\nimages can be ambiguous and contain artifacts, obtaining ground truth radiance\nfield samples is non-trivial. We address this challenge by pairing the\ndenoising formulation with a rendering loss, enabling our model to learn a\ndeviated prior that favours good image quality instead of trying to replicate\nfitting errors like floating artifacts. In contrast to 2D-diffusion models, our\nmodel learns multi-view consistent priors, enabling free-view synthesis and\naccurate shape generation. Compared to 3D GANs, our diffusion-based approach\nnaturally enables conditional generation such as masked completion or\nsingle-view 3D synthesis at inference time.\n","authors":["Norman Müller","Yawar Siddiqui","Lorenzo Porzi","Samuel Rota Bulò","Peter Kontschieder","Matthias Nießner"],"pdf_url":"https://arxiv.org/pdf/2212.01206v2.pdf","comment":"Project page: https://sirwyver.github.io/DiffRF/ Video:\n  https://youtu.be/qETBcLu8SUk - CVPR 2023 Highlight - updated evaluations\n  after fixing initial data mapping error on all methods"},{"id":"http://arxiv.org/abs/2303.15247v1","updated":"2023-03-27T14:31:25Z","published":"2023-03-27T14:31:25Z","title":"Zero-Shot Composed Image Retrieval with Textual Inversion","summary":"  Composed Image Retrieval (CIR) aims to retrieve a target image based on a\nquery composed of a reference image and a relative caption that describes the\ndifference between the two images. The high effort and cost required for\nlabeling datasets for CIR hamper the widespread usage of existing methods, as\nthey rely on supervised learning. In this work, we propose a new task,\nZero-Shot CIR (ZS-CIR), that aims to address CIR without requiring a labeled\ntraining dataset. Our approach, named zero-Shot composEd imAge Retrieval with\ntextuaL invErsion (SEARLE), maps the visual features of the reference image\ninto a pseudo-word token in CLIP token embedding space and integrates it with\nthe relative caption. To support research on ZS-CIR, we introduce an\nopen-domain benchmarking dataset named Composed Image Retrieval on Common\nObjects in context (CIRCO), which is the first dataset for CIR containing\nmultiple ground truths for each query. The experiments show that SEARLE\nexhibits better performance than the baselines on the two main datasets for CIR\ntasks, FashionIQ and CIRR, and on the proposed CIRCO. The dataset, the code and\nthe model are publicly available at https://github.com/miccunifi/SEARLE .\n","authors":["Alberto Baldrati","Lorenzo Agnolucci","Marco Bertini","Alberto Del Bimbo"],"pdf_url":"https://arxiv.org/pdf/2303.15247v1.pdf","comment":"SEARLE repository: https://github.com/miccunifi/SEARLE . CIRCO\n  repository: https://github.com/miccunifi/CIRCO"},{"id":"http://arxiv.org/abs/2303.15245v1","updated":"2023-03-27T14:29:18Z","published":"2023-03-27T14:29:18Z","title":"Comparison between layer-to-layer network training and conventional\n  network training using Convolutional Neural Networks","summary":"  Title: Comparison between layer-to-layer network training and conventional\nnetwork training using Convolutional Neural Networks\n  Abstract: Convolutional neural networks (CNNs) are widely used in various\napplications due to their effectiveness in extracting features from data.\nHowever, the performance of a CNN heavily depends on its architecture and\ntraining process. In this study, we propose a layer-to-layer training method\nand compare its performance with the conventional training method.\n  In the layer-to-layer training approach, we treat a portion of the early\nlayers as a student network and the later layers as a teacher network. During\neach training step, we incrementally train the student network to learn from\nthe output of the teacher network, and vice versa. We evaluate this approach on\na VGG16 network without pre-trained ImageNet weights and a regular CNN model.\n  Our experiments show that the layer-to-layer training method outperforms the\nconventional training method for both models. Specifically, we achieve higher\naccuracy on the test set for the VGG16 network and the CNN model using\nlayer-to-layer training compared to the conventional training method.\n  Overall, our study highlights the importance of layer-wise training in CNNs\nand suggests that layer-to-layer training can be a promising approach for\nimproving the accuracy of CNNs.\n","authors":["Kiran Kumar Ashish Bhyravabhottla","WonSook Lee"],"pdf_url":"https://arxiv.org/pdf/2303.15245v1.pdf","comment":"6 pages, 1 table"},{"id":"http://arxiv.org/abs/2303.14080v2","updated":"2023-03-27T14:24:03Z","published":"2023-03-24T15:44:42Z","title":"Best of Both Worlds: Multimodal Contrastive Learning with Tabular and\n  Imaging Data","summary":"  Medical datasets and especially biobanks, often contain extensive tabular\ndata with rich clinical information in addition to images. In practice,\nclinicians typically have less data, both in terms of diversity and scale, but\nstill wish to deploy deep learning solutions. Combined with increasing medical\ndataset sizes and expensive annotation costs, the necessity for unsupervised\nmethods that can pretrain multimodally and predict unimodally has risen.\n  To address these needs, we propose the first self-supervised contrastive\nlearning framework that takes advantage of images and tabular data to train\nunimodal encoders. Our solution combines SimCLR and SCARF, two leading\ncontrastive learning strategies, and is simple and effective. In our\nexperiments, we demonstrate the strength of our framework by predicting risks\nof myocardial infarction and coronary artery disease (CAD) using cardiac MR\nimages and 120 clinical features from 40,000 UK Biobank subjects. Furthermore,\nwe show the generalizability of our approach to natural images using the DVM\ncar advertisement dataset.\n  We take advantage of the high interpretability of tabular data and through\nattribution and ablation experiments find that morphometric tabular features,\ndescribing size and shape, have outsized importance during the contrastive\nlearning process and improve the quality of the learned embeddings. Finally, we\nintroduce a novel form of supervised contrastive learning, label as a feature\n(LaaF), by appending the ground truth label as a tabular feature during\nmultimodal pretraining, outperforming all supervised contrastive baselines.\n","authors":["Paul Hager","Martin J. Menten","Daniel Rueckert"],"pdf_url":"https://arxiv.org/pdf/2303.14080v2.pdf","comment":"Accepted in CVPR 2023"},{"id":"http://arxiv.org/abs/2212.07692v2","updated":"2023-03-27T14:22:54Z","published":"2022-12-15T09:57:19Z","title":"CNN-based real-time 2D-3D deformable registration from a single X-ray\n  projection","summary":"  Purpose: The purpose of this paper is to present a method for real-time 2D-3D\nnon-rigid registration using a single fluoroscopic image. Such a method can\nfind applications in surgery, interventional radiology and radiotherapy. By\nestimating a three-dimensional displacement field from a 2D X-ray image,\nanatomical structures segmented in the preoperative scan can be projected onto\nthe 2D image, thus providing a mixed reality view. Methods: A dataset composed\nof displacement fields and 2D projections of the anatomy is generated from the\npreoperative scan. From this dataset, a neural network is trained to recover\nthe unknown 3D displacement field from a single projection image. Results: Our\nmethod is validated on lung 4D CT data at different stages of the lung\ndeformation. The training is performed on a 3D CT using random (non\ndomain-specific) diffeomorphic deformations, to which perturbations mimicking\nthe pose uncertainty are added. The model achieves a mean TRE over a series of\nlandmarks ranging from 2.3 to 5.5 mm depending on the amplitude of deformation.\nConclusion: In this paper, a CNN-based method for real-time 2D-3D non-rigid\nregistration is presented. This method is able to cope with pose estimation\nuncertainties, making it applicable to actual clinical scenarios, such as lung\nsurgery, where the C-arm pose is planned before the intervention.\n","authors":["François Lecomte","Jean-Louis Dillenseger","Stéphane Cotin"],"pdf_url":"https://arxiv.org/pdf/2212.07692v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.09522v2","updated":"2023-03-27T14:22:28Z","published":"2023-03-16T17:38:15Z","title":"$P+$: Extended Textual Conditioning in Text-to-Image Generation","summary":"  We introduce an Extended Textual Conditioning space in text-to-image models,\nreferred to as $P+$. This space consists of multiple textual conditions,\nderived from per-layer prompts, each corresponding to a layer of the denoising\nU-net of the diffusion model.\n  We show that the extended space provides greater disentangling and control\nover image synthesis. We further introduce Extended Textual Inversion (XTI),\nwhere the images are inverted into $P+$, and represented by per-layer tokens.\n  We show that XTI is more expressive and precise, and converges faster than\nthe original Textual Inversion (TI) space. The extended inversion method does\nnot involve any noticeable trade-off between reconstruction and editability and\ninduces more regular inversions.\n  We conduct a series of extensive experiments to analyze and understand the\nproperties of the new space, and to showcase the effectiveness of our method\nfor personalizing text-to-image models. Furthermore, we utilize the unique\nproperties of this space to achieve previously unattainable results in\nobject-style mixing using text-to-image models. Project page:\nhttps://prompt-plus.github.io\n","authors":["Andrey Voynov","Qinghao Chu","Daniel Cohen-Or","Kfir Aberman"],"pdf_url":"https://arxiv.org/pdf/2303.09522v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.15233v1","updated":"2023-03-27T14:15:17Z","published":"2023-03-27T14:15:17Z","title":"Text-to-Image Diffusion Models are Zero-Shot Classifiers","summary":"  The excellent generative capabilities of text-to-image diffusion models\nsuggest they learn informative representations of image-text data. However,\nwhat knowledge their representations capture is not fully understood, and they\nhave not been thoroughly explored on downstream tasks. We investigate diffusion\nmodels by proposing a method for evaluating them as zero-shot classifiers. The\nkey idea is using a diffusion model's ability to denoise a noised image given a\ntext description of a label as a proxy for that label's likelihood. We apply\nour method to Imagen, using it to probe fine-grained aspects of Imagen's\nknowledge and comparing it with CLIP's zero-shot abilities. Imagen performs\ncompetitively with CLIP on a wide range of zero-shot image classification\ndatasets. Additionally, it achieves state-of-the-art results on shape/texture\nbias tests and can successfully perform attribute binding while CLIP cannot.\nAlthough generative pre-training is prevalent in NLP, visual foundation models\noften use other methods such as contrastive learning. Based on our findings, we\nargue that generative pre-training should be explored as a compelling\nalternative for vision and vision-language problems.\n","authors":["Kevin Clark","Priyank Jaini"],"pdf_url":"https://arxiv.org/pdf/2303.15233v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.15231v1","updated":"2023-03-27T14:13:02Z","published":"2023-03-27T14:13:02Z","title":"Advances and Applications of Computer Vision Techniques in Vehicle\n  Trajectory Generation and Surrogate Traffic Safety Indicators","summary":"  The application of Computer Vision (CV) techniques massively stimulates\nmicroscopic traffic safety analysis from the perspective of traffic conflicts\nand near misses, which is usually measured using Surrogate Safety Measures\n(SSM). However, as video processing and traffic safety modeling are two\nseparate research domains and few research have focused on systematically\nbridging the gap between them, it is necessary to provide transportation\nresearchers and practitioners with corresponding guidance. With this aim in\nmind, this paper focuses on reviewing the applications of CV techniques in\ntraffic safety modeling using SSM and suggesting the best way forward. The CV\nalgorithm that are used for vehicle detection and tracking from early\napproaches to the state-of-the-art models are summarized at a high level. Then,\nthe video pre-processing and post-processing techniques for vehicle trajectory\nextraction are introduced. A detailed review of SSMs for vehicle trajectory\ndata along with their application on traffic safety analysis is presented.\nFinally, practical issues in traffic video processing and SSM-based safety\nanalysis are discussed, and the available or potential solutions are provided.\nThis review is expected to assist transportation researchers and engineers with\nthe selection of suitable CV techniques for video processing, and the usage of\nSSMs for various traffic safety research objectives.\n","authors":["Mohamed Abdel-Aty","Zijin Wang","Ou Zheng","Amr Abdelraouf"],"pdf_url":"https://arxiv.org/pdf/2303.15231v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2207.11518v2","updated":"2023-03-27T14:12:20Z","published":"2022-07-23T13:39:01Z","title":"Online Knowledge Distillation via Mutual Contrastive Learning for Visual\n  Recognition","summary":"  The teacher-free online Knowledge Distillation (KD) aims to train an ensemble\nof multiple student models collaboratively and distill knowledge from each\nother. Although existing online KD methods achieve desirable performance, they\noften focus on class probabilities as the core knowledge type, ignoring the\nvaluable feature representational information. We present a Mutual Contrastive\nLearning (MCL) framework for online KD. The core idea of MCL is to perform\nmutual interaction and transfer of contrastive distributions among a cohort of\nnetworks in an online manner. Our MCL can aggregate cross-network embedding\ninformation and maximize the lower bound to the mutual information between two\nnetworks. This enables each network to learn extra contrastive knowledge from\nothers, leading to better feature representations, thus improving the\nperformance of visual recognition tasks. Beyond the final layer, we extend MCL\nto intermediate layers and perform an adaptive layer-matching mechanism trained\nby meta-optimization. Experiments on image classification and transfer learning\nto visual recognition tasks show that layer-wise MCL can lead to consistent\nperformance gains against state-of-the-art online KD approaches. The\nsuperiority demonstrates that layer-wise MCL can guide the network to generate\nbetter feature representations. Our code is publicly avaliable at\nhttps://github.com/winycg/L-MCL.\n","authors":["Chuanguang Yang","Zhulin An","Helong Zhou","Fuzhen Zhuang","Yongjun Xu","Qian Zhan"],"pdf_url":"https://arxiv.org/pdf/2207.11518v2.pdf","comment":"18 pages, accepted by IEEE Transactions on Pattern Analysis and\n  Machine Intelligence (TPAMI-2023)"},{"id":"http://arxiv.org/abs/2303.15230v1","updated":"2023-03-27T14:10:26Z","published":"2023-03-27T14:10:26Z","title":"Troika: Multi-Path Cross-Modal Traction for Compositional Zero-Shot\n  Learning","summary":"  Recent compositional zero-shot learning (CZSL) methods adapt pre-trained\nvision-language models (VLMs) by constructing trainable prompts only for\ncomposed state-object pairs. Relying on learning the joint representation of\nseen compositions, these methods ignore the explicit modeling of the state and\nobject, thus limiting the exploitation of pre-trained knowledge and\ngeneralization to unseen compositions. With a particular focus on the\nuniversality of the solution, in this work, we propose a novel paradigm for\nCZSL models that establishes three identification branches (i.e., Multi-Path)\nto jointly model the state, object, and composition. The presented Troika is\nour implementation that aligns the branch-specific prompt representations with\ndecomposed visual features. To calibrate the bias between semantically similar\nmulti-modal representations, we further devise a Cross-Modal Traction module\ninto Troika that shifts the prompt representation towards the current visual\ncontent. We conduct extensive experiments on three popular benchmarks, where\nour method significantly outperforms existing methods in both closed-world and\nopen-world settings.\n","authors":["Siteng Huang","Biao Gong","Yutong Feng","Yiliang Lv","Donglin Wang"],"pdf_url":"https://arxiv.org/pdf/2303.15230v1.pdf","comment":"14 pages"},{"id":"http://arxiv.org/abs/2303.15225v1","updated":"2023-03-27T14:05:34Z","published":"2023-03-27T14:05:34Z","title":"One-shot Feature-Preserving Point Cloud Simplification with Gaussian\n  Processes on Riemannian Manifolds","summary":"  The processing, storage and transmission of large-scale point clouds is an\nongoing challenge in the computer vision community which hinders progress in\nthe application of 3D models to real-world settings, such as autonomous\ndriving, virtual reality and remote sensing. We propose a novel, one-shot point\ncloud simplification method which preserves both the salient structural\nfeatures and the overall shape of a point cloud without any prior surface\nreconstruction step. Our method employs Gaussian processes with kernels defined\non Riemannian manifolds, allowing us to model the surface variation function\nacross any given point cloud. A simplified version of the original cloud is\nobtained by sequentially selecting points using a greedy sparsification scheme.\nThe selection criterion used for this scheme ensures that the simplified cloud\nbest represents the surface variation of the original point cloud. We evaluate\nour method on several benchmark datasets, compare it to a range of existing\nmethods and show that our method is competitive both in terms of empirical\nperformance and computational efficiency.\n","authors":["Stuti Pathak","Thomas M. McDonald","Rudi Penne"],"pdf_url":"https://arxiv.org/pdf/2303.15225v1.pdf","comment":"10 pages, 5 figures"},{"id":"http://arxiv.org/abs/2303.15223v1","updated":"2023-03-27T14:02:43Z","published":"2023-03-27T14:02:43Z","title":"How far generated data can impact Neural Networks performance?","summary":"  The success of deep learning models depends on the size and quality of the\ndataset to solve certain tasks. Here, we explore how far generated data can aid\nreal data in improving the performance of Neural Networks. In this work, we\nconsider facial expression recognition since it requires challenging local data\ngeneration at the level of local regions such as mouth, eyebrows, etc, rather\nthan simple augmentation. Generative Adversarial Networks (GANs) provide an\nalternative method for generating such local deformations but they need further\nvalidation. To answer our question, we consider noncomplex Convolutional Neural\nNetworks (CNNs) based classifiers for recognizing Ekman emotions. For the data\ngeneration process, we consider generating facial expressions (FEs) by relying\non two GANs. The first generates a random identity while the second imposes\nfacial deformations on top of it. We consider training the CNN classifier using\nFEs from: real-faces, GANs-generated, and finally using a combination of real\nand GAN-generated faces. We determine an upper bound regarding the data\ngeneration quantity to be mixed with the real one which contributes the most to\nenhancing FER accuracy. In our experiments, we find out that 5-times more\nsynthetic data to the real FEs dataset increases accuracy by 16%.\n","authors":["Sayeh Gholipour Picha","Dawood AL Chanti","Alice Caplier"],"pdf_url":"https://arxiv.org/pdf/2303.15223v1.pdf","comment":"Conference Publication in Proceedings of the 18th International Joint\n  Conference on Computer Vision, Imaging and Computer Graphics Theory and\n  Applications - Volume 5: VISAPP, 10 pages"},{"id":"http://arxiv.org/abs/2303.15219v1","updated":"2023-03-27T13:59:26Z","published":"2023-03-27T13:59:26Z","title":"Knowing the Distance: Understanding the Gap Between Synthetic and Real\n  Data For Face Parsing","summary":"  The use of synthetic data for training computer vision algorithms has become\nincreasingly popular due to its cost-effectiveness, scalability, and ability to\nprovide accurate multi-modality labels. Although recent studies have\ndemonstrated impressive results when training networks solely on synthetic\ndata, there remains a performance gap between synthetic and real data that is\ncommonly attributed to lack of photorealism. The aim of this study is to\ninvestigate the gap in greater detail for the face parsing task. We\ndifferentiate between three types of gaps: distribution gap, label gap, and\nphotorealism gap. Our findings show that the distribution gap is the largest\ncontributor to the performance gap, accounting for over 50% of the gap. By\naddressing this gap and accounting for the labels gap, we demonstrate that a\nmodel trained on synthetic data achieves comparable results to one trained on a\nsimilar amount of real data. This suggests that synthetic data is a viable\nalternative to real data, especially when real data is limited or difficult to\nobtain. Our study highlights the importance of content diversity in synthetic\ndatasets and challenges the notion that the photorealism gap is the most\ncritical factor affecting the performance of computer vision models trained on\nsynthetic data.\n","authors":["Eli Friedman","Assaf Lehr","Alexey Gruzdev","Vladimir Loginov","Max Kogan","Moran Rubin","Orly Zvitia"],"pdf_url":"https://arxiv.org/pdf/2303.15219v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.15214v1","updated":"2023-03-27T13:55:07Z","published":"2023-03-27T13:55:07Z","title":"CLIDiM: Contrastive Learning for Image Denoising in Microscopy","summary":"  Microscopy images often suffer from high levels of noise, which can hinder\nfurther analysis and interpretation. Content-aware image restoration (CARE)\nmethods have been proposed to address this issue, but they often require large\namounts of training data and suffer from over-fitting. To overcome these\nchallenges, we propose a novel framework for few-shot microscopy image\ndenoising. Our approach combines a generative adversarial network (GAN) trained\nvia contrastive learning (CL) with two structure preserving loss terms\n(Structural Similarity Index and Total Variation loss) to further improve the\nquality of the denoised images using little data. We demonstrate the\neffectiveness of our method on three well-known microscopy imaging datasets,\nand show that we can drastically reduce the amount of training data while\nretaining the quality of the denoising, thus alleviating the burden of\nacquiring paired data and enabling few-shot learning. The proposed framework\ncan be easily extended to other image restoration tasks and has the potential\nto significantly advance the field of microscopy image analysis.\n","authors":["Felix Fuentes-Hurtado","Jean-Baptiste Sibarita","Virgile Viasnoff"],"pdf_url":"https://arxiv.org/pdf/2303.15214v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.11726v2","updated":"2023-03-27T13:19:57Z","published":"2023-03-21T10:30:43Z","title":"3D Human Mesh Estimation from Virtual Markers","summary":"  Inspired by the success of volumetric 3D pose estimation, some recent human\nmesh estimators propose to estimate 3D skeletons as intermediate\nrepresentations, from which, the dense 3D meshes are regressed by exploiting\nthe mesh topology. However, body shape information is lost in extracting\nskeletons, leading to mediocre performance. The advanced motion capture systems\nsolve the problem by placing dense physical markers on the body surface, which\nallows to extract realistic meshes from their non-rigid motions. However, they\ncannot be applied to wild images without markers. In this work, we present an\nintermediate representation, named virtual markers, which learns 64 landmark\nkeypoints on the body surface based on the large-scale mocap data in a\ngenerative style, mimicking the effects of physical markers. The virtual\nmarkers can be accurately detected from wild images and can reconstruct the\nintact meshes with realistic shapes by simple interpolation. Our approach\noutperforms the state-of-the-art methods on three datasets. In particular, it\nsurpasses the existing methods by a notable margin on the SURREAL dataset,\nwhich has diverse body shapes. Code is available at\nhttps://github.com/ShirleyMaxx/VirtualMarker.\n","authors":["Xiaoxuan Ma","Jiajun Su","Chunyu Wang","Wentao Zhu","Yizhou Wang"],"pdf_url":"https://arxiv.org/pdf/2303.11726v2.pdf","comment":"CVPR 2023"},{"id":"http://arxiv.org/abs/2212.05973v2","updated":"2023-03-27T13:17:48Z","published":"2022-12-12T15:29:46Z","title":"Towards Practical Plug-and-Play Diffusion Models","summary":"  Diffusion-based generative models have achieved remarkable success in image\ngeneration. Their guidance formulation allows an external model to\nplug-and-play control the generation process for various tasks without\nfinetuning the diffusion model. However, the direct use of publicly available\noff-the-shelf models for guidance fails due to their poor performance on noisy\ninputs. For that, the existing practice is to fine-tune the guidance models\nwith labeled data corrupted with noises. In this paper, we argue that this\npractice has limitations in two aspects: (1) performing on inputs with\nextremely various noises is too hard for a single guidance model; (2)\ncollecting labeled datasets hinders scaling up for various tasks. To tackle the\nlimitations, we propose a novel strategy that leverages multiple experts where\neach expert is specialized in a particular noise range and guides the reverse\nprocess of the diffusion at its corresponding timesteps. However, as it is\ninfeasible to manage multiple networks and utilize labeled data, we present a\npractical guidance framework termed Practical Plug-And-Play (PPAP), which\nleverages parameter-efficient fine-tuning and data-free knowledge transfer. We\nexhaustively conduct ImageNet class conditional generation experiments to show\nthat our method can successfully guide diffusion with small trainable\nparameters and no labeled data. Finally, we show that image classifiers, depth\nestimators, and semantic segmentation models can guide publicly available GLIDE\nthrough our framework in a plug-and-play manner. Our code is available at\nhttps://github.com/riiid/PPAP.\n","authors":["Hyojun Go","Yunsung Lee","Jin-Young Kim","Seunghyun Lee","Myeongho Jeong","Hyun Seung Lee","Seungtaek Choi"],"pdf_url":"https://arxiv.org/pdf/2212.05973v2.pdf","comment":"CVPR 2023 camera-ready"},{"id":"http://arxiv.org/abs/2303.15167v1","updated":"2023-03-27T12:59:33Z","published":"2023-03-27T12:59:33Z","title":"Prompt-Guided Zero-Shot Anomaly Action Recognition using Pretrained Deep\n  Skeleton Features","summary":"  This study investigates unsupervised anomaly action recognition, which\nidentifies video-level abnormal-human-behavior events in an unsupervised manner\nwithout abnormal samples, and simultaneously addresses three limitations in the\nconventional skeleton-based approaches: target domain-dependent DNN training,\nrobustness against skeleton errors, and a lack of normal samples. We present a\nunified, user prompt-guided zero-shot learning framework using a target\ndomain-independent skeleton feature extractor, which is pretrained on a\nlarge-scale action recognition dataset. Particularly, during the training phase\nusing normal samples, the method models the distribution of skeleton features\nof the normal actions while freezing the weights of the DNNs and estimates the\nanomaly score using this distribution in the inference phase. Additionally, to\nincrease robustness against skeleton errors, we introduce a DNN architecture\ninspired by a point cloud deep learning paradigm, which sparsely propagates the\nfeatures between joints. Furthermore, to prevent the unobserved normal actions\nfrom being misidentified as abnormal actions, we incorporate a similarity score\nbetween the user prompt embeddings and skeleton features aligned in the common\nspace into the anomaly score, which indirectly supplements normal actions. On\ntwo publicly available datasets, we conduct experiments to test the\neffectiveness of the proposed method with respect to abovementioned\nlimitations.\n","authors":["Fumiaki Sato","Ryo Hachiuma","Taiki Sekii"],"pdf_url":"https://arxiv.org/pdf/2303.15167v1.pdf","comment":"CVPR 2023"},{"id":"http://arxiv.org/abs/2303.15166v1","updated":"2023-03-27T12:59:15Z","published":"2023-03-27T12:59:15Z","title":"Towards Artistic Image Aesthetics Assessment: a Large-scale Dataset and\n  a New Method","summary":"  Image aesthetics assessment (IAA) is a challenging task due to its highly\nsubjective nature. Most of the current studies rely on large-scale datasets\n(e.g., AVA and AADB) to learn a general model for all kinds of photography\nimages. However, little light has been shed on measuring the aesthetic quality\nof artistic images, and the existing datasets only contain relatively few\nartworks. Such a defect is a great obstacle to the aesthetic assessment of\nartistic images. To fill the gap in the field of artistic image aesthetics\nassessment (AIAA), we first introduce a large-scale AIAA dataset: Boldbrush\nArtistic Image Dataset (BAID), which consists of 60,337 artistic images\ncovering various art forms, with more than 360,000 votes from online users. We\nthen propose a new method, SAAN (Style-specific Art Assessment Network), which\ncan effectively extract and utilize style-specific and generic aesthetic\ninformation to evaluate artistic images. Experiments demonstrate that our\nproposed approach outperforms existing IAA methods on the proposed BAID dataset\naccording to quantitative comparisons. We believe the proposed dataset and\nmethod can serve as a foundation for future AIAA works and inspire more\nresearch in this field. Dataset and code are available at:\nhttps://github.com/Dreemurr-T/BAID.git\n","authors":["Ran Yi","Haoyuan Tian","Zhihao Gu","Yu-Kun Lai","Paul L. Rosin"],"pdf_url":"https://arxiv.org/pdf/2303.15166v1.pdf","comment":"Accepted by CVPR 2023"},{"id":"http://arxiv.org/abs/2211.04188v2","updated":"2023-03-27T12:54:49Z","published":"2022-11-08T12:01:31Z","title":"DepthFormer: Multimodal Positional Encodings and Cross-Input Attention\n  for Transformer-Based Segmentation Networks","summary":"  Most approaches for semantic segmentation use only information from color\ncameras to parse the scenes, yet recent advancements show that using depth data\nallows to further improve performances. In this work, we focus on\ntransformer-based deep learning architectures, that have achieved\nstate-of-the-art performances on the segmentation task, and we propose to\nemploy depth information by embedding it in the positional encoding.\nEffectively, we extend the network to multimodal data without adding any\nparameters and in a natural way that makes use of the strength of transformers'\nself-attention modules. We also investigate the idea of performing\ncross-modality operations inside the attention module, swapping the key inputs\nbetween the depth and color branches. Our approach consistently improves\nperformances on the Cityscapes benchmark.\n","authors":["Francesco Barbato","Giulia Rizzoli","Pietro Zanuttigh"],"pdf_url":"https://arxiv.org/pdf/2211.04188v2.pdf","comment":"Accepted at ICASSP 2023"},{"id":"http://arxiv.org/abs/2303.10644v2","updated":"2023-03-27T12:52:59Z","published":"2023-03-19T12:28:59Z","title":"Spatio-Temporal AU Relational Graph Representation Learning For Facial\n  Action Units Detection","summary":"  This paper presents our Facial Action Units (AUs) recognition submission to\nthe fifth Affective Behavior Analysis in-the-wild Competition (ABAW). Our\napproach consists of three main modules: (i) a pre-trained facial\nrepresentation encoder which produce a strong facial representation from each\ninput face image in the input sequence; (ii) an AU-specific feature generator\nthat specifically learns a set of AU features from each facial representation;\nand (iii) a spatio-temporal graph learning module that constructs a\nspatio-temporal graph representation. This graph representation describes AUs\ncontained in all frames and predicts the occurrence of each AU based on both\nthe modeled spatial information within the corresponding face and the learned\ntemporal dynamics among frames. The experimental results show that our approach\noutperformed the baseline and the spatio-temporal graph representation learning\nallows our model to generate the best results among all ablated systems. Our\nmodel ranks at the 4th place in the AU recognition track at the 5th ABAW\nCompetition.\n","authors":["Zihan Wang","Siyang Song","Cheng Luo","Yuzhi Zhou","Shiling Wu","Weicheng Xie","Linlin Shen"],"pdf_url":"https://arxiv.org/pdf/2303.10644v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.15149v1","updated":"2023-03-27T12:33:23Z","published":"2023-03-27T12:33:23Z","title":"What Can Human Sketches Do for Object Detection?","summary":"  Sketches are highly expressive, inherently capturing subjective and\nfine-grained visual cues. The exploration of such innate properties of human\nsketches has, however, been limited to that of image retrieval. In this paper,\nfor the first time, we cultivate the expressiveness of sketches but for the\nfundamental vision task of object detection. The end result is a sketch-enabled\nobject detection framework that detects based on what \\textit{you} sketch --\n\\textit{that} ``zebra'' (e.g., one that is eating the grass) in a herd of\nzebras (instance-aware detection), and only the \\textit{part} (e.g., ``head\" of\na ``zebra\") that you desire (part-aware detection). We further dictate that our\nmodel works without (i) knowing which category to expect at testing (zero-shot)\nand (ii) not requiring additional bounding boxes (as per fully supervised) and\nclass labels (as per weakly supervised). Instead of devising a model from the\nground up, we show an intuitive synergy between foundation models (e.g., CLIP)\nand existing sketch models build for sketch-based image retrieval (SBIR), which\ncan already elegantly solve the task -- CLIP to provide model generalisation,\nand SBIR to bridge the (sketch$\\rightarrow$photo) gap. In particular, we first\nperform independent prompting on both sketch and photo branches of an SBIR\nmodel to build highly generalisable sketch and photo encoders on the back of\nthe generalisation ability of CLIP. We then devise a training paradigm to adapt\nthe learned encoders for object detection, such that the region embeddings of\ndetected boxes are aligned with the sketch and photo embeddings from SBIR.\nEvaluating our framework on standard object detection datasets like PASCAL-VOC\nand MS-COCO outperforms both supervised (SOD) and weakly-supervised object\ndetectors (WSOD) on zero-shot setups. Project Page:\n\\url{https://pinakinathc.github.io/sketch-detect}\n","authors":["Pinaki Nath Chowdhury","Ayan Kumar Bhunia","Aneeshan Sain","Subhadeep Koley","Tao Xiang","Yi-Zhe Song"],"pdf_url":"https://arxiv.org/pdf/2303.15149v1.pdf","comment":"Accepted as Top 12 Best Papers. Will be presented in special\n  single-track plenary sessions to all attendees in Computer Vision and Pattern\n  Recognition (CVPR), 2023. Project Page: www.pinakinathc.me/sketch-detect"},{"id":"http://arxiv.org/abs/2303.15147v1","updated":"2023-03-27T12:32:49Z","published":"2023-03-27T12:32:49Z","title":"Pushing the Envelope for Depth-Based Semi-Supervised 3D Hand Pose\n  Estimation with Consistency Training","summary":"  Despite the significant progress that depth-based 3D hand pose estimation\nmethods have made in recent years, they still require a large amount of labeled\ntraining data to achieve high accuracy. However, collecting such data is both\ncostly and time-consuming. To tackle this issue, we propose a semi-supervised\nmethod to significantly reduce the dependence on labeled training data. The\nproposed method consists of two identical networks trained jointly: a teacher\nnetwork and a student network. The teacher network is trained using both the\navailable labeled and unlabeled samples. It leverages the unlabeled samples via\na loss formulation that encourages estimation equivariance under a set of\naffine transformations. The student network is trained using the unlabeled\nsamples with their pseudo-labels provided by the teacher network. For inference\nat test time, only the student network is used. Extensive experiments\ndemonstrate that the proposed method outperforms the state-of-the-art\nsemi-supervised methods by large margins.\n","authors":["Mohammad Rezaei","Farnaz Farahanipad","Alex Dillhoff","Vassilis Athitsos"],"pdf_url":"https://arxiv.org/pdf/2303.15147v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.15140v1","updated":"2023-03-27T12:18:52Z","published":"2023-03-27T12:18:52Z","title":"SimpleNet: A Simple Network for Image Anomaly Detection and Localization","summary":"  We propose a simple and application-friendly network (called SimpleNet) for\ndetecting and localizing anomalies. SimpleNet consists of four components: (1)\na pre-trained Feature Extractor that generates local features, (2) a shallow\nFeature Adapter that transfo local features towards target domain, (3) a simple\nAnomaly Feature Generator that counterfeits anomaly features by adding Gaussian\nnoise to normal features, and (4) a binary Anomaly Discriminator that\ndistinguishes anomaly features from normal features. During inference, the\nAnomaly Feature Generator would be discarded. Our approach is based on three\nintuitions. First, transforming pre-trained features to target-oriented\nfeatures helps avoid domain bias. Second, generating synthetic anomalies in\nfeature space is more effective, as defects may not have much commonality in\nthe image space. Third, a simple discriminator is much efficient and practical.\nIn spite of simplicity, SimpleNet outperforms previous methods quantitatively\nand qualitatively. On the MVTec AD benchmark, SimpleNet achieves an anomaly\ndetection AUROC of 99.6%, reducing the error by 55.5% compared to the next best\nperforming model. Furthermore, SimpleNet is faster than existing methods, with\na high frame rate of 77 FPS on a 3080ti GPU. Additionally, SimpleNet\ndemonstrates significant improvements in performance on the One-Class Novelty\nDetection task. Code: https://github.com/DonaldRR/SimpleNet.\n","authors":["Zhikang Liu","Yiming Zhou","Yuansheng Xu","Zilei Wang"],"pdf_url":"https://arxiv.org/pdf/2303.15140v1.pdf","comment":"Accepted to CVPR 2023"},{"id":"http://arxiv.org/abs/2303.15127v1","updated":"2023-03-27T12:00:54Z","published":"2023-03-27T12:00:54Z","title":"Learning the Unlearnable: Adversarial Augmentations Suppress Unlearnable\n  Example Attacks","summary":"  Unlearnable example attacks are data poisoning techniques that can be used to\nsafeguard public data against unauthorized use for training deep learning\nmodels. These methods add stealthy perturbations to the original image, thereby\nmaking it difficult for deep learning models to learn from these training data\neffectively. Current research suggests that adversarial training can, to a\ncertain degree, mitigate the impact of unlearnable example attacks, while\ncommon data augmentation methods are not effective against such poisons.\nAdversarial training, however, demands considerable computational resources and\ncan result in non-trivial accuracy loss. In this paper, we introduce the\nUEraser method, which outperforms current defenses against different types of\nstate-of-the-art unlearnable example attacks through a combination of effective\ndata augmentation policies and loss-maximizing adversarial augmentations. In\nstark contrast to the current SOTA adversarial training methods, UEraser uses\nadversarial augmentations, which extends beyond the confines of $ \\ell_p $\nperturbation budget assumed by current unlearning attacks and defenses. It also\nhelps to improve the model's generalization ability, thus protecting against\naccuracy loss. UEraser wipes out the unlearning effect with error-maximizing\ndata augmentations, thus restoring trained model accuracies. Interestingly,\nUEraser-Lite, a fast variant without adversarial augmentations, is also highly\neffective in preserving clean accuracies. On challenging unlearnable CIFAR-10,\nCIFAR-100, SVHN, and ImageNet-subset datasets produced with various attacks, it\nachieves results that are comparable to those obtained during clean training.\nWe also demonstrate its efficacy against possible adaptive attacks. Our code is\nopen source and available to the deep learning community:\nhttps://github.com/lafeat/ueraser.\n","authors":["Tianrui Qin","Xitong Gao","Juanjuan Zhao","Kejiang Ye","Cheng-Zhong Xu"],"pdf_url":"https://arxiv.org/pdf/2303.15127v1.pdf","comment":"UEraser introduces adversarial augmentations to suppress unlearnable\n  example attacks and outperforms current defenses"},{"id":"http://arxiv.org/abs/2303.15126v1","updated":"2023-03-27T11:58:52Z","published":"2023-03-27T11:58:52Z","title":"NeuralPCI: Spatio-temporal Neural Field for 3D Point Cloud Multi-frame\n  Non-linear Interpolation","summary":"  In recent years, there has been a significant increase in focus on the\ninterpolation task of computer vision. Despite the tremendous advancement of\nvideo interpolation, point cloud interpolation remains insufficiently explored.\nMeanwhile, the existence of numerous nonlinear large motions in real-world\nscenarios makes the point cloud interpolation task more challenging. In light\nof these issues, we present NeuralPCI: an end-to-end 4D spatio-temporal Neural\nfield for 3D Point Cloud Interpolation, which implicitly integrates multi-frame\ninformation to handle nonlinear large motions for both indoor and outdoor\nscenarios. Furthermore, we construct a new multi-frame point cloud\ninterpolation dataset called NL-Drive for large nonlinear motions in autonomous\ndriving scenes to better demonstrate the superiority of our method. Ultimately,\nNeuralPCI achieves state-of-the-art performance on both DHB (Dynamic Human\nBodies) and NL-Drive datasets. Beyond the interpolation task, our method can be\nnaturally extended to point cloud extrapolation, morphing, and auto-labeling,\nwhich indicates its substantial potential in other domains. Codes are available\nat https://github.com/ispc-lab/NeuralPCI.\n","authors":["Zehan Zheng","Danni Wu","Ruisi Lu","Fan Lu","Guang Chen","Changjun Jiang"],"pdf_url":"https://arxiv.org/pdf/2303.15126v1.pdf","comment":"Accepted by CVPR 2023. Project Page:\n  https://dyfcalid.github.io/NeuralPCI"},{"id":"http://arxiv.org/abs/2108.06932v6","updated":"2023-03-27T11:58:27Z","published":"2021-08-16T07:09:06Z","title":"Polyp-PVT: Polyp Segmentation with Pyramid Vision Transformers","summary":"  Most polyp segmentation methods use CNNs as their backbone, leading to two\nkey issues when exchanging information between the encoder and decoder: 1)\ntaking into account the differences in contribution between different-level\nfeatures and 2) designing an effective mechanism for fusing these features.\nUnlike existing CNN-based methods, we adopt a transformer encoder, which learns\nmore powerful and robust representations. In addition, considering the image\nacquisition influence and elusive properties of polyps, we introduce three\nstandard modules, including a cascaded fusion module (CFM), a camouflage\nidentification module (CIM), and a similarity aggregation module (SAM). Among\nthese, the CFM is used to collect the semantic and location information of\npolyps from high-level features; the CIM is applied to capture polyp\ninformation disguised in low-level features, and the SAM extends the pixel\nfeatures of the polyp area with high-level semantic position information to the\nentire polyp area, thereby effectively fusing cross-level features. The\nproposed model, named Polyp-PVT, effectively suppresses noises in the features\nand significantly improves their expressive capabilities. Extensive experiments\non five widely adopted datasets show that the proposed model is more robust to\nvarious challenging situations (\\emph{e.g.}, appearance changes, small objects,\nrotation) than existing representative methods. The proposed model is available\nat https://github.com/DengPingFan/Polyp-PVT.\n","authors":["Bo Dong","Wenhai Wang","Deng-Ping Fan","Jinpeng Li","Huazhu Fu","Ling Shao"],"pdf_url":"https://arxiv.org/pdf/2108.06932v6.pdf","comment":"Accepted to CAAI AIR2023"},{"id":"http://arxiv.org/abs/2209.08633v3","updated":"2023-03-27T11:56:44Z","published":"2022-09-18T19:12:43Z","title":"CNN based Intelligent Streetlight Management Using Smart CCTV Camera and\n  Semantic Segmentation","summary":"  One of the most neglected sources of energy loss is streetlights which\ngenerate too much light in areas where it is not required. Energy waste has\nenormous economic and environmental effects. In addition, due to the\nconventional manual nature of the operation, streetlights are frequently seen\nbeing turned ON during the day and OFF in the evening, which is regrettable\neven in the twenty-first century. These issues require automated streetlight\ncontrol in order to be resolved. This study aims to develop a novel streetlight\ncontrolling method by combining a smart transport monitoring system powered by\ncomputer vision technology with a closed circuit television (CCTV) camera that\nallows the light-emitting diode (LED) streetlight to automatically light up\nwith the appropriate brightness by detecting the presence of pedestrians or\nvehicles and dimming the streetlight in their absence using semantic image\nsegmentation from the CCTV video streaming. Consequently, our model\ndistinguishes daylight and nighttime, which made it feasible to automate the\nprocess of turning the streetlight 'ON' and 'OFF' to save energy consumption\ncosts. According to the aforementioned approach, geolocation sensor data could\nbe utilized to make more informed streetlight management decisions. To complete\nthe tasks, we consider training the U-net model with ResNet-34 as its backbone.\nThe validity of the models is guaranteed with the use of assessment matrices.\nThe suggested concept is straightforward, economical, energy-efficient,\nlong-lasting, and more resilient than conventional alternatives.\n","authors":["Md Sakib Ullah Sourav","Huidong Wang","Mohammad Raziuddin Chowdhury","Rejwan Bin Sulaiman"],"pdf_url":"https://arxiv.org/pdf/2209.08633v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.15124v1","updated":"2023-03-27T11:56:20Z","published":"2023-03-27T11:56:20Z","title":"Blind Inpainting with Object-aware Discrimination for Artificial Marker\n  Removal","summary":"  Medical images often contain artificial markers added by doctors, which can\nnegatively affect the accuracy of AI-based diagnosis. To address this issue and\nrecover the missing visual contents, inpainting techniques are highly needed.\nHowever, existing inpainting methods require manual mask input, limiting their\napplication scenarios. In this paper, we introduce a novel blind inpainting\nmethod that automatically completes visual contents without specifying masks\nfor target areas in an image. Our proposed model includes a mask-free\nreconstruction network and an object-aware discriminator. The reconstruction\nnetwork consists of two branches that predict the corrupted regions with\nartificial markers and simultaneously recover the missing visual contents. The\nobject-aware discriminator relies on the powerful recognition capabilities of\nthe dense object detector to ensure that the markers of reconstructed images\ncannot be detected in any local regions. As a result, the reconstructed image\ncan be close to the clean one as much as possible. Our proposed method is\nevaluated on different medical image datasets, covering multiple imaging\nmodalities such as ultrasound (US), magnetic resonance imaging (MRI), and\nelectron microscopy (EM), demonstrating that our method is effective and robust\nagainst various unknown missing region patterns.\n","authors":["Xuechen Guo","Wenhao Hu","Chiming Ni","Wenhao Chai","Shiyan Li","Gaoang Wang"],"pdf_url":"https://arxiv.org/pdf/2303.15124v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.15122v1","updated":"2023-03-27T11:50:27Z","published":"2023-03-27T11:50:27Z","title":"Parameter Efficient Local Implicit Image Function Network for Face\n  Segmentation","summary":"  Face parsing is defined as the per-pixel labeling of images containing human\nfaces. The labels are defined to identify key facial regions like eyes, lips,\nnose, hair, etc. In this work, we make use of the structural consistency of the\nhuman face to propose a lightweight face-parsing method using a Local Implicit\nFunction network, FP-LIIF. We propose a simple architecture having a\nconvolutional encoder and a pixel MLP decoder that uses 1/26th number of\nparameters compared to the state-of-the-art models and yet matches or\noutperforms state-of-the-art models on multiple datasets, like CelebAMask-HQ\nand LaPa. We do not use any pretraining, and compared to other works, our\nnetwork can also generate segmentation at different resolutions without any\nchanges in the input resolution. This work enables the use of facial\nsegmentation on low-compute or low-bandwidth devices because of its higher FPS\nand smaller model size.\n","authors":["Mausoom Sarkar","Nikitha SR","Mayur Hemani","Rishabh Jain","Balaji Krishnamurthy"],"pdf_url":"https://arxiv.org/pdf/2303.15122v1.pdf","comment":"Accepted at CVPR 2023"},{"id":"http://arxiv.org/abs/2303.09495v2","updated":"2023-03-27T11:42:13Z","published":"2023-03-16T17:15:25Z","title":"Among Us: Adversarially Robust Collaborative Perception by Consensus","summary":"  Multiple robots could perceive a scene (e.g., detect objects) collaboratively\nbetter than individuals, although easily suffer from adversarial attacks when\nusing deep learning. This could be addressed by the adversarial defense, but\nits training requires the often-unknown attacking mechanism. Differently, we\npropose ROBOSAC, a novel sampling-based defense strategy generalizable to\nunseen attackers. Our key idea is that collaborative perception should lead to\nconsensus rather than dissensus in results compared to individual perception.\nThis leads to our hypothesize-and-verify framework: perception results with and\nwithout collaboration from a random subset of teammates are compared until\nreaching a consensus. In such a framework, more teammates in the sampled subset\noften entail better perception performance but require longer sampling time to\nreject potential attackers. Thus, we derive how many sampling trials are needed\nto ensure the desired size of an attacker-free subset, or equivalently, the\nmaximum size of such a subset that we can successfully sample within a given\nnumber of trials. We validate our method on the task of collaborative 3D object\ndetection in autonomous driving scenarios.\n","authors":["Yiming Li","Qi Fang","Jiamu Bai","Siheng Chen","Felix Juefei-Xu","Chen Feng"],"pdf_url":"https://arxiv.org/pdf/2303.09495v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2301.01970v6","updated":"2023-03-27T11:37:27Z","published":"2023-01-05T09:11:16Z","title":"CAT: LoCalization and IdentificAtion Cascade Detection Transformer for\n  Open-World Object Detection","summary":"  Open-world object detection (OWOD), as a more general and challenging goal,\nrequires the model trained from data on known objects to detect both known and\nunknown objects and incrementally learn to identify these unknown objects. The\nexisting works which employ standard detection framework and fixed\npseudo-labelling mechanism (PLM) have the following problems: (i) The inclusion\nof detecting unknown objects substantially reduces the model's ability to\ndetect known ones. (ii) The PLM does not adequately utilize the priori\nknowledge of inputs. (iii) The fixed selection manner of PLM cannot guarantee\nthat the model is trained in the right direction. We observe that humans\nsubconsciously prefer to focus on all foreground objects and then identify each\none in detail, rather than localize and identify a single object\nsimultaneously, for alleviating the confusion. This motivates us to propose a\nnovel solution called CAT: LoCalization and IdentificAtion Cascade Detection\nTransformer which decouples the detection process via the shared decoder in the\ncascade decoding way. In the meanwhile, we propose the self-adaptive\npseudo-labelling mechanism which combines the model-driven with input-driven\nPLM and self-adaptively generates robust pseudo-labels for unknown objects,\nsignificantly improving the ability of CAT to retrieve unknown objects.\nComprehensive experiments on two benchmark datasets, i.e., MS-COCO and PASCAL\nVOC, show that our model outperforms the state-of-the-art in terms of all\nmetrics in the task of OWOD, incremental object detection (IOD) and open-set\ndetection.\n","authors":["Shuailei Ma","Yuefeng Wang","Jiaqi Fan","Ying Wei","Thomas H. Li","Hongli Liu","Fanbing Lv"],"pdf_url":"https://arxiv.org/pdf/2301.01970v6.pdf","comment":"CVPR 2023 camera-ready version"},{"id":"http://arxiv.org/abs/2303.13391v2","updated":"2023-03-27T11:36:12Z","published":"2023-03-23T16:07:31Z","title":"Xplainer: From X-Ray Observations to Explainable Zero-Shot Diagnosis","summary":"  Automated diagnosis prediction from medical images is a valuable resource to\nsupport clinical decision-making. However, such systems usually need to be\ntrained on large amounts of annotated data, which often is scarce in the\nmedical domain. Zero-shot methods address this challenge by allowing a flexible\nadaption to new settings with different clinical findings without relying on\nlabeled data. Further, to integrate automated diagnosis in the clinical\nworkflow, methods should be transparent and explainable, increasing medical\nprofessionals' trust and facilitating correctness verification. In this work,\nwe introduce Xplainer, a novel framework for explainable zero-shot diagnosis in\nthe clinical setting. Xplainer adapts the classification-by-description\napproach of contrastive vision-language models to the multi-label medical\ndiagnosis task. Specifically, instead of directly predicting a diagnosis, we\nprompt the model to classify the existence of descriptive observations, which a\nradiologist would look for on an X-Ray scan, and use the descriptor\nprobabilities to estimate the likelihood of a diagnosis. Our model is\nexplainable by design, as the final diagnosis prediction is directly based on\nthe prediction of the underlying descriptors. We evaluate Xplainer on two chest\nX-ray datasets, CheXpert and ChestX-ray14, and demonstrate its effectiveness in\nimproving the performance and explainability of zero-shot diagnosis. Our\nresults suggest that Xplainer provides a more detailed understanding of the\ndecision-making process and can be a valuable tool for clinical diagnosis.\n","authors":["Chantal Pellegrini","Matthias Keicher","Ege Özsoy","Petra Jiraskova","Rickmer Braren","Nassir Navab"],"pdf_url":"https://arxiv.org/pdf/2303.13391v2.pdf","comment":"9 pages, 2 figures, 6 tables"},{"id":"http://arxiv.org/abs/2303.15114v1","updated":"2023-03-27T11:32:14Z","published":"2023-03-27T11:32:14Z","title":"Automatic breach detection during spine pedicle drilling based on\n  vibroacoustic sensing","summary":"  Pedicle drilling is a complex and critical spinal surgery task. Detecting\nbreach or penetration of the surgical tool to the cortical wall during\npilot-hole drilling is essential to avoid damage to vital anatomical structures\nadjacent to the pedicle, such as the spinal cord, blood vessels, and nerves.\nCurrently, the guidance of pedicle drilling is done using image-guided methods\nthat are radiation intensive and limited to the preoperative information. This\nwork proposes a new radiation-free breach detection algorithm leveraging a\nnon-visual sensor setup in combination with deep learning approach. Multiple\nvibroacoustic sensors, such as a contact microphone, a free-field microphone, a\ntri-axial accelerometer, a uni-axial accelerometer, and an optical tracking\nsystem were integrated into the setup. Data were collected on four cadaveric\nhuman spines, ranging from L5 to T10. An experienced spine surgeon drilled the\npedicles relying on optical navigation. A new automatic labeling method based\non the tracking data was introduced. Labeled data was subsequently fed to the\nnetwork in mel-spectrograms, classifying the data into breach and non-breach.\nDifferent sensor types, sensor positioning, and their combinations were\nevaluated. The best results in breach recall for individual sensors could be\nachieved using contact microphones attached to the dorsal skin (85.8\\%) and\nuni-axial accelerometers clamped to the spinous process of the drilled vertebra\n(81.0\\%). The best-performing data fusion model combined the latter two sensors\nwith a breach recall of 98\\%. The proposed method shows the great potential of\nnon-visual sensor fusion for avoiding screw misplacement and accidental bone\nbreaches during pedicle drilling and could be extended to further surgical\napplications.\n","authors":["Aidana Massalimova","Maikel Timmermans","Nicola Cavalcanti","Daniel Suter","Matthias Seibold","Fabio Carrillo","Christoph J. Laux","Reto Sutter","Mazda Farshad","Kathleen Denis","Philipp Fürnstahl"],"pdf_url":"https://arxiv.org/pdf/2303.15114v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.15111v1","updated":"2023-03-27T11:29:54Z","published":"2023-03-27T11:29:54Z","title":"Learning Attention as Disentangler for Compositional Zero-shot Learning","summary":"  Compositional zero-shot learning (CZSL) aims at learning visual concepts\n(i.e., attributes and objects) from seen compositions and combining concept\nknowledge into unseen compositions. The key to CZSL is learning the\ndisentanglement of the attribute-object composition. To this end, we propose to\nexploit cross-attentions as compositional disentanglers to learn disentangled\nconcept embeddings. For example, if we want to recognize an unseen composition\n\"yellow flower\", we can learn the attribute concept \"yellow\" and object concept\n\"flower\" from different yellow objects and different flowers respectively. To\nfurther constrain the disentanglers to learn the concept of interest, we employ\na regularization at the attention level. Specifically, we adapt the earth\nmover's distance (EMD) as a feature similarity metric in the cross-attention\nmodule. Moreover, benefiting from concept disentanglement, we improve the\ninference process and tune the prediction score by combining multiple concept\nprobabilities. Comprehensive experiments on three CZSL benchmark datasets\ndemonstrate that our method significantly outperforms previous works in both\nclosed- and open-world settings, establishing a new state-of-the-art.\n","authors":["Shaozhe Hao","Kai Han","Kwan-Yee K. Wong"],"pdf_url":"https://arxiv.org/pdf/2303.15111v1.pdf","comment":"CVPR 2023, available at https://haoosz.github.io/ade-czsl/"},{"id":"http://arxiv.org/abs/2303.15109v1","updated":"2023-03-27T11:26:34Z","published":"2023-03-27T11:26:34Z","title":"Improving the Transferability of Adversarial Examples via Direction\n  Tuning","summary":"  In the transfer-based adversarial attacks, adversarial examples are only\ngenerated by the surrogate models and achieve effective perturbation in the\nvictim models. Although considerable efforts have been developed on improving\nthe transferability of adversarial examples generated by transfer-based\nadversarial attacks, our investigation found that, the big deviation between\nthe actual and steepest update directions of the current transfer-based\nadversarial attacks is caused by the large update step length, resulting in the\ngenerated adversarial examples can not converge well. However, directly\nreducing the update step length will lead to serious update oscillation so that\nthe generated adversarial examples also can not achieve great transferability\nto the victim models. To address these issues, a novel transfer-based attack,\nnamely direction tuning attack, is proposed to not only decrease the update\ndeviation in the large step length, but also mitigate the update oscillation in\nthe small sampling step length, thereby making the generated adversarial\nexamples converge well to achieve great transferability on victim models. In\naddition, a network pruning method is proposed to smooth the decision boundary,\nthereby further decreasing the update oscillation and enhancing the\ntransferability of the generated adversarial examples. The experiment results\non ImageNet demonstrate that the average attack success rate (ASR) of the\nadversarial examples generated by our method can be improved from 87.9\\% to\n94.5\\% on five victim models without defenses, and from 69.1\\% to 76.2\\% on\neight advanced defense methods, in comparison with that of latest\ngradient-based attacks.\n","authors":["Xiangyuan Yang","Jie Lin","Hanlin Zhang","Xinyu Yang","Peng Zhao"],"pdf_url":"https://arxiv.org/pdf/2303.15109v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.15108v1","updated":"2023-03-27T11:22:54Z","published":"2023-03-27T11:22:54Z","title":"High-fidelity 3D Human Digitization from Single 2K Resolution Images","summary":"  High-quality 3D human body reconstruction requires high-fidelity and\nlarge-scale training data and appropriate network design that effectively\nexploits the high-resolution input images. To tackle these problems, we propose\na simple yet effective 3D human digitization method called 2K2K, which\nconstructs a large-scale 2K human dataset and infers 3D human models from 2K\nresolution images. The proposed method separately recovers the global shape of\na human and its details. The low-resolution depth network predicts the global\nstructure from a low-resolution image, and the part-wise image-to-normal\nnetwork predicts the details of the 3D human body structure. The\nhigh-resolution depth network merges the global 3D shape and the detailed\nstructures to infer the high-resolution front and back side depth maps.\nFinally, an off-the-shelf mesh generator reconstructs the full 3D human model,\nwhich are available at https://github.com/SangHunHan92/2K2K. In addition, we\nalso provide 2,050 3D human models, including texture maps, 3D joints, and SMPL\nparameters for research purposes. In experiments, we demonstrate competitive\nperformance over the recent works on various datasets.\n","authors":["Sang-Hun Han","Min-Gyu Park","Ju Hong Yoon","Ju-Mi Kang","Young-Jae Park","Hae-Gon Jeon"],"pdf_url":"https://arxiv.org/pdf/2303.15108v1.pdf","comment":"code page : https://github.com/SangHunHan92/2K2K, Accepted to CVPR\n  2023 (Highlight)"},{"id":"http://arxiv.org/abs/2212.02802v2","updated":"2023-03-27T11:15:59Z","published":"2022-12-06T07:41:51Z","title":"Diffusion Video Autoencoders: Toward Temporally Consistent Face Video\n  Editing via Disentangled Video Encoding","summary":"  Inspired by the impressive performance of recent face image editing methods,\nseveral studies have been naturally proposed to extend these methods to the\nface video editing task. One of the main challenges here is temporal\nconsistency among edited frames, which is still unresolved. To this end, we\npropose a novel face video editing framework based on diffusion autoencoders\nthat can successfully extract the decomposed features - for the first time as a\nface video editing model - of identity and motion from a given video. This\nmodeling allows us to edit the video by simply manipulating the temporally\ninvariant feature to the desired direction for the consistency. Another unique\nstrength of our model is that, since our model is based on diffusion models, it\ncan satisfy both reconstruction and edit capabilities at the same time, and is\nrobust to corner cases in wild face videos (e.g. occluded faces) unlike the\nexisting GAN-based methods.\n","authors":["Gyeongman Kim","Hajin Shim","Hyunsu Kim","Yunjey Choi","Junho Kim","Eunho Yang"],"pdf_url":"https://arxiv.org/pdf/2212.02802v2.pdf","comment":"CVPR 2023. Our project page: https://diff-video-ae.github.io"},{"id":"http://arxiv.org/abs/2303.15105v1","updated":"2023-03-27T11:13:50Z","published":"2023-03-27T11:13:50Z","title":"Vision Transformer with Quadrangle Attention","summary":"  Window-based attention has become a popular choice in vision transformers due\nto its superior performance, lower computational complexity, and less memory\nfootprint. However, the design of hand-crafted windows, which is data-agnostic,\nconstrains the flexibility of transformers to adapt to objects of varying\nsizes, shapes, and orientations. To address this issue, we propose a novel\nquadrangle attention (QA) method that extends the window-based attention to a\ngeneral quadrangle formulation. Our method employs an end-to-end learnable\nquadrangle regression module that predicts a transformation matrix to transform\ndefault windows into target quadrangles for token sampling and attention\ncalculation, enabling the network to model various targets with different\nshapes and orientations and capture rich context information. We integrate QA\ninto plain and hierarchical vision transformers to create a new architecture\nnamed QFormer, which offers minor code modifications and negligible extra\ncomputational cost. Extensive experiments on public benchmarks demonstrate that\nQFormer outperforms existing representative vision transformers on various\nvision tasks, including classification, object detection, semantic\nsegmentation, and pose estimation. The code will be made publicly available at\n\\href{https://github.com/ViTAE-Transformer/QFormer}{QFormer}.\n","authors":["Qiming Zhang","Jing Zhang","Yufei Xu","Dacheng Tao"],"pdf_url":"https://arxiv.org/pdf/2303.15105v1.pdf","comment":"15 pages, the extension of the ECCV 2022 paper (VSA: Learning\n  Varied-Size Window Attention in Vision Transformers)"},{"id":"http://arxiv.org/abs/2303.15104v1","updated":"2023-03-27T11:13:46Z","published":"2023-03-27T11:13:46Z","title":"Generalizable Local Feature Pre-training for Deformable Shape Analysis","summary":"  Transfer learning is fundamental for addressing problems in settings with\nlittle training data. While several transfer learning approaches have been\nproposed in 3D, unfortunately, these solutions typically operate on an entire\n3D object or even scene-level and thus, as we show, fail to generalize to new\nclasses, such as deformable organic shapes. In addition, there is currently a\nlack of understanding of what makes pre-trained features transferable across\nsignificantly different 3D shape categories. In this paper, we make a step\ntoward addressing these challenges. First, we analyze the link between feature\nlocality and transferability in tasks involving deformable 3D objects, while\nalso comparing different backbones and losses for local feature pre-training.\nWe observe that with proper training, learned features can be useful in such\ntasks, but, crucially, only with an appropriate choice of the receptive field\nsize. We then propose a differentiable method for optimizing the receptive\nfield within 3D transfer learning. Jointly, this leads to the first learnable\nfeatures that can successfully generalize to unseen classes of 3D shapes such\nas humans and animals. Our extensive experiments show that this approach leads\nto state-of-the-art results on several downstream tasks such as segmentation,\nshape correspondence, and classification. Our code is available at\n\\url{https://github.com/pvnieo/vader}.\n","authors":["Souhaib Attaiki","Lei Li","Maks Ovsjanikov"],"pdf_url":"https://arxiv.org/pdf/2303.15104v1.pdf","comment":"16 pages, 14 figures, 7 tables, to be published in The IEEE\n  Conference on Computer Vision and Pattern Recognition (CVPR)"},{"id":"http://arxiv.org/abs/2303.15103v1","updated":"2023-03-27T11:13:35Z","published":"2023-03-27T11:13:35Z","title":"Contrastive Learning Is Spectral Clustering On Similarity Graph","summary":"  Contrastive learning is a powerful self-supervised learning method, but we\nhave a limited theoretical understanding of how it works and why it works. In\nthis paper, we prove that contrastive learning with the standard InfoNCE loss\nis equivalent to spectral clustering on the similarity graph. Using this\nequivalence as the building block, we extend our analysis to the CLIP model and\nrigorously characterize how similar multi-modal objects are embedded together.\nMotivated by our theoretical insights, we introduce the kernel mixture loss,\nincorporating novel kernel functions that outperform the standard Gaussian\nkernel on several vision datasets.\n","authors":["Zhiquan Tan","Yifan Zhang","Jingqin Yang","Yang Yuan"],"pdf_url":"https://arxiv.org/pdf/2303.15103v1.pdf","comment":"We express our gratitude to the anonymous reviewers for their\n  valuable feedback"},{"id":"http://arxiv.org/abs/2303.15101v1","updated":"2023-03-27T11:10:54Z","published":"2023-03-27T11:10:54Z","title":"DANI-Net: Uncalibrated Photometric Stereo by Differentiable Shadow\n  Handling, Anisotropic Reflectance Modeling, and Neural Inverse Rendering","summary":"  Uncalibrated photometric stereo (UPS) is challenging due to the inherent\nambiguity brought by the unknown light. Although the ambiguity is alleviated on\nnon-Lambertian objects, the problem is still difficult to solve for more\ngeneral objects with complex shapes introducing irregular shadows and general\nmaterials with complex reflectance like anisotropic reflectance. To exploit\ncues from shadow and reflectance to solve UPS and improve performance on\ngeneral materials, we propose DANI-Net, an inverse rendering framework with\ndifferentiable shadow handling and anisotropic reflectance modeling. Unlike\nmost previous methods that use non-differentiable shadow maps and assume\nisotropic material, our network benefits from cues of shadow and anisotropic\nreflectance through two differentiable paths. Experiments on multiple\nreal-world datasets demonstrate our superior and robust performance.\n","authors":["Zongrui Li","Qian Zheng","Boxin Shi","Gang Pan","Xudong Jiang"],"pdf_url":"https://arxiv.org/pdf/2303.15101v1.pdf","comment":"Accepted by CVPR 2023"},{"id":"http://arxiv.org/abs/2303.15092v1","updated":"2023-03-27T11:01:16Z","published":"2023-03-27T11:01:16Z","title":"Defect detection using weakly supervised learning","summary":"  In many real-world scenarios, obtaining large amounts of labeled data can be\na daunting task. Weakly supervised learning techniques have gained significant\nattention in recent years as an alternative to traditional supervised learning,\nas they enable training models using only a limited amount of labeled data. In\nthis paper, the performance of a weakly supervised classifier to its fully\nsupervised counterpart is compared on the task of defect detection. Experiments\nare conducted on a dataset of images containing defects, and evaluate the two\nclassifiers based on their accuracy, precision, and recall. Our results show\nthat the weakly supervised classifier achieves comparable performance to the\nsupervised classifier, while requiring significantly less labeled data.\n","authors":["Vasileios Sevetlidis","George Pavlidis","Vasiliki Balaska","Athanasios Psomoulis","Spyridon Mouroutsos","Antonios Gasteratos"],"pdf_url":"https://arxiv.org/pdf/2303.15092v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2302.08175v6","updated":"2023-03-27T10:59:42Z","published":"2023-02-16T09:44:55Z","title":"A numerical approximation method for the Fisher-Rao distance between\n  multivariate normal distributions","summary":"  We present a simple method to approximate Rao's distance between multivariate\nnormal distributions based on discretizing curves joining normal distributions\nand approximating Rao's distances between successive nearby normal\ndistributions on the curves by the square root of Jeffreys divergence, the\nsymmetrized Kullback-Leibler divergence. We consider experimentally the linear\ninterpolation curves in the ordinary, natural and expectation parameterizations\nof the normal distributions, and compare these curves with a curve derived from\nthe Calvo and Oller's isometric embedding of the Fisher-Rao $d$-variate normal\nmanifold into the cone of $(d+1)\\times (d+1)$ symmetric positive-definite\nmatrices [Journal of multivariate analysis 35.2 (1990): 223-242]. We report on\nour experiments and assess the quality of our approximation technique by\ncomparing the numerical approximations with both lower and upper bounds.\nFinally, we present several information-geometric properties of the Calvo and\nOller's isometric embedding.\n","authors":["Frank Nielsen"],"pdf_url":"https://arxiv.org/pdf/2302.08175v6.pdf","comment":"46 pages, 19 figures, 3 tables"},{"id":"http://arxiv.org/abs/2303.15086v1","updated":"2023-03-27T10:53:38Z","published":"2023-03-27T10:53:38Z","title":"Learning Action Changes by Measuring Verb-Adverb Textual Relationships","summary":"  The goal of this work is to understand the way actions are performed in\nvideos. That is, given a video, we aim to predict an adverb indicating a\nmodification applied to the action (e.g. cut \"finely\"). We cast this problem as\na regression task. We measure textual relationships between verbs and adverbs\nto generate a regression target representing the action change we aim to learn.\nWe test our approach on a range of datasets and achieve state-of-the-art\nresults on both adverb prediction and antonym classification. Furthermore, we\noutperform previous work when we lift two commonly assumed conditions: the\navailability of action labels during testing and the pairing of adverbs as\nantonyms. Existing datasets for adverb recognition are either noisy, which\nmakes learning difficult, or contain actions whose appearance is not influenced\nby adverbs, which makes evaluation less reliable. To address this, we collect a\nnew high quality dataset: Adverbs in Recipes (AIR). We focus on instructional\nrecipes videos, curating a set of actions that exhibit meaningful visual\nchanges when performed differently. Videos in AIR are more tightly trimmed and\nwere manually reviewed by multiple annotators to ensure high labelling quality.\nResults show that models learn better from AIR given its cleaner videos. At the\nsame time, adverb prediction on AIR is challenging, demonstrating that there is\nconsiderable room for improvement.\n","authors":["Davide Moltisanti","Frank Keller","Hakan Bilen","Laura Sevilla-Lara"],"pdf_url":"https://arxiv.org/pdf/2303.15086v1.pdf","comment":"CVPR 23. Code and dataset available at\n  https://github.com/dmoltisanti/air-cvpr23"},{"id":"http://arxiv.org/abs/2303.15083v1","updated":"2023-03-27T10:50:58Z","published":"2023-03-27T10:50:58Z","title":"UniDistill: A Universal Cross-Modality Knowledge Distillation Framework\n  for 3D Object Detection in Bird's-Eye View","summary":"  In the field of 3D object detection for autonomous driving, the sensor\nportfolio including multi-modality and single-modality is diverse and complex.\nSince the multi-modal methods have system complexity while the accuracy of\nsingle-modal ones is relatively low, how to make a tradeoff between them is\ndifficult. In this work, we propose a universal cross-modality knowledge\ndistillation framework (UniDistill) to improve the performance of\nsingle-modality detectors. Specifically, during training, UniDistill projects\nthe features of both the teacher and the student detector into Bird's-Eye-View\n(BEV), which is a friendly representation for different modalities. Then, three\ndistillation losses are calculated to sparsely align the foreground features,\nhelping the student learn from the teacher without introducing additional cost\nduring inference. Taking advantage of the similar detection paradigm of\ndifferent detectors in BEV, UniDistill easily supports LiDAR-to-camera,\ncamera-to-LiDAR, fusion-to-LiDAR and fusion-to-camera distillation paths.\nFurthermore, the three distillation losses can filter the effect of misaligned\nbackground information and balance between objects of different sizes,\nimproving the distillation effectiveness. Extensive experiments on nuScenes\ndemonstrate that UniDistill effectively improves the mAP and NDS of student\ndetectors by 2.0%~3.2%.\n","authors":["Shengchao Zhou","Weizhou Liu","Chen Hu","Shuchang Zhou","Chao Ma"],"pdf_url":"https://arxiv.org/pdf/2303.15083v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.15081v1","updated":"2023-03-27T10:45:00Z","published":"2023-03-27T10:45:00Z","title":"Exemplar-based Video Colorization with Long-term Spatiotemporal\n  Dependency","summary":"  Exemplar-based video colorization is an essential technique for applications\nlike old movie restoration. Although recent methods perform well in still\nscenes or scenes with regular movement, they always lack robustness in moving\nscenes due to their weak ability in modeling long-term dependency both\nspatially and temporally, leading to color fading, color discontinuity or other\nartifacts. To solve this problem, we propose an exemplar-based video\ncolorization framework with long-term spatiotemporal dependency. To enhance the\nlong-term spatial dependency, a parallelized CNN-Transformer block and a double\nhead non-local operation are designed. The proposed CNN-Transformer block can\nbetter incorporate long-term spatial dependency with local texture and\nstructural features, and the double head non-local operation further leverages\nthe performance of augmented feature. While for long-term temporal dependency\nenhancement, we further introduce the novel linkage subnet. The linkage subnet\npropagate motion information across adjacent frame blocks and help to maintain\ntemporal continuity. Experiments demonstrate that our model outperforms recent\nstate-of-the-art methods both quantitatively and qualitatively. Also, our model\ncan generate more colorful, realistic and stabilized results, especially for\nscenes where objects change greatly and irregularly.\n","authors":["Siqi Chen","Xueming Li","Xianlin Zhang","Mingdao Wang","Yu Zhang","Jiatong Han","Yue Zhang"],"pdf_url":"https://arxiv.org/pdf/2303.15081v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2207.09644v3","updated":"2023-03-27T10:35:11Z","published":"2022-07-20T04:21:05Z","title":"Hierarchically Self-Supervised Transformer for Human Skeleton\n  Representation Learning","summary":"  Despite the success of fully-supervised human skeleton sequence modeling,\nutilizing self-supervised pre-training for skeleton sequence representation\nlearning has been an active field because acquiring task-specific skeleton\nannotations at large scales is difficult. Recent studies focus on learning\nvideo-level temporal and discriminative information using contrastive learning,\nbut overlook the hierarchical spatial-temporal nature of human skeletons.\nDifferent from such superficial supervision at the video level, we propose a\nself-supervised hierarchical pre-training scheme incorporated into a\nhierarchical Transformer-based skeleton sequence encoder (Hi-TRS), to\nexplicitly capture spatial, short-term, and long-term temporal dependencies at\nframe, clip, and video levels, respectively. To evaluate the proposed\nself-supervised pre-training scheme with Hi-TRS, we conduct extensive\nexperiments covering three skeleton-based downstream tasks including action\nrecognition, action detection, and motion prediction. Under both supervised and\nsemi-supervised evaluation protocols, our method achieves the state-of-the-art\nperformance. Additionally, we demonstrate that the prior knowledge learned by\nour model in the pre-training stage has strong transfer capability for\ndifferent downstream tasks.\n","authors":["Yuxiao Chen","Long Zhao","Jianbo Yuan","Yu Tian","Zhaoyang Xia","Shijie Geng","Ligong Han","Dimitris N. Metaxas"],"pdf_url":"https://arxiv.org/pdf/2207.09644v3.pdf","comment":"Accepted to ECCV 2022"},{"id":"http://arxiv.org/abs/2207.13080v2","updated":"2023-03-27T10:27:04Z","published":"2022-07-26T17:52:14Z","title":"DETRs with Hybrid Matching","summary":"  One-to-one set matching is a key design for DETR to establish its end-to-end\ncapability, so that object detection does not require a hand-crafted NMS\n(non-maximum suppression) to remove duplicate detections. This end-to-end\nsignature is important for the versatility of DETR, and it has been generalized\nto broader vision tasks. However, we note that there are few queries assigned\nas positive samples and the one-to-one set matching significantly reduces the\ntraining efficacy of positive samples. We propose a simple yet effective method\nbased on a hybrid matching scheme that combines the original one-to-one\nmatching branch with an auxiliary one-to-many matching branch during training.\nOur hybrid strategy has been shown to significantly improve accuracy. In\ninference, only the original one-to-one match branch is used, thus maintaining\nthe end-to-end merit and the same inference efficiency of DETR. The method is\nnamed H-DETR, and it shows that a wide range of representative DETR methods can\nbe consistently improved across a wide range of visual tasks, including\nDeformableDETR, PETRv2, PETR, and TransTrack, among others. The code is\navailable at: https://github.com/HDETR\n","authors":["Ding Jia","Yuhui Yuan","Haodi He","Xiaopei Wu","Haojun Yu","Weihong Lin","Lei Sun","Chao Zhang","Han Hu"],"pdf_url":"https://arxiv.org/pdf/2207.13080v2.pdf","comment":"CVPR 2023. The code is available at: https://github.com/HDETR"},{"id":"http://arxiv.org/abs/2303.15067v1","updated":"2023-03-27T10:20:34Z","published":"2023-03-27T10:20:34Z","title":"Intersection over Union with smoothing for bounding box regression","summary":"  We focus on the construction of a loss function for the bounding box\nregression. The Intersection over Union (IoU) metric is improved to converge\nfaster, to make the surface of the loss function smooth and continuous over the\nwhole searched space, and to reach a more precise approximation of the labels.\nThe main principle is adding a smoothing part to the original IoU, where the\nsmoothing part is given by a linear space with values that increases from the\nground truth bounding box to the border of the input image, and thus covers the\nwhole spatial search space. We show the motivation and formalism behind this\nloss function and experimentally prove that it outperforms IoU, DIoU, CIoU, and\nSIoU by a large margin. We experimentally show that the proposed loss function\nis robust with respect to the noise in the dimension of ground truth bounding\nboxes. The reference implementation is available at\ngitlab.com/irafm-ai/smoothing-iou.\n","authors":["Petra Števuliáková","Petr Hurtik"],"pdf_url":"https://arxiv.org/pdf/2303.15067v1.pdf","comment":"11 pages, 4 figures, 4 tables, IWANN2023 conference"},{"id":"http://arxiv.org/abs/2303.15065v1","updated":"2023-03-27T10:18:42Z","published":"2023-03-27T10:18:42Z","title":"Multi-contrast MRI Super-resolution via Implicit Neural Representations","summary":"  Clinical routine and retrospective cohorts commonly include multi-parametric\nMagnetic Resonance Imaging; however, they are mostly acquired in different\nanisotropic 2D views due to signal-to-noise-ratio and scan-time constraints.\nThus acquired views suffer from poor out-of-plane resolution and affect\ndownstream volumetric image analysis that typically requires isotropic 3D\nscans. Combining different views of multi-contrast scans into high-resolution\nisotropic 3D scans is challenging due to the lack of a large training cohort,\nwhich calls for a subject-specific framework.This work proposes a novel\nsolution to this problem leveraging Implicit Neural Representations (INR). Our\nproposed INR jointly learns two different contrasts of complementary views in a\ncontinuous spatial function and benefits from exchanging anatomical information\nbetween them. Trained within minutes on a single commodity GPU, our model\nprovides realistic super-resolution across different pairs of contrasts in our\nexperiments with three datasets. Using Mutual Information (MI) as a metric, we\nfind that our model converges to an optimum MI amongst sequences, achieving\nanatomically faithful reconstruction. Code is available at:\nhttps://github.com/jqmcginnis/multi_contrast_inr.\n","authors":["Julian McGinnis","Suprosanna Shit","Hongwei Bran Li","Vasiliki Sideri-Lampretsa","Robert Graf","Maik Dannecker","Jiazhen Pan","Nil Stolt Ansó","Mark Mühlau","Jan S. Kirschke","Daniel Rueckert","Benedikt Wiestler"],"pdf_url":"https://arxiv.org/pdf/2303.15065v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.11791v2","updated":"2023-03-27T10:11:31Z","published":"2023-03-21T12:18:57Z","title":"Propagate And Calibrate: Real-time Passive Non-line-of-sight Tracking","summary":"  Non-line-of-sight (NLOS) tracking has drawn increasing attention in recent\nyears, due to its ability to detect object motion out of sight. Most previous\nworks on NLOS tracking rely on active illumination, e.g., laser, and suffer\nfrom high cost and elaborate experimental conditions. Besides, these techniques\nare still far from practical application due to oversimplified settings. In\ncontrast, we propose a purely passive method to track a person walking in an\ninvisible room by only observing a relay wall, which is more in line with real\napplication scenarios, e.g., security. To excavate imperceptible changes in\nvideos of the relay wall, we introduce difference frames as an essential\ncarrier of temporal-local motion messages. In addition, we propose PAC-Net,\nwhich consists of alternating propagation and calibration, making it capable of\nleveraging both dynamic and static messages on a frame-level granularity. To\nevaluate the proposed method, we build and publish the first dynamic passive\nNLOS tracking dataset, NLOS-Track, which fills the vacuum of realistic NLOS\ndatasets. NLOS-Track contains thousands of NLOS video clips and corresponding\ntrajectories. Both real-shot and synthetic data are included. Our codes and\ndataset are available at https://againstentropy.github.io/NLOS-Track/.\n","authors":["Yihao Wang","Zhigang Wang","Bin Zhao","Dong Wang","Mulin Chen","Xuelong Li"],"pdf_url":"https://arxiv.org/pdf/2303.11791v2.pdf","comment":"CVPR 2023 camera-ready version. Codes and dataset are available at\n  https://againstentropy.github.io/NLOS-Track/"},{"id":"http://arxiv.org/abs/2303.15062v1","updated":"2023-03-27T10:11:22Z","published":"2023-03-27T10:11:22Z","title":"The Devil is in the Points: Weakly Semi-Supervised Instance Segmentation\n  via Point-Guided Mask Representation","summary":"  In this paper, we introduce a novel learning scheme named weakly\nsemi-supervised instance segmentation (WSSIS) with point labels for\nbudget-efficient and high-performance instance segmentation. Namely, we\nconsider a dataset setting consisting of a few fully-labeled images and a lot\nof point-labeled images. Motivated by the main challenge of semi-supervised\napproaches mainly derives from the trade-off between false-negative and\nfalse-positive instance proposals, we propose a method for WSSIS that can\neffectively leverage the budget-friendly point labels as a powerful weak\nsupervision source to resolve the challenge. Furthermore, to deal with the hard\ncase where the amount of fully-labeled data is extremely limited, we propose a\nMaskRefineNet that refines noise in rough masks. We conduct extensive\nexperiments on COCO and BDD100K datasets, and the proposed method achieves\npromising results comparable to those of the fully-supervised model, even with\n50% of the fully labeled COCO data (38.8% vs. 39.7%). Moreover, when using as\nlittle as 5% of fully labeled COCO data, our method shows significantly\nsuperior performance over the state-of-the-art semi-supervised learning method\n(33.7% vs. 24.9%). The code is available at\nhttps://github.com/clovaai/PointWSSIS.\n","authors":["Beomyoung Kim","Joonhyun Jeong","Dongyoon Han","Sung Ju Hwang"],"pdf_url":"https://arxiv.org/pdf/2303.15062v1.pdf","comment":"CVPR 2023"},{"id":"http://arxiv.org/abs/2303.15060v1","updated":"2023-03-27T10:07:52Z","published":"2023-03-27T10:07:52Z","title":"TMO: Textured Mesh Acquisition of Objects with a Mobile Device by using\n  Differentiable Rendering","summary":"  We present a new pipeline for acquiring a textured mesh in the wild with a\nsingle smartphone which offers access to images, depth maps, and valid poses.\nOur method first introduces an RGBD-aided structure from motion, which can\nyield filtered depth maps and refines camera poses guided by corresponding\ndepth. Then, we adopt the neural implicit surface reconstruction method, which\nallows for high-quality mesh and develops a new training process for applying a\nregularization provided by classical multi-view stereo methods. Moreover, we\napply a differentiable rendering to fine-tune incomplete texture maps and\ngenerate textures which are perceptually closer to the original scene. Our\npipeline can be applied to any common objects in the real world without the\nneed for either in-the-lab environments or accurate mask images. We demonstrate\nresults of captured objects with complex shapes and validate our method\nnumerically against existing 3D reconstruction and texture mapping methods.\n","authors":["Jaehoon Choi","Dongki Jung","Taejae Lee","Sangwook Kim","Youngdong Jung","Dinesh Manocha","Donghwan Lee"],"pdf_url":"https://arxiv.org/pdf/2303.15060v1.pdf","comment":"Accepted to CVPR23. Project Page: https://jh-choi.github.io/TMO/"},{"id":"http://arxiv.org/abs/2303.15046v1","updated":"2023-03-27T09:44:40Z","published":"2023-03-27T09:44:40Z","title":"Nighttime Smartphone Reflective Flare Removal Using Optical Center\n  Symmetry Prior","summary":"  Reflective flare is a phenomenon that occurs when light reflects inside\nlenses, causing bright spots or a \"ghosting effect\" in photos, which can impact\ntheir quality. Eliminating reflective flare is highly desirable but\nchallenging. Many existing methods rely on manually designed features to detect\nthese bright spots, but they often fail to identify reflective flares created\nby various types of light and may even mistakenly remove the light sources in\nscenarios with multiple light sources. To address these challenges, we propose\nan optical center symmetry prior, which suggests that the reflective flare and\nlight source are always symmetrical around the lens's optical center. This\nprior helps to locate the reflective flare's proposal region more accurately\nand can be applied to most smartphone cameras. Building on this prior, we\ncreate the first reflective flare removal dataset called BracketFlare, which\ncontains diverse and realistic reflective flare patterns. We use continuous\nbracketing to capture the reflective flare pattern in the underexposed image\nand combine it with a normally exposed image to synthesize a pair of\nflare-corrupted and flare-free images. With the dataset, neural networks can be\ntrained to remove the reflective flares effectively. Extensive experiments\ndemonstrate the effectiveness of our method on both synthetic and real-world\ndatasets.\n","authors":["Yuekun Dai","Yihang Luo","Shangchen Zhou","Chongyi Li","Chen Change Loy"],"pdf_url":"https://arxiv.org/pdf/2303.15046v1.pdf","comment":"CVPR2023 (Highlight)"},{"id":"http://arxiv.org/abs/2303.15043v1","updated":"2023-03-27T09:43:42Z","published":"2023-03-27T09:43:42Z","title":"Joint Video Multi-Frame Interpolation and Deblurring under Unknown\n  Exposure Time","summary":"  Natural videos captured by consumer cameras often suffer from low framerate\nand motion blur due to the combination of dynamic scene complexity, lens and\nsensor imperfection, and less than ideal exposure setting. As a result,\ncomputational methods that jointly perform video frame interpolation and\ndeblurring begin to emerge with the unrealistic assumption that the exposure\ntime is known and fixed. In this work, we aim ambitiously for a more realistic\nand challenging task - joint video multi-frame interpolation and deblurring\nunder unknown exposure time. Toward this goal, we first adopt a variant of\nsupervised contrastive learning to construct an exposure-aware representation\nfrom input blurred frames. We then train two U-Nets for intra-motion and\ninter-motion analysis, respectively, adapting to the learned exposure\nrepresentation via gain tuning. We finally build our video reconstruction\nnetwork upon the exposure and motion representation by progressive\nexposure-adaptive convolution and motion refinement. Extensive experiments on\nboth simulated and real-world datasets show that our optimized method achieves\nnotable performance gains over the state-of-the-art on the joint video x8\ninterpolation and deblurring task. Moreover, on the seemingly implausible x16\ninterpolation task, our method outperforms existing methods by more than 1.5 dB\nin terms of PSNR.\n","authors":["Wei Shang","Dongwei Ren","Yi Yang","Hongzhi Zhang","Kede Ma","Wangmeng Zuo"],"pdf_url":"https://arxiv.org/pdf/2303.15043v1.pdf","comment":"Accepted by CVPR 2023, available at\n  https://github.com/shangwei5/VIDUE"},{"id":"http://arxiv.org/abs/2211.17174v2","updated":"2023-03-27T09:42:13Z","published":"2022-11-30T17:17:55Z","title":"Optimizing Explanations by Network Canonization and Hyperparameter\n  Search","summary":"  Explainable AI (XAI) is slowly becoming a key component for many AI\napplications. Rule-based and modified backpropagation XAI approaches however\noften face challenges when being applied to modern model architectures\nincluding innovative layer building blocks, which is caused by two reasons.\nFirstly, the high flexibility of rule-based XAI methods leads to numerous\npotential parameterizations. Secondly, many XAI methods break the\nimplementation-invariance axiom because they struggle with certain model\ncomponents, e.g., BatchNorm layers. The latter can be addressed with model\ncanonization, which is the process of re-structuring the model to disregard\nproblematic components without changing the underlying function. While model\ncanonization is straightforward for simple architectures (e.g., VGG, ResNet),\nit can be challenging for more complex and highly interconnected models (e.g.,\nDenseNet). Moreover, there is only little quantifiable evidence that model\ncanonization is beneficial for XAI. In this work, we propose canonizations for\ncurrently relevant model blocks applicable to popular deep neural network\narchitectures,including VGG, ResNet, EfficientNet, DenseNets, as well as\nRelation Networks. We further suggest a XAI evaluation framework with which we\nquantify and compare the effect sof model canonization for various XAI methods\nin image classification tasks on the Pascal-VOC and ILSVRC2017 datasets, as\nwell as for Visual Question Answering using CLEVR-XAI. Moreover, addressing the\nformer issue outlined above, we demonstrate how our evaluation framework can be\napplied to perform hyperparameter search for XAI methods to optimize the\nquality of explanations.\n","authors":["Frederik Pahde","Galip Ümit Yolcu","Alexander Binder","Wojciech Samek","Sebastian Lapuschkin"],"pdf_url":"https://arxiv.org/pdf/2211.17174v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.15038v1","updated":"2023-03-27T09:35:44Z","published":"2023-03-27T09:35:44Z","title":"Image Quality-aware Diagnosis via Meta-knowledge Co-embedding","summary":"  Medical images usually suffer from image degradation in clinical practice,\nleading to decreased performance of deep learning-based models. To resolve this\nproblem, most previous works have focused on filtering out degradation-causing\nlow-quality images while ignoring their potential value for models. Through\neffectively learning and leveraging the knowledge of degradations, models can\nbetter resist their adverse effects and avoid misdiagnosis. In this paper, we\nraise the problem of image quality-aware diagnosis, which aims to take\nadvantage of low-quality images and image quality labels to achieve a more\naccurate and robust diagnosis. However, the diversity of degradations and\nsuperficially unrelated targets between image quality assessment and disease\ndiagnosis makes it still quite challenging to effectively leverage quality\nlabels to assist diagnosis. Thus, to tackle these issues, we propose a novel\nmeta-knowledge co-embedding network, consisting of two subnets: Task Net and\nMeta Learner. Task Net constructs an explicit quality information utilization\nmechanism to enhance diagnosis via knowledge co-embedding features, while Meta\nLearner ensures the effectiveness and constrains the semantics of these\nfeatures via meta-learning and joint-encoding masking. Superior performance on\nfive datasets with four widely-used medical imaging modalities demonstrates the\neffectiveness and generalizability of our method.\n","authors":["Haoxuan Che","Siyu Chen","Hao Chen"],"pdf_url":"https://arxiv.org/pdf/2303.15038v1.pdf","comment":"Accepted by CVPR 2023"},{"id":"http://arxiv.org/abs/2302.04820v2","updated":"2023-03-27T09:21:29Z","published":"2023-02-09T18:15:08Z","title":"High-fidelity Interpretable Inverse Rig: An Accurate and Sparse Solution\n  Optimizing the Quartic Blendshape Model","summary":"  We propose a method to fit arbitrarily accurate blendshape rig models by\nsolving the inverse rig problem in realistic human face animation. The method\nconsiders blendshape models with different levels of added corrections and\nsolves the regularized least-squares problem using coordinate descent, i.e.,\niteratively estimating blendshape weights. Besides making the optimization\neasier to solve, this approach ensures that mutually exclusive controllers will\nnot be activated simultaneously and improves the goodness of fit after each\niteration. We show experimentally that the proposed method yields solutions\nwith mesh error comparable to or lower than the state-of-the-art approaches\nwhile significantly reducing the cardinality of the weight vector (over 20\npercent), hence giving a high-fidelity reconstruction of the reference\nexpression that is easier to manipulate in the post-production manually. Python\nscripts for the algorithm will be publicly available upon acceptance of the\npaper.\n","authors":["Stevo Racković","Cláudia Soares","Dušan Jakovetić","Zoranka Desnica"],"pdf_url":"https://arxiv.org/pdf/2302.04820v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.15024v1","updated":"2023-03-27T09:16:10Z","published":"2023-03-27T09:16:10Z","title":"An End-to-End Framework For Universal Lesion Detection With Missing\n  Annotations","summary":"  Fully annotated large-scale medical image datasets are highly valuable.\nHowever, because labeling medical images is tedious and requires specialized\nknowledge, the large-scale datasets available often have missing annotation\nissues. For instance, DeepLesion, a large-scale CT image dataset with labels\nfor various kinds of lesions, is reported to have a missing annotation rate of\n50\\%. Directly training a lesion detector on it would suffer from false\nnegative supervision caused by unannotated lesions. To address this issue,\nprevious works have used sophisticated multi-stage strategies to switch between\nlesion mining and detector training. In this work, we present a novel\nend-to-end framework for mining unlabeled lesions while simultaneously training\nthe detector. Our framework follows the teacher-student paradigm. In each\niteration, the teacher model infers the input data and creates a set of\npredictions. High-confidence predictions are combined with partially-labeled\nground truth for training the student model. On the DeepLesion dataset, using\nthe original partially labeled training set, our model can outperform all other\nmore complicated methods and surpass the previous best method by 2.3\\% on\naverage sensitivity and 2.7\\% on average precision, achieving state-of-the-art\nuniversal lesion detection results.\n","authors":["Xiaoyu Bai","Yong Xia"],"pdf_url":"https://arxiv.org/pdf/2303.15024v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.15023v1","updated":"2023-03-27T09:15:53Z","published":"2023-03-27T09:15:53Z","title":"ScarceNet: Animal Pose Estimation with Scarce Annotations","summary":"  Animal pose estimation is an important but under-explored task due to the\nlack of labeled data. In this paper, we tackle the task of animal pose\nestimation with scarce annotations, where only a small set of labeled data and\nunlabeled images are available. At the core of the solution to this problem\nsetting is the use of the unlabeled data to compensate for the lack of\nwell-labeled animal pose data. To this end, we propose the ScarceNet, a pseudo\nlabel-based approach to generate artificial labels for the unlabeled images.\nThe pseudo labels, which are generated with a model trained with the small set\nof labeled images, are generally noisy and can hurt the performance when\ndirectly used for training. To solve this problem, we first use a small-loss\ntrick to select reliable pseudo labels. Although effective, the selection\nprocess is improvident since numerous high-loss samples are left unused. We\nfurther propose to identify reusable samples from the high-loss samples based\non an agreement check. Pseudo labels are re-generated to provide supervision\nfor those reusable samples. Lastly, we introduce a student-teacher framework to\nenforce a consistency constraint since there are still samples that are neither\nreliable nor reusable. By combining the reliable pseudo label selection with\nthe reusable sample re-labeling and the consistency constraint, we can make\nfull use of the unlabeled data. We evaluate our approach on the challenging\nAP-10K dataset, where our approach outperforms existing semi-supervised\napproaches by a large margin. We also test on the TigDog dataset, where our\napproach can achieve better performance than domain adaptation based approaches\nwhen only very few annotations are available. Our code is available at the\nproject website.\n","authors":["Chen Li","Gim Hee Lee"],"pdf_url":"https://arxiv.org/pdf/2303.15023v1.pdf","comment":"Accepted to CVPR 2023"},{"id":"http://arxiv.org/abs/2303.15014v1","updated":"2023-03-27T08:57:28Z","published":"2023-03-27T08:57:28Z","title":"Leveraging Hidden Positives for Unsupervised Semantic Segmentation","summary":"  Dramatic demand for manpower to label pixel-level annotations triggered the\nadvent of unsupervised semantic segmentation. Although the recent work\nemploying the vision transformer (ViT) backbone shows exceptional performance,\nthere is still a lack of consideration for task-specific training guidance and\nlocal semantic consistency. To tackle these issues, we leverage contrastive\nlearning by excavating hidden positives to learn rich semantic relationships\nand ensure semantic consistency in local regions. Specifically, we first\ndiscover two types of global hidden positives, task-agnostic and task-specific\nones for each anchor based on the feature similarities defined by a fixed\npre-trained backbone and a segmentation head-in-training, respectively. A\ngradual increase in the contribution of the latter induces the model to capture\ntask-specific semantic features. In addition, we introduce a gradient\npropagation strategy to learn semantic consistency between adjacent patches,\nunder the inherent premise that nearby patches are highly likely to possess the\nsame semantics. Specifically, we add the loss propagating to local hidden\npositives, semantically similar nearby patches, in proportion to the predefined\nsimilarity scores. With these training schemes, our proposed method achieves\nnew state-of-the-art (SOTA) results in COCO-stuff, Cityscapes, and Potsdam-3\ndatasets. Our code is available at: https://github.com/hynnsk/HP.\n","authors":["Hyun Seok Seong","WonJun Moon","SuBeen Lee","Jae-Pil Heo"],"pdf_url":"https://arxiv.org/pdf/2303.15014v1.pdf","comment":"Accepted to CVPR 2023"},{"id":"http://arxiv.org/abs/2203.11242v2","updated":"2023-03-27T08:57:21Z","published":"2022-03-21T18:06:03Z","title":"A survey on GANs for computer vision: Recent research, analysis and\n  taxonomy","summary":"  In the last few years, there have been several revolutions in the field of\ndeep learning, mainly headlined by the large impact of Generative Adversarial\nNetworks (GANs). GANs not only provide an unique architecture when defining\ntheir models, but also generate incredible results which have had a direct\nimpact on society. Due to the significant improvements and new areas of\nresearch that GANs have brought, the community is constantly coming up with new\nresearches that make it almost impossible to keep up with the times. Our survey\naims to provide a general overview of GANs, showing the latest architectures,\noptimizations of the loss functions, validation metrics and application areas\nof the most widely recognized variants. The efficiency of the different\nvariants of the model architecture will be evaluated, as well as showing the\nbest application area; as a vital part of the process, the different metrics\nfor evaluating the performance of GANs and the frequently used loss functions\nwill be analyzed. The final objective of this survey is to provide a summary of\nthe evolution and performance of the GANs which are having better results to\nguide future researchers in the field.\n","authors":["Guillermo Iglesias","Edgar Talavera","Alberto Díaz-Álvarez"],"pdf_url":"https://arxiv.org/pdf/2203.11242v2.pdf","comment":"77 pages, 11 figures, 4 tables"},{"id":"http://arxiv.org/abs/2303.15012v1","updated":"2023-03-27T08:54:51Z","published":"2023-03-27T08:54:51Z","title":"3D-Aware Multi-Class Image-to-Image Translation with NeRFs","summary":"  Recent advances in 3D-aware generative models (3D-aware GANs) combined with\nNeural Radiance Fields (NeRF) have achieved impressive results. However no\nprior works investigate 3D-aware GANs for 3D consistent multi-class\nimage-to-image (3D-aware I2I) translation. Naively using 2D-I2I translation\nmethods suffers from unrealistic shape/identity change. To perform 3D-aware\nmulti-class I2I translation, we decouple this learning process into a\nmulti-class 3D-aware GAN step and a 3D-aware I2I translation step. In the first\nstep, we propose two novel techniques: a new conditional architecture and an\neffective training strategy. In the second step, based on the well-trained\nmulti-class 3D-aware GAN architecture, that preserves view-consistency, we\nconstruct a 3D-aware I2I translation system. To further reduce the\nview-consistency problems, we propose several new techniques, including a\nU-net-like adaptor network design, a hierarchical representation constrain and\na relative regularization loss. In extensive experiments on two datasets,\nquantitative and qualitative results demonstrate that we successfully perform\n3D-aware I2I translation with multi-view consistency.\n","authors":["Senmao Li","Joost van de Weijer","Yaxing Wang","Fahad Shahbaz Khan","Meiqin Liu","Jian Yang"],"pdf_url":"https://arxiv.org/pdf/2303.15012v1.pdf","comment":"Accepted by CVPR2023"},{"id":"http://arxiv.org/abs/2303.11251v2","updated":"2023-03-27T08:53:58Z","published":"2023-03-20T16:35:38Z","title":"Towards End-to-End Generative Modeling of Long Videos with\n  Memory-Efficient Bidirectional Transformers","summary":"  Autoregressive transformers have shown remarkable success in video\ngeneration. However, the transformers are prohibited from directly learning the\nlong-term dependency in videos due to the quadratic complexity of\nself-attention, and inherently suffering from slow inference time and error\npropagation due to the autoregressive process. In this paper, we propose\nMemory-efficient Bidirectional Transformer (MeBT) for end-to-end learning of\nlong-term dependency in videos and fast inference. Based on recent advances in\nbidirectional transformers, our method learns to decode the entire\nspatio-temporal volume of a video in parallel from partially observed patches.\nThe proposed transformer achieves a linear time complexity in both encoding and\ndecoding, by projecting observable context tokens into a fixed number of latent\ntokens and conditioning them to decode the masked tokens through the\ncross-attention. Empowered by linear complexity and bidirectional modeling, our\nmethod demonstrates significant improvement over the autoregressive\nTransformers for generating moderately long videos in both quality and speed.\nVideos and code are available at https://sites.google.com/view/mebt-cvpr2023 .\n","authors":["Jaehoon Yoo","Semin Kim","Doyup Lee","Chiheon Kim","Seunghoon Hong"],"pdf_url":"https://arxiv.org/pdf/2303.11251v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.15006v1","updated":"2023-03-27T08:47:18Z","published":"2023-03-27T08:47:18Z","title":"Curriculum Learning for Compositional Visual Reasoning","summary":"  Visual Question Answering (VQA) is a complex task requiring large datasets\nand expensive training. Neural Module Networks (NMN) first translate the\nquestion to a reasoning path, then follow that path to analyze the image and\nprovide an answer. We propose an NMN method that relies on predefined\ncross-modal embeddings to ``warm start'' learning on the GQA dataset, then\nfocus on Curriculum Learning (CL) as a way to improve training and make a\nbetter use of the data. Several difficulty criteria are employed for defining\nCL methods. We show that by an appropriate selection of the CL method the cost\nof training and the amount of training data can be greatly reduced, with a\nlimited impact on the final VQA accuracy. Furthermore, we introduce\nintermediate losses during training and find that this allows to simplify the\nCL strategy.\n","authors":["Wafa Aissa","Marin Ferecatu","Michel Crucianu"],"pdf_url":"https://arxiv.org/pdf/2303.15006v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2104.02864v4","updated":"2023-03-27T08:46:45Z","published":"2021-04-07T02:32:06Z","title":"Self-Supervised Learning for Gastritis Detection with Gastric X-ray\n  Images","summary":"  Purpose: Manual annotation of gastric X-ray images by doctors for gastritis\ndetection is time-consuming and expensive. To solve this, a self-supervised\nlearning method is developed in this study. The effectiveness of the proposed\nself-supervised learning method in gastritis detection is verified using a few\nannotated gastric X-ray images. Methods: In this study, we develop a novel\nmethod that can perform explicit self-supervised learning and learn\ndiscriminative representations from gastric X-ray images. Models trained based\non the proposed method were fine-tuned on datasets comprising a few annotated\ngastric X-ray images. Five self-supervised learning methods, i.e., SimSiam,\nBYOL, PIRL-jigsaw, PIRL-rotation, and SimCLR, were compared with the proposed\nmethod. Furthermore, three previous methods, one pretrained on ImageNet, one\ntrained from scratch, and one semi-supervised learning method, were compared\nwith the proposed method. Results: The proposed method's harmonic mean score of\nsensitivity and specificity after fine-tuning with the annotated data of 10,\n20, 30, and 40 patients were 0.875, 0.911, 0.915, and 0.931, respectively. The\nproposed method outperformed all comparative methods, including the five\nself-supervised learning and three previous methods. Experimental results\nshowed the effectiveness of the proposed method in gastritis detection using a\nfew annotated gastric X-ray images. Conclusions: This paper proposes a novel\nself-supervised learning method based on a teacher-student architecture for\ngastritis detection using gastric X-ray images. The proposed method can perform\nexplicit self-supervised learning and learn discriminative representations from\ngastric X-ray images. The proposed method exhibits potential clinical use in\ngastritis detection using a few annotated gastric X-ray images.\n","authors":["Guang Li","Ren Togo","Takahiro Ogawa","Miki Haseyama"],"pdf_url":"https://arxiv.org/pdf/2104.02864v4.pdf","comment":"Published as a journal paper at Springer IJCARS"},{"id":"http://arxiv.org/abs/2303.15005v1","updated":"2023-03-27T08:46:31Z","published":"2023-03-27T08:46:31Z","title":"Architecturing Binarized Neural Networks for Traffic Sign Recognition","summary":"  Traffic signs support road safety and managing the flow of traffic, hence are\nan integral part of any vision system for autonomous driving. While the use of\ndeep learning is well-known in traffic signs classification due to the high\naccuracy results obtained using convolutional neural networks (CNNs) (state of\nthe art is 99.46\\%), little is known about binarized neural networks (BNNs).\nCompared to CNNs, BNNs reduce the model size and simplify convolution\noperations and have shown promising results in computationally limited and\nenergy-constrained devices which appear in the context of autonomous driving.\n  This work presents a bottom-up approach for architecturing BNNs by studying\ncharacteristics of the constituent layers. These constituent layers (binarized\nconvolutional layers, max pooling, batch normalization, fully connected layers)\nare studied in various combinations and with different values of kernel size,\nnumber of filters and of neurons by using the German Traffic Sign Recognition\nBenchmark (GTSRB) for training. As a result, we propose BNNs architectures\nwhich achieve more than $90\\%$ for GTSRB (the maximum is $96.45\\%$) and an\naverage greater than $80\\%$ (the maximum is $88.99\\%$) considering also the\nBelgian and Chinese datasets for testing. The number of parameters of these\narchitectures varies from 100k to less than 2M. The accompanying material of\nthis paper is publicly available at\nhttps://github.com/apostovan21/BinarizedNeuralNetwork.\n","authors":["Andreea Postovan","Mădălina Eraşcu"],"pdf_url":"https://arxiv.org/pdf/2303.15005v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.14999v1","updated":"2023-03-27T08:42:45Z","published":"2023-03-27T08:42:45Z","title":"Transformer-based Multi-Instance Learning for Weakly Supervised Object\n  Detection","summary":"  Weakly Supervised Object Detection (WSOD) enables the training of object\ndetection models using only image-level annotations. State-of-the-art WSOD\ndetectors commonly rely on multi-instance learning (MIL) as the backbone of\ntheir detectors and assume that the bounding box proposals of an image are\nindependent of each other. However, since such approaches only utilize the\nhighest score proposal and discard the potentially useful information from\nother proposals, their independent MIL backbone often limits models to salient\nparts of an object or causes them to detect only one object per class. To solve\nthe above problems, we propose a novel backbone for WSOD based on our tailored\nVision Transformer named Weakly Supervised Transformer Detection Network\n(WSTDN). Our algorithm is not only the first to demonstrate that self-attention\nmodules that consider inter-instance relationships are effective backbones for\nWSOD, but also we introduce a novel bounding box mining method (BBM) integrated\nwith a memory transfer refinement (MTR) procedure to utilize the instance\ndependencies for facilitating instance refinements. Experimental results on\nPASCAL VOC2007 and VOC2012 benchmarks demonstrate the effectiveness of our\nproposed WSTDN and modified instance refinement modules.\n","authors":["Zhaofei Wang","Weijia Zhang","Min-Ling Zhang"],"pdf_url":"https://arxiv.org/pdf/2303.14999v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.14998v1","updated":"2023-03-27T08:42:10Z","published":"2023-03-27T08:42:10Z","title":"Multi-view Cross-Modality MR Image Translation for Vestibular Schwannoma\n  and Cochlea Segmentation","summary":"  In this work, we propose a multi-view image translation framework, which can\ntranslate contrast-enhanced T1 (ceT1) MR imaging to high-resolution T2 (hrT2)\nMR imaging for unsupervised vestibular schwannoma and cochlea segmentation. We\nadopt two image translation models in parallel that use a pixel-level\nconsistent constraint and a patch-level contrastive constraint, respectively.\nThereby, we can augment pseudo-hrT2 images reflecting different perspectives,\nwhich eventually lead to a high-performing segmentation model. Our experimental\nresults on the CrossMoDA challenge show that the proposed method achieved\nenhanced performance on the vestibular schwannoma and cochlea segmentation.\n","authors":["Bogyeong Kang","Hyeonyeong Nam","Ji-Wung Han","Keun-Soo Heo","Tae-Eui Kam"],"pdf_url":"https://arxiv.org/pdf/2303.14998v1.pdf","comment":"9 pages, 4 figures"},{"id":"http://arxiv.org/abs/2211.14905v2","updated":"2023-03-27T08:39:13Z","published":"2022-11-27T18:13:05Z","title":"Multi-Modal Few-Shot Temporal Action Detection","summary":"  Few-shot (FS) and zero-shot (ZS) learning are two different approaches for\nscaling temporal action detection (TAD) to new classes. The former adapts a\npretrained vision model to a new task represented by as few as a single video\nper class, whilst the latter requires no training examples by exploiting a\nsemantic description of the new class. In this work, we introduce a new\nmulti-modality few-shot (MMFS) TAD problem, which can be considered as a\nmarriage of FS-TAD and ZS-TAD by leveraging few-shot support videos and new\nclass names jointly. To tackle this problem, we further introduce a novel\nMUlti-modality PromPt mETa-learning (MUPPET) method. This is enabled by\nefficiently bridging pretrained vision and language models whilst maximally\nreusing already learned capacity. Concretely, we construct multi-modal prompts\nby mapping support videos into the textual token space of a vision-language\nmodel using a meta-learned adapter-equipped visual semantics tokenizer. To\ntackle large intra-class variation, we further design a query feature\nregulation scheme. Extensive experiments on ActivityNetv1.3 and THUMOS14\ndemonstrate that our MUPPET outperforms state-of-the-art alternative methods,\noften by a large margin. We also show that our MUPPET can be easily extended to\ntackle the few-shot object detection problem and again achieves the\nstate-of-the-art performance on MS-COCO dataset. The code will be available in\nhttps://github.com/sauradip/MUPPET\n","authors":["Sauradip Nag","Mengmeng Xu","Xiatian Zhu","Juan-Manuel Perez-Rua","Bernard Ghanem","Yi-Zhe Song","Tao Xiang"],"pdf_url":"https://arxiv.org/pdf/2211.14905v2.pdf","comment":"Technical Report"},{"id":"http://arxiv.org/abs/2301.13166v2","updated":"2023-03-27T08:33:40Z","published":"2023-01-30T18:37:32Z","title":"ESC: Exploration with Soft Commonsense Constraints for Zero-shot Object\n  Navigation","summary":"  The ability to accurately locate and navigate to a specific object is a\ncrucial capability for embodied agents that operate in the real world and\ninteract with objects to complete tasks. Such object navigation tasks usually\nrequire large-scale training in visual environments with labeled objects, which\ngeneralizes poorly to novel objects in unknown environments. In this work, we\npresent a novel zero-shot object navigation method, Exploration with Soft\nCommonsense constraints (ESC), that transfers commonsense knowledge in\npre-trained models to open-world object navigation without any navigation\nexperience nor any other training on the visual environments. First, ESC\nleverages a pre-trained vision and language model for open-world prompt-based\ngrounding and a pre-trained commonsense language model for room and object\nreasoning. Then ESC converts commonsense knowledge into navigation actions by\nmodeling it as soft logic predicates for efficient exploration. Extensive\nexperiments on MP3D, HM3D, and RoboTHOR benchmarks show that our ESC method\nimproves significantly over baselines, and achieves new state-of-the-art\nresults for zero-shot object navigation (e.g., 158% relative Success Rate\nimprovement than CoW on MP3D).\n","authors":["Kaiwen Zhou","Kaizhi Zheng","Connor Pryor","Yilin Shen","Hongxia Jin","Lise Getoor","Xin Eric Wang"],"pdf_url":"https://arxiv.org/pdf/2301.13166v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.14077v2","updated":"2023-03-27T08:32:43Z","published":"2023-03-24T15:41:40Z","title":"Improved Adversarial Training Through Adaptive Instance-wise Loss\n  Smoothing","summary":"  Deep neural networks can be easily fooled into making incorrect predictions\nthrough corruption of the input by adversarial perturbations:\nhuman-imperceptible artificial noise. So far adversarial training has been the\nmost successful defense against such adversarial attacks. This work focuses on\nimproving adversarial training to boost adversarial robustness. We first\nanalyze, from an instance-wise perspective, how adversarial vulnerability\nevolves during adversarial training. We find that during training an overall\nreduction of adversarial loss is achieved by sacrificing a considerable\nproportion of training samples to be more vulnerable to adversarial attack,\nwhich results in an uneven distribution of adversarial vulnerability among\ndata. Such \"uneven vulnerability\", is prevalent across several popular robust\ntraining methods and, more importantly, relates to overfitting in adversarial\ntraining. Motivated by this observation, we propose a new adversarial training\nmethod: Instance-adaptive Smoothness Enhanced Adversarial Training (ISEAT). It\njointly smooths both input and weight loss landscapes in an adaptive,\ninstance-specific, way to enhance robustness more for those samples with higher\nadversarial vulnerability. Extensive experiments demonstrate the superiority of\nour method over existing defense methods. Noticeably, our method, when combined\nwith the latest data augmentation and semi-supervised learning techniques,\nachieves state-of-the-art robustness against $\\ell_{\\infty}$-norm constrained\nattacks on CIFAR10 of 59.32% for Wide ResNet34-10 without extra data, and\n61.55% for Wide ResNet28-10 with extra data. Code is available at\nhttps://github.com/TreeLLi/Instance-adaptive-Smoothness-Enhanced-AT.\n","authors":["Lin Li","Michael Spratling"],"pdf_url":"https://arxiv.org/pdf/2303.14077v2.pdf","comment":"12 pages, work in submission"},{"id":"http://arxiv.org/abs/2204.10588v2","updated":"2023-03-27T08:30:34Z","published":"2022-04-22T09:19:49Z","title":"A Note on the Regularity of Images Generated by Convolutional Neural\n  Networks","summary":"  The regularity of images generated by convolutional neural networks, such as\nthe U-net, generative networks, or the deep image prior, is analyzed. In a\nresolution-independent, infinite dimensional setting, it is shown that such\nimages, represented as functions, are always continuous and, in some\ncircumstances, even continuously differentiable, contradicting the widely\naccepted modeling of sharp edges in images via jump discontinuities. While such\nstatements require an infinite dimensional setting, the connection to\n(discretized) neural networks used in practice is made by considering the limit\nas the resolution approaches infinity. As practical consequence, the results of\nthis paper in particular provide analytical evidence that basic L2\nregularization of network weights might lead to over-smoothed outputs.\n","authors":["Andreas Habring","Martin Holler"],"pdf_url":"https://arxiv.org/pdf/2204.10588v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.14978v1","updated":"2023-03-27T08:19:01Z","published":"2023-03-27T08:19:01Z","title":"Learned Image Compression with Mixed Transformer-CNN Architectures","summary":"  Learned image compression (LIC) methods have exhibited promising progress and\nsuperior rate-distortion performance compared with classical image compression\nstandards. Most existing LIC methods are Convolutional Neural Networks-based\n(CNN-based) or Transformer-based, which have different advantages. Exploiting\nboth advantages is a point worth exploring, which has two challenges: 1) how to\neffectively fuse the two methods? 2) how to achieve higher performance with a\nsuitable complexity? In this paper, we propose an efficient parallel\nTransformer-CNN Mixture (TCM) block with a controllable complexity to\nincorporate the local modeling ability of CNN and the non-local modeling\nability of transformers to improve the overall architecture of image\ncompression models. Besides, inspired by the recent progress of entropy\nestimation models and attention modules, we propose a channel-wise entropy\nmodel with parameter-efficient swin-transformer-based attention (SWAtten)\nmodules by using channel squeezing. Experimental results demonstrate our\nproposed method achieves state-of-the-art rate-distortion performances on three\ndifferent resolution datasets (i.e., Kodak, Tecnick, CLIC Professional\nValidation) compared to existing LIC methods. The code is at\nhttps://github.com/jmliu206/LIC_TCM.\n","authors":["Jinming Liu","Heming Sun","Jiro Katto"],"pdf_url":"https://arxiv.org/pdf/2303.14978v1.pdf","comment":"Accepted by CVPR2023 (Highlight)"},{"id":"http://arxiv.org/abs/2303.14977v1","updated":"2023-03-27T08:17:22Z","published":"2023-03-27T08:17:22Z","title":"A novel Multi to Single Module for small object detection","summary":"  Small object detection presents a significant challenge in computer vision\nand object detection. The performance of small object detectors is often\ncompromised by a lack of pixels and less significant features. This issue stems\nfrom information misalignment caused by variations in feature scale and\ninformation loss during feature processing. In response to this challenge, this\npaper proposes a novel the Multi to Single Module (M2S), which enhances a\nspecific layer through improving feature extraction and refining features.\nSpecifically, M2S includes the proposed Cross-scale Aggregation Module (CAM)\nand explored Dual Relationship Module (DRM) to improve information extraction\ncapabilities and feature refinement effects. Moreover, this paper enhances the\naccuracy of small object detection by utilizing M2S to generate an additional\ndetection head. The effectiveness of the proposed method is evaluated on two\ndatasets, VisDrone2021-DET and SeaDronesSeeV2. The experimental results\ndemonstrate its improved performance compared with existing methods. Compared\nto the baseline model (YOLOv5s), M2S improves the accuracy by about 1.1\\% on\nthe VisDrone2021-DET testing dataset and 15.68\\% on the SeaDronesSeeV2\nvalidation set.\n","authors":["Xiaohui Guo"],"pdf_url":"https://arxiv.org/pdf/2303.14977v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.14969v1","updated":"2023-03-27T07:58:42Z","published":"2023-03-27T07:58:42Z","title":"Universal Few-shot Learning of Dense Prediction Tasks with Visual Token\n  Matching","summary":"  Dense prediction tasks are a fundamental class of problems in computer\nvision. As supervised methods suffer from high pixel-wise labeling cost, a\nfew-shot learning solution that can learn any dense task from a few labeled\nimages is desired. Yet, current few-shot learning methods target a restricted\nset of tasks such as semantic segmentation, presumably due to challenges in\ndesigning a general and unified model that is able to flexibly and efficiently\nadapt to arbitrary tasks of unseen semantics. We propose Visual Token Matching\n(VTM), a universal few-shot learner for arbitrary dense prediction tasks. It\nemploys non-parametric matching on patch-level embedded tokens of images and\nlabels that encapsulates all tasks. Also, VTM flexibly adapts to any task with\na tiny amount of task-specific parameters that modulate the matching algorithm.\nWe implement VTM as a powerful hierarchical encoder-decoder architecture\ninvolving ViT backbones where token matching is performed at multiple feature\nhierarchies. We experiment VTM on a challenging variant of Taskonomy dataset\nand observe that it robustly few-shot learns various unseen dense prediction\ntasks. Surprisingly, it is competitive with fully supervised baselines using\nonly 10 labeled examples of novel tasks (0.004% of full supervision) and\nsometimes outperforms using 0.1% of full supervision. Codes are available at\nhttps://github.com/GitGyun/visual_token_matching.\n","authors":["Donggyun Kim","Jinwoo Kim","Seongwoong Cho","Chong Luo","Seunghoon Hong"],"pdf_url":"https://arxiv.org/pdf/2303.14969v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.14968v1","updated":"2023-03-27T07:58:09Z","published":"2023-03-27T07:58:09Z","title":"Blind Image Quality Assessment via Vision-Language Correspondence: A\n  Multitask Learning Perspective","summary":"  We aim at advancing blind image quality assessment (BIQA), which predicts the\nhuman perception of image quality without any reference information. We develop\na general and automated multitask learning scheme for BIQA to exploit auxiliary\nknowledge from other tasks, in a way that the model parameter sharing and the\nloss weighting are determined automatically. Specifically, we first describe\nall candidate label combinations (from multiple tasks) using a textual\ntemplate, and compute the joint probability from the cosine similarities of the\nvisual-textual embeddings. Predictions of each task can be inferred from the\njoint distribution, and optimized by carefully designed loss functions. Through\ncomprehensive experiments on learning three tasks - BIQA, scene classification,\nand distortion type identification, we verify that the proposed BIQA method 1)\nbenefits from the scene classification and distortion type identification tasks\nand outperforms the state-of-the-art on multiple IQA datasets, 2) is more\nrobust in the group maximum differentiation competition, and 3) realigns the\nquality annotations from different IQA datasets more effectively. The source\ncode is available at https://github.com/zwx8981/LIQE.\n","authors":["Weixia Zhang","Guangtao Zhai","Ying Wei","Xiaokang Yang","Kede Ma"],"pdf_url":"https://arxiv.org/pdf/2303.14968v1.pdf","comment":"CVPR2023"},{"id":"http://arxiv.org/abs/2303.08597v4","updated":"2023-03-27T07:56:21Z","published":"2023-03-15T13:07:21Z","title":"Aerial-Ground Person Re-ID","summary":"  Person re-ID matches persons across multiple non-overlapping cameras. Despite\nthe increasing deployment of airborne platforms in surveillance, current\nexisting person re-ID benchmarks' focus is on ground-ground matching and very\nlimited efforts on aerial-aerial matching. We propose a new benchmark dataset -\nAG-ReID, which performs person re-ID matching in a new setting: across aerial\nand ground cameras. Our dataset contains 21,983 images of 388 identities and 15\nsoft attributes for each identity. The data was collected by a UAV flying at\naltitudes between 15 to 45 meters and a ground-based CCTV camera on a\nuniversity campus. Our dataset presents a novel elevated-viewpoint challenge\nfor person re-ID due to the significant difference in person appearance across\nthese cameras. We propose an explainable algorithm to guide the person re-ID\nmodel's training with soft attributes to address this challenge. Experiments\ndemonstrate the efficacy of our method on the aerial-ground person re-ID task.\nThe dataset will be published and the baseline codes will be open-sourced at\nhttps://github.com/huynguyen792/AG-ReID to facilitate research in this area.\n","authors":["Huy Nguyen","Kien Nguyen","Sridha Sridharan","Clinton Fookes"],"pdf_url":"https://arxiv.org/pdf/2303.08597v4.pdf","comment":"The paper will be temporarily withdrawn from arXiv for updates and\n  revisions"},{"id":"http://arxiv.org/abs/2207.06124v3","updated":"2023-03-27T07:55:32Z","published":"2022-07-13T11:12:03Z","title":"DynaST: Dynamic Sparse Transformer for Exemplar-Guided Image Generation","summary":"  One key challenge of exemplar-guided image generation lies in establishing\nfine-grained correspondences between input and guided images. Prior approaches,\ndespite the promising results, have relied on either estimating dense attention\nto compute per-point matching, which is limited to only coarse scales due to\nthe quadratic memory cost, or fixing the number of correspondences to achieve\nlinear complexity, which lacks flexibility. In this paper, we propose a dynamic\nsparse attention based Transformer model, termed Dynamic Sparse Transformer\n(DynaST), to achieve fine-level matching with favorable efficiency. The heart\nof our approach is a novel dynamic-attention unit, dedicated to covering the\nvariation on the optimal number of tokens one position should focus on.\nSpecifically, DynaST leverages the multi-layer nature of Transformer structure,\nand performs the dynamic attention scheme in a cascaded manner to refine\nmatching results and synthesize visually-pleasing outputs. In addition, we\nintroduce a unified training objective for DynaST, making it a versatile\nreference-based image translation framework for both supervised and\nunsupervised scenarios. Extensive experiments on three applications,\npose-guided person image generation, edge-based face synthesis, and undistorted\nimage style transfer, demonstrate that DynaST achieves superior performance in\nlocal details, outperforming the state of the art while reducing the\ncomputational cost significantly. Our code is available at\nhttps://github.com/Huage001/DynaST\n","authors":["Songhua Liu","Jingwen Ye","Sucheng Ren","Xinchao Wang"],"pdf_url":"https://arxiv.org/pdf/2207.06124v3.pdf","comment":"ECCV 2022"},{"id":"http://arxiv.org/abs/2303.14964v1","updated":"2023-03-27T07:54:09Z","published":"2023-03-27T07:54:09Z","title":"Learning a Deep Color Difference Metric for Photographic Images","summary":"  Most well-established and widely used color difference (CD) metrics are\nhandcrafted and subject-calibrated against uniformly colored patches, which do\nnot generalize well to photographic images characterized by natural scene\ncomplexities. Constructing CD formulae for photographic images is still an\nactive research topic in imaging/illumination, vision science, and color\nscience communities. In this paper, we aim to learn a deep CD metric for\nphotographic images with four desirable properties. First, it well aligns with\nthe observations in vision science that color and form are linked inextricably\nin visual cortical processing. Second, it is a proper metric in the\nmathematical sense. Third, it computes accurate CDs between photographic\nimages, differing mainly in color appearances. Fourth, it is robust to mild\ngeometric distortions (e.g., translation or due to parallax), which are often\npresent in photographic images of the same scene captured by different digital\ncameras. We show that all these properties can be satisfied at once by learning\na multi-scale autoregressive normalizing flow for feature transform, followed\nby the Euclidean distance which is linearly proportional to the human\nperceptual CD. Quantitative and qualitative experiments on the large-scale SPCD\ndataset demonstrate the promise of the learned CD metric.\n","authors":["Haoyu Chen","Zhihua Wang","Yang Yang","Qilin Sun","Kede Ma"],"pdf_url":"https://arxiv.org/pdf/2303.14964v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.13540v2","updated":"2023-03-27T07:53:49Z","published":"2023-03-17T16:01:39Z","title":"Artificial Intelligence for Sustainability: Facilitating Sustainable\n  Smart Product-Service Systems with Computer Vision","summary":"  The usage and impact of deep learning for cleaner production and\nsustainability purposes remain little explored. This work shows how deep\nlearning can be harnessed to increase sustainability in production and product\nusage. Specifically, we utilize deep learning-based computer vision to\ndetermine the wear states of products. The resulting insights serve as a basis\nfor novel product-service systems with improved integration and result\norientation. Moreover, these insights are expected to facilitate product usage\nimprovements and R&D innovations. We demonstrate our approach on two products:\nmachining tools and rotating X-ray anodes. From a technical standpoint, we show\nthat it is possible to recognize the wear state of these products using\ndeep-learning-based computer vision. In particular, we detect wear through\nmicroscopic images of the two products. We utilize a U-Net for semantic\nsegmentation to detect wear based on pixel granularity. The resulting mean dice\ncoefficients of 0.631 and 0.603 demonstrate the feasibility of the proposed\napproach. Consequently, experts can now make better decisions, for example, to\nimprove the machining process parameters. To assess the impact of the proposed\napproach on environmental sustainability, we perform life cycle assessments\nthat show gains for both products. The results indicate that the emissions of\nCO2 equivalents are reduced by 12% for machining tools and by 44% for rotating\nanodes. This work can serve as a guideline and inspire researchers and\npractitioners to utilize computer vision in similar scenarios to develop\nsustainable smart product-service systems and enable cleaner production.\n","authors":["Jannis Walk","Niklas Kühl","Michael Saidani","Jürgen Schatte"],"pdf_url":"https://arxiv.org/pdf/2303.13540v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.14962v1","updated":"2023-03-27T07:53:23Z","published":"2023-03-27T07:53:23Z","title":"Forget-free Continual Learning with Soft-Winning SubNetworks","summary":"  Inspired by Regularized Lottery Ticket Hypothesis (RLTH), which states that\ncompetitive smooth (non-binary) subnetworks exist within a dense network in\ncontinual learning tasks, we investigate two proposed architecture-based\ncontinual learning methods which sequentially learn and select adaptive binary-\n(WSN) and non-binary Soft-Subnetworks (SoftNet) for each task. WSN and SoftNet\njointly learn the regularized model weights and task-adaptive non-binary masks\nof subnetworks associated with each task whilst attempting to select a small\nset of weights to be activated (winning ticket) by reusing weights of the prior\nsubnetworks. Our proposed WSN and SoftNet are inherently immune to catastrophic\nforgetting as each selected subnetwork model does not infringe upon other\nsubnetworks in Task Incremental Learning (TIL). In TIL, binary masks spawned\nper winning ticket are encoded into one N-bit binary digit mask, then\ncompressed using Huffman coding for a sub-linear increase in network capacity\nto the number of tasks. Surprisingly, in the inference step, SoftNet generated\nby injecting small noises to the backgrounds of acquired WSN (holding the\nforegrounds of WSN) provides excellent forward transfer power for future tasks\nin TIL. SoftNet shows its effectiveness over WSN in regularizing parameters to\ntackle the overfitting, to a few examples in Few-shot Class Incremental\nLearning (FSCIL).\n","authors":["Haeyong Kang","Jaehong Yoon","Sultan Rizky Madjid","Sung Ju Hwang","Chang D. Yoo"],"pdf_url":"https://arxiv.org/pdf/2303.14962v1.pdf","comment":"arXiv admin note: text overlap with arXiv:2209.07529"},{"id":"http://arxiv.org/abs/2303.14961v1","updated":"2023-03-27T07:52:58Z","published":"2023-03-27T07:52:58Z","title":"Diffusion Denoised Smoothing for Certified and Adversarial Robust\n  Out-Of-Distribution Detection","summary":"  As the use of machine learning continues to expand, the importance of\nensuring its safety cannot be overstated. A key concern in this regard is the\nability to identify whether a given sample is from the training distribution,\nor is an \"Out-Of-Distribution\" (OOD) sample. In addition, adversaries can\nmanipulate OOD samples in ways that lead a classifier to make a confident\nprediction. In this study, we present a novel approach for certifying the\nrobustness of OOD detection within a $\\ell_2$-norm around the input, regardless\nof network architecture and without the need for specific components or\nadditional training. Further, we improve current techniques for detecting\nadversarial attacks on OOD samples, while providing high levels of certified\nand adversarial robustness on in-distribution samples. The average of all OOD\ndetection metrics on CIFAR10/100 shows an increase of $\\sim 13 \\% / 5\\%$\nrelative to previous approaches.\n","authors":["Nicola Franco","Daniel Korth","Jeanette Miriam Lorenz","Karsten Roscher","Stephan Guennemann"],"pdf_url":"https://arxiv.org/pdf/2303.14961v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.14960v1","updated":"2023-03-27T07:46:58Z","published":"2023-03-27T07:46:58Z","title":"Ambiguity-Resistant Semi-Supervised Learning for Dense Object Detection","summary":"  With basic Semi-Supervised Object Detection (SSOD) techniques, one-stage\ndetectors generally obtain limited promotions compared with two-stage clusters.\nWe experimentally find that the root lies in two kinds of ambiguities: (1)\nSelection ambiguity that selected pseudo labels are less accurate, since\nclassification scores cannot properly represent the localization quality. (2)\nAssignment ambiguity that samples are matched with improper labels in\npseudo-label assignment, as the strategy is misguided by missed objects and\ninaccurate pseudo boxes. To tackle these problems, we propose a\nAmbiguity-Resistant Semi-supervised Learning (ARSL) for one-stage detectors.\nSpecifically, to alleviate the selection ambiguity, Joint-Confidence Estimation\n(JCE) is proposed to jointly quantifies the classification and localization\nquality of pseudo labels. As for the assignment ambiguity, Task-Separation\nAssignment (TSA) is introduced to assign labels based on pixel-level\npredictions rather than unreliable pseudo boxes. It employs a\n\"divide-and-conquer\" strategy and separately exploits positives for the\nclassification and localization task, which is more robust to the assignment\nambiguity. Comprehensive experiments demonstrate that ARSL effectively\nmitigates the ambiguities and achieves state-of-the-art SSOD performance on MS\nCOCO and PASCAL VOC. Codes can be found at\nhttps://github.com/PaddlePaddle/PaddleDetection.\n","authors":["Chang Liu","Weiming Zhang","Xiangru Lin","Wei Zhang","Xiao Tan","Junyu Han","Xiaomao Li","Errui Ding","Jingdong Wang"],"pdf_url":"https://arxiv.org/pdf/2303.14960v1.pdf","comment":"Accepted to CVPR 2023"},{"id":"http://arxiv.org/abs/2303.12641v2","updated":"2023-03-27T07:43:17Z","published":"2023-03-22T15:23:09Z","title":"Reveal to Revise: An Explainable AI Life Cycle for Iterative Bias\n  Correction of Deep Models","summary":"  State-of-the-art machine learning models often learn spurious correlations\nembedded in the training data. This poses risks when deploying these models for\nhigh-stake decision-making, such as in medical applications like skin cancer\ndetection. To tackle this problem, we propose Reveal to Revise (R2R), a\nframework entailing the entire eXplainable Artificial Intelligence (XAI) life\ncycle, enabling practitioners to iteratively identify, mitigate, and\n(re-)evaluate spurious model behavior with a minimal amount of human\ninteraction. In the first step (1), R2R reveals model weaknesses by finding\noutliers in attributions or through inspection of latent concepts learned by\nthe model. Secondly (2), the responsible artifacts are detected and spatially\nlocalized in the input data, which is then leveraged to (3) revise the model\nbehavior. Concretely, we apply the methods of RRR, CDEP and ClArC for model\ncorrection, and (4) (re-)evaluate the model's performance and remaining\nsensitivity towards the artifact. Using two medical benchmark datasets for\nMelanoma detection and bone age estimation, we apply our R2R framework to VGG,\nResNet and EfficientNet architectures and thereby reveal and correct real\ndataset-intrinsic artifacts, as well as synthetic variants in a controlled\nsetting. Completing the XAI life cycle, we demonstrate multiple R2R iterations\nto mitigate different biases. Code is available on\nhttps://github.com/maxdreyer/Reveal2Revise.\n","authors":["Frederik Pahde","Maximilian Dreyer","Wojciech Samek","Sebastian Lapuschkin"],"pdf_url":"https://arxiv.org/pdf/2303.12641v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.00917v2","updated":"2023-03-27T07:42:24Z","published":"2023-03-02T02:26:04Z","title":"Enhancing General Face Forgery Detection via Vision Transformer with\n  Low-Rank Adaptation","summary":"  Nowadays, forgery faces pose pressing security concerns over fake news,\nfraud, impersonation, etc. Despite the demonstrated success in intra-domain\nface forgery detection, existing detection methods lack generalization\ncapability and tend to suffer from dramatic performance drops when deployed to\nunforeseen domains. To mitigate this issue, this paper designs a more general\nfake face detection model based on the vision transformer(ViT) architecture. In\nthe training phase, the pretrained ViT weights are freezed, and only the\nLow-Rank Adaptation(LoRA) modules are updated. Additionally, the Single Center\nLoss(SCL) is applied to supervise the training process, further improving the\ngeneralization capability of the model. The proposed method achieves\nstate-of-the-arts detection performances in both cross-manipulation and\ncross-dataset evaluations.\n","authors":["Chenqi Kong","Haoliang Li","Shiqi Wang"],"pdf_url":"https://arxiv.org/pdf/2303.00917v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.14953v1","updated":"2023-03-27T07:36:47Z","published":"2023-03-27T07:36:47Z","title":"DyGait: Exploiting Dynamic Representations for High-performance Gait\n  Recognition","summary":"  Gait recognition is a biometric technology that recognizes the identity of\nhumans through their walking patterns. Compared with other biometric\ntechnologies, gait recognition is more difficult to disguise and can be applied\nto the condition of long-distance without the cooperation of subjects. Thus, it\nhas unique potential and wide application for crime prevention and social\nsecurity. At present, most gait recognition methods directly extract features\nfrom the video frames to establish representations. However, these\narchitectures learn representations from different features equally but do not\npay enough attention to dynamic features, which refers to a representation of\ndynamic parts of silhouettes over time (e.g. legs). Since dynamic parts of the\nhuman body are more informative than other parts (e.g. bags) during walking, in\nthis paper, we propose a novel and high-performance framework named DyGait.\nThis is the first framework on gait recognition that is designed to focus on\nthe extraction of dynamic features. Specifically, to take full advantage of the\ndynamic information, we propose a Dynamic Augmentation Module (DAM), which can\nautomatically establish spatial-temporal feature representations of the dynamic\nparts of the human body. The experimental results show that our DyGait network\noutperforms other state-of-the-art gait recognition methods. It achieves an\naverage Rank-1 accuracy of 71.4% on the GREW dataset, 66.3% on the Gait3D\ndataset, 98.4% on the CASIA-B dataset and 98.3% on the OU-MVLP dataset.\n","authors":["Ming Wang","Xianda Guo","Beibei Lin","Tian Yang","Zheng Zhu","Lincheng Li","Shunli Zhang","Xin Yu"],"pdf_url":"https://arxiv.org/pdf/2303.14953v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.08435v2","updated":"2023-03-27T07:11:17Z","published":"2023-03-15T08:17:07Z","title":"Physics-Informed Optical Kernel Regression Using Complex-valued Neural\n  Fields","summary":"  Lithography is fundamental to integrated circuit fabrication, necessitating\nlarge computation overhead. The advancement of machine learning (ML)-based\nlithography models alleviates the trade-offs between manufacturing process\nexpense and capability. However, all previous methods regard the lithography\nsystem as an image-to-image black box mapping, utilizing network parameters to\nlearn by rote mappings from massive mask-to-aerial or mask-to-resist image\npairs, resulting in poor generalization capability. In this paper, we propose a\nnew ML-based paradigm disassembling the rigorous lithographic model into\nnon-parametric mask operations and learned optical kernels containing\ndeterminant source, pupil, and lithography information. By optimizing\ncomplex-valued neural fields to perform optical kernel regression from\ncoordinates, our method can accurately restore lithography system using a\nsmall-scale training dataset with fewer parameters, demonstrating superior\ngeneralization capability as well. Experiments show that our framework can use\n31% of parameters while achieving 69$\\times$ smaller mean squared error with\n1.3$\\times$ higher throughput than the state-of-the-art.\n","authors":["Guojin Chen","Zehua Pei","Haoyu Yang","Yuzhe Ma","Bei Yu","Martin D. F. Wong"],"pdf_url":"https://arxiv.org/pdf/2303.08435v2.pdf","comment":"Accepted by DAC23"},{"id":"http://arxiv.org/abs/2211.01146v2","updated":"2023-03-27T07:02:09Z","published":"2022-11-02T14:22:50Z","title":"DynamicISP: Dynamically Controlled Image Signal Processor for Image\n  Recognition","summary":"  Image Signal Processors (ISPs) play important roles in image recognition\ntasks as well as in the perceptual quality of captured images. In most cases,\nexperts make a lot of effort to manually tune many parameters of ISPs, but the\nparameters are sub-optimal. In the literature, two types of techniques have\nbeen actively studied: a machine learning-based parameter tuning technique and\na DNN-based ISP technique. The former is lightweight but lacks expressive\npower. The latter has expressive power, but the computational cost is too heavy\non edge devices. To solve these problems, we propose \"DynamicISP,\" which\nconsists of multiple classical ISP functions and dynamically controls the\nparameters of each frame according to the recognition result of the previous\nframe. We show our method successfully controls the parameters of multiple ISP\nfunctions and achieves state-of-the-art accuracy with low computational cost in\nsingle and multi-category object detection tasks.\n","authors":["Masakazu Yoshimura","Junji Otsuka","Atsushi Irie","Takeshi Ohashi"],"pdf_url":"https://arxiv.org/pdf/2211.01146v2.pdf","comment":"Several updates from v1 including additional experiments,\n  visualization results, and modified document"},{"id":"http://arxiv.org/abs/2301.05175v3","updated":"2023-03-27T06:59:55Z","published":"2023-01-12T18:01:28Z","title":"Scene-Aware 3D Multi-Human Motion Capture from a Single Camera","summary":"  In this work, we consider the problem of estimating the 3D position of\nmultiple humans in a scene as well as their body shape and articulation from a\nsingle RGB video recorded with a static camera. In contrast to expensive\nmarker-based or multi-view systems, our lightweight setup is ideal for private\nusers as it enables an affordable 3D motion capture that is easy to install and\ndoes not require expert knowledge. To deal with this challenging setting, we\nleverage recent advances in computer vision using large-scale pre-trained\nmodels for a variety of modalities, including 2D body joints, joint angles,\nnormalized disparity maps, and human segmentation masks. Thus, we introduce the\nfirst non-linear optimization-based approach that jointly solves for the\nabsolute 3D position of each human, their articulated pose, their individual\nshapes as well as the scale of the scene. In particular, we estimate the scene\ndepth and person unique scale from normalized disparity predictions using the\n2D body joints and joint angles. Given the per-frame scene depth, we\nreconstruct a point-cloud of the static scene in 3D space. Finally, given the\nper-frame 3D estimates of the humans and scene point-cloud, we perform a\nspace-time coherent optimization over the video to ensure temporal, spatial and\nphysical plausibility. We evaluate our method on established multi-person 3D\nhuman pose benchmarks where we consistently outperform previous methods and we\nqualitatively demonstrate that our method is robust to in-the-wild conditions\nincluding challenging scenes with people of different sizes.\n","authors":["Diogo Luvizon","Marc Habermann","Vladislav Golyanik","Adam Kortylewski","Christian Theobalt"],"pdf_url":"https://arxiv.org/pdf/2301.05175v3.pdf","comment":"Accepted to Eurographics 2023. See also github:\n  https://github.com/dluvizon/scene-aware-3d-multi-human project page:\n  https://vcai.mpi-inf.mpg.de/projects/scene-aware-3d-multi-human/"},{"id":"http://arxiv.org/abs/2109.14196v3","updated":"2023-03-27T06:50:45Z","published":"2021-09-29T05:19:58Z","title":"WEDGE: Web-Image Assisted Domain Generalization for Semantic\n  Segmentation","summary":"  Domain generalization for semantic segmentation is highly demanded in real\napplications, where a trained model is expected to work well in previously\nunseen domains. One challenge lies in the lack of data which could cover the\ndiverse distributions of the possible unseen domains for training. In this\npaper, we propose a WEb-image assisted Domain GEneralization (WEDGE) scheme,\nwhich is the first to exploit the diversity of web-crawled images for\ngeneralizable semantic segmentation. To explore and exploit the real-world data\ndistributions, we collect a web-crawled dataset which presents large diversity\nin terms of weather conditions, sites, lighting, camera styles, etc. We also\npresent a method which injects the style representation of the web-crawled data\ninto the source domain on-the-fly during training, which enables the network to\nexperience images of diverse styles with reliable labels for effective\ntraining. Moreover, we use the web-crawled dataset with predicted pseudo labels\nfor training to further enhance the capability of the network. Extensive\nexperiments demonstrate that our method clearly outperforms existing domain\ngeneralization techniques.\n","authors":["Namyup Kim","Taeyoung Son","Jaehyun Pahk","Cuiling Lan","Wenjun Zeng","Suha Kwak"],"pdf_url":"https://arxiv.org/pdf/2109.14196v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.14935v1","updated":"2023-03-27T06:31:21Z","published":"2023-03-27T06:31:21Z","title":"TabIQA: Table Questions Answering on Business Document Images","summary":"  Table answering questions from business documents has many challenges that\nrequire understanding tabular structures, cross-document referencing, and\nadditional numeric computations beyond simple search queries. This paper\nintroduces a novel pipeline, named TabIQA, to answer questions about business\ndocument images. TabIQA combines state-of-the-art deep learning techniques 1)\nto extract table content and structural information from images and 2) to\nanswer various questions related to numerical data, text-based information, and\ncomplex queries from structured tables. The evaluation results on VQAonBD 2023\ndataset demonstrate the effectiveness of TabIQA in achieving promising\nperformance in answering table-related questions. The TabIQA repository is\navailable at https://github.com/phucty/itabqa.\n","authors":["Phuc Nguyen","Nam Tuan Ly","Hideaki Takeda","Atsuhiro Takasu"],"pdf_url":"https://arxiv.org/pdf/2303.14935v1.pdf","comment":"First two authors contributed equally"},{"id":"http://arxiv.org/abs/2301.01482v4","updated":"2023-03-27T06:23:07Z","published":"2023-01-04T08:22:34Z","title":"Sample Imbalance Adjustment and Similar Object Exclusion in Underwater\n  Object Tracking","summary":"  Although modern trackers exhibit competitive performance for underwater image\ndegradation assessment, two problems remain when these are applied to\nunderwater object tracking (UOT). A single-object tracker is trained on\nopen-air datasets, which results in a serious sample imbalance between\nunderwater objects and open-air objects when it is applied to UOT. Moreover,\nunderwater targets such as fish and dolphins usually have a similar appearance,\nand it is challenging for models to discriminate weak discriminative features.\nExisting detection-based post-processing approaches struggle to distinguish a\ntracked target from similar objects. In this study, the UOSTrack is proposed,\nwhich involves the use of underwater images and open-air sequence hybrid\ntraining (UOHT), and motion-based post-processing (MBPP). The UOHT training\nparadigm is designed to train the sample-imbalanced underwater tracker. In\nparticular, underwater object detection (UOD) images are converted into image\npairs through customised data augmentation, such that the tracker is exposed to\nmore underwater domain training samples and learns the feature expressions of\nunderwater objects. The MBPP paradigm is proposed to exclude similar objects\nnear the target. In particular, it employs the estimation box predicted using a\nKalman filter and the candidate boxes in each frame to reconfirm the tracked\ntarget that is hidden in the candidate area when it has been lost. UOSTrack\nprovides an average performance improvement of 3.5 % compared to OSTrack on\nsimilar object challenge attribute in UOT100 and UTB180. The average\nperformance improvements provided by UOSTrack are 1 % and 3 %, respectively.\nThe results from two UOT benchmarks demonstrate that UOSTrack sets a new\nstate-of-the-art benchmark, and the effectiveness of UOHT and MBPP, and the\ngeneralisation and applicability of the MBPP for use in UOT.\n","authors":["Yunfeng Li","Bo Wang","Ye Li","Wei Huo","Zhuoyan Liu"],"pdf_url":"https://arxiv.org/pdf/2301.01482v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2211.12018v2","updated":"2023-03-27T06:20:51Z","published":"2022-11-22T05:21:21Z","title":"Level-S$^2$fM: Structure from Motion on Neural Level Set of Implicit\n  Surfaces","summary":"  This paper presents a neural incremental Structure-from-Motion (SfM)\napproach, Level-S$^2$fM, which estimates the camera poses and scene geometry\nfrom a set of uncalibrated images by learning coordinate MLPs for the implicit\nsurfaces and the radiance fields from the established keypoint correspondences.\nOur novel formulation poses some new challenges due to inevitable two-view and\nfew-view configurations in the incremental SfM pipeline, which complicates the\noptimization of coordinate MLPs for volumetric neural rendering with unknown\ncamera poses. Nevertheless, we demonstrate that the strong inductive basis\nconveying in the 2D correspondences is promising to tackle those challenges by\nexploiting the relationship between the ray sampling schemes. Based on this, we\nrevisit the pipeline of incremental SfM and renew the key components, including\ntwo-view geometry initialization, the camera poses registration, the 3D points\ntriangulation, and Bundle Adjustment, with a fresh perspective based on neural\nimplicit surfaces. By unifying the scene geometry in small MLP networks through\ncoordinate MLPs, our Level-S$^2$fM treats the zero-level set of the implicit\nsurface as an informative top-down regularization to manage the reconstructed\n3D points, reject the outliers in correspondences via querying SDF, and refine\nthe estimated geometries by NBA (Neural BA). Not only does our Level-S$^2$fM\nlead to promising results on camera pose estimation and scene geometry\nreconstruction, but it also shows a promising way for neural implicit rendering\nwithout knowing camera extrinsic beforehand.\n","authors":["Yuxi Xiao","Nan Xue","Tianfu Wu","Gui-Song Xia"],"pdf_url":"https://arxiv.org/pdf/2211.12018v2.pdf","comment":"camera-ready version (CVPR 2023). Project page:\n  https://henry123-boy.github.io/level-s2fm/"},{"id":"http://arxiv.org/abs/2303.14934v1","updated":"2023-03-27T06:18:20Z","published":"2023-03-27T06:18:20Z","title":"Spatially Adaptive Self-Supervised Learning for Real-World Image\n  Denoising","summary":"  Significant progress has been made in self-supervised image denoising (SSID)\nin the recent few years. However, most methods focus on dealing with spatially\nindependent noise, and they have little practicality on real-world sRGB images\nwith spatially correlated noise. Although pixel-shuffle downsampling has been\nsuggested for breaking the noise correlation, it breaks the original\ninformation of images, which limits the denoising performance. In this paper,\nwe propose a novel perspective to solve this problem, i.e., seeking for\nspatially adaptive supervision for real-world sRGB image denoising.\nSpecifically, we take into account the respective characteristics of flat and\ntextured regions in noisy images, and construct supervisions for them\nseparately. For flat areas, the supervision can be safely derived from\nnon-adjacent pixels, which are much far from the current pixel for excluding\nthe influence of the noise-correlated ones. And we extend the blind-spot\nnetwork to a blind-neighborhood network (BNN) for providing supervision on flat\nareas. For textured regions, the supervision has to be closely related to the\ncontent of adjacent pixels. And we present a locally aware network (LAN) to\nmeet the requirement, while LAN itself is selectively supervised with the\noutput of BNN. Combining these two supervisions, a denoising network (e.g.,\nU-Net) can be well-trained. Extensive experiments show that our method performs\nfavorably against state-of-the-art SSID methods on real-world sRGB photographs.\nThe code is available at https://github.com/nagejacob/SpatiallyAdaptiveSSID.\n","authors":["Junyi Li","Zhilu Zhang","Xiaoyu Liu","Chaoyu Feng","Xiaotao Wang","Lei Lei","Wangmeng Zuo"],"pdf_url":"https://arxiv.org/pdf/2303.14934v1.pdf","comment":"CVPR 2023 Camera Ready"},{"id":"http://arxiv.org/abs/2210.16046v2","updated":"2023-03-27T06:17:13Z","published":"2022-10-28T10:33:45Z","title":"Rawgment: Noise-Accounted RAW Augmentation Enables Recognition in a Wide\n  Variety of Environments","summary":"  Image recognition models that work in challenging environments (e.g.,\nextremely dark, blurry, or high dynamic range conditions) must be useful.\nHowever, creating training datasets for such environments is expensive and hard\ndue to the difficulties of data collection and annotation. It is desirable if\nwe could get a robust model without the need for hard-to-obtain datasets. One\nsimple approach is to apply data augmentation such as color jitter and blur to\nstandard RGB (sRGB) images in simple scenes. Unfortunately, this approach\nstruggles to yield realistic images in terms of pixel intensity and noise\ndistribution due to not considering the non-linearity of Image Signal\nProcessors (ISPs) and noise characteristics of image sensors. Instead, we\npropose a noise-accounted RAW image augmentation method. In essence, color\njitter and blur augmentation are applied to a RAW image before applying\nnon-linear ISP, resulting in realistic intensity. Furthermore, we introduce a\nnoise amount alignment method that calibrates the domain gap in the noise\nproperty caused by the augmentation. We show that our proposed noise-accounted\nRAW augmentation method doubles the image recognition accuracy in challenging\nenvironments only with simple training data.\n","authors":["Masakazu Yoshimura","Junji Otsuka","Atsushi Irie","Takeshi Ohashi"],"pdf_url":"https://arxiv.org/pdf/2210.16046v2.pdf","comment":"Accepted to CVPR2023"},{"id":"http://arxiv.org/abs/2303.14933v1","updated":"2023-03-27T06:17:10Z","published":"2023-03-27T06:17:10Z","title":"MD-VQA: Multi-Dimensional Quality Assessment for UGC Live Videos","summary":"  User-generated content (UGC) live videos are often bothered by various\ndistortions during capture procedures and thus exhibit diverse visual\nqualities. Such source videos are further compressed and transcoded by media\nserver providers before being distributed to end-users. Because of the\nflourishing of UGC live videos, effective video quality assessment (VQA) tools\nare needed to monitor and perceptually optimize live streaming videos in the\ndistributing process. In this paper, we address \\textbf{UGC Live VQA} problems\nby constructing a first-of-a-kind subjective UGC Live VQA database and\ndeveloping an effective evaluation tool. Concretely, 418 source UGC videos are\ncollected in real live streaming scenarios and 3,762 compressed ones at\ndifferent bit rates are generated for the subsequent subjective VQA\nexperiments. Based on the built database, we develop a\n\\underline{M}ulti-\\underline{D}imensional \\underline{VQA} (\\textbf{MD-VQA})\nevaluator to measure the visual quality of UGC live videos from semantic,\ndistortion, and motion aspects respectively. Extensive experimental results\nshow that MD-VQA achieves state-of-the-art performance on both our UGC Live VQA\ndatabase and existing compressed UGC VQA databases.\n","authors":["Zicheng Zhang","Wei Wu","Wei Sun","Dangyang Tu","Wei Lu","Xiongkuo Min","Ying Chen","Guangtao Zhai"],"pdf_url":"https://arxiv.org/pdf/2303.14933v1.pdf","comment":"Accepted to CVPR2023"},{"id":"http://arxiv.org/abs/2303.12371v2","updated":"2023-03-27T06:15:09Z","published":"2023-03-22T08:14:23Z","title":"$P^{3}O$: Transferring Visual Representations for Reinforcement Learning\n  via Prompting","summary":"  It is important for deep reinforcement learning (DRL) algorithms to transfer\ntheir learned policies to new environments that have different visual inputs.\nIn this paper, we introduce Prompt based Proximal Policy Optimization\n($P^{3}O$), a three-stage DRL algorithm that transfers visual representations\nfrom a target to a source environment by applying prompting. The process of\n$P^{3}O$ consists of three stages: pre-training, prompting, and predicting. In\nparticular, we specify a prompt-transformer for representation conversion and\npropose a two-step training process to train the prompt-transformer for the\ntarget environment, while the rest of the DRL pipeline remains unchanged. We\nimplement $P^{3}O$ and evaluate it on the OpenAI CarRacing video game. The\nexperimental results show that $P^{3}O$ outperforms the state-of-the-art visual\ntransferring schemes. In particular, $P^{3}O$ allows the learned policies to\nperform well in environments with different visual inputs, which is much more\neffective than retraining the policies in these environments.\n","authors":["Guoliang You","Xiaomeng Chu","Yifan Duan","Jie Peng","Jianmin Ji","Yu Zhang","Yanyong Zhang"],"pdf_url":"https://arxiv.org/pdf/2303.12371v2.pdf","comment":"This paper has been accepted to be presented at the upcoming IEEE\n  International Conference on Multimedia & Expo (ICME) in 2023"},{"id":"http://arxiv.org/abs/2303.14930v1","updated":"2023-03-27T06:11:28Z","published":"2023-03-27T06:11:28Z","title":"Addressing the Challenges of Open-World Object Detection","summary":"  We address the challenging problem of open world object detection (OWOD),\nwhere object detectors must identify objects from known classes while also\nidentifying and continually learning to detect novel objects. Prior work has\nresulted in detectors that have a relatively low ability to detect novel\nobjects, and a high likelihood of classifying a novel object as one of the\nknown classes. We approach the problem by identifying the three main challenges\nthat OWOD presents and introduce OW-RCNN, an open world object detector that\naddresses each of these three challenges. OW-RCNN establishes a new state of\nthe art using the open-world evaluation protocol on MS-COCO, showing a\ndrastically increased ability to detect novel objects (16-21% absolute increase\nin U-Recall), to avoid their misclassification as one of the known classes (up\nto 52% reduction in A-OSE), and to incrementally learn to detect them while\nmaintaining performance on previously known classes (1-6% absolute increase in\nmAP).\n","authors":["David Pershouse","Feras Dayoub","Dimity Miller","Niko Sünderhauf"],"pdf_url":"https://arxiv.org/pdf/2303.14930v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.13800v2","updated":"2023-03-27T05:53:10Z","published":"2023-03-24T04:45:45Z","title":"Aligning Step-by-Step Instructional Diagrams to Video Demonstrations","summary":"  Multimodal alignment facilitates the retrieval of instances from one modality\nwhen queried using another. In this paper, we consider a novel setting where\nsuch an alignment is between (i) instruction steps that are depicted as\nassembly diagrams (commonly seen in Ikea assembly manuals) and (ii) video\nsegments from in-the-wild videos; these videos comprising an enactment of the\nassembly actions in the real world. To learn this alignment, we introduce a\nnovel supervised contrastive learning method that learns to align videos with\nthe subtle details in the assembly diagrams, guided by a set of novel losses.\nTo study this problem and demonstrate the effectiveness of our method, we\nintroduce a novel dataset: IAW for Ikea assembly in the wild consisting of 183\nhours of videos from diverse furniture assembly collections and nearly 8,300\nillustrations from their associated instruction manuals and annotated for their\nground truth alignments. We define two tasks on this dataset: First, nearest\nneighbor retrieval between video segments and illustrations, and, second,\nalignment of instruction steps and the segments for each video. Extensive\nexperiments on IAW demonstrate superior performances of our approach against\nalternatives.\n","authors":["Jiahao Zhang","Anoop Cherian","Yanbin Liu","Yizhak Ben-Shabat","Cristian Rodriguez","Stephen Gould"],"pdf_url":"https://arxiv.org/pdf/2303.13800v2.pdf","comment":"Project website:\n  https://academic.davidz.cn/en/publication/zhang-cvpr-2023/"},{"id":"http://arxiv.org/abs/2303.14926v1","updated":"2023-03-27T05:53:01Z","published":"2023-03-27T05:53:01Z","title":"Continuous Intermediate Token Learning with Implicit Motion Manifold for\n  Keyframe Based Motion Interpolation","summary":"  Deriving sophisticated 3D motions from sparse keyframes is a particularly\nchallenging problem, due to continuity and exceptionally skeletal precision.\nThe action features are often derivable accurately from the full series of\nkeyframes, and thus, leveraging the global context with transformers has been a\npromising data-driven embedding approach. However, existing methods are often\nwith inputs of interpolated intermediate frame for continuity using basic\ninterpolation methods with keyframes, which result in a trivial local minimum\nduring training. In this paper, we propose a novel framework to formulate\nlatent motion manifolds with keyframe-based constraints, from which the\ncontinuous nature of intermediate token representations is considered.\nParticularly, our proposed framework consists of two stages for identifying a\nlatent motion subspace, i.e., a keyframe encoding stage and an intermediate\ntoken generation stage, and a subsequent motion synthesis stage to extrapolate\nand compose motion data from manifolds. Through our extensive experiments\nconducted on both the LaFAN1 and CMU Mocap datasets, our proposed method\ndemonstrates both superior interpolation accuracy and high visual similarity to\nground truth motions.\n","authors":["Clinton Ansun Mo","Kun Hu","Chengjiang Long","Zhiyong Wang"],"pdf_url":"https://arxiv.org/pdf/2303.14926v1.pdf","comment":"Accepted by CVPR 2023"}],"Information Retrieval":[{"id":"http://arxiv.org/abs/2211.09832v2","updated":"2023-03-27T17:45:19Z","published":"2022-11-17T19:00:24Z","title":"Latent User Intent Modeling for Sequential Recommenders","summary":"  Sequential recommender models are essential components of modern industrial\nrecommender systems. These models learn to predict the next items a user is\nlikely to interact with based on his/her interaction history on the platform.\nMost sequential recommenders however lack a higher-level understanding of user\nintents, which often drive user behaviors online. Intent modeling is thus\ncritical for understanding users and optimizing long-term user experience. We\npropose a probabilistic modeling approach and formulate user intent as latent\nvariables, which are inferred based on user behavior signals using variational\nautoencoders (VAE). The recommendation policy is then adjusted accordingly\ngiven the inferred user intent. We demonstrate the effectiveness of the latent\nuser intent modeling via offline analyses as well as live experiments on a\nlarge-scale industrial recommendation platform.\n","authors":["Bo Chang","Alexandros Karatzoglou","Yuyan Wang","Can Xu","Ed H. Chi","Minmin Chen"],"pdf_url":"https://arxiv.org/pdf/2211.09832v2.pdf","comment":"The Web Conference 2023, Industry Track"},{"id":"http://arxiv.org/abs/2303.15350v1","updated":"2023-03-27T16:07:44Z","published":"2023-03-27T16:07:44Z","title":"Improving Neural Topic Models with Wasserstein Knowledge Distillation","summary":"  Topic modeling is a dominant method for exploring document collections on the\nweb and in digital libraries. Recent approaches to topic modeling use\npretrained contextualized language models and variational autoencoders.\nHowever, large neural topic models have a considerable memory footprint. In\nthis paper, we propose a knowledge distillation framework to compress a\ncontextualized topic model without loss in topic quality. In particular, the\nproposed distillation objective is to minimize the cross-entropy of the soft\nlabels produced by the teacher and the student models, as well as to minimize\nthe squared 2-Wasserstein distance between the latent distributions learned by\nthe two models. Experiments on two publicly available datasets show that the\nstudent trained with knowledge distillation achieves topic coherence much\nhigher than that of the original student model, and even surpasses the teacher\nwhile containing far fewer parameters than the teacher's. The distilled model\nalso outperforms several other competitive topic models on topic coherence.\n","authors":["Suman Adhya","Debarshi Kumar Sanyal"],"pdf_url":"https://arxiv.org/pdf/2303.15350v1.pdf","comment":"Accepted at ECIR 2023"},{"id":"http://arxiv.org/abs/2303.15247v1","updated":"2023-03-27T14:31:25Z","published":"2023-03-27T14:31:25Z","title":"Zero-Shot Composed Image Retrieval with Textual Inversion","summary":"  Composed Image Retrieval (CIR) aims to retrieve a target image based on a\nquery composed of a reference image and a relative caption that describes the\ndifference between the two images. The high effort and cost required for\nlabeling datasets for CIR hamper the widespread usage of existing methods, as\nthey rely on supervised learning. In this work, we propose a new task,\nZero-Shot CIR (ZS-CIR), that aims to address CIR without requiring a labeled\ntraining dataset. Our approach, named zero-Shot composEd imAge Retrieval with\ntextuaL invErsion (SEARLE), maps the visual features of the reference image\ninto a pseudo-word token in CLIP token embedding space and integrates it with\nthe relative caption. To support research on ZS-CIR, we introduce an\nopen-domain benchmarking dataset named Composed Image Retrieval on Common\nObjects in context (CIRCO), which is the first dataset for CIR containing\nmultiple ground truths for each query. The experiments show that SEARLE\nexhibits better performance than the baselines on the two main datasets for CIR\ntasks, FashionIQ and CIRR, and on the proposed CIRCO. The dataset, the code and\nthe model are publicly available at https://github.com/miccunifi/SEARLE .\n","authors":["Alberto Baldrati","Lorenzo Agnolucci","Marco Bertini","Alberto Del Bimbo"],"pdf_url":"https://arxiv.org/pdf/2303.15247v1.pdf","comment":"SEARLE repository: https://github.com/miccunifi/SEARLE . CIRCO\n  repository: https://github.com/miccunifi/CIRCO"},{"id":"http://arxiv.org/abs/2303.15036v1","updated":"2023-03-27T09:33:16Z","published":"2023-03-27T09:33:16Z","title":"Retrievability in an Integrated Retrieval System: An Extended Study","summary":"  Retrievability measures the influence a retrieval system has on the access to\ninformation in a given collection of items. This measure can help in making an\nevaluation of the search system based on which insights can be drawn. In this\npaper, we investigate the retrievability in an integrated search system\nconsisting of items from various categories, particularly focussing on\ndatasets, publications \\ijdl{and variables} in a real-life Digital Library\n(DL). The traditional metrics, that is, the Lorenz curve and Gini coefficient,\nare employed to visualize the diversity in retrievability scores of the\n\\ijdl{three} retrievable document types (specifically datasets, publications,\nand variables). Our results show a significant popularity bias with certain\nitems being retrieved more often than others. Particularly, it has been shown\nthat certain datasets are more likely to be retrieved than other datasets in\nthe same category. In contrast, the retrievability scores of items from the\nvariable or publication category are more evenly distributed. We have observed\nthat the distribution of document retrievability is more diverse for datasets\nas compared to publications and variables.\n","authors":["Dwaipayan Roy","Zeljko Carevic","Philipp Mayr"],"pdf_url":"https://arxiv.org/pdf/2303.15036v1.pdf","comment":"To appear in International Journal on Digital Libraries (IJDL). arXiv\n  admin note: substantial text overlap with arXiv:2205.00937"},{"id":"http://arxiv.org/abs/2303.15016v1","updated":"2023-03-27T08:59:55Z","published":"2023-03-27T08:59:55Z","title":"Borrowing Human Senses: Comment-Aware Self-Training for Social Media\n  Multimodal Classification","summary":"  Social media is daily creating massive multimedia content with paired image\nand text, presenting the pressing need to automate the vision and language\nunderstanding for various multimodal classification tasks. Compared to the\ncommonly researched visual-lingual data, social media posts tend to exhibit\nmore implicit image-text relations. To better glue the cross-modal semantics\ntherein, we capture hinting features from user comments, which are retrieved\nvia jointly leveraging visual and lingual similarity. Afterwards, the\nclassification tasks are explored via self-training in a teacher-student\nframework, motivated by the usually limited labeled data scales in existing\nbenchmarks. Substantial experiments are conducted on four multimodal social\nmedia benchmarks for image text relation classification, sarcasm detection,\nsentiment classification, and hate speech detection. The results show that our\nmethod further advances the performance of previous state-of-the-art models,\nwhich do not employ comment modeling or self-training.\n","authors":["Chunpu Xu","Jing Li"],"pdf_url":"https://arxiv.org/pdf/2303.15016v1.pdf","comment":"accepted to EMNLP 2022"},{"id":"http://arxiv.org/abs/2303.14991v1","updated":"2023-03-27T08:34:42Z","published":"2023-03-27T08:34:42Z","title":"Empowering Dual-Encoder with Query Generator for Cross-Lingual Dense\n  Retrieval","summary":"  In monolingual dense retrieval, lots of works focus on how to distill\nknowledge from cross-encoder re-ranker to dual-encoder retriever and these\nmethods achieve better performance due to the effectiveness of cross-encoder\nre-ranker. However, we find that the performance of the cross-encoder re-ranker\nis heavily influenced by the number of training samples and the quality of\nnegative samples, which is hard to obtain in the cross-lingual setting. In this\npaper, we propose to use a query generator as the teacher in the cross-lingual\nsetting, which is less dependent on enough training samples and high-quality\nnegative samples. In addition to traditional knowledge distillation, we further\npropose a novel enhancement method, which uses the query generator to help the\ndual-encoder align queries from different languages, but does not need any\nadditional parallel sentences. The experimental results show that our method\noutperforms the state-of-the-art methods on two benchmark datasets.\n","authors":["Houxing Ren","Linjun Shou","Ning Wu","Ming Gong","Daxin Jiang"],"pdf_url":"https://arxiv.org/pdf/2303.14991v1.pdf","comment":"EMNLP 2022 main conference"},{"id":"http://arxiv.org/abs/2303.14979v1","updated":"2023-03-27T08:22:21Z","published":"2023-03-27T08:22:21Z","title":"Lexicon-Enhanced Self-Supervised Training for Multilingual Dense\n  Retrieval","summary":"  Recent multilingual pre-trained models have shown better performance in\nvarious multilingual tasks. However, these models perform poorly on\nmultilingual retrieval tasks due to lacking multilingual training data. In this\npaper, we propose to mine and generate self-supervised training data based on a\nlarge-scale unlabeled corpus. We carefully design a mining method which\ncombines the sparse and dense models to mine the relevance of unlabeled queries\nand passages. And we introduce a query generator to generate more queries in\ntarget languages for unlabeled passages. Through extensive experiments on Mr.\nTYDI dataset and an industrial dataset from a commercial search engine, we\ndemonstrate that our method performs better than baselines based on various\npre-trained multilingual models. Our method even achieves on-par performance\nwith the supervised method on the latter dataset.\n","authors":["Houxing Ren","Linjun Shou","Jian Pei","Ning Wu","Ming Gong","Daxin Jiang"],"pdf_url":"https://arxiv.org/pdf/2303.14979v1.pdf","comment":"EMNLP 2022 Findings"},{"id":"http://arxiv.org/abs/2303.15595v1","updated":"2023-03-27T20:54:49Z","published":"2023-03-27T20:54:49Z","title":"Model Cascades for Efficient Image Search","summary":"  Modern neural encoders offer unprecedented text-image retrieval (TIR)\naccuracy. However, their high computational cost impedes an adoption to\nlarge-scale image searches. We propose a novel image ranking algorithm that\nuses a cascade of increasingly powerful neural encoders to progressively filter\nimages by how well they match a given text. Our algorithm reduces lifetime TIR\ncosts by over 3x.\n","authors":["Robert Hönig","Mingyuan Chi"],"pdf_url":"https://arxiv.org/pdf/2303.15595v1.pdf","comment":"Under review as a short paper at the DEEM '23 workshop"}],"Machine Learning":[{"id":"http://arxiv.org/abs/2303.15443v1","updated":"2023-03-27T17:59:34Z","published":"2023-03-27T17:59:34Z","title":"GeoNet: Benchmarking Unsupervised Adaptation across Geographies","summary":"  In recent years, several efforts have been aimed at improving the robustness\nof vision models to domains and environments unseen during training. An\nimportant practical problem pertains to models deployed in a new geography that\nis under-represented in the training dataset, posing a direct challenge to fair\nand inclusive computer vision. In this paper, we study the problem of\ngeographic robustness and make three main contributions. First, we introduce a\nlarge-scale dataset GeoNet for geographic adaptation containing benchmarks\nacross diverse tasks like scene recognition (GeoPlaces), image classification\n(GeoImNet) and universal adaptation (GeoUniDA). Second, we investigate the\nnature of distribution shifts typical to the problem of geographic adaptation\nand hypothesize that the major source of domain shifts arise from significant\nvariations in scene context (context shift), object design (design shift) and\nlabel distribution (prior shift) across geographies. Third, we conduct an\nextensive evaluation of several state-of-the-art unsupervised domain adaptation\nalgorithms and architectures on GeoNet, showing that they do not suffice for\ngeographical adaptation, and that large-scale pre-training using large vision\nmodels also does not lead to geographic robustness. Our dataset is publicly\navailable at https://tarun005.github.io/GeoNet.\n","authors":["Tarun Kalluri","Wangdong Xu","Manmohan Chandraker"],"pdf_url":"https://arxiv.org/pdf/2303.15443v1.pdf","comment":"CVPR 2023 Camera Ready. Project Page:\n  https://tarun005.github.io/GeoNet"},{"id":"http://arxiv.org/abs/2303.15441v1","updated":"2023-03-27T17:59:33Z","published":"2023-03-27T17:59:33Z","title":"Zero-shot Model Diagnosis","summary":"  When it comes to deploying deep vision models, the behavior of these systems\nmust be explicable to ensure confidence in their reliability and fairness. A\ncommon approach to evaluate deep learning models is to build a labeled test set\nwith attributes of interest and assess how well it performs. However, creating\na balanced test set (i.e., one that is uniformly sampled over all the important\ntraits) is often time-consuming, expensive, and prone to mistakes. The question\nwe try to address is: can we evaluate the sensitivity of deep learning models\nto arbitrary visual attributes without an annotated test set? This paper argues\nthe case that Zero-shot Model Diagnosis (ZOOM) is possible without the need for\na test set nor labeling. To avoid the need for test sets, our system relies on\na generative model and CLIP. The key idea is enabling the user to select a set\nof prompts (relevant to the problem) and our system will automatically search\nfor semantic counterfactual images (i.e., synthesized images that flip the\nprediction in the case of a binary classifier) using the generative model. We\nevaluate several visual tasks (classification, key-point detection, and\nsegmentation) in multiple visual domains to demonstrate the viability of our\nmethodology. Extensive experiments demonstrate that our method is capable of\nproducing counterfactual images and offering sensitivity analysis for model\ndiagnosis without the need for a test set.\n","authors":["Jinqi Luo","Zhaoning Wang","Chen Henry Wu","Dong Huang","Fernando De la Torre"],"pdf_url":"https://arxiv.org/pdf/2303.15441v1.pdf","comment":"Accepted in CVPR 2023"},{"id":"http://arxiv.org/abs/2303.15438v1","updated":"2023-03-27T17:59:20Z","published":"2023-03-27T17:59:20Z","title":"On the stepwise nature of self-supervised learning","summary":"  We present a simple picture of the training process of self-supervised\nlearning methods with joint embedding networks. We find that these methods\nlearn their high-dimensional embeddings one dimension at a time in a sequence\nof discrete, well-separated steps. We arrive at this conclusion via the study\nof a linearized model of Barlow Twins applicable to the case in which the\ntrained network is infinitely wide. We solve the training dynamics of this\nmodel from small initialization, finding that the model learns the top\neigenmodes of a certain contrastive kernel in a stepwise fashion, and obtain a\nclosed-form expression for the final learned representations. Remarkably, we\nthen see the same stepwise learning phenomenon when training deep ResNets using\nthe Barlow Twins, SimCLR, and VICReg losses. Our theory suggests that, just as\nkernel regression can be thought of as a model of supervised learning,\n\\textit{kernel PCA} may serve as a useful model of self-supervised learning.\n","authors":["James B. Simon","Maksis Knutins","Liu Ziyin","Daniel Geisz","Abraham J. Fetterman","Joshua Albrecht"],"pdf_url":"https://arxiv.org/pdf/2303.15438v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2006.12293v2","updated":"2023-03-27T17:58:34Z","published":"2020-06-22T14:26:55Z","title":"DeepTopPush: Simple and Scalable Method for Accuracy at the Top","summary":"  Accuracy at the top is a special class of binary classification problems\nwhere the performance is evaluated only on a small number of relevant (top)\nsamples. Applications include information retrieval systems or processes with\nmanual (expensive) postprocessing. This leads to minimizing the number of\nirrelevant samples above a threshold. We consider classifiers in the form of an\narbitrary (deep) network and propose a new method DeepTopPush for minimizing\nthe loss function at the top. Since the threshold depends on all samples, the\nproblem is non-decomposable. We modify the stochastic gradient descent to\nhandle the non-decomposability in an end-to-end training manner and propose a\nway to estimate the threshold only from values on the current minibatch and one\ndelayed value. We demonstrate the excellent performance of DeepTopPush on\nvisual recognition datasets and two real-world applications. The first one\nselects a small number of molecules for further drug testing. The second one\nuses real malware data, where we detected 46\\% malware at an extremely low\nfalse alarm rate of $10^{-5}$.\n","authors":["Václav Mácha","Lukáš Adam","Václav Šmídl"],"pdf_url":"https://arxiv.org/pdf/2006.12293v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.15433v1","updated":"2023-03-27T17:55:44Z","published":"2023-03-27T17:55:44Z","title":"Anti-DreamBooth: Protecting users from personalized text-to-image\n  synthesis","summary":"  Text-to-image diffusion models are nothing but a revolution, allowing anyone,\neven without design skills, to create realistic images from simple text inputs.\nWith powerful personalization tools like DreamBooth, they can generate images\nof a specific person just by learning from his/her few reference images.\nHowever, when misused, such a powerful and convenient tool can produce fake\nnews or disturbing content targeting any individual victim, posing a severe\nnegative social impact. In this paper, we explore a defense system called\nAnti-DreamBooth against such malicious use of DreamBooth. The system aims to\nadd subtle noise perturbation to each user's image before publishing in order\nto disrupt the generation quality of any DreamBooth model trained on these\nperturbed images. We investigate a wide range of algorithms for perturbation\noptimization and extensively evaluate them on two facial datasets over various\ntext-to-image model versions. Despite the complicated formulation of DreamBooth\nand Diffusion-based text-to-image models, our methods effectively defend users\nfrom the malicious use of those models. Their effectiveness withstands even\nadverse conditions, such as model or prompt/term mismatching between training\nand testing. Our code will be available at\n\\href{https://github.com/VinAIResearch/Anti-DreamBooth.git}{https://github.com/VinAIResearch/Anti-DreamBooth.git}.\n","authors":["Thanh Van Le","Hao Phung","Thuan Hoang Nguyen","Quan Dao","Ngoc Tran","Anh Tran"],"pdf_url":"https://arxiv.org/pdf/2303.15433v1.pdf","comment":"Project page: https://anti-dreambooth.github.io/"},{"id":"http://arxiv.org/abs/2303.15430v1","updated":"2023-03-27T17:54:32Z","published":"2023-03-27T17:54:32Z","title":"TextMI: Textualize Multimodal Information for Integrating Non-verbal\n  Cues in Pre-trained Language Models","summary":"  Pre-trained large language models have recently achieved ground-breaking\nperformance in a wide variety of language understanding tasks. However, the\nsame model can not be applied to multimodal behavior understanding tasks (e.g.,\nvideo sentiment/humor detection) unless non-verbal features (e.g., acoustic and\nvisual) can be integrated with language. Jointly modeling multiple modalities\nsignificantly increases the model complexity, and makes the training process\ndata-hungry. While an enormous amount of text data is available via the web,\ncollecting large-scale multimodal behavioral video datasets is extremely\nexpensive, both in terms of time and money. In this paper, we investigate\nwhether large language models alone can successfully incorporate non-verbal\ninformation when they are presented in textual form. We present a way to\nconvert the acoustic and visual information into corresponding textual\ndescriptions and concatenate them with the spoken text. We feed this augmented\ninput to a pre-trained BERT model and fine-tune it on three downstream\nmultimodal tasks: sentiment, humor, and sarcasm detection. Our approach,\nTextMI, significantly reduces model complexity, adds interpretability to the\nmodel's decision, and can be applied for a diverse set of tasks while achieving\nsuperior (multimodal sarcasm detection) or near SOTA (multimodal sentiment\nanalysis and multimodal humor detection) performance. We propose TextMI as a\ngeneral, competitive baseline for multimodal behavioral analysis tasks,\nparticularly in a low-resource setting.\n","authors":["Md Kamrul Hasan","Md Saiful Islam","Sangwu Lee","Wasifur Rahman","Iftekhar Naim","Mohammed Ibrahim Khan","Ehsan Hoque"],"pdf_url":"https://arxiv.org/pdf/2303.15430v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2202.13603v2","updated":"2023-03-27T17:52:52Z","published":"2022-02-28T08:25:26Z","title":"Optimal Online Generalized Linear Regression with Stochastic Noise and\n  Its Application to Heteroscedastic Bandits","summary":"  We study the problem of online generalized linear regression in the\nstochastic setting, where the label is generated from a generalized linear\nmodel with possibly unbounded additive noise. We provide a sharp analysis of\nthe classical follow-the-regularized-leader (FTRL) algorithm to cope with the\nlabel noise. More specifically, for $\\sigma$-sub-Gaussian label noise, our\nanalysis provides a regret upper bound of $O(\\sigma^2 d \\log T) + o(\\log T)$,\nwhere $d$ is the dimension of the input vector, $T$ is the total number of\nrounds. We also prove a $\\Omega(\\sigma^2d\\log(T/d))$ lower bound for stochastic\nonline linear regression, which indicates that our upper bound is nearly\noptimal. In addition, we extend our analysis to a more refined Bernstein noise\ncondition. As an application, we study generalized linear bandits with\nheteroscedastic noise and propose an algorithm based on FTRL to achieve the\nfirst variance-aware regret bound.\n","authors":["Heyang Zhao","Dongruo Zhou","Jiafan He","Quanquan Gu"],"pdf_url":"https://arxiv.org/pdf/2202.13603v2.pdf","comment":"27 pages, 3 figures. In this updated version, we have changed the\n  paper title, added new theoretical results on the FTRL algorithm and mainly\n  focused on stochastic online regression. Refer to arXiv:2202.13603v1 for the\n  previous version, which contains more results on heteroscedastic nonlinear\n  bandits"},{"id":"http://arxiv.org/abs/2011.14033v5","updated":"2023-03-27T17:47:44Z","published":"2020-11-28T00:20:36Z","title":"A Tractable Online Learning Algorithm for the Multinomial Logit\n  Contextual Bandit","summary":"  In this paper, we consider the contextual variant of the MNL-Bandit problem.\nMore specifically, we consider a dynamic set optimization problem, where a\ndecision-maker offers a subset (assortment) of products to a consumer and\nobserves the response in every round. Consumers purchase products to maximize\ntheir utility. We assume that a set of attributes describe the products, and\nthe mean utility of a product is linear in the values of these attributes. We\nmodel consumer choice behavior using the widely used Multinomial Logit (MNL)\nmodel and consider the decision maker problem of dynamically learning the model\nparameters while optimizing cumulative revenue over the selling horizon $T$.\nThough this problem has attracted considerable attention in recent times, many\nexisting methods often involve solving an intractable non-convex optimization\nproblem. Their theoretical performance guarantees depend on a problem-dependent\nparameter which could be prohibitively large. In particular, existing\nalgorithms for this problem have regret bounded by $O(\\sqrt{\\kappa d T})$,\nwhere $\\kappa$ is a problem-dependent constant that can have an exponential\ndependency on the number of attributes. In this paper, we propose an optimistic\nalgorithm and show that the regret is bounded by $O(\\sqrt{dT} + \\kappa)$,\nsignificantly improving the performance over existing methods. Further, we\npropose a convex relaxation of the optimization step, which allows for\ntractable decision-making while retaining the favourable regret guarantee.\n","authors":["Priyank Agrawal","Theja Tulabandhula","Vashist Avadhanula"],"pdf_url":"https://arxiv.org/pdf/2011.14033v5.pdf","comment":"Accepted to be published at Elsevier European Journal of Operational\n  Research (EJOR)"},{"id":"http://arxiv.org/abs/2211.09832v2","updated":"2023-03-27T17:45:19Z","published":"2022-11-17T19:00:24Z","title":"Latent User Intent Modeling for Sequential Recommenders","summary":"  Sequential recommender models are essential components of modern industrial\nrecommender systems. These models learn to predict the next items a user is\nlikely to interact with based on his/her interaction history on the platform.\nMost sequential recommenders however lack a higher-level understanding of user\nintents, which often drive user behaviors online. Intent modeling is thus\ncritical for understanding users and optimizing long-term user experience. We\npropose a probabilistic modeling approach and formulate user intent as latent\nvariables, which are inferred based on user behavior signals using variational\nautoencoders (VAE). The recommendation policy is then adjusted accordingly\ngiven the inferred user intent. We demonstrate the effectiveness of the latent\nuser intent modeling via offline analyses as well as live experiments on a\nlarge-scale industrial recommendation platform.\n","authors":["Bo Chang","Alexandros Karatzoglou","Yuyan Wang","Can Xu","Ed H. Chi","Minmin Chen"],"pdf_url":"https://arxiv.org/pdf/2211.09832v2.pdf","comment":"The Web Conference 2023, Industry Track"},{"id":"http://arxiv.org/abs/2303.15421v1","updated":"2023-03-27T17:43:57Z","published":"2023-03-27T17:43:57Z","title":"ACAT: Adversarial Counterfactual Attention for Classification and\n  Detection in Medical Imaging","summary":"  In some medical imaging tasks and other settings where only small parts of\nthe image are informative for the classification task, traditional CNNs can\nsometimes struggle to generalise. Manually annotated Regions of Interest (ROI)\nare sometimes used to isolate the most informative parts of the image. However,\nthese are expensive to collect and may vary significantly across annotators. To\novercome these issues, we propose a framework that employs saliency maps to\nobtain soft spatial attention masks that modulate the image features at\ndifferent scales. We refer to our method as Adversarial Counterfactual\nAttention (ACAT). ACAT increases the baseline classification accuracy of\nlesions in brain CT scans from 71.39% to 72.55% and of COVID-19 related\nfindings in lung CT scans from 67.71% to 70.84% and exceeds the performance of\ncompeting methods. We investigate the best way to generate the saliency maps\nemployed in our architecture and propose a way to obtain them from\nadversarially generated counterfactual images. They are able to isolate the\narea of interest in brain and lung CT scans without using any manual\nannotations. In the task of localising the lesion location out of 6 possible\nregions, they obtain a score of 65.05% on brain CT scans, improving the score\nof 61.29% obtained with the best competing method.\n","authors":["Alessandro Fontanella","Antreas Antoniou","Wenwen Li","Joanna Wardlaw","Grant Mair","Emanuele Trucco","Amos Storkey"],"pdf_url":"https://arxiv.org/pdf/2303.15421v1.pdf","comment":"17 pages, 7 figures"},{"id":"http://arxiv.org/abs/2211.09107v2","updated":"2023-03-27T17:43:18Z","published":"2022-11-16T18:50:11Z","title":"Interpretable Few-shot Learning with Online Attribute Selection","summary":"  Few-shot learning (FSL) is a challenging learning problem in which only a few\nsamples are available for each class. Decision interpretation is more important\nin few-shot classification since there is a greater chance of error than in\ntraditional classification. However, most of the previous FSL methods are\nblack-box models. In this paper, we propose an inherently interpretable model\nfor FSL based on human-friendly attributes. Moreover, we propose an online\nattribute selection mechanism that can effectively filter out irrelevant\nattributes in each episode. The attribute selection mechanism improves the\naccuracy and helps with interpretability by reducing the number of participated\nattributes in each episode. We propose a mechanism that automatically detects\nthe episodes where the pool of human-friendly attributes are not adequate, and\ncompensates by engaging learned unknown attributes. We demonstrate that the\nproposed method achieves results on par with black-box few-shot-learning models\non four widely used datasets.\n","authors":["Mohammad Reza Zarei","Majid Komeili"],"pdf_url":"https://arxiv.org/pdf/2211.09107v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.15413v1","updated":"2023-03-27T17:31:13Z","published":"2023-03-27T17:31:13Z","title":"Debiasing Scores and Prompts of 2D Diffusion for Robust Text-to-3D\n  Generation","summary":"  The view inconsistency problem in score-distilling text-to-3D generation,\nalso known as the Janus problem, arises from the intrinsic bias of 2D diffusion\nmodels, which leads to the unrealistic generation of 3D objects. In this work,\nwe explore score-distilling text-to-3D generation and identify the main causes\nof the Janus problem. Based on these findings, we propose two approaches to\ndebias the score-distillation frameworks for robust text-to-3D generation. Our\nfirst approach, called score debiasing, involves gradually increasing the\ntruncation value for the score estimated by 2D diffusion models throughout the\noptimization process. Our second approach, called prompt debiasing, identifies\nconflicting words between user prompts and view prompts utilizing a language\nmodel and adjusts the discrepancy between view prompts and object-space camera\nposes. Our experimental results show that our methods improve realism by\nsignificantly reducing artifacts and achieve a good trade-off between\nfaithfulness to the 2D diffusion models and 3D consistency with little\noverhead.\n","authors":["Susung Hong","Donghoon Ahn","Seungryong Kim"],"pdf_url":"https://arxiv.org/pdf/2303.15413v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.15407v1","updated":"2023-03-27T17:25:04Z","published":"2023-03-27T17:25:04Z","title":"Dimensionality Collapse: Optimal Measurement Selection for Low-Error\n  Infinite-Horizon Forecasting","summary":"  This work introduces a method to select linear functional measurements of a\nvector-valued time series optimized for forecasting distant time-horizons. By\nformulating and solving the problem of sequential linear measurement design as\nan infinite-horizon problem with the time-averaged trace of the Cram\\'{e}r-Rao\nlower bound (CRLB) for forecasting as the cost, the most informative data can\nbe collected irrespective of the eventual forecasting algorithm. By introducing\ntheoretical results regarding measurements under additive noise from natural\nexponential families, we construct an equivalent problem from which a local\ndimensionality reduction can be derived. This alternative formulation is based\non the future collapse of dimensionality inherent in the limiting behavior of\nmany differential equations and can be directly observed in the low-rank\nstructure of the CRLB for forecasting. Implementations of both an approximate\ndynamic programming formulation and the proposed alternative are illustrated\nusing an extended Kalman filter for state estimation, with results on simulated\nsystems with limit cycles and chaotic behavior demonstrating a linear\nimprovement in the CRLB as a function of the number of collapsing dimensions of\nthe system.\n","authors":["Helmuth Naumer","Farzad Kamalabadi"],"pdf_url":"https://arxiv.org/pdf/2303.15407v1.pdf","comment":"33 Pages, 9 Figures, To appear in Proceedings of the 26th\n  International Conference on Artificial Intelligence and Statistics (AISTATS)\n  2023"},{"id":"http://arxiv.org/abs/2209.08938v2","updated":"2023-03-27T17:16:03Z","published":"2022-09-19T11:46:05Z","title":"Accelerating Neural Network Inference with Processing-in-DRAM: From the\n  Edge to the Cloud","summary":"  Neural networks (NNs) are growing in importance and complexity. A neural\nnetwork's performance (and energy efficiency) can be bound either by\ncomputation or memory resources. The processing-in-memory (PIM) paradigm, where\ncomputation is placed near or within memory arrays, is a viable solution to\naccelerate memory-bound NNs. However, PIM architectures vary in form, where\ndifferent PIM approaches lead to different trade-offs. Our goal is to analyze,\ndiscuss, and contrast DRAM-based PIM architectures for NN performance and\nenergy efficiency. To do so, we analyze three state-of-the-art PIM\narchitectures: (1) UPMEM, which integrates processors and DRAM arrays into a\nsingle 2D chip; (2) Mensa, a 3D-stack-based PIM architecture tailored for edge\ndevices; and (3) SIMDRAM, which uses the analog principles of DRAM to execute\nbit-serial operations. Our analysis reveals that PIM greatly benefits\nmemory-bound NNs: (1) UPMEM provides 23x the performance of a high-end GPU when\nthe GPU requires memory oversubscription for a general matrix-vector\nmultiplication kernel; (2) Mensa improves energy efficiency and throughput by\n3.0x and 3.1x over the Google Edge TPU for 24 Google edge NN models; and (3)\nSIMDRAM outperforms a CPU/GPU by 16.7x/1.4x for three binary NNs. We conclude\nthat the ideal PIM architecture for NN models depends on a model's distinct\nattributes, due to the inherent architectural design choices.\n","authors":["Geraldo F. Oliveira","Juan Gómez-Luna","Saugata Ghose","Amirali Boroumand","Onur Mutlu"],"pdf_url":"https://arxiv.org/pdf/2209.08938v2.pdf","comment":"This is an extended and updated version of a paper published in IEEE\n  Micro, pp. 1-14, 29 Aug. 2022. arXiv admin note: text overlap with\n  arXiv:2109.14320"},{"id":"http://arxiv.org/abs/2303.08797v2","updated":"2023-03-27T17:07:52Z","published":"2023-03-15T17:43:42Z","title":"Stochastic Interpolants: A Unifying Framework for Flows and Diffusions","summary":"  A class of generative models that unifies flow-based and diffusion-based\nmethods is introduced. These models extend the framework proposed in Albergo &\nVanden-Eijnden (2023), enabling the use of a broad class of continuous-time\nstochastic processes called `stochastic interpolants' to bridge any two\narbitrary probability density functions exactly in finite time. These\ninterpolants are built by combining data from the two prescribed densities with\nan additional latent variable that shapes the bridge in a flexible way. The\ntime-dependent probability density function of the stochastic interpolant is\nshown to satisfy a first-order transport equation as well as a family of\nforward and backward Fokker-Planck equations with tunable diffusion. Upon\nconsideration of the time evolution of an individual sample, this viewpoint\nimmediately leads to both deterministic and stochastic generative models based\non probability flow equations or stochastic differential equations with an\nadjustable level of noise. The drift coefficients entering these models are\ntime-dependent velocity fields characterized as the unique minimizers of simple\nquadratic objective functions, one of which is a new objective for the score of\nthe interpolant density. Remarkably, we show that minimization of these\nquadratic objectives leads to control of the likelihood for any of our\ngenerative models built upon stochastic dynamics. By contrast, we establish\nthat generative models based upon a deterministic dynamics must, in addition,\ncontrol the Fisher divergence between the target and the model. We also\nconstruct estimators for the likelihood and the cross-entropy of\ninterpolant-based generative models, discuss connections with other stochastic\nbridges, and demonstrate that such models recover the Schr\\\"odinger bridge\nbetween the two target densities when explicitly optimizing over the\ninterpolant.\n","authors":["Michael S. Albergo","Nicholas M. Boffi","Eric Vanden-Eijnden"],"pdf_url":"https://arxiv.org/pdf/2303.08797v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.15383v1","updated":"2023-03-27T16:56:57Z","published":"2023-03-27T16:56:57Z","title":"List Online Classification","summary":"  We study multiclass online prediction where the learner can predict using a\nlist of multiple labels (as opposed to just one label in the traditional\nsetting). We characterize learnability in this model using the $b$-ary\nLittlestone dimension. This dimension is a variation of the classical\nLittlestone dimension with the difference that binary mistake trees are\nreplaced with $(k+1)$-ary mistake trees, where $k$ is the number of labels in\nthe list. In the agnostic setting, we explore different scenarios depending on\nwhether the comparator class consists of single-labeled or multi-labeled\nfunctions and its tradeoff with the size of the lists the algorithm uses. We\nfind that it is possible to achieve negative regret in some cases and provide a\ncomplete characterization of when this is possible. As part of our work, we\nadapt classical algorithms such as Littlestone's SOA and Rosenblatt's\nPerceptron to predict using lists of labels. We also establish combinatorial\nresults for list-learnable classes, including an list online version of the\nSauer-Shelah-Perles Lemma. We state our results within the framework of pattern\nclasses -- a generalization of hypothesis classes which can represent adaptive\nhypotheses (i.e. functions with memory), and model data-dependent assumptions\nsuch as linear classification with margin.\n","authors":["Shay Moran","Ohad Sharon","Iska Tsubari"],"pdf_url":"https://arxiv.org/pdf/2303.15383v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.13035v2","updated":"2023-03-27T16:56:26Z","published":"2023-03-23T04:47:46Z","title":"SPeC: A Soft Prompt-Based Calibration on Mitigating Performance\n  Variability in Clinical Notes Summarization","summary":"  Electronic health records (EHRs) store an extensive array of patient\ninformation, encompassing medical histories, diagnoses, treatments, and test\noutcomes. These records are crucial for enabling healthcare providers to make\nwell-informed decisions regarding patient care. Summarizing clinical notes\nfurther assists healthcare professionals in pinpointing potential health risks\nand making better-informed decisions. This process contributes to reducing\nerrors and enhancing patient outcomes by ensuring providers have access to the\nmost pertinent and current patient data. Recent research has shown that\nincorporating prompts with large language models (LLMs) substantially boosts\nthe efficacy of summarization tasks. However, we show that this approach also\nleads to increased output variance, resulting in notably divergent outputs even\nwhen prompts share similar meanings. To tackle this challenge, we introduce a\nmodel-agnostic Soft Prompt-Based Calibration (SPeC) pipeline that employs soft\nprompts to diminish variance while preserving the advantages of prompt-based\nsummarization. Experimental findings on multiple clinical note tasks and LLMs\nindicate that our method not only bolsters performance but also effectively\ncurbs variance for various LLMs, providing a more uniform and dependable\nsolution for summarizing vital medical information.\n","authors":["Yu-Neng Chuang","Ruixiang Tang","Xiaoqian Jiang","Xia Hu"],"pdf_url":"https://arxiv.org/pdf/2303.13035v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.15378v1","updated":"2023-03-27T16:52:17Z","published":"2023-03-27T16:52:17Z","title":"CoDeC: Communication-Efficient Decentralized Continual Learning","summary":"  Training at the edge utilizes continuously evolving data generated at\ndifferent locations. Privacy concerns prohibit the co-location of this\nspatially as well as temporally distributed data, deeming it crucial to design\ntraining algorithms that enable efficient continual learning over decentralized\nprivate data. Decentralized learning allows serverless training with spatially\ndistributed data. A fundamental barrier in such distributed learning is the\nhigh bandwidth cost of communicating model updates between agents. Moreover,\nexisting works under this training paradigm are not inherently suitable for\nlearning a temporal sequence of tasks while retaining the previously acquired\nknowledge. In this work, we propose CoDeC, a novel communication-efficient\ndecentralized continual learning algorithm which addresses these challenges. We\nmitigate catastrophic forgetting while learning a task sequence in a\ndecentralized learning setup by combining orthogonal gradient projection with\ngossip averaging across decentralized agents. Further, CoDeC includes a novel\nlossless communication compression scheme based on the gradient subspaces. We\nexpress layer-wise gradients as a linear combination of the basis vectors of\nthese gradient subspaces and communicate the associated coefficients. We\ntheoretically analyze the convergence rate for our algorithm and demonstrate\nthrough an extensive set of experiments that CoDeC successfully learns\ndistributed continual tasks with minimal forgetting. The proposed compression\nscheme results in up to 4.8x reduction in communication costs with\niso-performance as the full communication baseline.\n","authors":["Sakshi Choudhary","Sai Aparna Aketi","Gobinda Saha","Kaushik Roy"],"pdf_url":"https://arxiv.org/pdf/2303.15378v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.12642v2","updated":"2023-03-27T16:34:55Z","published":"2023-03-22T15:23:22Z","title":"Democratising AI: Multiple Meanings, Goals, and Methods","summary":"  Numerous parties are calling for the democratisation of AI, but the phrase is\nused to refer to a variety of goals, the pursuit of which sometimes conflict.\nThis paper identifies four kinds of AI democratisation that are commonly\ndiscussed: (1) the democratisation of AI use, (2) the democratisation of AI\ndevelopment, (3) the democratisation of AI profits, and (4) the democratisation\nof AI governance. Numerous goals and methods of achieving each form of\ndemocratisation are discussed. The main takeaway from this paper is that AI\ndemocratisation is a multifarious and sometimes conflicting concept that should\nnot be conflated with improving AI accessibility. If we want to move beyond\nambiguous commitments to democratising AI, to productive discussions of\nconcrete policies and trade-offs, then we need to recognise the principal role\nof the democratisation of AI governance in navigating tradeoffs and risks\nacross decisions around use, development, and profits.\n","authors":["Elizabeth Seger","Aviv Ovadya","Ben Garfinkel","Divya Siddarth","Allan Dafoe"],"pdf_url":"https://arxiv.org/pdf/2303.12642v2.pdf","comment":"Changed second author affiliation; added citation to section 5.2;\n  edit to author contribution statement"},{"id":"http://arxiv.org/abs/2303.15361v1","updated":"2023-03-27T16:32:21Z","published":"2023-03-27T16:32:21Z","title":"A Comprehensive Survey on Test-Time Adaptation under Distribution Shifts","summary":"  Machine learning methods strive to acquire a robust model during training\nthat can generalize well to test samples, even under distribution shifts.\nHowever, these methods often suffer from a performance drop due to unknown test\ndistributions. Test-time adaptation (TTA), an emerging paradigm, has the\npotential to adapt a pre-trained model to unlabeled data during testing, before\nmaking predictions. Recent progress in this paradigm highlights the significant\nbenefits of utilizing unlabeled data for training self-adapted models prior to\ninference. In this survey, we divide TTA into several distinct categories,\nnamely, test-time (source-free) domain adaptation, test-time batch adaptation,\nonline test-time adaptation, and test-time prior adaptation. For each category,\nwe provide a comprehensive taxonomy of advanced algorithms, followed by a\ndiscussion of different learning scenarios. Furthermore, we analyze relevant\napplications of TTA and discuss open challenges and promising areas for future\nresearch. A comprehensive list of TTA methods can be found at\n\\url{https://github.com/tim-learn/awesome-test-time-adaptation}.\n","authors":["Jian Liang","Ran He","Tieniu Tan"],"pdf_url":"https://arxiv.org/pdf/2303.15361v1.pdf","comment":"Discussions, comments, and questions are all welcomed in\n  \\url{https://github.com/tim-learn/awesome-test-time-adaptation}"},{"id":"http://arxiv.org/abs/2303.15354v1","updated":"2023-03-27T16:13:54Z","published":"2023-03-27T16:13:54Z","title":"Generalisability of deep learning-based early warning in the intensive\n  care unit: a retrospective empirical evaluation","summary":"  Deep learning (DL) can aid doctors in detecting worsening patient states\nearly, affording them time to react and prevent bad outcomes. While DL-based\nearly warning models usually work well in the hospitals they were trained for,\nthey tend to be less reliable when applied at new hospitals. This makes it\ndifficult to deploy them at scale. Using carefully harmonised intensive care\ndata from four data sources across Europe and the US (totalling 334,812 stays),\nwe systematically assessed the reliability of DL models for three common\nadverse events: death, acute kidney injury (AKI), and sepsis. We tested whether\nusing more than one data source and/or explicitly optimising for\ngeneralisability during training improves model performance at new hospitals.\nWe found that models achieved high AUROC for mortality (0.838-0.869), AKI\n(0.823-0.866), and sepsis (0.749-0.824) at the training hospital. As expected,\nperformance dropped at new hospitals, sometimes by as much as -0.200. Using\nmore than one data source for training mitigated the performance drop, with\nmulti-source models performing roughly on par with the best single-source\nmodel. This suggests that as data from more hospitals become available for\ntraining, model robustness is likely to increase, lower-bounding robustness\nwith the performance of the most applicable data source in the training data.\nDedicated methods promoting generalisability did not noticeably improve\nperformance in our experiments.\n","authors":["Patrick Rockenschaub","Adam Hilbert","Tabea Kossen","Falk von Dincklage","Vince Istvan Madai","Dietmar Frey"],"pdf_url":"https://arxiv.org/pdf/2303.15354v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2206.15025v2","updated":"2023-03-27T16:09:27Z","published":"2022-06-30T05:29:52Z","title":"On the Convergence of Distributed Stochastic Bilevel Optimization\n  Algorithms over a Network","summary":"  Bilevel optimization has been applied to a wide variety of machine learning\nmodels, and numerous stochastic bilevel optimization algorithms have been\ndeveloped in recent years. However, most existing algorithms restrict their\nfocus on the single-machine setting so that they are incapable of handling the\ndistributed data. To address this issue, under the setting where all\nparticipants compose a network and perform peer-to-peer communication in this\nnetwork, we developed two novel decentralized stochastic bilevel optimization\nalgorithms based on the gradient tracking communication mechanism and two\ndifferent gradient estimators. Additionally, we established their convergence\nrates for nonconvex-strongly-convex problems with novel theoretical analysis\nstrategies. To our knowledge, this is the first work achieving these\ntheoretical results. Finally, we applied our algorithms to practical machine\nlearning models, and the experimental results confirmed the efficacy of our\nalgorithms.\n","authors":["Hongchang Gao","Bin Gu","My T. Thai"],"pdf_url":"https://arxiv.org/pdf/2206.15025v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.15350v1","updated":"2023-03-27T16:07:44Z","published":"2023-03-27T16:07:44Z","title":"Improving Neural Topic Models with Wasserstein Knowledge Distillation","summary":"  Topic modeling is a dominant method for exploring document collections on the\nweb and in digital libraries. Recent approaches to topic modeling use\npretrained contextualized language models and variational autoencoders.\nHowever, large neural topic models have a considerable memory footprint. In\nthis paper, we propose a knowledge distillation framework to compress a\ncontextualized topic model without loss in topic quality. In particular, the\nproposed distillation objective is to minimize the cross-entropy of the soft\nlabels produced by the teacher and the student models, as well as to minimize\nthe squared 2-Wasserstein distance between the latent distributions learned by\nthe two models. Experiments on two publicly available datasets show that the\nstudent trained with knowledge distillation achieves topic coherence much\nhigher than that of the original student model, and even surpasses the teacher\nwhile containing far fewer parameters than the teacher's. The distilled model\nalso outperforms several other competitive topic models on topic coherence.\n","authors":["Suman Adhya","Debarshi Kumar Sanyal"],"pdf_url":"https://arxiv.org/pdf/2303.15350v1.pdf","comment":"Accepted at ECIR 2023"},{"id":"http://arxiv.org/abs/2303.15349v1","updated":"2023-03-27T16:02:50Z","published":"2023-03-27T16:02:50Z","title":"Information Maximizing Curriculum: A Curriculum-Based Approach for\n  Training Mixtures of Experts","summary":"  Mixtures of Experts (MoE) are known for their ability to learn complex\nconditional distributions with multiple modes. However, despite their\npotential, these models are challenging to train and often tend to produce poor\nperformance, explaining their limited popularity. Our hypothesis is that this\nunder-performance is a result of the commonly utilized maximum likelihood (ML)\noptimization, which leads to mode averaging and a higher likelihood of getting\nstuck in local maxima. We propose a novel curriculum-based approach to learning\nmixture models in which each component of the MoE is able to select its own\nsubset of the training data for learning. This approach allows for independent\noptimization of each component, resulting in a more modular architecture that\nenables the addition and deletion of components on the fly, leading to an\noptimization less susceptible to local optima. The curricula can ignore\ndata-points from modes not represented by the MoE, reducing the mode-averaging\nproblem. To achieve a good data coverage, we couple the optimization of the\ncurricula with a joint entropy objective and optimize a lower bound of this\nobjective. We evaluate our curriculum-based approach on a variety of multimodal\nbehavior learning tasks and demonstrate its superiority over competing methods\nfor learning MoE models and conditional generative models.\n","authors":["Denis Blessing","Onur Celik","Xiaogang Jia","Moritz Reuss","Maximilian Xiling Li","Rudolf Lioutikov","Gerhard Neumann"],"pdf_url":"https://arxiv.org/pdf/2303.15349v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.15342v1","updated":"2023-03-27T15:52:14Z","published":"2023-03-27T15:52:14Z","title":"Exploring Continual Learning of Diffusion Models","summary":"  Diffusion models have achieved remarkable success in generating high-quality\nimages thanks to their novel training procedures applied to unprecedented\namounts of data. However, training a diffusion model from scratch is\ncomputationally expensive. This highlights the need to investigate the\npossibility of training these models iteratively, reusing computation while the\ndata distribution changes. In this study, we take the first step in this\ndirection and evaluate the continual learning (CL) properties of diffusion\nmodels. We begin by benchmarking the most common CL methods applied to\nDenoising Diffusion Probabilistic Models (DDPMs), where we note the strong\nperformance of the experience replay with the reduced rehearsal coefficient.\nFurthermore, we provide insights into the dynamics of forgetting, which exhibit\ndiverse behavior across diffusion timesteps. We also uncover certain pitfalls\nof using the bits-per-dimension metric for evaluating CL.\n","authors":["Michał Zając","Kamil Deja","Anna Kuzina","Jakub M. Tomczak","Tomasz Trzciński","Florian Shkurti","Piotr Miłoś"],"pdf_url":"https://arxiv.org/pdf/2303.15342v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2202.03360v2","updated":"2023-03-27T15:51:56Z","published":"2022-02-07T17:10:00Z","title":"Discrete-Event Controller Synthesis for Autonomous Systems with\n  Deep-Learning Perception Components","summary":"  We present DeepDECS, a new method for the synthesis of\ncorrect-by-construction discrete-event controllers for autonomous systems that\nuse deep neural network (DNN) classifiers for the perception step of their\ndecision-making processes. Despite major advances in deep learning in recent\nyears, providing safety guarantees for these systems remains very challenging.\nOur controller synthesis method addresses this challenge by integrating DNN\nverification with the synthesis of verified Markov models. The synthesised\nmodels correspond to discrete-event controllers guaranteed to satisfy the\nsafety, dependability and performance requirements of the autonomous system,\nand to be Pareto optimal with respect to a set of optimisation objectives. We\nuse the method in simulation to synthesise controllers for mobile-robot\ncollision mitigation and for maintaining driver attentiveness in shared-control\nautonomous driving.\n","authors":["Radu Calinescu","Calum Imrie","Ravi Mangal","Genaína Nunes Rodrigues","Corina Păsăreanu","Misael Alpizar Santana","Gricel Vázquez"],"pdf_url":"https://arxiv.org/pdf/2202.03360v2.pdf","comment":"18 pages 6 Figures 2 Tables"},{"id":"http://arxiv.org/abs/2208.01647v2","updated":"2023-03-27T15:43:43Z","published":"2022-08-02T14:12:03Z","title":"AI-driven Hypergraph Network of Organic Chemistry: Network Statistics\n  and Applications in Reaction Classification","summary":"  Rapid discovery of new reactions and molecules in recent years has been\nfacilitated by the advancements in high throughput screening, accessibility to\na much more complex chemical design space, and the development of accurate\nmolecular modeling frameworks. A holistic study of the growing chemistry\nliterature is, therefore, required that focuses on understanding the recent\ntrends and extrapolating them into possible future trajectories. To this end,\nseveral network theory-based studies have been reported that use a directed\ngraph representation of chemical reactions. Here, we perform a study based on\nrepresenting chemical reactions as hypergraphs where the hyperedges represent\nchemical reactions and nodes represent the participating molecules. We use a\nstandard reactions dataset to construct a hypernetwork and report its\nstatistics such as degree distributions, average path length, assortativity or\ndegree correlations, PageRank centrality, and graph-based clusters (or\ncommunities). We also compute each statistic for an equivalent directed graph\nrepresentation of reactions to draw parallels and highlight differences between\nthe two. To demonstrate the AI applicability of hypergraph reaction\nrepresentation, we generate dense hypergraph embeddings and use them in the\nreaction classification problem. We conclude that the hypernetwork\nrepresentation is flexible, preserves reaction context, and uncovers hidden\ninsights that are otherwise not apparent in a traditional directed graph\nrepresentation of chemical reactions.\n","authors":["Vipul Mann","Venkat Venkatasubramanian"],"pdf_url":"https://arxiv.org/pdf/2208.01647v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.15318v1","updated":"2023-03-27T15:20:10Z","published":"2023-03-27T15:20:10Z","title":"Closed-Loop Koopman Operator Approximation","summary":"  The Koopman operator allows a nonlinear system to be rewritten as an\ninfinite-dimensional linear system by viewing it in terms of an infinite set of\nlifting functions instead of a state vector. The main feature of this\nrepresentation is its linearity, making it compatible with existing linear\nsystems theory. A finite-dimensional approximation of the Koopman operator can\nbe identified from experimental data by choosing a finite subset of lifting\nfunctions, applying it to the data, and solving a least squares problem in the\nlifted space. Existing Koopman operator approximation methods are designed to\nidentify open-loop systems. However, it is impractical or impossible to run\nexperiments on some systems without a feedback controller. Unfortunately, the\nintroduction of feedback control results in correlations between the system's\ninput and output, making some plant dynamics difficult to identify if the\ncontroller is neglected. This paper addresses this limitation by introducing a\nmethod to identify a Koopman model of the closed-loop system, and then extract\na Koopman model of the plant given knowledge of the controller. This is\naccomplished by leveraging the linearity of the Koopman representation of the\nsystem. The proposed approach widens the applicability of Koopman operator\nidentification methods to a broader class of systems. The effectiveness of the\nproposed closed-loop Koopman operator approximation method is demonstrated\nexperimentally using a Harmonic Drive gearbox exhibiting nonlinear vibrations.\n","authors":["Steven Dahdah","James Richard Forbes"],"pdf_url":"https://arxiv.org/pdf/2303.15318v1.pdf","comment":"21 pages, 13 figures"},{"id":"http://arxiv.org/abs/2303.15311v1","updated":"2023-03-27T15:18:32Z","published":"2023-03-27T15:18:32Z","title":"Improving Dual-Encoder Training through Dynamic Indexes for Negative\n  Mining","summary":"  Dual encoder models are ubiquitous in modern classification and retrieval.\nCrucial for training such dual encoders is an accurate estimation of gradients\nfrom the partition function of the softmax over the large output space; this\nrequires finding negative targets that contribute most significantly (\"hard\nnegatives\"). Since dual encoder model parameters change during training, the\nuse of traditional static nearest neighbor indexes can be sub-optimal. These\nstatic indexes (1) periodically require expensive re-building of the index,\nwhich in turn requires (2) expensive re-encoding of all targets using updated\nmodel parameters. This paper addresses both of these challenges. First, we\nintroduce an algorithm that uses a tree structure to approximate the softmax\nwith provable bounds and that dynamically maintains the tree. Second, we\napproximate the effect of a gradient update on target encodings with an\nefficient Nystrom low-rank approximation. In our empirical study on datasets\nwith over twenty million targets, our approach cuts error by half in relation\nto oracle brute-force negative mining. Furthermore, our method surpasses prior\nstate-of-the-art while using 150x less accelerator memory.\n","authors":["Nicholas Monath","Manzil Zaheer","Kelsey Allen","Andrew McCallum"],"pdf_url":"https://arxiv.org/pdf/2303.15311v1.pdf","comment":"To appear at AISTATS 2023"},{"id":"http://arxiv.org/abs/2303.07522v2","updated":"2023-03-27T15:10:51Z","published":"2023-03-13T23:17:51Z","title":"Audio Visual Language Maps for Robot Navigation","summary":"  While interacting in the world is a multi-sensory experience, many robots\ncontinue to predominantly rely on visual perception to map and navigate in\ntheir environments. In this work, we propose Audio-Visual-Language Maps\n(AVLMaps), a unified 3D spatial map representation for storing cross-modal\ninformation from audio, visual, and language cues. AVLMaps integrate the\nopen-vocabulary capabilities of multimodal foundation models pre-trained on\nInternet-scale data by fusing their features into a centralized 3D voxel grid.\nIn the context of navigation, we show that AVLMaps enable robot systems to\nindex goals in the map based on multimodal queries, e.g., textual descriptions,\nimages, or audio snippets of landmarks. In particular, the addition of audio\ninformation enables robots to more reliably disambiguate goal locations.\nExtensive experiments in simulation show that AVLMaps enable zero-shot\nmultimodal goal navigation from multimodal prompts and provide 50% better\nrecall in ambiguous scenarios. These capabilities extend to mobile robots in\nthe real world - navigating to landmarks referring to visual, audio, and\nspatial concepts. Videos and code are available at: https://avlmaps.github.io.\n","authors":["Chenguang Huang","Oier Mees","Andy Zeng","Wolfram Burgard"],"pdf_url":"https://arxiv.org/pdf/2303.07522v2.pdf","comment":"Project page: https://avlmaps.github.io/"},{"id":"http://arxiv.org/abs/2103.14636v2","updated":"2023-03-27T15:10:28Z","published":"2021-03-26T17:54:47Z","title":"A Practical Survey on Faster and Lighter Transformers","summary":"  Recurrent neural networks are effective models to process sequences. However,\nthey are unable to learn long-term dependencies because of their inherent\nsequential nature. As a solution, Vaswani et al. introduced the Transformer, a\nmodel solely based on the attention mechanism that is able to relate any two\npositions of the input sequence, hence modelling arbitrary long dependencies.\nThe Transformer has improved the state-of-the-art across numerous sequence\nmodelling tasks. However, its effectiveness comes at the expense of a quadratic\ncomputational and memory complexity with respect to the sequence length,\nhindering its adoption. Fortunately, the deep learning community has always\nbeen interested in improving the models' efficiency, leading to a plethora of\nsolutions such as parameter sharing, pruning, mixed-precision, and knowledge\ndistillation. Recently, researchers have directly addressed the Transformer's\nlimitation by designing lower-complexity alternatives such as the Longformer,\nReformer, Linformer, and Performer. However, due to the wide range of\nsolutions, it has become challenging for researchers and practitioners to\ndetermine which methods to apply in practice in order to meet the desired\ntrade-off between capacity, computation, and memory. This survey addresses this\nissue by investigating popular approaches to make Transformers faster and\nlighter and by providing a comprehensive explanation of the methods' strengths,\nlimitations, and underlying assumptions.\n","authors":["Quentin Fournier","Gaétan Marceau Caron","Daniel Aloise"],"pdf_url":"https://arxiv.org/pdf/2103.14636v2.pdf","comment":"ACM Computing Surveys; 40 pages, 18 figures, 4 tables"},{"id":"http://arxiv.org/abs/2303.15288v1","updated":"2023-03-27T15:10:19Z","published":"2023-03-27T15:10:19Z","title":"Diffusion Models for Memory-efficient Processing of 3D Medical Images","summary":"  Denoising diffusion models have recently achieved state-of-the-art\nperformance in many image-generation tasks. They do, however, require a large\namount of computational resources. This limits their application to medical\ntasks, where we often deal with large 3D volumes, like high-resolution\nthree-dimensional data. In this work, we present a number of different ways to\nreduce the resource consumption for 3D diffusion models and apply them to a\ndataset of 3D images. The main contribution of this paper is the\nmemory-efficient patch-based diffusion model \\textit{PatchDDM}, which can be\napplied to the total volume during inference while the training is performed\nonly on patches. While the proposed diffusion model can be applied to any image\ngeneration tasks, we evaluate the method on the tumor segmentation task of the\nBraTS2020 dataset and demonstrate that we can generate meaningful\nthree-dimensional segmentations.\n","authors":["Florentin Bieder","Julia Wolleb","Alicia Durrer","Robin Sandkühler","Philippe C. Cattin"],"pdf_url":"https://arxiv.org/pdf/2303.15288v1.pdf","comment":"Accepted at MIDL 2023"},{"id":"http://arxiv.org/abs/2303.15286v1","updated":"2023-03-27T15:07:55Z","published":"2023-03-27T15:07:55Z","title":"Unsupervised Adaptation from Repeated Traversals for Autonomous Driving","summary":"  For a self-driving car to operate reliably, its perceptual system must\ngeneralize to the end-user's environment -- ideally without additional\nannotation efforts. One potential solution is to leverage unlabeled data (e.g.,\nunlabeled LiDAR point clouds) collected from the end-users' environments (i.e.\ntarget domain) to adapt the system to the difference between training and\ntesting environments. While extensive research has been done on such an\nunsupervised domain adaptation problem, one fundamental problem lingers: there\nis no reliable signal in the target domain to supervise the adaptation process.\nTo overcome this issue we observe that it is easy to collect unsupervised data\nfrom multiple traversals of repeated routes. While different from conventional\nunsupervised domain adaptation, this assumption is extremely realistic since\nmany drivers share the same roads. We show that this simple additional\nassumption is sufficient to obtain a potent signal that allows us to perform\niterative self-training of 3D object detectors on the target domain.\nConcretely, we generate pseudo-labels with the out-of-domain detector but\nreduce false positives by removing detections of supposedly mobile objects that\nare persistent across traversals. Further, we reduce false negatives by\nencouraging predictions in regions that are not persistent. We experiment with\nour approach on two large-scale driving datasets and show remarkable\nimprovement in 3D object detection of cars, pedestrians, and cyclists, bringing\nus a step closer to generalizable autonomous driving.\n","authors":["Yurong You","Cheng Perng Phoo","Katie Z Luo","Travis Zhang","Wei-Lun Chao","Bharath Hariharan","Mark Campbell","Kilian Q. Weinberger"],"pdf_url":"https://arxiv.org/pdf/2303.15286v1.pdf","comment":"Accepted by NeurIPS 2022. Code is available at\n  https://github.com/YurongYou/Rote-DA"},{"id":"http://arxiv.org/abs/2210.07453v2","updated":"2023-03-27T15:04:30Z","published":"2022-10-14T01:41:10Z","title":"Using Graph Algorithms to Pretrain Graph Completion Transformers","summary":"  Recent work on Graph Neural Networks has demonstrated that self-supervised\npretraining can further enhance performance on downstream graph, link, and node\nclassification tasks. However, the efficacy of pretraining tasks has not been\nfully investigated for downstream large knowledge graph completion tasks. Using\na contextualized knowledge graph embedding approach, we investigate five\ndifferent pretraining signals, constructed using several graph algorithms and\nno external data, as well as their combination. We leverage the versatility of\nour Transformer-based model to explore graph structure generation pretraining\ntasks (i.e. path and k-hop neighborhood generation), typically inapplicable to\nmost graph embedding methods. We further propose a new path-finding algorithm\nguided by information gain and find that it is the best-performing pretraining\ntask across three downstream knowledge graph completion datasets. While using\nour new path-finding algorithm as a pretraining signal provides 2-3% MRR\nimprovements, we show that pretraining on all signals together gives the best\nknowledge graph completion results. In a multitask setting that combines all\npretraining tasks, our method surpasses the latest and strong performing\nknowledge graph embedding methods on all metrics for FB15K-237, on MRR and\nHit@1 for WN18RRand on MRR and hit@10 for JF17K (a knowledge hypergraph\ndataset).\n","authors":["Jonathan Pilault","Michael Galkin","Bahare Fatemi","Perouz Taslakian","David Vasquez","Christopher Pal"],"pdf_url":"https://arxiv.org/pdf/2210.07453v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.15270v1","updated":"2023-03-27T14:59:08Z","published":"2023-03-27T14:59:08Z","title":"Unified Keypoint-based Action Recognition Framework via Structured\n  Keypoint Pooling","summary":"  This paper simultaneously addresses three limitations associated with\nconventional skeleton-based action recognition; skeleton detection and tracking\nerrors, poor variety of the targeted actions, as well as person-wise and\nframe-wise action recognition. A point cloud deep-learning paradigm is\nintroduced to the action recognition, and a unified framework along with a\nnovel deep neural network architecture called Structured Keypoint Pooling is\nproposed. The proposed method sparsely aggregates keypoint features in a\ncascaded manner based on prior knowledge of the data structure (which is\ninherent in skeletons), such as the instances and frames to which each keypoint\nbelongs, and achieves robustness against input errors. Its less constrained and\ntracking-free architecture enables time-series keypoints consisting of human\nskeletons and nonhuman object contours to be efficiently treated as an input 3D\npoint cloud and extends the variety of the targeted action. Furthermore, we\npropose a Pooling-Switching Trick inspired by Structured Keypoint Pooling. This\ntrick switches the pooling kernels between the training and inference phases to\ndetect person-wise and frame-wise actions in a weakly supervised manner using\nonly video-level action labels. This trick enables our training scheme to\nnaturally introduce novel data augmentation, which mixes multiple point clouds\nextracted from different videos. In the experiments, we comprehensively verify\nthe effectiveness of the proposed method against the limitations, and the\nmethod outperforms state-of-the-art skeleton-based action recognition and\nspatio-temporal action localization methods.\n","authors":["Ryo Hachiuma","Fumiaki Sato","Taiki Sekii"],"pdf_url":"https://arxiv.org/pdf/2303.15270v1.pdf","comment":"CVPR 2023"},{"id":"http://arxiv.org/abs/2303.15265v1","updated":"2023-03-27T14:54:43Z","published":"2023-03-27T14:54:43Z","title":"Bilex Rx: Lexical Data Augmentation for Massively Multilingual Machine\n  Translation","summary":"  Neural machine translation (NMT) has progressed rapidly over the past several\nyears, and modern models are able to achieve relatively high quality using only\nmonolingual text data, an approach dubbed Unsupervised Machine Translation\n(UNMT). However, these models still struggle in a variety of ways, including\naspects of translation that for a human are the easiest - for instance,\ncorrectly translating common nouns. This work explores a cheap and abundant\nresource to combat this problem: bilingual lexica. We test the efficacy of\nbilingual lexica in a real-world set-up, on 200-language translation models\ntrained on web-crawled text. We present several findings: (1) using lexical\ndata augmentation, we demonstrate sizable performance gains for unsupervised\ntranslation; (2) we compare several families of data augmentation,\ndemonstrating that they yield similar improvements, and can be combined for\neven greater improvements; (3) we demonstrate the importance of carefully\ncurated lexica over larger, noisier ones, especially with larger models; and\n(4) we compare the efficacy of multilingual lexicon data versus\nhuman-translated parallel data. Finally, we open-source GATITOS (available at\nhttps://github.com/google-research/url-nlp/tree/main/gatitos), a new\nmultilingual lexicon for 26 low-resource languages, which had the highest\nperformance among lexica in our experiments.\n","authors":["Alex Jones","Isaac Caswell","Ishank Saxena","Orhan Firat"],"pdf_url":"https://arxiv.org/pdf/2303.15265v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2210.12101v2","updated":"2023-03-27T14:49:07Z","published":"2022-10-21T16:53:18Z","title":"Neural Network Approximations of PDEs Beyond Linearity: A\n  Representational Perspective","summary":"  A burgeoning line of research leverages deep neural networks to approximate\nthe solutions to high dimensional PDEs, opening lines of theoretical inquiry\nfocused on explaining how it is that these models appear to evade the curse of\ndimensionality. However, most prior theoretical analyses have been limited to\nlinear PDEs. In this work, we take a step towards studying the representational\npower of neural networks for approximating solutions to nonlinear PDEs. We\nfocus on a class of PDEs known as \\emph{nonlinear elliptic variational PDEs},\nwhose solutions minimize an \\emph{Euler-Lagrange} energy functional\n$\\mathcal{E}(u) = \\int_\\Omega L(x, u(x), \\nabla u(x)) - f(x) u(x)dx$. We show\nthat if composing a function with Barron norm $b$ with partial derivatives of\n$L$ produces a function of Barron norm at most $B_L b^p$, the solution to the\nPDE can be $\\epsilon$-approximated in the $L^2$ sense by a function with Barron\nnorm $O\\left(\\left(dB_L\\right)^{\\max\\{p \\log(1/ \\epsilon),\np^{\\log(1/\\epsilon)}\\}}\\right)$. By a classical result due to Barron [1993],\nthis correspondingly bounds the size of a 2-layer neural network needed to\napproximate the solution. Treating $p, \\epsilon, B_L$ as constants, this\nquantity is polynomial in dimension, thus showing neural networks can evade the\ncurse of dimensionality. Our proof technique involves neurally simulating\n(preconditioned) gradient in an appropriate Hilbert space, which converges\nexponentially fast to the solution of the PDE, and such that we can bound the\nincrease of the Barron norm at each iterate. Our results subsume and\nsubstantially generalize analogous prior results for linear elliptic PDEs over\na unit hypercube.\n","authors":["Tanya Marwah","Zachary C. Lipton","Jianfeng Lu","Andrej Risteski"],"pdf_url":"https://arxiv.org/pdf/2210.12101v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.15256v1","updated":"2023-03-27T14:44:39Z","published":"2023-03-27T14:44:39Z","title":"Active Self-Supervised Learning: A Few Low-Cost Relationships Are All\n  You Need","summary":"  Self-Supervised Learning (SSL) has emerged as the solution of choice to learn\ntransferable representations from unlabeled data. However, SSL requires to\nbuild samples that are known to be semantically akin, i.e. positive views.\nRequiring such knowledge is the main limitation of SSL and is often tackled by\nad-hoc strategies e.g. applying known data-augmentations to the same input. In\nthis work, we generalize and formalize this principle through Positive Active\nLearning (PAL) where an oracle queries semantic relationships between samples.\nPAL achieves three main objectives. First, it unveils a theoretically grounded\nlearning framework beyond SSL, that can be extended to tackle supervised and\nsemi-supervised learning depending on the employed oracle. Second, it provides\na consistent algorithm to embed a priori knowledge, e.g. some observed labels,\ninto any SSL losses without any change in the training pipeline. Third, it\nprovides a proper active learning framework yielding low-cost solutions to\nannotate datasets, arguably bringing the gap between theory and practice of\nactive learning that is based on simple-to-answer-by-non-experts queries of\nsemantic relationships between inputs.\n","authors":["Vivien Cabannes","Leon Bottou","Yann Lecun","Randall Balestriero"],"pdf_url":"https://arxiv.org/pdf/2303.15256v1.pdf","comment":"8 main pages, 20 totals, 10 figures"},{"id":"http://arxiv.org/abs/2303.15244v1","updated":"2023-03-27T14:29:04Z","published":"2023-03-27T14:29:04Z","title":"Manifold Learning by Mixture Models of VAEs for Inverse Problems","summary":"  Representing a manifold of very high-dimensional data with generative models\nhas been shown to be computationally efficient in practice. However, this\nrequires that the data manifold admits a global parameterization. In order to\nrepresent manifolds of arbitrary topology, we propose to learn a mixture model\nof variational autoencoders. Here, every encoder-decoder pair represents one\nchart of a manifold. We propose a loss function for maximum likelihood\nestimation of the model weights and choose an architecture that provides us the\nanalytical expression of the charts and of their inverses. Once the manifold is\nlearned, we use it for solving inverse problems by minimizing a data fidelity\nterm restricted to the learned manifold. To solve the arising minimization\nproblem we propose a Riemannian gradient descent algorithm on the learned\nmanifold. We demonstrate the performance of our method for low-dimensional toy\nexamples as well as for deblurring and electrical impedance tomography on\ncertain image manifolds.\n","authors":["Giovanni S. Alberti","Johannes Hertrich","Matteo Santacesaria","Silvia Sciutto"],"pdf_url":"https://arxiv.org/pdf/2303.15244v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2211.09694v2","updated":"2023-03-27T14:27:57Z","published":"2022-11-17T17:35:37Z","title":"Thermodynamics of bidirectional associative memories","summary":"  In this paper we investigate the equilibrium properties of bidirectional\nassociative memories (BAMs). Introduced by Kosko in 1988 as a generalization of\nthe Hopfield model to a bipartite structure, the simplest architecture is\ndefined by two layers of neurons, with synaptic connections only between units\nof different layers: even without internal connections within each layer,\ninformation storage and retrieval are still possible through the reverberation\nof neural activities passing from one layer to another. We characterize the\ncomputational capabilities of a stochastic extension of this model in the\nthermodynamic limit, by applying rigorous techniques from statistical physics.\nA detailed picture of the phase diagram at the replica symmetric level is\nprovided, both at finite temperature and in the noiseless regimes. Also for the\nlatter, the critical load is further investigated up to one step of replica\nsymmetry breaking. An analytical and numerical inspection of the transition\ncurves (namely critical lines splitting the various modes of operation of the\nmachine) is carried out as the control parameters - noise, load and asymmetry\nbetween the two layer sizes - are tuned. In particular, with a finite asymmetry\nbetween the two layers, it is shown how the BAM can store information more\nefficiently than the Hopfield model by requiring less parameters to encode a\nfixed number of patterns. Comparisons are made with numerical simulations of\nneural dynamics. Finally, a low-load analysis is carried out to explain the\nretrieval mechanism in the BAM by analogy with two interacting Hopfield models.\nA potential equivalence with two coupled Restricted Boltmzann Machines is also\ndiscussed.\n","authors":["Adriano Barra","Giovanni Catania","Aurélien Decelle","Beatriz Seoane"],"pdf_url":"https://arxiv.org/pdf/2211.09694v2.pdf","comment":"25 pages, 11 figures"},{"id":"http://arxiv.org/abs/2303.13937v2","updated":"2023-03-27T14:27:45Z","published":"2023-03-24T11:50:08Z","title":"Topological Reconstruction of Particle Physics Processes using Graph\n  Neural Networks","summary":"  We present a new approach, the Topograph, which reconstructs underlying\nphysics processes, including the intermediary particles, by leveraging\nunderlying priors from the nature of particle physics decays and the\nflexibility of message passing graph neural networks. The Topograph not only\nsolves the combinatoric assignment of observed final state objects, associating\nthem to their original mother particles, but directly predicts the properties\nof intermediate particles in hard scatter processes and their subsequent\ndecays. In comparison to standard combinatoric approaches or modern approaches\nusing graph neural networks, which scale exponentially or quadratically, the\ncomplexity of Topographs scales linearly with the number of reconstructed\nobjects.\n  We apply Topographs to top quark pair production in the all hadronic decay\nchannel, where we outperform the standard approach and match the performance of\nthe state-of-the-art machine learning technique.\n","authors":["Lukas Ehrke","John Andrew Raine","Knut Zoch","Manuel Guth","Tobias Golling"],"pdf_url":"https://arxiv.org/pdf/2303.13937v2.pdf","comment":"24 pages, 24 figures, 7 tables"},{"id":"http://arxiv.org/abs/2303.09522v2","updated":"2023-03-27T14:22:28Z","published":"2023-03-16T17:38:15Z","title":"$P+$: Extended Textual Conditioning in Text-to-Image Generation","summary":"  We introduce an Extended Textual Conditioning space in text-to-image models,\nreferred to as $P+$. This space consists of multiple textual conditions,\nderived from per-layer prompts, each corresponding to a layer of the denoising\nU-net of the diffusion model.\n  We show that the extended space provides greater disentangling and control\nover image synthesis. We further introduce Extended Textual Inversion (XTI),\nwhere the images are inverted into $P+$, and represented by per-layer tokens.\n  We show that XTI is more expressive and precise, and converges faster than\nthe original Textual Inversion (TI) space. The extended inversion method does\nnot involve any noticeable trade-off between reconstruction and editability and\ninduces more regular inversions.\n  We conduct a series of extensive experiments to analyze and understand the\nproperties of the new space, and to showcase the effectiveness of our method\nfor personalizing text-to-image models. Furthermore, we utilize the unique\nproperties of this space to achieve previously unattainable results in\nobject-style mixing using text-to-image models. Project page:\nhttps://prompt-plus.github.io\n","authors":["Andrey Voynov","Qinghao Chu","Daniel Cohen-Or","Kfir Aberman"],"pdf_url":"https://arxiv.org/pdf/2303.09522v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2302.07061v2","updated":"2023-03-27T14:15:37Z","published":"2023-02-14T14:21:26Z","title":"Do Deep Learning Methods Really Perform Better in Molecular Conformation\n  Generation?","summary":"  Molecular conformation generation (MCG) is a fundamental and important\nproblem in drug discovery. Many traditional methods have been developed to\nsolve the MCG problem, such as systematic searching, model-building, random\nsearching, distance geometry, molecular dynamics, Monte Carlo methods, etc.\nHowever, they have some limitations depending on the molecular structures.\nRecently, there are plenty of deep learning based MCG methods, which claim they\nlargely outperform the traditional methods. However, to our surprise, we design\na simple and cheap algorithm (parameter-free) based on the traditional methods\nand find it is comparable to or even outperforms deep learning based MCG\nmethods in the widely used GEOM-QM9 and GEOM-Drugs benchmarks. In particular,\nour design algorithm is simply the clustering of the RDKIT-generated\nconformations. We hope our findings can help the community to revise the deep\nlearning methods for MCG. The code of the proposed algorithm could be found at\nhttps://gist.github.com/ZhouGengmo/5b565f51adafcd911c0bc115b2ef027c.\n","authors":["Gengmo Zhou","Zhifeng Gao","Zhewei Wei","Hang Zheng","Guolin Ke"],"pdf_url":"https://arxiv.org/pdf/2302.07061v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.15233v1","updated":"2023-03-27T14:15:17Z","published":"2023-03-27T14:15:17Z","title":"Text-to-Image Diffusion Models are Zero-Shot Classifiers","summary":"  The excellent generative capabilities of text-to-image diffusion models\nsuggest they learn informative representations of image-text data. However,\nwhat knowledge their representations capture is not fully understood, and they\nhave not been thoroughly explored on downstream tasks. We investigate diffusion\nmodels by proposing a method for evaluating them as zero-shot classifiers. The\nkey idea is using a diffusion model's ability to denoise a noised image given a\ntext description of a label as a proxy for that label's likelihood. We apply\nour method to Imagen, using it to probe fine-grained aspects of Imagen's\nknowledge and comparing it with CLIP's zero-shot abilities. Imagen performs\ncompetitively with CLIP on a wide range of zero-shot image classification\ndatasets. Additionally, it achieves state-of-the-art results on shape/texture\nbias tests and can successfully perform attribute binding while CLIP cannot.\nAlthough generative pre-training is prevalent in NLP, visual foundation models\noften use other methods such as contrastive learning. Based on our findings, we\nargue that generative pre-training should be explored as a compelling\nalternative for vision and vision-language problems.\n","authors":["Kevin Clark","Priyank Jaini"],"pdf_url":"https://arxiv.org/pdf/2303.15233v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.15230v1","updated":"2023-03-27T14:10:26Z","published":"2023-03-27T14:10:26Z","title":"Troika: Multi-Path Cross-Modal Traction for Compositional Zero-Shot\n  Learning","summary":"  Recent compositional zero-shot learning (CZSL) methods adapt pre-trained\nvision-language models (VLMs) by constructing trainable prompts only for\ncomposed state-object pairs. Relying on learning the joint representation of\nseen compositions, these methods ignore the explicit modeling of the state and\nobject, thus limiting the exploitation of pre-trained knowledge and\ngeneralization to unseen compositions. With a particular focus on the\nuniversality of the solution, in this work, we propose a novel paradigm for\nCZSL models that establishes three identification branches (i.e., Multi-Path)\nto jointly model the state, object, and composition. The presented Troika is\nour implementation that aligns the branch-specific prompt representations with\ndecomposed visual features. To calibrate the bias between semantically similar\nmulti-modal representations, we further devise a Cross-Modal Traction module\ninto Troika that shifts the prompt representation towards the current visual\ncontent. We conduct extensive experiments on three popular benchmarks, where\nour method significantly outperforms existing methods in both closed-world and\nopen-world settings.\n","authors":["Siteng Huang","Biao Gong","Yutong Feng","Yiliang Lv","Donglin Wang"],"pdf_url":"https://arxiv.org/pdf/2303.15230v1.pdf","comment":"14 pages"},{"id":"http://arxiv.org/abs/2303.15228v1","updated":"2023-03-27T14:08:27Z","published":"2023-03-27T14:08:27Z","title":"Interpretable machine learning of amino acid patterns in proteins: a\n  statistical ensemble approach","summary":"  Explainable and interpretable unsupervised machine learning helps understand\nthe underlying structure of data. We introduce an ensemble analysis of machine\nlearning models to consolidate their interpretation. Its application shows that\nrestricted Boltzmann machines compress consistently into a few bits the\ninformation stored in a sequence of five amino acids at the start or end of\n$\\alpha$-helices or $\\beta$-sheets. The weights learned by the machines reveal\nunexpected properties of the amino acids and the secondary structure of\nproteins: (i) His and Thr have a negligible contribution to the amphiphilic\npattern of $\\alpha$-helices; (ii) there is a class of $\\alpha$-helices\nparticularly rich in Ala at their end; (iii) Pro occupies most often slots\notherwise occupied by polar or charged amino acids, and its presence at the\nstart of helices is relevant; (iv) Glu and especially Asp on one side, and Val,\nLeu, Iso, and Phe on the other, display the strongest tendency to mark\namphiphilic patterns, i.e., extreme values of an \"effective hydrophobicity\",\nthough they are not the most powerful (non) hydrophobic amino acids.\n","authors":["Anna Braghetto","Enzo Orlandini","Marco Baiesi"],"pdf_url":"https://arxiv.org/pdf/2303.15228v1.pdf","comment":"15 pages, 9 figures"},{"id":"http://arxiv.org/abs/2303.15226v1","updated":"2023-03-27T14:06:05Z","published":"2023-03-27T14:06:05Z","title":"Asynchronous Online Federated Learning with Reduced Communication\n  Requirements","summary":"  Online federated learning (FL) enables geographically distributed devices to\nlearn a global shared model from locally available streaming data. Most online\nFL literature considers a best-case scenario regarding the participating\nclients and the communication channels. However, these assumptions are often\nnot met in real-world applications. Asynchronous settings can reflect a more\nrealistic environment, such as heterogeneous client participation due to\navailable computational power and battery constraints, as well as delays caused\nby communication channels or straggler devices. Further, in most applications,\nenergy efficiency must be taken into consideration. Using the principles of\npartial-sharing-based communications, we propose a communication-efficient\nasynchronous online federated learning (PAO-Fed) strategy. By reducing the\ncommunication overhead of the participants, the proposed method renders\nparticipation in the learning task more accessible and efficient. In addition,\nthe proposed aggregation mechanism accounts for random participation, handles\ndelayed updates and mitigates their effect on accuracy. We prove the first and\nsecond-order convergence of the proposed PAO-Fed method and obtain an\nexpression for its steady-state mean square deviation. Finally, we conduct\ncomprehensive simulations to study the performance of the proposed method on\nboth synthetic and real-life datasets. The simulations reveal that in\nasynchronous settings, the proposed PAO-Fed is able to achieve the same\nconvergence properties as that of the online federated stochastic gradient\nwhile reducing the communication overhead by 98 percent.\n","authors":["Francois Gauthier","Vinay Chakravarthi Gogineni","Stefan Werner","Yih-Fang Huang","Anthony Kuh"],"pdf_url":"https://arxiv.org/pdf/2303.15226v1.pdf","comment":"A conference precursor of this work appears in the 2022 IEEE ICC"},{"id":"http://arxiv.org/abs/2303.15219v1","updated":"2023-03-27T13:59:26Z","published":"2023-03-27T13:59:26Z","title":"Knowing the Distance: Understanding the Gap Between Synthetic and Real\n  Data For Face Parsing","summary":"  The use of synthetic data for training computer vision algorithms has become\nincreasingly popular due to its cost-effectiveness, scalability, and ability to\nprovide accurate multi-modality labels. Although recent studies have\ndemonstrated impressive results when training networks solely on synthetic\ndata, there remains a performance gap between synthetic and real data that is\ncommonly attributed to lack of photorealism. The aim of this study is to\ninvestigate the gap in greater detail for the face parsing task. We\ndifferentiate between three types of gaps: distribution gap, label gap, and\nphotorealism gap. Our findings show that the distribution gap is the largest\ncontributor to the performance gap, accounting for over 50% of the gap. By\naddressing this gap and accounting for the labels gap, we demonstrate that a\nmodel trained on synthetic data achieves comparable results to one trained on a\nsimilar amount of real data. This suggests that synthetic data is a viable\nalternative to real data, especially when real data is limited or difficult to\nobtain. Our study highlights the importance of content diversity in synthetic\ndatasets and challenges the notion that the photorealism gap is the most\ncritical factor affecting the performance of computer vision models trained on\nsynthetic data.\n","authors":["Eli Friedman","Assaf Lehr","Alexey Gruzdev","Vladimir Loginov","Max Kogan","Moran Rubin","Orly Zvitia"],"pdf_url":"https://arxiv.org/pdf/2303.15219v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.15218v1","updated":"2023-03-27T13:59:22Z","published":"2023-03-27T13:59:22Z","title":"Evaluating XGBoost for Balanced and Imbalanced Data: Application to\n  Fraud Detection","summary":"  This paper evaluates XGboost's performance given different dataset sizes and\nclass distributions, from perfectly balanced to highly imbalanced. XGBoost has\nbeen selected for evaluation, as it stands out in several benchmarks due to its\ndetection performance and speed. After introducing the problem of fraud\ndetection, the paper reviews evaluation metrics for detection systems or binary\nclassifiers, and illustrates with examples how different metrics work for\nbalanced and imbalanced datasets. Then, it examines the principles of XGBoost.\nIt proposes a pipeline for data preparation and compares a Vanilla XGBoost\nagainst a random search-tuned XGBoost. Random search fine-tuning provides\nconsistent improvement for large datasets of 100 thousand samples, not so for\nmedium and small datasets of 10 and 1 thousand samples, respectively. Besides,\nas expected, XGBoost recognition performance improves as more data is\navailable, and deteriorates detection performance as the datasets become more\nimbalanced. Tests on distributions with 50, 45, 25, and 5 percent positive\nsamples show that the largest drop in detection performance occurs for the\ndistribution with only 5 percent positive samples. Sampling to balance the\ntraining set does not provide consistent improvement. Therefore, future work\nwill include a systematic study of different techniques to deal with data\nimbalance and evaluating other approaches, including graphs, autoencoders, and\ngenerative adversarial methods, to deal with the lack of labels.\n","authors":["Gissel Velarde","Anindya Sudhir","Sanjay Deshmane","Anuj Deshmunkh","Khushboo Sharma","Vaibhav Joshi"],"pdf_url":"https://arxiv.org/pdf/2303.15218v1.pdf","comment":"17 pages, 8 figures, 9 tables, Presented at NVIDIA GTC, The\n  Conference for the Era of AI and the Metaverse, March 23, 2023. [S51129]"},{"id":"http://arxiv.org/abs/2303.15216v1","updated":"2023-03-27T13:57:13Z","published":"2023-03-27T13:57:13Z","title":"Robust Risk-Aware Option Hedging","summary":"  The objectives of option hedging/trading extend beyond mere protection\nagainst downside risks, with a desire to seek gains also driving agent's\nstrategies. In this study, we showcase the potential of robust risk-aware\nreinforcement learning (RL) in mitigating the risks associated with\npath-dependent financial derivatives. We accomplish this by leveraging the\nJaimungal, Pesenti, Wang, Tatsat (2022) and their policy gradient approach,\nwhich optimises robust risk-aware performance criteria. We specifically apply\nthis methodology to the hedging of barrier options, and highlight how the\noptimal hedging strategy undergoes distortions as the agent moves from being\nrisk-averse to risk-seeking. As well as how the agent robustifies their\nstrategy. We further investigate the performance of the hedge when the data\ngenerating process (DGP) varies from the training DGP, and demonstrate that the\nrobust strategies outperform the non-robust ones.\n","authors":["David Wu","Sebastian Jaimungal"],"pdf_url":"https://arxiv.org/pdf/2303.15216v1.pdf","comment":"16 pages, 14 figures, 1 table"},{"id":"http://arxiv.org/abs/2303.15214v1","updated":"2023-03-27T13:55:07Z","published":"2023-03-27T13:55:07Z","title":"CLIDiM: Contrastive Learning for Image Denoising in Microscopy","summary":"  Microscopy images often suffer from high levels of noise, which can hinder\nfurther analysis and interpretation. Content-aware image restoration (CARE)\nmethods have been proposed to address this issue, but they often require large\namounts of training data and suffer from over-fitting. To overcome these\nchallenges, we propose a novel framework for few-shot microscopy image\ndenoising. Our approach combines a generative adversarial network (GAN) trained\nvia contrastive learning (CL) with two structure preserving loss terms\n(Structural Similarity Index and Total Variation loss) to further improve the\nquality of the denoised images using little data. We demonstrate the\neffectiveness of our method on three well-known microscopy imaging datasets,\nand show that we can drastically reduce the amount of training data while\nretaining the quality of the denoising, thus alleviating the burden of\nacquiring paired data and enabling few-shot learning. The proposed framework\ncan be easily extended to other image restoration tasks and has the potential\nto significantly advance the field of microscopy image analysis.\n","authors":["Felix Fuentes-Hurtado","Jean-Baptiste Sibarita","Virgile Viasnoff"],"pdf_url":"https://arxiv.org/pdf/2303.15214v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2210.06422v2","updated":"2023-03-27T13:53:56Z","published":"2022-10-12T17:15:44Z","title":"A New Family of Generalization Bounds Using Samplewise Evaluated CMI","summary":"  We present a new family of information-theoretic generalization bounds, in\nwhich the training loss and the population loss are compared through a jointly\nconvex function. This function is upper-bounded in terms of the disintegrated,\nsamplewise, evaluated conditional mutual information (CMI), an information\nmeasure that depends on the losses incurred by the selected hypothesis, rather\nthan on the hypothesis itself, as is common in probably approximately correct\n(PAC)-Bayesian results. We demonstrate the generality of this framework by\nrecovering and extending previously known information-theoretic bounds.\nFurthermore, using the evaluated CMI, we derive a samplewise, average version\nof Seeger's PAC-Bayesian bound, where the convex function is the binary KL\ndivergence. In some scenarios, this novel bound results in a tighter\ncharacterization of the population loss of deep neural networks than previous\nbounds. Finally, we derive high-probability versions of some of these average\nbounds. We demonstrate the unifying nature of the evaluated CMI bounds by using\nthem to recover average and high-probability generalization bounds for\nmulticlass classification with finite Natarajan dimension.\n","authors":["Fredrik Hellström","Giuseppe Durisi"],"pdf_url":"https://arxiv.org/pdf/2210.06422v2.pdf","comment":"NeurIPS 2022"},{"id":"http://arxiv.org/abs/2303.15212v1","updated":"2023-03-27T13:52:40Z","published":"2023-03-27T13:52:40Z","title":"Deep Ranking Ensembles for Hyperparameter Optimization","summary":"  Automatically optimizing the hyperparameters of Machine Learning algorithms\nis one of the primary open questions in AI. Existing work in Hyperparameter\nOptimization (HPO) trains surrogate models for approximating the response\nsurface of hyperparameters as a regression task. In contrast, we hypothesize\nthat the optimal strategy for training surrogates is to preserve the ranks of\nthe performances of hyperparameter configurations as a Learning to Rank\nproblem. As a result, we present a novel method that meta-learns neural network\nsurrogates optimized for ranking the configurations' performances while\nmodeling their uncertainty via ensembling. In a large-scale experimental\nprotocol comprising 12 baselines, 16 HPO search spaces and 86 datasets/tasks,\nwe demonstrate that our method achieves new state-of-the-art results in HPO.\n","authors":["Abdus Salam Khazi","Sebastian Pineda Arango","Josif Grabocka"],"pdf_url":"https://arxiv.org/pdf/2303.15212v1.pdf","comment":"Published in ICLR 2023"},{"id":"http://arxiv.org/abs/2303.15210v1","updated":"2023-03-27T13:51:56Z","published":"2023-03-27T13:51:56Z","title":"On the Connection between $L_p$ and Risk Consistency and its\n  Implications on Regularized Kernel Methods","summary":"  As a predictor's quality is often assessed by means of its risk, it is\nnatural to regard risk consistency as a desirable property of learning methods,\nand many such methods have indeed been shown to be risk consistent. The first\naim of this paper is to establish the close connection between risk consistency\nand $L_p$-consistency for a considerably wider class of loss functions than has\nbeen done before. The attempt to transfer this connection to shifted loss\nfunctions surprisingly reveals that this shift does not reduce the assumptions\nneeded on the underlying probability measure to the same extent as it does for\nmany other results. The results are applied to regularized kernel methods such\nas support vector machines.\n","authors":["Hannes Köhler"],"pdf_url":"https://arxiv.org/pdf/2303.15210v1.pdf","comment":"33 pages, 1 figure"},{"id":"http://arxiv.org/abs/2303.15201v1","updated":"2023-03-27T13:39:26Z","published":"2023-03-27T13:39:26Z","title":"An active inference model of car following: Advantages and applications","summary":"  Driver process models play a central role in the testing, verification, and\ndevelopment of automated and autonomous vehicle technologies. Prior models\ndeveloped from control theory and physics-based rules are limited in automated\nvehicle applications due to their restricted behavioral repertoire. Data-driven\nmachine learning models are more capable than rule-based models but are limited\nby the need for large training datasets and their lack of interpretability,\ni.e., an understandable link between input data and output behaviors. We\npropose a novel car following modeling approach using active inference, which\nhas comparable behavioral flexibility to data-driven models while maintaining\ninterpretability. We assessed the proposed model, the Active Inference Driving\nAgent (AIDA), through a benchmark analysis against the rule-based Intelligent\nDriver Model, and two neural network Behavior Cloning models. The models were\ntrained and tested on a real-world driving dataset using a consistent process.\nThe testing results showed that the AIDA predicted driving controls\nsignificantly better than the rule-based Intelligent Driver Model and had\nsimilar accuracy to the data-driven neural network models in three out of four\nevaluations. Subsequent interpretability analyses illustrated that the AIDA's\nlearned distributions were consistent with driver behavior theory and that\nvisualizations of the distributions could be used to directly comprehend the\nmodel's decision making process and correct model errors attributable to\nlimited training data. The results indicate that the AIDA is a promising\nalternative to black-box data-driven models and suggest a need for further\nresearch focused on modeling driving style and model training with more diverse\ndatasets.\n","authors":["Ran Wei","Anthony D. McDonald","Alfredo Garcia","Gustav Markkula","Johan Engstrom","Matthew O'Kelly"],"pdf_url":"https://arxiv.org/pdf/2303.15201v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.15196v1","updated":"2023-03-27T13:35:28Z","published":"2023-03-27T13:35:28Z","title":"Probing optimisation in physics-informed neural networks","summary":"  A novel comparison is presented of the effect of optimiser choice on the\naccuracy of physics-informed neural networks (PINNs). To give insight into why\nsome optimisers are better, a new approach is proposed that tracks the training\ntrajectory curvature and can be evaluated on the fly at a low computational\ncost. The linear advection equation is studied for several advective\nvelocities, and we show that the optimiser choice substantially impacts PINNs\nmodel performance and accuracy. Furthermore, using the curvature measure, we\nfound a negative correlation between the convergence error and the curvature in\nthe optimiser local reference frame. It is concluded that, in this case, larger\nlocal curvature values result in better solutions. Consequently, optimisation\nof PINNs is made more difficult as minima are in highly curved regions.\n","authors":["Nayara Fonseca","Veronica Guidetti","Will Trojak"],"pdf_url":"https://arxiv.org/pdf/2303.15196v1.pdf","comment":"Accepted at the ICLR 2023 Workshop on Physics for Machine Learning"},{"id":"http://arxiv.org/abs/2109.08549v5","updated":"2023-03-27T13:33:16Z","published":"2021-09-17T13:45:46Z","title":"Measuring Fairness Under Unawareness of Sensitive Attributes: A\n  Quantification-Based Approach","summary":"  Algorithms and models are increasingly deployed to inform decisions about\npeople, inevitably affecting their lives. As a consequence, those in charge of\ndeveloping these models must carefully evaluate their impact on different\ngroups of people and favour group fairness, that is, ensure that groups\ndetermined by sensitive demographic attributes, such as race or sex, are not\ntreated unjustly. To achieve this goal, the availability (awareness) of these\ndemographic attributes to those evaluating the impact of these models is\nfundamental. Unfortunately, collecting and storing these attributes is often in\nconflict with industry practices and legislation on data minimisation and\nprivacy. For this reason, it can be hard to measure the group fairness of\ntrained models, even from within the companies developing them. In this work,\nwe tackle the problem of measuring group fairness under unawareness of\nsensitive attributes, by using techniques from quantification, a supervised\nlearning task concerned with directly providing group-level prevalence\nestimates (rather than individual-level class labels). We show that\nquantification approaches are particularly suited to tackle the\nfairness-under-unawareness problem, as they are robust to inevitable\ndistribution shifts while at the same time decoupling the (desirable) objective\nof measuring group fairness from the (undesirable) side effect of allowing the\ninference of sensitive attributes of individuals. More in detail, we show that\nfairness under unawareness can be cast as a quantification problem and solved\nwith proven methods from the quantification literature. We show that these\nmethods outperform previous approaches to measure demographic parity in five\nexperimental protocols, corresponding to important challenges that complicate\nthe estimation of classifier fairness under unawareness.\n","authors":["Alessandro Fabris","Andrea Esuli","Alejandro Moreo","Fabrizio Sebastiani"],"pdf_url":"https://arxiv.org/pdf/2109.08549v5.pdf","comment":"Accepted for publication in the Journal of Artificial Intelligence\n  Research"},{"id":"http://arxiv.org/abs/2202.02831v2","updated":"2023-03-27T13:29:55Z","published":"2022-02-06T18:52:21Z","title":"Anticorrelated Noise Injection for Improved Generalization","summary":"  Injecting artificial noise into gradient descent (GD) is commonly employed to\nimprove the performance of machine learning models. Usually, uncorrelated noise\nis used in such perturbed gradient descent (PGD) methods. It is, however, not\nknown if this is optimal or whether other types of noise could provide better\ngeneralization performance. In this paper, we zoom in on the problem of\ncorrelating the perturbations of consecutive PGD steps. We consider a variety\nof objective functions for which we find that GD with anticorrelated\nperturbations (\"Anti-PGD\") generalizes significantly better than GD and\nstandard (uncorrelated) PGD. To support these experimental findings, we also\nderive a theoretical analysis that demonstrates that Anti-PGD moves to wider\nminima, while GD and PGD remain stuck in suboptimal regions or even diverge.\nThis new connection between anticorrelated noise and generalization opens the\nfield to novel ways to exploit noise for training machine learning models.\n","authors":["Antonio Orvieto","Hans Kersting","Frank Proske","Francis Bach","Aurelien Lucchi"],"pdf_url":"https://arxiv.org/pdf/2202.02831v2.pdf","comment":"22 pages, 16 figures"},{"id":"http://arxiv.org/abs/2303.15190v1","updated":"2023-03-27T13:26:02Z","published":"2023-03-27T13:26:02Z","title":"Evaluating self-attention interpretability through human-grounded\n  experimental protocol","summary":"  Attention mechanisms have played a crucial role in the development of complex\narchitectures such as Transformers in natural language processing. However,\nTransformers remain hard to interpret and are considered as black-boxes. This\npaper aims to assess how attention coefficients from Transformers can help in\nproviding interpretability. A new attention-based interpretability method\ncalled CLaSsification-Attention (CLS-A) is proposed. CLS-A computes an\ninterpretability score for each word based on the attention coefficient\ndistribution related to the part specific to the classification task within the\nTransformer architecture. A human-grounded experiment is conducted to evaluate\nand compare CLS-A to other interpretability methods. The experimental protocol\nrelies on the capacity of an interpretability method to provide explanation in\nline with human reasoning. Experiment design includes measuring reaction times\nand correct response rates by human subjects. CLS-A performs comparably to\nusual interpretability methods regarding average participant reaction time and\naccuracy. The lower computational cost of CLS-A compared to other\ninterpretability methods and its availability by design within the classifier\nmake it particularly interesting. Data analysis also highlights the link\nbetween the probability score of a classifier prediction and adequate\nexplanations. Finally, our work confirms the relevancy of the use of CLS-A and\nshows to which extent self-attention contains rich information to explain\nTransformer classifiers.\n","authors":["Milan Bhan","Nina Achache","Victor Legrand","Annabelle Blangero","Nicolas Chesneau"],"pdf_url":"https://arxiv.org/pdf/2303.15190v1.pdf","comment":"11 pages, 7 figures"},{"id":"http://arxiv.org/abs/2303.15168v1","updated":"2023-03-27T13:00:20Z","published":"2023-03-27T13:00:20Z","title":"Personalized Federated Learning on Long-Tailed Data via Adversarial\n  Feature Augmentation","summary":"  Personalized Federated Learning (PFL) aims to learn personalized models for\neach client based on the knowledge across all clients in a privacy-preserving\nmanner. Existing PFL methods generally assume that the underlying global data\nacross all clients are uniformly distributed without considering the long-tail\ndistribution. The joint problem of data heterogeneity and long-tail\ndistribution in the FL environment is more challenging and severely affects the\nperformance of personalized models. In this paper, we propose a PFL method\ncalled Federated Learning with Adversarial Feature Augmentation (FedAFA) to\naddress this joint problem in PFL. FedAFA optimizes the personalized model for\neach client by producing a balanced feature set to enhance the local minority\nclasses. The local minority class features are generated by transferring the\nknowledge from the local majority class features extracted by the global model\nin an adversarial example learning manner. The experimental results on\nbenchmarks under different settings of data heterogeneity and long-tail\ndistribution demonstrate that FedAFA significantly improves the personalized\nperformance of each client compared with the state-of-the-art PFL algorithm.\nThe code is available at https://github.com/pxqian/FedAFA.\n","authors":["Yang Lu","Pinxin Qian","Gang Huang","Hanzi Wang"],"pdf_url":"https://arxiv.org/pdf/2303.15168v1.pdf","comment":"Accepted by ICASSP 2023"},{"id":"http://arxiv.org/abs/2303.15167v1","updated":"2023-03-27T12:59:33Z","published":"2023-03-27T12:59:33Z","title":"Prompt-Guided Zero-Shot Anomaly Action Recognition using Pretrained Deep\n  Skeleton Features","summary":"  This study investigates unsupervised anomaly action recognition, which\nidentifies video-level abnormal-human-behavior events in an unsupervised manner\nwithout abnormal samples, and simultaneously addresses three limitations in the\nconventional skeleton-based approaches: target domain-dependent DNN training,\nrobustness against skeleton errors, and a lack of normal samples. We present a\nunified, user prompt-guided zero-shot learning framework using a target\ndomain-independent skeleton feature extractor, which is pretrained on a\nlarge-scale action recognition dataset. Particularly, during the training phase\nusing normal samples, the method models the distribution of skeleton features\nof the normal actions while freezing the weights of the DNNs and estimates the\nanomaly score using this distribution in the inference phase. Additionally, to\nincrease robustness against skeleton errors, we introduce a DNN architecture\ninspired by a point cloud deep learning paradigm, which sparsely propagates the\nfeatures between joints. Furthermore, to prevent the unobserved normal actions\nfrom being misidentified as abnormal actions, we incorporate a similarity score\nbetween the user prompt embeddings and skeleton features aligned in the common\nspace into the anomaly score, which indirectly supplements normal actions. On\ntwo publicly available datasets, we conduct experiments to test the\neffectiveness of the proposed method with respect to abovementioned\nlimitations.\n","authors":["Fumiaki Sato","Ryo Hachiuma","Taiki Sekii"],"pdf_url":"https://arxiv.org/pdf/2303.15167v1.pdf","comment":"CVPR 2023"},{"id":"http://arxiv.org/abs/2104.02317v4","updated":"2023-03-27T12:52:42Z","published":"2021-04-06T06:45:14Z","title":"A hybrid ensemble method with negative correlation learning for\n  regression","summary":"  Hybrid ensemble, an essential branch of ensembles, has flourished in the\nregression field, with studies confirming diversity's importance. However,\nprevious ensembles consider diversity in the sub-model training stage, with\nlimited improvement compared to single models. In contrast, this study\nautomatically selects and weights sub-models from a heterogeneous model pool.\nIt solves an optimization problem using an interior-point filtering\nlinear-search algorithm. The objective function innovatively incorporates\nnegative correlation learning as a penalty term, with which a diverse model\nsubset can be selected. The best sub-models from each model class are selected\nto build the NCL ensemble, which performance is better than the simple average\nand other state-of-the-art weighting methods. It is also possible to improve\nthe NCL ensemble with a regularization term in the objective function. In\npractice, it is difficult to conclude the optimal sub-model for a dataset prior\ndue to the model uncertainty. Regardless, our method would achieve comparable\naccuracy as the potential optimal sub-models. In conclusion, the value of this\nstudy lies in its ease of use and effectiveness, allowing the hybrid ensemble\nto embrace diversity and accuracy.\n","authors":["Yun Bai","Ganglin Tian","Yanfei Kang","Suling Jia"],"pdf_url":"https://arxiv.org/pdf/2104.02317v4.pdf","comment":"38 pages, 10 figures, 9 tables"},{"id":"http://arxiv.org/abs/2303.15132v1","updated":"2023-03-27T12:08:05Z","published":"2023-03-27T12:08:05Z","title":"Cross-utterance ASR Rescoring with Graph-based Label Propagation","summary":"  We propose a novel approach for ASR N-best hypothesis rescoring with\ngraph-based label propagation by leveraging cross-utterance acoustic\nsimilarity. In contrast to conventional neural language model (LM) based ASR\nrescoring/reranking models, our approach focuses on acoustic information and\nconducts the rescoring collaboratively among utterances, instead of\nindividually. Experiments on the VCTK dataset demonstrate that our approach\nconsistently improves ASR performance, as well as fairness across speaker\ngroups with different accents. Our approach provides a low-cost solution for\nmitigating the majoritarian bias of ASR systems, without the need to train new\ndomain- or accent-specific models.\n","authors":["Srinath Tankasala","Long Chen","Andreas Stolcke","Anirudh Raju","Qianli Deng","Chander Chandak","Aparna Khare","Roland Maas","Venkatesh Ravichandran"],"pdf_url":"https://arxiv.org/pdf/2303.15132v1.pdf","comment":"To appear in IEEE ICASSP 2023"},{"id":"http://arxiv.org/abs/2303.13763v2","updated":"2023-03-27T12:06:35Z","published":"2023-03-24T02:28:55Z","title":"Edge-free but Structure-aware: Prototype-Guided Knowledge Distillation\n  from GNNs to MLPs","summary":"  Distilling high-accuracy Graph Neural Networks~(GNNs) to low-latency\nmultilayer perceptrons~(MLPs) on graph tasks has become a hot research topic.\nHowever, MLPs rely exclusively on the node features and fail to capture the\ngraph structural information. Previous methods address this issue by processing\ngraph edges into extra inputs for MLPs, but such graph structures may be\nunavailable for various scenarios. To this end, we propose a Prototype-Guided\nKnowledge Distillation~(PGKD) method, which does not require graph\nedges~(edge-free) yet learns structure-aware MLPs. Specifically, we analyze the\ngraph structural information in GNN teachers, and distill such information from\nGNNs to MLPs via prototypes in an edge-free setting. Experimental results on\npopular graph benchmarks demonstrate the effectiveness and robustness of the\nproposed PGKD.\n","authors":["Taiqiang Wu","Zhe Zhao","Jiahao Wang","Xingyu Bai","Lei Wang","Ngai Wong","Yujiu Yang"],"pdf_url":"https://arxiv.org/pdf/2303.13763v2.pdf","comment":"8 pages, 4 figures, 9 tables"},{"id":"http://arxiv.org/abs/2303.15127v1","updated":"2023-03-27T12:00:54Z","published":"2023-03-27T12:00:54Z","title":"Learning the Unlearnable: Adversarial Augmentations Suppress Unlearnable\n  Example Attacks","summary":"  Unlearnable example attacks are data poisoning techniques that can be used to\nsafeguard public data against unauthorized use for training deep learning\nmodels. These methods add stealthy perturbations to the original image, thereby\nmaking it difficult for deep learning models to learn from these training data\neffectively. Current research suggests that adversarial training can, to a\ncertain degree, mitigate the impact of unlearnable example attacks, while\ncommon data augmentation methods are not effective against such poisons.\nAdversarial training, however, demands considerable computational resources and\ncan result in non-trivial accuracy loss. In this paper, we introduce the\nUEraser method, which outperforms current defenses against different types of\nstate-of-the-art unlearnable example attacks through a combination of effective\ndata augmentation policies and loss-maximizing adversarial augmentations. In\nstark contrast to the current SOTA adversarial training methods, UEraser uses\nadversarial augmentations, which extends beyond the confines of $ \\ell_p $\nperturbation budget assumed by current unlearning attacks and defenses. It also\nhelps to improve the model's generalization ability, thus protecting against\naccuracy loss. UEraser wipes out the unlearning effect with error-maximizing\ndata augmentations, thus restoring trained model accuracies. Interestingly,\nUEraser-Lite, a fast variant without adversarial augmentations, is also highly\neffective in preserving clean accuracies. On challenging unlearnable CIFAR-10,\nCIFAR-100, SVHN, and ImageNet-subset datasets produced with various attacks, it\nachieves results that are comparable to those obtained during clean training.\nWe also demonstrate its efficacy against possible adaptive attacks. Our code is\nopen source and available to the deep learning community:\nhttps://github.com/lafeat/ueraser.\n","authors":["Tianrui Qin","Xitong Gao","Juanjuan Zhao","Kejiang Ye","Cheng-Zhong Xu"],"pdf_url":"https://arxiv.org/pdf/2303.15127v1.pdf","comment":"UEraser introduces adversarial augmentations to suppress unlearnable\n  example attacks and outperforms current defenses"},{"id":"http://arxiv.org/abs/2303.15124v1","updated":"2023-03-27T11:56:20Z","published":"2023-03-27T11:56:20Z","title":"Blind Inpainting with Object-aware Discrimination for Artificial Marker\n  Removal","summary":"  Medical images often contain artificial markers added by doctors, which can\nnegatively affect the accuracy of AI-based diagnosis. To address this issue and\nrecover the missing visual contents, inpainting techniques are highly needed.\nHowever, existing inpainting methods require manual mask input, limiting their\napplication scenarios. In this paper, we introduce a novel blind inpainting\nmethod that automatically completes visual contents without specifying masks\nfor target areas in an image. Our proposed model includes a mask-free\nreconstruction network and an object-aware discriminator. The reconstruction\nnetwork consists of two branches that predict the corrupted regions with\nartificial markers and simultaneously recover the missing visual contents. The\nobject-aware discriminator relies on the powerful recognition capabilities of\nthe dense object detector to ensure that the markers of reconstructed images\ncannot be detected in any local regions. As a result, the reconstructed image\ncan be close to the clean one as much as possible. Our proposed method is\nevaluated on different medical image datasets, covering multiple imaging\nmodalities such as ultrasound (US), magnetic resonance imaging (MRI), and\nelectron microscopy (EM), demonstrating that our method is effective and robust\nagainst various unknown missing region patterns.\n","authors":["Xuechen Guo","Wenhao Hu","Chiming Ni","Wenhao Chai","Shiyan Li","Gaoang Wang"],"pdf_url":"https://arxiv.org/pdf/2303.15124v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.10880v4","updated":"2023-03-27T11:52:20Z","published":"2023-03-20T05:38:30Z","title":"Rotating without Seeing: Towards In-hand Dexterity through Touch","summary":"  Tactile information plays a critical role in human dexterity. It reveals\nuseful contact information that may not be inferred directly from vision. In\nfact, humans can even perform in-hand dexterous manipulation without using\nvision. Can we enable the same ability for the multi-finger robot hand? In this\npaper, we present Touch Dexterity, a new system that can perform in-hand object\nrotation using only touching without seeing the object. Instead of relying on\nprecise tactile sensing in a small region, we introduce a new system design\nusing dense binary force sensors (touch or no touch) overlaying one side of the\nwhole robot hand (palm, finger links, fingertips). Such a design is low-cost,\ngiving a larger coverage of the object, and minimizing the Sim2Real gap at the\nsame time. We train an in-hand rotation policy using Reinforcement Learning on\ndiverse objects in simulation. Relying on touch-only sensing, we can directly\ndeploy the policy in a real robot hand and rotate novel objects that are not\npresented in training. Extensive ablations are performed on how tactile\ninformation help in-hand manipulation.Our project is available at\nhttps://touchdexterity.github.io.\n","authors":["Zhao-Heng Yin","Binghao Huang","Yuzhe Qin","Qifeng Chen","Xiaolong Wang"],"pdf_url":"https://arxiv.org/pdf/2303.10880v4.pdf","comment":"Project page: https://touchdexterity.github.io"},{"id":"http://arxiv.org/abs/2201.07513v2","updated":"2023-03-27T11:45:43Z","published":"2022-01-19T10:27:28Z","title":"Can't Steal? Cont-Steal! Contrastive Stealing Attacks Against Image\n  Encoders","summary":"  Self-supervised representation learning techniques have been developing\nrapidly to make full use of unlabeled images. They encode images into rich\nfeatures that are oblivious to downstream tasks. Behind their revolutionary\nrepresentation power, the requirements for dedicated model designs and a\nmassive amount of computation resources expose image encoders to the risks of\npotential model stealing attacks - a cheap way to mimic the well-trained\nencoder performance while circumventing the demanding requirements. Yet\nconventional attacks only target supervised classifiers given their predicted\nlabels and/or posteriors, which leaves the vulnerability of unsupervised\nencoders unexplored.\n  In this paper, we first instantiate the conventional stealing attacks against\nencoders and demonstrate their severer vulnerability compared with downstream\nclassifiers. To better leverage the rich representation of encoders, we further\npropose Cont-Steal, a contrastive-learning-based attack, and validate its\nimproved stealing effectiveness in various experiment settings. As a takeaway,\nwe appeal to our community's attention to the intellectual property protection\nof representation learning techniques, especially to the defenses against\nencoder stealing attacks like ours.\n","authors":["Zeyang Sha","Xinlei He","Ning Yu","Michael Backes","Yang Zhang"],"pdf_url":"https://arxiv.org/pdf/2201.07513v2.pdf","comment":"Accepted by CVPR 2023"},{"id":"http://arxiv.org/abs/2303.13391v2","updated":"2023-03-27T11:36:12Z","published":"2023-03-23T16:07:31Z","title":"Xplainer: From X-Ray Observations to Explainable Zero-Shot Diagnosis","summary":"  Automated diagnosis prediction from medical images is a valuable resource to\nsupport clinical decision-making. However, such systems usually need to be\ntrained on large amounts of annotated data, which often is scarce in the\nmedical domain. Zero-shot methods address this challenge by allowing a flexible\nadaption to new settings with different clinical findings without relying on\nlabeled data. Further, to integrate automated diagnosis in the clinical\nworkflow, methods should be transparent and explainable, increasing medical\nprofessionals' trust and facilitating correctness verification. In this work,\nwe introduce Xplainer, a novel framework for explainable zero-shot diagnosis in\nthe clinical setting. Xplainer adapts the classification-by-description\napproach of contrastive vision-language models to the multi-label medical\ndiagnosis task. Specifically, instead of directly predicting a diagnosis, we\nprompt the model to classify the existence of descriptive observations, which a\nradiologist would look for on an X-Ray scan, and use the descriptor\nprobabilities to estimate the likelihood of a diagnosis. Our model is\nexplainable by design, as the final diagnosis prediction is directly based on\nthe prediction of the underlying descriptors. We evaluate Xplainer on two chest\nX-ray datasets, CheXpert and ChestX-ray14, and demonstrate its effectiveness in\nimproving the performance and explainability of zero-shot diagnosis. Our\nresults suggest that Xplainer provides a more detailed understanding of the\ndecision-making process and can be a valuable tool for clinical diagnosis.\n","authors":["Chantal Pellegrini","Matthias Keicher","Ege Özsoy","Petra Jiraskova","Rickmer Braren","Nassir Navab"],"pdf_url":"https://arxiv.org/pdf/2303.13391v2.pdf","comment":"9 pages, 2 figures, 6 tables"},{"id":"http://arxiv.org/abs/2301.12896v2","updated":"2023-03-27T11:35:15Z","published":"2023-01-30T13:58:14Z","title":"Identifying Adversarially Attackable and Robust Samples","summary":"  Adversarial attacks insert small, imperceptible perturbations to input\nsamples that cause large, undesired changes to the output of deep learning\nmodels. Despite extensive research on generating adversarial attacks and\nbuilding defense systems, there has been limited research on understanding\nadversarial attacks from an input-data perspective. This work introduces the\nnotion of sample attackability, where we aim to identify samples that are most\nsusceptible to adversarial attacks (attackable samples) and conversely also\nidentify the least susceptible samples (robust samples). We propose a\ndeep-learning-based method to detect the adversarially attackable and robust\nsamples in an unseen dataset for an unseen target model. Experiments on\nstandard image classification datasets enables us to assess the portability of\nthe deep attackability detector across a range of architectures. We find that\nthe deep attackability detector performs better than simple model\nuncertainty-based measures for identifying the attackable/robust samples. This\nsuggests that uncertainty is an inadequate proxy for measuring sample distance\nto a decision boundary. In addition to better understanding adversarial attack\ntheory, it is found that the ability to identify the adversarially attackable\nand robust samples has implications for improving the efficiency of\nsample-selection tasks, e.g. active learning in augmentation for adversarial\ntraining.\n","authors":["Vyas Raina","Mark Gales"],"pdf_url":"https://arxiv.org/pdf/2301.12896v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.15109v1","updated":"2023-03-27T11:26:34Z","published":"2023-03-27T11:26:34Z","title":"Improving the Transferability of Adversarial Examples via Direction\n  Tuning","summary":"  In the transfer-based adversarial attacks, adversarial examples are only\ngenerated by the surrogate models and achieve effective perturbation in the\nvictim models. Although considerable efforts have been developed on improving\nthe transferability of adversarial examples generated by transfer-based\nadversarial attacks, our investigation found that, the big deviation between\nthe actual and steepest update directions of the current transfer-based\nadversarial attacks is caused by the large update step length, resulting in the\ngenerated adversarial examples can not converge well. However, directly\nreducing the update step length will lead to serious update oscillation so that\nthe generated adversarial examples also can not achieve great transferability\nto the victim models. To address these issues, a novel transfer-based attack,\nnamely direction tuning attack, is proposed to not only decrease the update\ndeviation in the large step length, but also mitigate the update oscillation in\nthe small sampling step length, thereby making the generated adversarial\nexamples converge well to achieve great transferability on victim models. In\naddition, a network pruning method is proposed to smooth the decision boundary,\nthereby further decreasing the update oscillation and enhancing the\ntransferability of the generated adversarial examples. The experiment results\non ImageNet demonstrate that the average attack success rate (ASR) of the\nadversarial examples generated by our method can be improved from 87.9\\% to\n94.5\\% on five victim models without defenses, and from 69.1\\% to 76.2\\% on\neight advanced defense methods, in comparison with that of latest\ngradient-based attacks.\n","authors":["Xiangyuan Yang","Jie Lin","Hanlin Zhang","Xinyu Yang","Peng Zhao"],"pdf_url":"https://arxiv.org/pdf/2303.15109v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2212.02802v2","updated":"2023-03-27T11:15:59Z","published":"2022-12-06T07:41:51Z","title":"Diffusion Video Autoencoders: Toward Temporally Consistent Face Video\n  Editing via Disentangled Video Encoding","summary":"  Inspired by the impressive performance of recent face image editing methods,\nseveral studies have been naturally proposed to extend these methods to the\nface video editing task. One of the main challenges here is temporal\nconsistency among edited frames, which is still unresolved. To this end, we\npropose a novel face video editing framework based on diffusion autoencoders\nthat can successfully extract the decomposed features - for the first time as a\nface video editing model - of identity and motion from a given video. This\nmodeling allows us to edit the video by simply manipulating the temporally\ninvariant feature to the desired direction for the consistency. Another unique\nstrength of our model is that, since our model is based on diffusion models, it\ncan satisfy both reconstruction and edit capabilities at the same time, and is\nrobust to corner cases in wild face videos (e.g. occluded faces) unlike the\nexisting GAN-based methods.\n","authors":["Gyeongman Kim","Hajin Shim","Hyunsu Kim","Yunjey Choi","Junho Kim","Eunho Yang"],"pdf_url":"https://arxiv.org/pdf/2212.02802v2.pdf","comment":"CVPR 2023. Our project page: https://diff-video-ae.github.io"},{"id":"http://arxiv.org/abs/2303.15103v1","updated":"2023-03-27T11:13:35Z","published":"2023-03-27T11:13:35Z","title":"Contrastive Learning Is Spectral Clustering On Similarity Graph","summary":"  Contrastive learning is a powerful self-supervised learning method, but we\nhave a limited theoretical understanding of how it works and why it works. In\nthis paper, we prove that contrastive learning with the standard InfoNCE loss\nis equivalent to spectral clustering on the similarity graph. Using this\nequivalence as the building block, we extend our analysis to the CLIP model and\nrigorously characterize how similar multi-modal objects are embedded together.\nMotivated by our theoretical insights, we introduce the kernel mixture loss,\nincorporating novel kernel functions that outperform the standard Gaussian\nkernel on several vision datasets.\n","authors":["Zhiquan Tan","Yifan Zhang","Jingqin Yang","Yang Yuan"],"pdf_url":"https://arxiv.org/pdf/2303.15103v1.pdf","comment":"We express our gratitude to the anonymous reviewers for their\n  valuable feedback"},{"id":"http://arxiv.org/abs/2303.15092v1","updated":"2023-03-27T11:01:16Z","published":"2023-03-27T11:01:16Z","title":"Defect detection using weakly supervised learning","summary":"  In many real-world scenarios, obtaining large amounts of labeled data can be\na daunting task. Weakly supervised learning techniques have gained significant\nattention in recent years as an alternative to traditional supervised learning,\nas they enable training models using only a limited amount of labeled data. In\nthis paper, the performance of a weakly supervised classifier to its fully\nsupervised counterpart is compared on the task of defect detection. Experiments\nare conducted on a dataset of images containing defects, and evaluate the two\nclassifiers based on their accuracy, precision, and recall. Our results show\nthat the weakly supervised classifier achieves comparable performance to the\nsupervised classifier, while requiring significantly less labeled data.\n","authors":["Vasileios Sevetlidis","George Pavlidis","Vasiliki Balaska","Athanasios Psomoulis","Spyridon Mouroutsos","Antonios Gasteratos"],"pdf_url":"https://arxiv.org/pdf/2303.15092v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2302.08175v6","updated":"2023-03-27T10:59:42Z","published":"2023-02-16T09:44:55Z","title":"A numerical approximation method for the Fisher-Rao distance between\n  multivariate normal distributions","summary":"  We present a simple method to approximate Rao's distance between multivariate\nnormal distributions based on discretizing curves joining normal distributions\nand approximating Rao's distances between successive nearby normal\ndistributions on the curves by the square root of Jeffreys divergence, the\nsymmetrized Kullback-Leibler divergence. We consider experimentally the linear\ninterpolation curves in the ordinary, natural and expectation parameterizations\nof the normal distributions, and compare these curves with a curve derived from\nthe Calvo and Oller's isometric embedding of the Fisher-Rao $d$-variate normal\nmanifold into the cone of $(d+1)\\times (d+1)$ symmetric positive-definite\nmatrices [Journal of multivariate analysis 35.2 (1990): 223-242]. We report on\nour experiments and assess the quality of our approximation technique by\ncomparing the numerical approximations with both lower and upper bounds.\nFinally, we present several information-geometric properties of the Calvo and\nOller's isometric embedding.\n","authors":["Frank Nielsen"],"pdf_url":"https://arxiv.org/pdf/2302.08175v6.pdf","comment":"46 pages, 19 figures, 3 tables"},{"id":"http://arxiv.org/abs/2303.15087v1","updated":"2023-03-27T10:54:32Z","published":"2023-03-27T10:54:32Z","title":"Prediction of Time and Distance of Trips Using Explainable\n  Attention-based LSTMs","summary":"  In this paper, we propose machine learning solutions to predict the time of\nfuture trips and the possible distance the vehicle will travel. For this\nprediction task, we develop and investigate four methods. In the first method,\nwe use long short-term memory (LSTM)-based structures specifically designed to\nhandle multi-dimensional historical data of trip time and distances\nsimultaneously. Using it, we predict the future trip time and forecast the\ndistance a vehicle will travel by concatenating the outputs of LSTM networks\nthrough fully connected layers. The second method uses attention-based LSTM\nnetworks (At-LSTM) to perform the same tasks. The third method utilizes two\nLSTM networks in parallel, one for forecasting the time of the trip and the\nother for predicting the distance. The output of each LSTM is then concatenated\nthrough fully connected layers. Finally, the last model is based on two\nparallel At-LSTMs, where similarly, each At-LSTM predicts time and distance\nseparately through fully connected layers. Among the proposed methods, the most\nadvanced one, i.e., parallel At-LSTM, predicts the next trip's distance and\ntime with 3.99% error margin where it is 23.89% better than LSTM, the first\nmethod. We also propose TimeSHAP as an explainability method for understanding\nhow the networks perform learning and model the sequence of information.\n","authors":["Ebrahim Balouji","Jonas Sjöblom","Nikolce Murgovski","Morteza Haghir Chehreghani"],"pdf_url":"https://arxiv.org/pdf/2303.15087v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2209.07171v2","updated":"2023-03-27T10:40:03Z","published":"2022-09-15T09:43:17Z","title":"Learning to Exploit Elastic Actuators for Quadruped Locomotion","summary":"  Spring-based actuators in legged locomotion provide energy-efficiency and\nimproved performance, but increase the difficulty of controller design. While\nprevious work has focused on extensive modeling and simulation to find optimal\ncontrollers for such systems, we propose to learn model-free controllers\ndirectly on the real robot. In our approach, gaits are first synthesized by\ncentral pattern generators (CPGs), whose parameters are optimized to quickly\nobtain an open-loop controller that achieves efficient locomotion. Then, to\nmake this controller more robust and further improve the performance, we use\nreinforcement learning to close the loop, to learn corrective actions on top of\nthe CPGs. We evaluate the proposed approach on the DLR elastic quadruped bert.\nOur results in learning trotting and pronking gaits show that exploitation of\nthe spring actuator dynamics emerges naturally from optimizing for dynamic\nmotions, yielding high-performing locomotion despite being model-free. The\nwhole process takes no more than 1.5 hours on the real robot and results in\nnatural-looking gaits.\n","authors":["Antonin Raffin","Daniel Seidel","Jens Kober","Alin Albu-Schäffer","João Silvério","Freek Stulp"],"pdf_url":"https://arxiv.org/pdf/2209.07171v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.15074v1","updated":"2023-03-27T10:28:35Z","published":"2023-03-27T10:28:35Z","title":"Conjunction Data Messages for Space Collision Behave as a Poisson\n  Process","summary":"  Space debris is a major problem in space exploration. International bodies\ncontinuously monitor a large database of orbiting objects and emit warnings in\nthe form of conjunction data messages. An important question for satellite\noperators is to estimate when fresh information will arrive so that they can\nreact timely but sparingly with satellite maneuvers. We propose a statistical\nlearning model of the message arrival process, allowing us to answer two\nimportant questions: (1) Will there be any new message in the next specified\ntime interval? (2) When exactly and with what uncertainty will the next message\narrive? The average prediction error for question (2) of our Bayesian Poisson\nprocess model is smaller than the baseline in more than 4 hours in a test set\nof 50k close encounter events.\n","authors":["Francisco Caldas","Cláudia Soares","Cláudia Nunes","Marta Guimarães"],"pdf_url":"https://arxiv.org/pdf/2303.15074v1.pdf","comment":"Submitted to EUSIPCO '23. arXiv admin note: substantial text overlap\n  with arXiv:2105.08509"},{"id":"http://arxiv.org/abs/2303.15072v1","updated":"2023-03-27T10:25:46Z","published":"2023-03-27T10:25:46Z","title":"Exposing the Functionalities of Neurons for Gated Recurrent Unit Based\n  Sequence-to-Sequence Model","summary":"  The goal of this paper is to report certain scientific discoveries about a\nSeq2Seq model. It is known that analyzing the behavior of RNN-based models at\nthe neuron level is considered a more challenging task than analyzing a DNN or\nCNN models due to their recursive mechanism in nature. This paper aims to\nprovide neuron-level analysis to explain why a vanilla GRU-based Seq2Seq model\nwithout attention can achieve token-positioning. We found four different types\nof neurons: storing, counting, triggering, and outputting and further uncover\nthe mechanism for these neurons to work together in order to produce the right\ntoken in the right position.\n","authors":["Yi-Ting Lee","Da-Yi Wu","Chih-Chun Yang","Shou-De Lin"],"pdf_url":"https://arxiv.org/pdf/2303.15072v1.pdf","comment":"9 pages (excluding reference), 10 figures"},{"id":"http://arxiv.org/abs/2205.13602v3","updated":"2023-03-27T10:20:36Z","published":"2022-05-26T20:19:28Z","title":"Consistent and fast inference in compartmental models of epidemics using\n  Poisson Approximate Likelihoods","summary":"  Addressing the challenge of scaling-up epidemiological inference to complex\nand heterogeneous models, we introduce Poisson Approximate Likelihood (PAL)\nmethods. In contrast to the popular ODE approach to compartmental modelling, in\nwhich a large population limit is used to motivate a deterministic model, PALs\nare derived from approximate filtering equations for finite-population,\nstochastic compartmental models, and the large population limit drives\nconsistency of maximum PAL estimators. Our theoretical results appear to be the\nfirst likelihood-based parameter estimation consistency results which apply to\na broad class of partially observed stochastic compartmental models and address\nthe large population limit. PALs are simple to implement, involving only\nelementary arithmetic operations and no tuning parameters, and fast to\nevaluate, requiring no simulation from the model and having computational cost\nindependent of population size. Through examples we demonstrate how PALs can be\nused to: fit an age-structured model of influenza, taking advantage of\nautomatic differentiation in Stan; compare over-dispersion mechanisms in a\nmodel of rotavirus by embedding PALs within sequential Monte Carlo; and\nevaluate the role of unit-specific parameters in a meta-population model of\nmeasles.\n","authors":["Michael Whitehouse","Nick Whiteley","Lorenzo Rimella"],"pdf_url":"https://arxiv.org/pdf/2205.13602v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2210.10535v2","updated":"2023-03-27T10:01:30Z","published":"2022-10-19T13:17:03Z","title":"Stability of Entropic Wasserstein Barycenters and application to random\n  geometric graphs","summary":"  As interest in graph data has grown in recent years, the computation of\nvarious geometric tools has become essential. In some area such as mesh\nprocessing, they often rely on the computation of geodesics and shortest paths\nin discretized manifolds. A recent example of such a tool is the computation of\nWasserstein barycenters (WB), a very general notion of barycenters derived from\nthe theory of Optimal Transport, and their entropic-regularized variant. In\nthis paper, we examine how WBs on discretized meshes relate to the geometry of\nthe underlying manifold. We first provide a generic stability result with\nrespect to the input cost matrices. We then apply this result to random\ngeometric graphs on manifolds, whose shortest paths converge to geodesics,\nhence proving the consistency of WBs computed on discretized shapes.\n","authors":["Marc Theveneau","Nicolas Keriven"],"pdf_url":"https://arxiv.org/pdf/2210.10535v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.15057v1","updated":"2023-03-27T10:00:50Z","published":"2023-03-27T10:00:50Z","title":"Meta-Calibration Regularized Neural Networks","summary":"  Miscalibration-the mismatch between predicted probability and the true\ncorrectness likelihood-has been frequently identified in modern deep neural\nnetworks. Recent work in the field aims to address this problem by training\ncalibrated models directly by optimizing a proxy of the calibration error\nalongside the conventional objective. Recently, Meta-Calibration (MC) showed\nthe effectiveness of using meta-learning for learning better calibrated models.\nIn this work, we extend MC with two main components: (1) gamma network\n(gamma-net), a meta network to learn a sample-wise gamma at a continuous space\nfor focal loss for optimizing backbone network; (2) smooth expected calibration\nerror (SECE), a Gaussian-kernel based unbiased and differentiable ECE which\naims to smoothly optimizing gamma-net. The proposed method regularizes neural\nnetwork towards better calibration meanwhile retain predictive performance. Our\nexperiments show that (a) learning sample-wise gamma at continuous space can\neffectively perform calibration; (b) SECE smoothly optimise gamma-net towards\nbetter robustness to binning schemes; (c) the combination of gamma-net and SECE\nachieve the best calibration performance across various calibration metrics and\nretain very competitive predictive performance as compared to multiple recently\nproposed methods on three datasets.\n","authors":["Cheng Wang","Jacek Golebiowski"],"pdf_url":"https://arxiv.org/pdf/2303.15057v1.pdf","comment":"15 pages"},{"id":"http://arxiv.org/abs/2303.15053v1","updated":"2023-03-27T09:55:33Z","published":"2023-03-27T09:55:33Z","title":"Hyperparameter optimization, quantum-assisted model performance\n  prediction, and benchmarking of AI-based High Energy Physics workloads using\n  HPC","summary":"  Training and Hyperparameter Optimization (HPO) of deep learning-based AI\nmodels are often compute resource intensive and calls for the use of\nlarge-scale distributed resources as well as scalable and resource efficient\nhyperparameter search algorithms. This work studies the potential of using\nmodel performance prediction to aid the HPO process carried out on High\nPerformance Computing systems. In addition, a quantum annealer is used to train\nthe performance predictor and a method is proposed to overcome some of the\nproblems derived from the current limitations in quantum systems as well as to\nincrease the stability of solutions. This allows for achieving results on a\nquantum machine comparable to those obtained on a classical machine, showing\nhow quantum computers could be integrated within classical machine learning\ntuning pipelines.\n  Furthermore, results are presented from the development of a containerized\nbenchmark based on an AI-model for collision event reconstruction that allows\nus to compare and assess the suitability of different hardware accelerators for\ntraining deep neural networks.\n","authors":["Eric Wulff","Maria Girone","David Southwick","Juan Pablo García Amboage","Eduard Cuba"],"pdf_url":"https://arxiv.org/pdf/2303.15053v1.pdf","comment":"5 pages, 7 figures. Submitted to the proceedings of the ACAT 2022\n  conference and is to be published in the Journal Of Physics: Conference\n  Series"},{"id":"http://arxiv.org/abs/2209.07154v2","updated":"2023-03-27T09:49:45Z","published":"2022-09-15T09:09:53Z","title":"Risk-aware linear bandits with convex loss","summary":"  In decision-making problems such as the multi-armed bandit, an agent learns\nsequentially by optimizing a certain feedback. While the mean reward criterion\nhas been extensively studied, other measures that reflect an aversion to\nadverse outcomes, such as mean-variance or conditional value-at-risk (CVaR),\ncan be of interest for critical applications (healthcare, agriculture).\nAlgorithms have been proposed for such risk-aware measures under bandit\nfeedback without contextual information. In this work, we study contextual\nbandits where such risk measures can be elicited as linear functions of the\ncontexts through the minimization of a convex loss. A typical example that fits\nwithin this framework is the expectile measure, which is obtained as the\nsolution of an asymmetric least-square problem. Using the method of mixtures\nfor supermartingales, we derive confidence sequences for the estimation of such\nrisk measures. We then propose an optimistic UCB algorithm to learn optimal\nrisk-aware actions, with regret guarantees similar to those of generalized\nlinear bandits. This approach requires solving a convex problem at each round\nof the algorithm, which we can relax by allowing only approximated solution\nobtained by online gradient descent, at the cost of slightly higher regret. We\nconclude by evaluating the resulting algorithms on numerical experiments.\n","authors":["Patrick Saux","Odalric-Ambrym Maillard"],"pdf_url":"https://arxiv.org/pdf/2209.07154v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2211.17174v2","updated":"2023-03-27T09:42:13Z","published":"2022-11-30T17:17:55Z","title":"Optimizing Explanations by Network Canonization and Hyperparameter\n  Search","summary":"  Explainable AI (XAI) is slowly becoming a key component for many AI\napplications. Rule-based and modified backpropagation XAI approaches however\noften face challenges when being applied to modern model architectures\nincluding innovative layer building blocks, which is caused by two reasons.\nFirstly, the high flexibility of rule-based XAI methods leads to numerous\npotential parameterizations. Secondly, many XAI methods break the\nimplementation-invariance axiom because they struggle with certain model\ncomponents, e.g., BatchNorm layers. The latter can be addressed with model\ncanonization, which is the process of re-structuring the model to disregard\nproblematic components without changing the underlying function. While model\ncanonization is straightforward for simple architectures (e.g., VGG, ResNet),\nit can be challenging for more complex and highly interconnected models (e.g.,\nDenseNet). Moreover, there is only little quantifiable evidence that model\ncanonization is beneficial for XAI. In this work, we propose canonizations for\ncurrently relevant model blocks applicable to popular deep neural network\narchitectures,including VGG, ResNet, EfficientNet, DenseNets, as well as\nRelation Networks. We further suggest a XAI evaluation framework with which we\nquantify and compare the effect sof model canonization for various XAI methods\nin image classification tasks on the Pascal-VOC and ILSVRC2017 datasets, as\nwell as for Visual Question Answering using CLEVR-XAI. Moreover, addressing the\nformer issue outlined above, we demonstrate how our evaluation framework can be\napplied to perform hyperparameter search for XAI methods to optimize the\nquality of explanations.\n","authors":["Frederik Pahde","Galip Ümit Yolcu","Alexander Binder","Wojciech Samek","Sebastian Lapuschkin"],"pdf_url":"https://arxiv.org/pdf/2211.17174v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.15042v1","updated":"2023-03-27T09:40:14Z","published":"2023-03-27T09:40:14Z","title":"Partially Adaptive Multichannel Joint Reduction of Ego-noise and\n  Environmental Noise","summary":"  Human-robot interaction relies on a noise-robust audio processing module\ncapable of estimating target speech from audio recordings impacted by\nenvironmental noise, as well as self-induced noise, so-called ego-noise. While\nexternal ambient noise sources vary from environment to environment, ego-noise\nis mainly caused by the internal motors and joints of a robot. Ego-noise and\nenvironmental noise reduction are often decoupled, i.e., ego-noise reduction is\nperformed without considering environmental noise. Recently, a variational\nautoencoder (VAE)-based speech model has been combined with a fully adaptive\nnon-negative matrix factorization (NMF) noise model to recover clean speech\nunder different environmental noise disturbances. However, its enhancement\nperformance is limited in adverse acoustic scenarios involving, e.g. ego-noise.\nIn this paper, we propose a multichannel partially adaptive scheme to jointly\nmodel ego-noise and environmental noise utilizing the VAE-NMF framework, where\nwe take advantage of spatially and spectrally structured characteristics of\nego-noise by pre-training the ego-noise model, while retaining the ability to\nadapt to unknown environmental noise. Experimental results show that our\nproposed approach outperforms the methods based on a completely fixed scheme\nand a fully adaptive scheme when ego-noise and environmental noise are present\nsimultaneously.\n","authors":["Huajian Fang","Niklas Wittmer","Johannes Twiefel","Stefan Wermter","Timo Gerkmann"],"pdf_url":"https://arxiv.org/pdf/2303.15042v1.pdf","comment":"Accepted to the 2023 IEEE International Conference on Acoustics,\n  Speech, and Signal Processing (ICASSP 2023)"},{"id":"http://arxiv.org/abs/2303.15041v1","updated":"2023-03-27T09:39:38Z","published":"2023-03-27T09:39:38Z","title":"Towards black-box parameter estimation","summary":"  Deep learning algorithms have recently shown to be a successful tool in\nestimating parameters of statistical models for which simulation is easy, but\nlikelihood computation is challenging. But the success of these approaches\ndepends on simulating parameters that sufficiently reproduce the observed data,\nand, at present, there is a lack of efficient methods to produce these\nsimulations. We develop new black-box procedures to estimate parameters of\nstatistical models based only on weak parameter structure assumptions. For\nwell-structured likelihoods with frequent occurrences, such as in time series,\nthis is achieved by pre-training a deep neural network on an extensive\nsimulated database that covers a wide range of data sizes. For other types of\ncomplex dependencies, an iterative algorithm guides simulations to the correct\nparameter region in multiple rounds. These approaches can successfully estimate\nand quantify the uncertainty of parameters from non-Gaussian models with\ncomplex spatial and temporal dependencies. The success of our methods is a\nfirst step towards a fully flexible automatic black-box estimation framework.\n","authors":["Amanda Lenzi","Haavard Rue"],"pdf_url":"https://arxiv.org/pdf/2303.15041v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2206.12342v3","updated":"2023-03-27T09:26:59Z","published":"2022-06-24T15:27:58Z","title":"FEATHERS: Federated Architecture and Hyperparameter Search","summary":"  Deep neural architectures have profound impact on achieved performance in\nmany of today's AI tasks, yet, their design still heavily relies on human prior\nknowledge and experience. Neural architecture search (NAS) together with\nhyperparameter optimization (HO) helps to reduce this dependence. However,\nstate of the art NAS and HO rapidly become infeasible with increasing amount of\ndata being stored in a distributed fashion, typically violating data privacy\nregulations such as GDPR and CCPA. As a remedy, we introduce FEATHERS -\n$\\textbf{FE}$derated $\\textbf{A}$rchi$\\textbf{T}$ecture and\n$\\textbf{H}$yp$\\textbf{ER}$parameter $\\textbf{S}$earch, a method that not only\noptimizes both neural architectures and optimization-related hyperparameters\njointly in distributed data settings, but further adheres to data privacy\nthrough the use of differential privacy (DP). We show that FEATHERS efficiently\noptimizes architectural and optimization-related hyperparameters alike, while\ndemonstrating convergence on classification tasks at no detriment to model\nperformance when complying with privacy constraints.\n","authors":["Jonas Seng","Pooja Prasad","Martin Mundt","Devendra Singh Dhami","Kristian Kersting"],"pdf_url":"https://arxiv.org/pdf/2206.12342v3.pdf","comment":"Main paper: 8 pages, References: 2 pages, Supplement: 4.5 pages, Main\n  paper: 3 figures, 2 tables, 1 algorithm, Supplement: 2 figure, 4 algorithms,\n  extended previous version by Differential Privacy, theoretical results and\n  more experiments. Updated author list as it was incomplete"},{"id":"http://arxiv.org/abs/2303.15015v1","updated":"2023-03-27T08:58:22Z","published":"2023-03-27T08:58:22Z","title":"Towards Open Temporal Graph Neural Networks","summary":"  Graph neural networks (GNNs) for temporal graphs have recently attracted\nincreasing attentions, where a common assumption is that the class set for\nnodes is closed. However, in real-world scenarios, it often faces the open set\nproblem with the dynamically increased class set as the time passes by. This\nwill bring two big challenges to the existing dynamic GNN methods: (i) How to\ndynamically propagate appropriate information in an open temporal graph, where\nnew class nodes are often linked to old class nodes. This case will lead to a\nsharp contradiction. This is because typical GNNs are prone to make the\nembeddings of connected nodes become similar, while we expect the embeddings of\nthese two interactive nodes to be distinguishable since they belong to\ndifferent classes. (ii) How to avoid catastrophic knowledge forgetting over old\nclasses when learning new classes occurred in temporal graphs. In this paper,\nwe propose a general and principled learning approach for open temporal graphs,\ncalled OTGNet, with the goal of addressing the above two challenges. We assume\nthe knowledge of a node can be disentangled into class-relevant and\nclass-agnostic one, and thus explore a new message passing mechanism by\nextending the information bottleneck principle to only propagate class-agnostic\nknowledge between nodes of different classes, avoiding aggregating conflictive\ninformation. Moreover, we devise a strategy to select both important and\ndiverse triad sub-graph structures for effective class-incremental learning.\nExtensive experiments on three real-world datasets of different domains\ndemonstrate the superiority of our method, compared to the baselines.\n","authors":["Kaituo Feng","Changsheng Li","Xiaolu Zhang","Jun Zhou"],"pdf_url":"https://arxiv.org/pdf/2303.15015v1.pdf","comment":"ICLR 2023 Oral"},{"id":"http://arxiv.org/abs/2203.11242v2","updated":"2023-03-27T08:57:21Z","published":"2022-03-21T18:06:03Z","title":"A survey on GANs for computer vision: Recent research, analysis and\n  taxonomy","summary":"  In the last few years, there have been several revolutions in the field of\ndeep learning, mainly headlined by the large impact of Generative Adversarial\nNetworks (GANs). GANs not only provide an unique architecture when defining\ntheir models, but also generate incredible results which have had a direct\nimpact on society. Due to the significant improvements and new areas of\nresearch that GANs have brought, the community is constantly coming up with new\nresearches that make it almost impossible to keep up with the times. Our survey\naims to provide a general overview of GANs, showing the latest architectures,\noptimizations of the loss functions, validation metrics and application areas\nof the most widely recognized variants. The efficiency of the different\nvariants of the model architecture will be evaluated, as well as showing the\nbest application area; as a vital part of the process, the different metrics\nfor evaluating the performance of GANs and the frequently used loss functions\nwill be analyzed. The final objective of this survey is to provide a summary of\nthe evolution and performance of the GANs which are having better results to\nguide future researchers in the field.\n","authors":["Guillermo Iglesias","Edgar Talavera","Alberto Díaz-Álvarez"],"pdf_url":"https://arxiv.org/pdf/2203.11242v2.pdf","comment":"77 pages, 11 figures, 4 tables"},{"id":"http://arxiv.org/abs/2303.15006v1","updated":"2023-03-27T08:47:18Z","published":"2023-03-27T08:47:18Z","title":"Curriculum Learning for Compositional Visual Reasoning","summary":"  Visual Question Answering (VQA) is a complex task requiring large datasets\nand expensive training. Neural Module Networks (NMN) first translate the\nquestion to a reasoning path, then follow that path to analyze the image and\nprovide an answer. We propose an NMN method that relies on predefined\ncross-modal embeddings to ``warm start'' learning on the GQA dataset, then\nfocus on Curriculum Learning (CL) as a way to improve training and make a\nbetter use of the data. Several difficulty criteria are employed for defining\nCL methods. We show that by an appropriate selection of the CL method the cost\nof training and the amount of training data can be greatly reduced, with a\nlimited impact on the final VQA accuracy. Furthermore, we introduce\nintermediate losses during training and find that this allows to simplify the\nCL strategy.\n","authors":["Wafa Aissa","Marin Ferecatu","Michel Crucianu"],"pdf_url":"https://arxiv.org/pdf/2303.15006v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.15005v1","updated":"2023-03-27T08:46:31Z","published":"2023-03-27T08:46:31Z","title":"Architecturing Binarized Neural Networks for Traffic Sign Recognition","summary":"  Traffic signs support road safety and managing the flow of traffic, hence are\nan integral part of any vision system for autonomous driving. While the use of\ndeep learning is well-known in traffic signs classification due to the high\naccuracy results obtained using convolutional neural networks (CNNs) (state of\nthe art is 99.46\\%), little is known about binarized neural networks (BNNs).\nCompared to CNNs, BNNs reduce the model size and simplify convolution\noperations and have shown promising results in computationally limited and\nenergy-constrained devices which appear in the context of autonomous driving.\n  This work presents a bottom-up approach for architecturing BNNs by studying\ncharacteristics of the constituent layers. These constituent layers (binarized\nconvolutional layers, max pooling, batch normalization, fully connected layers)\nare studied in various combinations and with different values of kernel size,\nnumber of filters and of neurons by using the German Traffic Sign Recognition\nBenchmark (GTSRB) for training. As a result, we propose BNNs architectures\nwhich achieve more than $90\\%$ for GTSRB (the maximum is $96.45\\%$) and an\naverage greater than $80\\%$ (the maximum is $88.99\\%$) considering also the\nBelgian and Chinese datasets for testing. The number of parameters of these\narchitectures varies from 100k to less than 2M. The accompanying material of\nthis paper is publicly available at\nhttps://github.com/apostovan21/BinarizedNeuralNetwork.\n","authors":["Andreea Postovan","Mădălina Eraşcu"],"pdf_url":"https://arxiv.org/pdf/2303.15005v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2211.14905v2","updated":"2023-03-27T08:39:13Z","published":"2022-11-27T18:13:05Z","title":"Multi-Modal Few-Shot Temporal Action Detection","summary":"  Few-shot (FS) and zero-shot (ZS) learning are two different approaches for\nscaling temporal action detection (TAD) to new classes. The former adapts a\npretrained vision model to a new task represented by as few as a single video\nper class, whilst the latter requires no training examples by exploiting a\nsemantic description of the new class. In this work, we introduce a new\nmulti-modality few-shot (MMFS) TAD problem, which can be considered as a\nmarriage of FS-TAD and ZS-TAD by leveraging few-shot support videos and new\nclass names jointly. To tackle this problem, we further introduce a novel\nMUlti-modality PromPt mETa-learning (MUPPET) method. This is enabled by\nefficiently bridging pretrained vision and language models whilst maximally\nreusing already learned capacity. Concretely, we construct multi-modal prompts\nby mapping support videos into the textual token space of a vision-language\nmodel using a meta-learned adapter-equipped visual semantics tokenizer. To\ntackle large intra-class variation, we further design a query feature\nregulation scheme. Extensive experiments on ActivityNetv1.3 and THUMOS14\ndemonstrate that our MUPPET outperforms state-of-the-art alternative methods,\noften by a large margin. We also show that our MUPPET can be easily extended to\ntackle the few-shot object detection problem and again achieves the\nstate-of-the-art performance on MS-COCO dataset. The code will be available in\nhttps://github.com/sauradip/MUPPET\n","authors":["Sauradip Nag","Mengmeng Xu","Xiatian Zhu","Juan-Manuel Perez-Rua","Bernard Ghanem","Yi-Zhe Song","Tao Xiang"],"pdf_url":"https://arxiv.org/pdf/2211.14905v2.pdf","comment":"Technical Report"},{"id":"http://arxiv.org/abs/2301.13166v2","updated":"2023-03-27T08:33:40Z","published":"2023-01-30T18:37:32Z","title":"ESC: Exploration with Soft Commonsense Constraints for Zero-shot Object\n  Navigation","summary":"  The ability to accurately locate and navigate to a specific object is a\ncrucial capability for embodied agents that operate in the real world and\ninteract with objects to complete tasks. Such object navigation tasks usually\nrequire large-scale training in visual environments with labeled objects, which\ngeneralizes poorly to novel objects in unknown environments. In this work, we\npresent a novel zero-shot object navigation method, Exploration with Soft\nCommonsense constraints (ESC), that transfers commonsense knowledge in\npre-trained models to open-world object navigation without any navigation\nexperience nor any other training on the visual environments. First, ESC\nleverages a pre-trained vision and language model for open-world prompt-based\ngrounding and a pre-trained commonsense language model for room and object\nreasoning. Then ESC converts commonsense knowledge into navigation actions by\nmodeling it as soft logic predicates for efficient exploration. Extensive\nexperiments on MP3D, HM3D, and RoboTHOR benchmarks show that our ESC method\nimproves significantly over baselines, and achieves new state-of-the-art\nresults for zero-shot object navigation (e.g., 158% relative Success Rate\nimprovement than CoW on MP3D).\n","authors":["Kaiwen Zhou","Kaizhi Zheng","Connor Pryor","Yilin Shen","Hongxia Jin","Lise Getoor","Xin Eric Wang"],"pdf_url":"https://arxiv.org/pdf/2301.13166v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.14077v2","updated":"2023-03-27T08:32:43Z","published":"2023-03-24T15:41:40Z","title":"Improved Adversarial Training Through Adaptive Instance-wise Loss\n  Smoothing","summary":"  Deep neural networks can be easily fooled into making incorrect predictions\nthrough corruption of the input by adversarial perturbations:\nhuman-imperceptible artificial noise. So far adversarial training has been the\nmost successful defense against such adversarial attacks. This work focuses on\nimproving adversarial training to boost adversarial robustness. We first\nanalyze, from an instance-wise perspective, how adversarial vulnerability\nevolves during adversarial training. We find that during training an overall\nreduction of adversarial loss is achieved by sacrificing a considerable\nproportion of training samples to be more vulnerable to adversarial attack,\nwhich results in an uneven distribution of adversarial vulnerability among\ndata. Such \"uneven vulnerability\", is prevalent across several popular robust\ntraining methods and, more importantly, relates to overfitting in adversarial\ntraining. Motivated by this observation, we propose a new adversarial training\nmethod: Instance-adaptive Smoothness Enhanced Adversarial Training (ISEAT). It\njointly smooths both input and weight loss landscapes in an adaptive,\ninstance-specific, way to enhance robustness more for those samples with higher\nadversarial vulnerability. Extensive experiments demonstrate the superiority of\nour method over existing defense methods. Noticeably, our method, when combined\nwith the latest data augmentation and semi-supervised learning techniques,\nachieves state-of-the-art robustness against $\\ell_{\\infty}$-norm constrained\nattacks on CIFAR10 of 59.32% for Wide ResNet34-10 without extra data, and\n61.55% for Wide ResNet28-10 with extra data. Code is available at\nhttps://github.com/TreeLLi/Instance-adaptive-Smoothness-Enhanced-AT.\n","authors":["Lin Li","Michael Spratling"],"pdf_url":"https://arxiv.org/pdf/2303.14077v2.pdf","comment":"12 pages, work in submission"},{"id":"http://arxiv.org/abs/2303.14989v1","updated":"2023-03-27T08:32:20Z","published":"2023-03-27T08:32:20Z","title":"Regularized EM algorithm","summary":"  Expectation-Maximization (EM) algorithm is a widely used iterative algorithm\nfor computing (local) maximum likelihood estimate (MLE). It can be used in an\nextensive range of problems, including the clustering of data based on the\nGaussian mixture model (GMM). Numerical instability and convergence problems\nmay arise in situations where the sample size is not much larger than the data\ndimensionality. In such low sample support (LSS) settings, the covariance\nmatrix update in the EM-GMM algorithm may become singular or poorly\nconditioned, causing the algorithm to crash. On the other hand, in many signal\nprocessing problems, a priori information can be available indicating certain\nstructures for different cluster covariance matrices. In this paper, we present\na regularized EM algorithm for GMM-s that can make efficient use of such prior\nknowledge as well as cope with LSS situations. The method aims to maximize a\npenalized GMM likelihood where regularized estimation may be used to ensure\npositive definiteness of covariance matrix updates and shrink the estimators\ntowards some structured target covariance matrices. We show that the\ntheoretical guarantees of convergence hold, leading to better performing EM\nalgorithm for structured covariance matrix models or with low sample settings.\n","authors":["Pierre Houdouin","Esa Ollila","Frederic Pascal"],"pdf_url":"https://arxiv.org/pdf/2303.14989v1.pdf","comment":"ICASSP Conference, 4 pages, 8 figures"},{"id":"http://arxiv.org/abs/2303.14986v1","updated":"2023-03-27T08:30:11Z","published":"2023-03-27T08:30:11Z","title":"mSPD-NN: A Geometrically Aware Neural Framework for Biomarker Discovery\n  from Functional Connectomics Manifolds","summary":"  Connectomics has emerged as a powerful tool in neuroimaging and has spurred\nrecent advancements in statistical and machine learning methods for\nconnectivity data. Despite connectomes inhabiting a matrix manifold, most\nanalytical frameworks ignore the underlying data geometry. This is largely\nbecause simple operations, such as mean estimation, do not have easily\ncomputable closed-form solutions. We propose a geometrically aware neural\nframework for connectomes, i.e., the mSPD-NN, designed to estimate the geodesic\nmean of a collections of symmetric positive definite (SPD) matrices. The\nmSPD-NN is comprised of bilinear fully connected layers with tied weights and\nutilizes a novel loss function to optimize the matrix-normal equation arising\nfrom Fr\\'echet mean estimation. Via experiments on synthetic data, we\ndemonstrate the efficacy of our mSPD-NN against common alternatives for SPD\nmean estimation, providing competitive performance in terms of scalability and\nrobustness to noise. We illustrate the real-world flexibility of the mSPD-NN in\nmultiple experiments on rs-fMRI data and demonstrate that it uncovers stable\nbiomarkers associated with subtle network differences among patients with\nADHD-ASD comorbidities and healthy controls.\n","authors":["Niharika S. D'Souza","Archana Venkataraman"],"pdf_url":"https://arxiv.org/pdf/2303.14986v1.pdf","comment":"Accepted into IPMI 2023"},{"id":"http://arxiv.org/abs/2206.13508v2","updated":"2023-03-27T08:07:11Z","published":"2022-06-25T17:09:00Z","title":"Data Augmentation techniques in time series domain: A survey and\n  taxonomy","summary":"  With the latest advances in Deep Learning-based} generative models, it has\nnot taken long to take advantage of their remarkable performance in the area of\ntime series. Deep neural networks used to work with time series heavily depend\non the size and consistency of the datasets used in training. These features\nare not usually abundant in the real world, where they are usually limited and\noften have constraints that must be guaranteed. Therefore, an effective way to\nincrease the amount of data is by using Data Augmentation techniques, either by\nadding noise or permutations and by generating new synthetic data. This work\nsystematically reviews the current state-of-the-art in the area to provide an\noverview of all available algorithms and proposes a taxonomy of the most\nrelevant research. The efficiency of the different variants will be evaluated\nas a central part of the process, as well as the different metrics to evaluate\nthe performance and the main problems concerning each model will be analysed.\nThe ultimate aim of this study is to provide a summary of the evolution and\nperformance of areas that produce better results to guide future researchers in\nthis field.\n","authors":["Edgar Talavera","Guillermo Iglesias","Ángel González-Prieto","Alberto Mozo","Sandra Gómez-Canaval"],"pdf_url":"https://arxiv.org/pdf/2206.13508v2.pdf","comment":"33 pages, 9 figures"},{"id":"http://arxiv.org/abs/2303.14968v1","updated":"2023-03-27T07:58:09Z","published":"2023-03-27T07:58:09Z","title":"Blind Image Quality Assessment via Vision-Language Correspondence: A\n  Multitask Learning Perspective","summary":"  We aim at advancing blind image quality assessment (BIQA), which predicts the\nhuman perception of image quality without any reference information. We develop\na general and automated multitask learning scheme for BIQA to exploit auxiliary\nknowledge from other tasks, in a way that the model parameter sharing and the\nloss weighting are determined automatically. Specifically, we first describe\nall candidate label combinations (from multiple tasks) using a textual\ntemplate, and compute the joint probability from the cosine similarities of the\nvisual-textual embeddings. Predictions of each task can be inferred from the\njoint distribution, and optimized by carefully designed loss functions. Through\ncomprehensive experiments on learning three tasks - BIQA, scene classification,\nand distortion type identification, we verify that the proposed BIQA method 1)\nbenefits from the scene classification and distortion type identification tasks\nand outperforms the state-of-the-art on multiple IQA datasets, 2) is more\nrobust in the group maximum differentiation competition, and 3) realigns the\nquality annotations from different IQA datasets more effectively. The source\ncode is available at https://github.com/zwx8981/LIQE.\n","authors":["Weixia Zhang","Guangtao Zhai","Ying Wei","Xiaokang Yang","Kede Ma"],"pdf_url":"https://arxiv.org/pdf/2303.14968v1.pdf","comment":"CVPR2023"},{"id":"http://arxiv.org/abs/2303.14966v1","updated":"2023-03-27T07:57:04Z","published":"2023-03-27T07:57:04Z","title":"Adaptive Federated Learning via Entropy Approach","summary":"  Federated Learning (FL) has recently emerged as a popular framework, which\nallows resource-constrained discrete clients to cooperatively learn the global\nmodel under the orchestration of a central server while storing\nprivacy-sensitive data locally. However, due to the difference in equipment and\ndata divergence of heterogeneous clients, there will be parameter deviation\nbetween local models, resulting in a slow convergence rate and a reduction of\nthe accuracy of the global model. The current FL algorithms use the static\nclient learning strategy pervasively and can not adapt to the dynamic training\nparameters of different clients. In this paper, by considering the deviation\nbetween different local model parameters, we propose an adaptive learning rate\nscheme for each client based on entropy theory to alleviate the deviation\nbetween heterogeneous clients and achieve fast convergence of the global model.\nIt's difficult to design the optimal dynamic learning rate for each client as\nthe local information of other clients is unknown, especially during the local\ntraining epochs without communications between local clients and the central\nserver. To enable a decentralized learning rate design for each client, we\nfirst introduce mean-field schemes to estimate the terms related to other\nclients' local model parameters. Then the decentralized adaptive learning rate\nfor each client is obtained in closed form by constructing the Hamilton\nequation. Moreover, we prove that there exist fixed point solutions for the\nmean-field estimators, and an algorithm is proposed to obtain them. Finally,\nextensive experimental results on real datasets show that our algorithm can\neffectively eliminate the deviation between local model parameters compared to\nother recent FL algorithms.\n","authors":["Shensheng Zheng","Xuehe Wang","Lingjie Duan"],"pdf_url":"https://arxiv.org/pdf/2303.14966v1.pdf","comment":"7 pages, 4 figures"},{"id":"http://arxiv.org/abs/2303.14965v1","updated":"2023-03-27T07:54:56Z","published":"2023-03-27T07:54:56Z","title":"Disruption Precursor Onset Time Study Based on Semi-supervised Anomaly\n  Detection","summary":"  The full understanding of plasma disruption in tokamaks is currently lacking,\nand data-driven methods are extensively used for disruption prediction.\nHowever, most existing data-driven disruption predictors employ supervised\nlearning techniques, which require labeled training data. The manual labeling\nof disruption precursors is a tedious and challenging task, as some precursors\nare difficult to accurately identify, limiting the potential of machine\nlearning models. To address this issue, commonly used labeling methods assume\nthat the precursor onset occurs at a fixed time before the disruption, which\nmay not be consistent for different types of disruptions or even the same type\nof disruption, due to the different speeds at which plasma instabilities\nescalate. This leads to mislabeled samples and suboptimal performance of the\nsupervised learning predictor. In this paper, we present a disruption\nprediction method based on anomaly detection that overcomes the drawbacks of\nunbalanced positive and negative data samples and inaccurately labeled\ndisruption precursor samples. We demonstrate the effectiveness and reliability\nof anomaly detection predictors based on different algorithms on J-TEXT and\nEAST to evaluate the reliability of the precursor onset time inferred by the\nanomaly detection predictor. The precursor onset times inferred by these\npredictors reveal that the labeling methods have room for improvement as the\nonset times of different shots are not necessarily the same. Finally, we\noptimize precursor labeling using the onset times inferred by the anomaly\ndetection predictor and test the optimized labels on supervised learning\ndisruption predictors. The results on J-TEXT and EAST show that the models\ntrained on the optimized labels outperform those trained on fixed onset time\nlabels.\n","authors":["Xinkun Ai","Wei Zheng","Ming Zhang","Dalong Chen","Chengshuo Shen","Bihao Guo","Bingjia Xiao","Yu Zhong","Nengchao Wang","Zhoujun Yang","Zhipeng Chen","Zhongyong Chen","Yonghua Ding","Yuan Pan","J-TEXT team"],"pdf_url":"https://arxiv.org/pdf/2303.14965v1.pdf","comment":"21 pages, 11 figures"},{"id":"http://arxiv.org/abs/2303.13540v2","updated":"2023-03-27T07:53:49Z","published":"2023-03-17T16:01:39Z","title":"Artificial Intelligence for Sustainability: Facilitating Sustainable\n  Smart Product-Service Systems with Computer Vision","summary":"  The usage and impact of deep learning for cleaner production and\nsustainability purposes remain little explored. This work shows how deep\nlearning can be harnessed to increase sustainability in production and product\nusage. Specifically, we utilize deep learning-based computer vision to\ndetermine the wear states of products. The resulting insights serve as a basis\nfor novel product-service systems with improved integration and result\norientation. Moreover, these insights are expected to facilitate product usage\nimprovements and R&D innovations. We demonstrate our approach on two products:\nmachining tools and rotating X-ray anodes. From a technical standpoint, we show\nthat it is possible to recognize the wear state of these products using\ndeep-learning-based computer vision. In particular, we detect wear through\nmicroscopic images of the two products. We utilize a U-Net for semantic\nsegmentation to detect wear based on pixel granularity. The resulting mean dice\ncoefficients of 0.631 and 0.603 demonstrate the feasibility of the proposed\napproach. Consequently, experts can now make better decisions, for example, to\nimprove the machining process parameters. To assess the impact of the proposed\napproach on environmental sustainability, we perform life cycle assessments\nthat show gains for both products. The results indicate that the emissions of\nCO2 equivalents are reduced by 12% for machining tools and by 44% for rotating\nanodes. This work can serve as a guideline and inspire researchers and\npractitioners to utilize computer vision in similar scenarios to develop\nsustainable smart product-service systems and enable cleaner production.\n","authors":["Jannis Walk","Niklas Kühl","Michael Saidani","Jürgen Schatte"],"pdf_url":"https://arxiv.org/pdf/2303.13540v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.14962v1","updated":"2023-03-27T07:53:23Z","published":"2023-03-27T07:53:23Z","title":"Forget-free Continual Learning with Soft-Winning SubNetworks","summary":"  Inspired by Regularized Lottery Ticket Hypothesis (RLTH), which states that\ncompetitive smooth (non-binary) subnetworks exist within a dense network in\ncontinual learning tasks, we investigate two proposed architecture-based\ncontinual learning methods which sequentially learn and select adaptive binary-\n(WSN) and non-binary Soft-Subnetworks (SoftNet) for each task. WSN and SoftNet\njointly learn the regularized model weights and task-adaptive non-binary masks\nof subnetworks associated with each task whilst attempting to select a small\nset of weights to be activated (winning ticket) by reusing weights of the prior\nsubnetworks. Our proposed WSN and SoftNet are inherently immune to catastrophic\nforgetting as each selected subnetwork model does not infringe upon other\nsubnetworks in Task Incremental Learning (TIL). In TIL, binary masks spawned\nper winning ticket are encoded into one N-bit binary digit mask, then\ncompressed using Huffman coding for a sub-linear increase in network capacity\nto the number of tasks. Surprisingly, in the inference step, SoftNet generated\nby injecting small noises to the backgrounds of acquired WSN (holding the\nforegrounds of WSN) provides excellent forward transfer power for future tasks\nin TIL. SoftNet shows its effectiveness over WSN in regularizing parameters to\ntackle the overfitting, to a few examples in Few-shot Class Incremental\nLearning (FSCIL).\n","authors":["Haeyong Kang","Jaehong Yoon","Sultan Rizky Madjid","Sung Ju Hwang","Chang D. Yoo"],"pdf_url":"https://arxiv.org/pdf/2303.14962v1.pdf","comment":"arXiv admin note: text overlap with arXiv:2209.07529"},{"id":"http://arxiv.org/abs/2303.14961v1","updated":"2023-03-27T07:52:58Z","published":"2023-03-27T07:52:58Z","title":"Diffusion Denoised Smoothing for Certified and Adversarial Robust\n  Out-Of-Distribution Detection","summary":"  As the use of machine learning continues to expand, the importance of\nensuring its safety cannot be overstated. A key concern in this regard is the\nability to identify whether a given sample is from the training distribution,\nor is an \"Out-Of-Distribution\" (OOD) sample. In addition, adversaries can\nmanipulate OOD samples in ways that lead a classifier to make a confident\nprediction. In this study, we present a novel approach for certifying the\nrobustness of OOD detection within a $\\ell_2$-norm around the input, regardless\nof network architecture and without the need for specific components or\nadditional training. Further, we improve current techniques for detecting\nadversarial attacks on OOD samples, while providing high levels of certified\nand adversarial robustness on in-distribution samples. The average of all OOD\ndetection metrics on CIFAR10/100 shows an increase of $\\sim 13 \\% / 5\\%$\nrelative to previous approaches.\n","authors":["Nicola Franco","Daniel Korth","Jeanette Miriam Lorenz","Karsten Roscher","Stephan Guennemann"],"pdf_url":"https://arxiv.org/pdf/2303.14961v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.14951v1","updated":"2023-03-27T07:28:46Z","published":"2023-03-27T07:28:46Z","title":"Improving Contextualized Topic Models with Negative Sampling","summary":"  Topic modeling has emerged as a dominant method for exploring large document\ncollections. Recent approaches to topic modeling use large contextualized\nlanguage models and variational autoencoders. In this paper, we propose a\nnegative sampling mechanism for a contextualized topic model to improve the\nquality of the generated topics. In particular, during model training, we\nperturb the generated document-topic vector and use a triplet loss to encourage\nthe document reconstructed from the correct document-topic vector to be similar\nto the input document and dissimilar to the document reconstructed from the\nperturbed vector. Experiments for different topic counts on three publicly\navailable benchmark datasets show that in most cases, our approach leads to an\nincrease in topic coherence over that of the baselines. Our model also achieves\nvery high topic diversity.\n","authors":["Suman Adhya","Avishek Lahiri","Debarshi Kumar Sanyal","Partha Pratim Das"],"pdf_url":"https://arxiv.org/pdf/2303.14951v1.pdf","comment":"Accepted at 19th International Conference on Natural Language\n  Processing (ICON 2022)"},{"id":"http://arxiv.org/abs/2209.07116v4","updated":"2023-03-27T07:28:31Z","published":"2022-09-15T07:59:05Z","title":"On Generalization of Decentralized Learning with Separable Data","summary":"  Decentralized learning offers privacy and communication efficiency when data\nare naturally distributed among agents communicating over an underlying graph.\nMotivated by overparameterized learning settings, in which models are trained\nto zero training loss, we study algorithmic and generalization properties of\ndecentralized learning with gradient descent on separable data. Specifically,\nfor decentralized gradient descent (DGD) and a variety of loss functions that\nasymptote to zero at infinity (including exponential and logistic losses), we\nderive novel finite-time generalization bounds. This complements a long line of\nrecent work that studies the generalization performance and the implicit bias\nof gradient descent over separable data, but has thus far been limited to\ncentralized learning scenarios. Notably, our generalization bounds\napproximately match in order their centralized counterparts. Critical behind\nthis, and of independent interest, is establishing novel bounds on the training\nloss and the rate-of-consensus of DGD for a class of self-bounded losses.\nFinally, on the algorithmic front, we design improved gradient-based routines\nfor decentralized learning with separable data and empirically demonstrate\norders-of-magnitude of speed-up in terms of both training and generalization\nperformance.\n","authors":["Hossein Taheri","Christos Thrampoulidis"],"pdf_url":"https://arxiv.org/pdf/2209.07116v4.pdf","comment":"Minor changes: fixing typos, few more references. Title changed to\n  the title of conference version"},{"id":"http://arxiv.org/abs/2303.08435v2","updated":"2023-03-27T07:11:17Z","published":"2023-03-15T08:17:07Z","title":"Physics-Informed Optical Kernel Regression Using Complex-valued Neural\n  Fields","summary":"  Lithography is fundamental to integrated circuit fabrication, necessitating\nlarge computation overhead. The advancement of machine learning (ML)-based\nlithography models alleviates the trade-offs between manufacturing process\nexpense and capability. However, all previous methods regard the lithography\nsystem as an image-to-image black box mapping, utilizing network parameters to\nlearn by rote mappings from massive mask-to-aerial or mask-to-resist image\npairs, resulting in poor generalization capability. In this paper, we propose a\nnew ML-based paradigm disassembling the rigorous lithographic model into\nnon-parametric mask operations and learned optical kernels containing\ndeterminant source, pupil, and lithography information. By optimizing\ncomplex-valued neural fields to perform optical kernel regression from\ncoordinates, our method can accurately restore lithography system using a\nsmall-scale training dataset with fewer parameters, demonstrating superior\ngeneralization capability as well. Experiments show that our framework can use\n31% of parameters while achieving 69$\\times$ smaller mean squared error with\n1.3$\\times$ higher throughput than the state-of-the-art.\n","authors":["Guojin Chen","Zehua Pei","Haoyu Yang","Yuzhe Ma","Bei Yu","Martin D. F. Wong"],"pdf_url":"https://arxiv.org/pdf/2303.08435v2.pdf","comment":"Accepted by DAC23"},{"id":"http://arxiv.org/abs/2208.08221v3","updated":"2023-03-27T06:58:11Z","published":"2022-08-17T11:06:44Z","title":"Which Factors are associated with Open Access Publishing? A Springer\n  Nature Case Study","summary":"  Open Access (OA) facilitates access to articles. But, authors or funders\noften must pay the publishing costs preventing authors who do not receive\nfinancial support from participating in OA publishing and citation advantage\nfor OA articles. OA may exacerbate existing inequalities in the publication\nsystem rather than overcome them. To investigate this, we studied 522,411\narticles published by Springer Nature. Employing correlation and regression\nanalyses, we describe the relationship between authors affiliated with\ncountries from different income levels, their choice of publishing model, and\nthe citation impact of their papers. A machine learning classification method\nhelped us to explore the importance of different features in predicting the\npublishing model. The results show that authors eligible for APC waivers\npublish more in gold-OA journals than others. In contrast, authors eligible for\nan APC discount have the lowest ratio of OA publications, leading to the\nassumption that this discount insufficiently motivates authors to publish in\ngold-OA journals. We found a strong correlation between the journal rank and\nthe publishing model in gold-OA journals, whereas the OA option is mostly\navoided in hybrid journals. Also, results show that the countries' income\nlevel, seniority, and experience with OA publications are the most predictive\nfactors for OA publishing in hybrid journals.\n","authors":["Fakhri Momeni","Stefan Dietze","Philipp Mayr","Kristin Biesenbender","Isabella Peters"],"pdf_url":"https://arxiv.org/pdf/2208.08221v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2212.12380v3","updated":"2023-03-27T06:44:53Z","published":"2022-12-23T14:53:09Z","title":"Towards Scalable Physically Consistent Neural Networks: an Application\n  to Data-driven Multi-zone Thermal Building Models","summary":"  With more and more data being collected, data-driven modeling methods have\nbeen gaining in popularity in recent years. While physically sound, classical\ngray-box models are often cumbersome to identify and scale, and their accuracy\nmight be hindered by their limited expressiveness. On the other hand, classical\nblack-box methods, typically relying on Neural Networks (NNs) nowadays, often\nachieve impressive performance, even at scale, by deriving statistical patterns\nfrom data. However, they remain completely oblivious to the underlying physical\nlaws, which may lead to potentially catastrophic failures if decisions for\nreal-world physical systems are based on them. Physically Consistent Neural\nNetworks (PCNNs) were recently developed to address these aforementioned\nissues, ensuring physical consistency while still leveraging NNs to attain\nstate-of-the-art accuracy.\n  In this work, we scale PCNNs to model building temperature dynamics and\npropose a thorough comparison with classical gray-box and black-box methods.\nMore precisely, we design three distinct PCNN extensions, thereby exemplifying\nthe modularity and flexibility of the architecture, and formally prove their\nphysical consistency. In the presented case study, PCNNs are shown to achieve\nstate-of-the-art accuracy, even outperforming classical NN-based models despite\ntheir constrained structure. Our investigations furthermore provide a clear\nillustration of NNs achieving seemingly good performance while remaining\ncompletely physics-agnostic, which can be misleading in practice. While this\nperformance comes at the cost of computational complexity, PCNNs on the other\nhand show accuracy improvements of 17-35% compared to all other physically\nconsistent methods, paving the way for scalable physically consistent models\nwith state-of-the-art performance.\n","authors":["Loris Di Natale","Bratislav Svetozarevic","Philipp Heer","Colin Neil Jones"],"pdf_url":"https://arxiv.org/pdf/2212.12380v3.pdf","comment":"Submitted to Applied Energy"},{"id":"http://arxiv.org/abs/2303.14939v1","updated":"2023-03-27T06:37:55Z","published":"2023-03-27T06:37:55Z","title":"Explain, Adapt and Retrain: How to improve the accuracy of a PPM\n  classifier through different explanation styles","summary":"  Recent papers have introduced a novel approach to explain why a Predictive\nProcess Monitoring (PPM) model for outcome-oriented predictions provides wrong\npredictions. Moreover, they have shown how to exploit the explanations,\nobtained using state-of-the art post-hoc explainers, to identify the most\ncommon features that induce a predictor to make mistakes in a semi-automated\nway, and, in turn, to reduce the impact of those features and increase the\naccuracy of the predictive model. This work starts from the assumption that\nfrequent control flow patterns in event logs may represent important features\nthat characterize, and therefore explain, a certain prediction. Therefore, in\nthis paper, we (i) employ a novel encoding able to leverage DECLARE constraints\nin Predictive Process Monitoring and compare the effectiveness of this encoding\nwith Predictive Process Monitoring state-of-the art encodings, in particular\nfor the task of outcome-oriented predictions; (ii) introduce a completely\nautomated pipeline for the identification of the most common features inducing\na predictor to make mistakes; and (iii) show the effectiveness of the proposed\npipeline in increasing the accuracy of the predictive model by validating it on\ndifferent real-life datasets.\n","authors":["Williams Rizzi","Chiara Di Francescomarino","Chiara Ghidini","Fabrizio Maria Maggi"],"pdf_url":"https://arxiv.org/pdf/2303.14939v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.14937v1","updated":"2023-03-27T06:34:42Z","published":"2023-03-27T06:34:42Z","title":"LEURN: Learning Explainable Univariate Rules with Neural Networks","summary":"  In this paper, we propose LEURN: a neural network architecture that learns\nunivariate decision rules. LEURN is a white-box algorithm that results into\nunivariate trees and makes explainable decisions in every stage. In each layer,\nLEURN finds a set of univariate rules based on an embedding of the previously\nchecked rules and their corresponding responses. Both rule finding and final\ndecision mechanisms are weighted linear combinations of these embeddings, hence\ncontribution of all rules are clearly formulated and explainable. LEURN can\nselect features, extract feature importance, provide semantic similarity\nbetween a pair of samples, be used in a generative manner and can give a\nconfidence score. Thanks to a smoothness parameter, LEURN can also controllably\nbehave like decision trees or vanilla neural networks. Besides these\nadvantages, LEURN achieves comparable performance to state-of-the-art methods\nacross 30 tabular datasets for classification and regression problems.\n","authors":["Caglar Aytekin"],"pdf_url":"https://arxiv.org/pdf/2303.14937v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2206.06561v4","updated":"2023-03-27T05:59:30Z","published":"2022-06-14T02:24:38Z","title":"FreeKD: Free-direction Knowledge Distillation for Graph Neural Networks","summary":"  Knowledge distillation (KD) has demonstrated its effectiveness to boost the\nperformance of graph neural networks (GNNs), where its goal is to distill\nknowledge from a deeper teacher GNN into a shallower student GNN. However, it\nis actually difficult to train a satisfactory teacher GNN due to the well-known\nover-parametrized and over-smoothing issues, leading to invalid knowledge\ntransfer in practical applications. In this paper, we propose the first\nFree-direction Knowledge Distillation framework via Reinforcement learning for\nGNNs, called FreeKD, which is no longer required to provide a deeper\nwell-optimized teacher GNN. The core idea of our work is to collaboratively\nbuild two shallower GNNs in an effort to exchange knowledge between them via\nreinforcement learning in a hierarchical way. As we observe that one typical\nGNN model often has better and worse performances at different nodes during\ntraining, we devise a dynamic and free-direction knowledge transfer strategy\nthat consists of two levels of actions: 1) node-level action determines the\ndirections of knowledge transfer between the corresponding nodes of two\nnetworks; and then 2) structure-level action determines which of the local\nstructures generated by the node-level actions to be propagated. In essence,\nour FreeKD is a general and principled framework which can be naturally\ncompatible with GNNs of different architectures. Extensive experiments on five\nbenchmark datasets demonstrate our FreeKD outperforms two base GNNs in a large\nmargin, and shows its efficacy to various GNNs. More surprisingly, our FreeKD\nhas comparable or even better performance than traditional KD algorithms that\ndistill knowledge from a deeper and stronger teacher GNN.\n","authors":["Kaituo Feng","Changsheng Li","Ye Yuan","Guoren Wang"],"pdf_url":"https://arxiv.org/pdf/2206.06561v4.pdf","comment":"Accepted to KDD 2022"},{"id":"http://arxiv.org/abs/2303.14920v1","updated":"2023-03-27T05:34:19Z","published":"2023-03-27T05:34:19Z","title":"Adapting Pretrained Language Models for Solving Tabular Prediction\n  Problems in the Electronic Health Record","summary":"  We propose an approach for adapting the DeBERTa model for electronic health\nrecord (EHR) tasks using domain adaptation. We pretrain a small DeBERTa model\non a dataset consisting of MIMIC-III discharge summaries, clinical notes,\nradiology reports, and PubMed abstracts. We compare this model's performance\nwith a DeBERTa model pre-trained on clinical texts from our institutional EHR\n(MeDeBERTa) and an XGBoost model. We evaluate performance on three benchmark\ntasks for emergency department outcomes using the MIMIC-IV-ED dataset. We\npreprocess the data to convert it into text format and generate four versions\nof the original datasets to compare data processing and data inclusion. The\nresults show that our proposed approach outperforms the alternative models on\ntwo of three tasks (p<0.001) and matches performance on the third task, with\nthe use of descriptive columns improving performance over the original column\nnames.\n","authors":["Christopher McMaster","David FL Liew","Douglas EV Pires"],"pdf_url":"https://arxiv.org/pdf/2303.14920v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.11577v3","updated":"2023-03-27T05:29:33Z","published":"2023-03-21T03:51:15Z","title":"Feature-adjacent multi-fidelity physics-informed machine learning for\n  partial differential equations","summary":"  Physics-informed neural networks have emerged as an alternative method for\nsolving partial differential equations. However, for complex problems, the\ntraining of such networks can still require high-fidelity data which can be\nexpensive to generate. To reduce or even eliminate the dependency on\nhigh-fidelity data, we propose a novel multi-fidelity architecture which is\nbased on a feature space shared by the low- and high-fidelity solutions. In the\nfeature space, the projections of the low-fidelity and high-fidelity solutions\nare adjacent by constraining their relative distance. The feature space is\nrepresented with an encoder and its mapping to the original solution space is\neffected through a decoder. The proposed multi-fidelity approach is validated\non forward and inverse problems for steady and unsteady problems described by\npartial differential equations.\n","authors":["Wenqian Chen","Panos Stinis"],"pdf_url":"https://arxiv.org/pdf/2303.11577v3.pdf","comment":"12 figures"},{"id":"http://arxiv.org/abs/2205.15531v2","updated":"2023-03-27T04:30:25Z","published":"2022-05-31T04:25:37Z","title":"itKD: Interchange Transfer-based Knowledge Distillation for 3D Object\n  Detection","summary":"  Point-cloud based 3D object detectors recently have achieved remarkable\nprogress. However, most studies are limited to the development of network\narchitectures for improving only their accuracy without consideration of the\ncomputational efficiency. In this paper, we first propose an autoencoder-style\nframework comprising channel-wise compression and decompression via interchange\ntransfer-based knowledge distillation. To learn the map-view feature of a\nteacher network, the features from teacher and student networks are\nindependently passed through the shared autoencoder; here, we use a compressed\nrepresentation loss that binds the channel-wised compression knowledge from\nboth student and teacher networks as a kind of regularization. The decompressed\nfeatures are transferred in opposite directions to reduce the gap in the\ninterchange reconstructions. Lastly, we present an head attention loss to match\nthe 3D object detection information drawn by the multi-head self-attention\nmechanism. Through extensive experiments, we verify that our method can train\nthe lightweight model that is well-aligned with the 3D point cloud detection\ntask and we demonstrate its superiority using the well-known public datasets;\ne.g., Waymo and nuScenes.\n","authors":["Hyeon Cho","Junyong Choi","Geonwoo Baek","Wonjun Hwang"],"pdf_url":"https://arxiv.org/pdf/2205.15531v2.pdf","comment":"Accepted at CVPR 2023"},{"id":"http://arxiv.org/abs/2211.08179v2","updated":"2023-03-27T03:29:48Z","published":"2022-11-15T14:41:11Z","title":"Artificial intelligence approaches for materials-by-design of energetic\n  materials: state-of-the-art, challenges, and future directions","summary":"  Artificial intelligence (AI) is rapidly emerging as an enabling tool for\nsolving various complex materials design problems. This paper aims to review\nrecent advances in AI-driven materials-by-design and their applications to\nenergetic materials (EM). Trained with data from numerical simulations and/or\nphysical experiments, AI models can assimilate trends and patterns within the\ndesign parameter space, identify optimal material designs (micro-morphologies,\ncombinations of materials in composites, etc.), and point to designs with\nsuperior/targeted property and performance metrics. We review approaches\nfocusing on such capabilities with respect to the three main stages of\nmaterials-by-design, namely representation learning of microstructure\nmorphology (i.e., shape descriptors), structure-property-performance (S-P-P)\nlinkage estimation, and optimization/design exploration. We provide a\nperspective view of these methods in terms of their potential, practicality,\nand efficacy towards the realization of materials-by-design. Specifically,\nmethods in the literature are evaluated in terms of their capacity to learn\nfrom a small/limited number of data, computational complexity,\ngeneralizability/scalability to other material species and operating\nconditions, interpretability of the model predictions, and the burden of\nsupervision/data annotation. Finally, we suggest a few promising future\nresearch directions for EM materials-by-design, such as meta-learning, active\nlearning, Bayesian learning, and semi-/weakly-supervised learning, to bridge\nthe gap between machine learning research and EM research.\n","authors":["Joseph B. Choi","Phong C. H. Nguyen","Oishik Sen","H. S. Udaykumar","Stephen Baek"],"pdf_url":"https://arxiv.org/pdf/2211.08179v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.14901v1","updated":"2023-03-27T03:22:25Z","published":"2023-03-27T03:22:25Z","title":"Identifying Suspicious Regions of Covid-19 by Abnormality-Sensitive\n  Activation Mapping","summary":"  This paper presents a fully-automated method for the identification of\nsuspicious regions of a coronavirus disease (COVID-19) on chest CT volumes. One\nmajor role of chest CT scanning in COVID-19 diagnoses is identification of an\ninflammation particular to the disease. This task is generally performed by\nradiologists through an interpretation of the CT volumes, however, because of\nthe heavy workload, an automatic analysis method using a computer is desired.\nMost computer-aided diagnosis studies have addressed only a portion of the\nelements necessary for the identification. In this work, we realize the\nidentification method through a classification task by using a 2.5-dimensional\nCNN with three-dimensional attention mechanisms. We visualize the suspicious\nregions by applying a backpropagation based on positive gradients to\nattention-weighted features. We perform experiments on an in-house dataset and\ntwo public datasets to reveal the generalization ability of the proposed\nmethod. The proposed architecture achieved AUCs of over 0.900 for all the\ndatasets, and mean sensitivity $0.853 \\pm 0.036$ and specificity $0.870 \\pm\n0.040$. The method can also identify notable lesions pointed out in the\nradiology report as suspicious regions.\n","authors":["Ryo Toda","Hayato Itoh","Masahiro Oda","Yuichiro Hayashi","Yoshito Otake","Masahiro Hashimoto","Toshiaki Akashi","Shigeki Aoki","Kensaku Mori"],"pdf_url":"https://arxiv.org/pdf/2303.14901v1.pdf","comment":"10 pages, 3 figures"},{"id":"http://arxiv.org/abs/2303.14898v1","updated":"2023-03-27T03:15:27Z","published":"2023-03-27T03:15:27Z","title":"Mutually-paced Knowledge Distillation for Cross-lingual Temporal\n  Knowledge Graph Reasoning","summary":"  This paper investigates cross-lingual temporal knowledge graph reasoning\nproblem, which aims to facilitate reasoning on Temporal Knowledge Graphs (TKGs)\nin low-resource languages by transfering knowledge from TKGs in high-resource\nones. The cross-lingual distillation ability across TKGs becomes increasingly\ncrucial, in light of the unsatisfying performance of existing reasoning methods\non those severely incomplete TKGs, especially in low-resource languages.\nHowever, it poses tremendous challenges in two aspects. First, the\ncross-lingual alignments, which serve as bridges for knowledge transfer, are\nusually too scarce to transfer sufficient knowledge between two TKGs. Second,\ntemporal knowledge discrepancy of the aligned entities, especially when\nalignments are unreliable, can mislead the knowledge distillation process. We\ncorrespondingly propose a mutually-paced knowledge distillation model MP-KD,\nwhere a teacher network trained on a source TKG can guide the training of a\nstudent network on target TKGs with an alignment module. Concretely, to deal\nwith the scarcity issue, MP-KD generates pseudo alignments between TKGs based\non the temporal information extracted by our representation module. To maximize\nthe efficacy of knowledge transfer and control the noise caused by the temporal\nknowledge discrepancy, we enhance MP-KD with a temporal cross-lingual attention\nmechanism to dynamically estimate the alignment strength. The two procedures\nare mutually paced along with model training. Extensive experiments on twelve\ncross-lingual TKG transfer tasks in the EventKG benchmark demonstrate the\neffectiveness of the proposed MP-KD method.\n","authors":["Ruijie Wang","Zheng Li","Jingfeng Yang","Tianyu Cao","Chao Zhang","Bing Yin","Tarek Abdelzaher"],"pdf_url":"https://arxiv.org/pdf/2303.14898v1.pdf","comment":"This paper is accepted by The Web Conference 2023"},{"id":"http://arxiv.org/abs/2203.02194v3","updated":"2023-03-27T03:12:05Z","published":"2022-03-04T09:04:55Z","title":"Rethinking Reconstruction Autoencoder-Based Out-of-Distribution\n  Detection","summary":"  In some scenarios, classifier requires detecting out-of-distribution samples\nfar from its training data. With desirable characteristics, reconstruction\nautoencoder-based methods deal with this problem by using input reconstruction\nerror as a metric of novelty vs. normality. We formulate the essence of such\napproach as a quadruplet domain translation with an intrinsic bias to only\nquery for a proxy of conditional data uncertainty. Accordingly, an improvement\ndirection is formalized as maximumly compressing the autoencoder's latent space\nwhile ensuring its reconstructive power for acting as a described domain\ntranslator. From it, strategies are introduced including semantic\nreconstruction, data certainty decomposition and normalized L2 distance to\nsubstantially improve original methods, which together establish\nstate-of-the-art performance on various benchmarks, e.g., the FPR@95%TPR of\nCIFAR-100 vs. TinyImagenet-crop on Wide-ResNet is 0.2%. Importantly, our method\nworks without any additional data, hard-to-implement structure, time-consuming\npipeline, and even harming the classification accuracy of known classes.\n","authors":["Yibo Zhou"],"pdf_url":"https://arxiv.org/pdf/2203.02194v3.pdf","comment":"Accepted('Poster' presentation) as main conference paper of CVPR2022"},{"id":"http://arxiv.org/abs/2301.11517v3","updated":"2023-03-27T02:59:10Z","published":"2023-01-27T03:33:11Z","title":"Task-Agnostic Graph Neural Network Evaluation via Adversarial\n  Collaboration","summary":"  It has been increasingly demanding to develop reliable methods to evaluate\nthe progress of Graph Neural Network (GNN) research for molecular\nrepresentation learning. Existing GNN benchmarking methods for molecular\nrepresentation learning focus on comparing the GNNs' performances on some\nnode/graph classification/regression tasks on certain datasets. However, there\nlacks a principled, task-agnostic method to directly compare two GNNs.\nAdditionally, most of the existing self-supervised learning works incorporate\nhandcrafted augmentations to the data, which has several severe difficulties to\nbe applied on graphs due to their unique characteristics. To address the\naforementioned issues, we propose GraphAC (Graph Adversarial Collaboration) --\na conceptually novel, principled, task-agnostic, and stable framework for\nevaluating GNNs through contrastive self-supervision. We introduce a novel\nobjective function: the Competitive Barlow Twins, that allow two GNNs to\njointly update themselves from direct competitions against each other. GraphAC\nsucceeds in distinguishing GNNs of different expressiveness across various\naspects, and has demonstrated to be a principled and reliable GNN evaluation\nmethod, without necessitating any augmentations.\n","authors":["Xiangyu Zhao","Hannes Stärk","Dominique Beaini","Yiren Zhao","Pietro Liò"],"pdf_url":"https://arxiv.org/pdf/2301.11517v3.pdf","comment":"11th International Conference on Learning Representations (ICLR 2023)\n  Machine Learning for Drug Discovery (MLDD) Workshop. 17 pages, 6 figures, 4\n  tables"},{"id":"http://arxiv.org/abs/2303.14889v1","updated":"2023-03-27T02:55:56Z","published":"2023-03-27T02:55:56Z","title":"Model-Based Reinforcement Learning with Isolated Imaginations","summary":"  World models learn the consequences of actions in vision-based interactive\nsystems. However, in practical scenarios like autonomous driving,\nnoncontrollable dynamics that are independent or sparsely dependent on action\nsignals often exist, making it challenging to learn effective world models. To\naddress this issue, we propose Iso-Dream++, a model-based reinforcement\nlearning approach that has two main contributions. First, we optimize the\ninverse dynamics to encourage the world model to isolate controllable state\ntransitions from the mixed spatiotemporal variations of the environment.\nSecond, we perform policy optimization based on the decoupled latent\nimaginations, where we roll out noncontrollable states into the future and\nadaptively associate them with the current controllable state. This enables\nlong-horizon visuomotor control tasks to benefit from isolating mixed dynamics\nsources in the wild, such as self-driving cars that can anticipate the movement\nof other vehicles, thereby avoiding potential risks. On top of our previous\nwork, we further consider the sparse dependencies between controllable and\nnoncontrollable states, address the training collapse problem of state\ndecoupling, and validate our approach in transfer learning setups. Our\nempirical study demonstrates that Iso-Dream++ outperforms existing\nreinforcement learning models significantly on CARLA and DeepMind Control.\n","authors":["Minting Pan","Xiangming Zhu","Yunbo Wang","Xiaokang Yang"],"pdf_url":"https://arxiv.org/pdf/2303.14889v1.pdf","comment":"arXiv admin note: substantial text overlap with arXiv:2205.13817"},{"id":"http://arxiv.org/abs/2303.14885v1","updated":"2023-03-27T02:50:02Z","published":"2023-03-27T02:50:02Z","title":"Text is All You Need: Personalizing ASR Models using Controllable Speech\n  Synthesis","summary":"  Adapting generic speech recognition models to specific individuals is a\nchallenging problem due to the scarcity of personalized data. Recent works have\nproposed boosting the amount of training data using personalized text-to-speech\nsynthesis. Here, we ask two fundamental questions about this strategy: when is\nsynthetic data effective for personalization, and why is it effective in those\ncases? To address the first question, we adapt a state-of-the-art automatic\nspeech recognition (ASR) model to target speakers from four benchmark datasets\nrepresentative of different speaker types. We show that ASR personalization\nwith synthetic data is effective in all cases, but particularly when (i) the\ntarget speaker is underrepresented in the global data, and (ii) the capacity of\nthe global model is limited. To address the second question of why personalized\nsynthetic data is effective, we use controllable speech synthesis to generate\nspeech with varied styles and content. Surprisingly, we find that the text\ncontent of the synthetic data, rather than style, is important for speaker\nadaptation. These results lead us to propose a data selection strategy for ASR\npersonalization based on speech content.\n","authors":["Karren Yang","Ting-Yao Hu","Jen-Hao Rick Chang","Hema Swetha Koppula","Oncel Tuzel"],"pdf_url":"https://arxiv.org/pdf/2303.14885v1.pdf","comment":"ICASSP 2023"},{"id":"http://arxiv.org/abs/2303.14882v1","updated":"2023-03-27T02:45:18Z","published":"2023-03-27T02:45:18Z","title":"TransCODE: Co-design of Transformers and Accelerators for Efficient\n  Training and Inference","summary":"  Automated co-design of machine learning models and evaluation hardware is\ncritical for efficiently deploying such models at scale. Despite the\nstate-of-the-art performance of transformer models, they are not yet ready for\nexecution on resource-constrained hardware platforms. High memory requirements\nand low parallelizability of the transformer architecture exacerbate this\nproblem. Recently-proposed accelerators attempt to optimize the throughput and\nenergy consumption of transformer models. However, such works are either\nlimited to a one-sided search of the model architecture or a restricted set of\noff-the-shelf devices. Furthermore, previous works only accelerate model\ninference and not training, which incurs substantially higher memory and\ncompute resources, making the problem even more challenging. To address these\nlimitations, this work proposes a dynamic training framework, called DynaProp,\nthat speeds up the training process and reduces memory consumption. DynaProp is\na low-overhead pruning method that prunes activations and gradients at runtime.\nTo effectively execute this method on hardware for a diverse set of transformer\narchitectures, we propose ELECTOR, a framework that simulates transformer\ninference and training on a design space of accelerators. We use this simulator\nin conjunction with the proposed co-design technique, called TransCODE, to\nobtain the best-performing models with high accuracy on the given task and\nminimize latency, energy consumption, and chip area. The obtained\ntransformer-accelerator pair achieves 0.3% higher accuracy than the\nstate-of-the-art pair while incurring 5.2$\\times$ lower latency and 3.0$\\times$\nlower energy consumption.\n","authors":["Shikhar Tuli","Niraj K. Jha"],"pdf_url":"https://arxiv.org/pdf/2303.14882v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.14878v1","updated":"2023-03-27T02:22:09Z","published":"2023-03-27T02:22:09Z","title":"GPT-PINN: Generative Pre-Trained Physics-Informed Neural Networks toward\n  non-intrusive Meta-learning of parametric PDEs","summary":"  Physics-Informed Neural Network (PINN) has proven itself a powerful tool to\nobtain the numerical solutions of nonlinear partial differential equations\n(PDEs) leveraging the expressivity of deep neural networks and the computing\npower of modern heterogeneous hardware. However, its training is still\ntime-consuming, especially in the multi-query and real-time simulation\nsettings, and its parameterization often overly excessive. In this paper, we\npropose the Generative Pre-Trained PINN (GPT-PINN) to mitigate both challenges\nin the setting of parametric PDEs. GPT-PINN represents a brand-new\nmeta-learning paradigm for parametric systems. As a network of networks, its\nouter-/meta-network is hyper-reduced with only one hidden layer having\nsignificantly reduced number of neurons. Moreover, its activation function at\neach hidden neuron is a (full) PINN pre-trained at a judiciously selected\nsystem configuration. The meta-network adaptively ``learns'' the parametric\ndependence of the system and ``grows'' this hidden layer one neuron at a time.\nIn the end, by encompassing a very small number of networks trained at this set\nof adaptively-selected parameter values, the meta-network is capable of\ngenerating surrogate solutions for the parametric system across the entire\nparameter domain accurately and efficiently.\n","authors":["Yanlai Chen","Shawn Koohy"],"pdf_url":"https://arxiv.org/pdf/2303.14878v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.14877v1","updated":"2023-03-27T02:14:56Z","published":"2023-03-27T02:14:56Z","title":"Error-mitigated Quantum Approximate Optimization via Learning-based\n  Adaptive Optimization","summary":"  Combinatorial optimization problems are ubiquitous and computationally hard\nto solve in general. Quantum computing is envisioned as a powerful tool\noffering potential computational advantages for solving some of these problems.\nQuantum approximate optimization algorithm (QAOA), one of the most\nrepresentative quantum-classical hybrid algorithms, is designed to solve\ncertain combinatorial optimization problems by transforming a discrete\noptimization problem into a classical optimization problem over a continuous\ncircuit parameter domain. QAOA objective landscape over the parameter variables\nis notorious for pervasive local minima and barren plateaus, and its viability\nin training significantly relies on the efficacy of the classical optimization\nalgorithm. To enhance the performance of QAOA, we design double adaptive-region\nBayesian optimization (DARBO), an adaptive classical optimizer for QAOA. Our\nexperimental results demonstrate that the algorithm greatly outperforms\nconventional gradient-based and gradient-free optimizers in terms of speed,\naccuracy, and stability. We also address the issues of measurement efficiency\nand the suppression of quantum noise by successfully conducting the full\noptimization loop on the superconducting quantum processor. This work helps to\nunlock the full power of QAOA and paves the way toward achieving quantum\nadvantage in practical classical tasks.\n","authors":["Lixue Cheng","Yu-Qin Chen","Shi-Xin Zhang","Shengyu Zhang"],"pdf_url":"https://arxiv.org/pdf/2303.14877v1.pdf","comment":"Main text: 11 pages, 4 figures, SI: 5 pages, 5 figures"},{"id":"http://arxiv.org/abs/2301.00896v2","updated":"2023-03-27T01:57:56Z","published":"2023-01-03T00:28:57Z","title":"Efficient Robustness Assessment via Adversarial Spatial-Temporal Focus\n  on Videos","summary":"  Adversarial robustness assessment for video recognition models has raised\nconcerns owing to their wide applications on safety-critical tasks. Compared\nwith images, videos have much high dimension, which brings huge computational\ncosts when generating adversarial videos. This is especially serious for the\nquery-based black-box attacks where gradient estimation for the threat models\nis usually utilized, and high dimensions will lead to a large number of\nqueries. To mitigate this issue, we propose to simultaneously eliminate the\ntemporal and spatial redundancy within the video to achieve an effective and\nefficient gradient estimation on the reduced searching space, and thus query\nnumber could decrease. To implement this idea, we design the novel Adversarial\nspatial-temporal Focus (AstFocus) attack on videos, which performs attacks on\nthe simultaneously focused key frames and key regions from the inter-frames and\nintra-frames in the video. AstFocus attack is based on the cooperative\nMulti-Agent Reinforcement Learning (MARL) framework. One agent is responsible\nfor selecting key frames, and another agent is responsible for selecting key\nregions. These two agents are jointly trained by the common rewards received\nfrom the black-box threat models to perform a cooperative prediction. By\ncontinuously querying, the reduced searching space composed of key frames and\nkey regions is becoming precise, and the whole query number becomes less than\nthat on the original video. Extensive experiments on four mainstream video\nrecognition models and three widely used action recognition datasets\ndemonstrate that the proposed AstFocus attack outperforms the SOTA methods,\nwhich is prevenient in fooling rate, query number, time, and perturbation\nmagnitude at the same.\n","authors":["Wei Xingxing","Wang Songping","Yan Huanqian"],"pdf_url":"https://arxiv.org/pdf/2301.00896v2.pdf","comment":"accepted by TPAMI2023"},{"id":"http://arxiv.org/abs/2303.14870v1","updated":"2023-03-27T01:25:24Z","published":"2023-03-27T01:25:24Z","title":"Bi-Manual Block Assembly via Sim-to-Real Reinforcement Learning","summary":"  Most successes in robotic manipulation have been restricted to single-arm\ngripper robots, whose low dexterity limits the range of solvable tasks to\npick-and-place, inser-tion, and object rearrangement. More complex tasks such\nas assembly require dual and multi-arm platforms, but entail a suite of unique\nchallenges such as bi-arm coordination and collision avoidance, robust\ngrasping, and long-horizon planning. In this work we investigate the\nfeasibility of training deep reinforcement learning (RL) policies in simulation\nand transferring them to the real world (Sim2Real) as a generic methodology for\nobtaining performant controllers for real-world bi-manual robotic manipulation\ntasks. As a testbed for bi-manual manipulation, we develop the U-Shape Magnetic\nBlockAssembly Task, wherein two robots with parallel grippers must connect 3\nmagnetic blocks to form a U-shape. Without manually-designed controller nor\nhuman demonstrations, we demonstrate that with careful Sim2Real considerations,\nour policies trained with RL in simulation enable two xArm6 robots to solve the\nU-shape assembly task with a success rate of above90% in simulation, and 50% on\nreal hardware without any additional real-world fine-tuning. Through careful\nablations,we highlight how each component of the system is critical for such\nsimple and successful policy learning and transfer,including task\nspecification, learning algorithm, direct joint-space control, behavior\nconstraints, perception and actuation noises, action delays and action\ninterpolation. Our results present a significant step forward for bi-arm\ncapability on real hardware, and we hope our system can inspire future research\non deep RL and Sim2Real transfer of bi-manualpolicies, drastically scaling up\nthe capability of real-world robot manipulators.\n","authors":["Satoshi Kataoka","Youngseog Chung","Seyed Kamyar Seyed Ghasemipour","Pannag Sanketi","Shixiang Shane Gu","Igor Mordatch"],"pdf_url":"https://arxiv.org/pdf/2303.14870v1.pdf","comment":"Our accompanying project webpage can be found at:\n  https://sites.google.com/view/u-shape-block-assembly. arXiv admin note:\n  substantial text overlap with arXiv:2203.08277"},{"id":"http://arxiv.org/abs/2303.14869v1","updated":"2023-03-27T01:22:12Z","published":"2023-03-27T01:22:12Z","title":"Label-Free Liver Tumor Segmentation","summary":"  We demonstrate that AI models can accurately segment liver tumors without the\nneed for manual annotation by using synthetic tumors in CT scans. Our synthetic\ntumors have two intriguing advantages: (I) realistic in shape and texture,\nwhich even medical professionals can confuse with real tumors; (II) effective\nfor training AI models, which can perform liver tumor segmentation similarly to\nthe model trained on real tumors -- this result is exciting because no existing\nwork, using synthetic tumors only, has thus far reached a similar or even close\nperformance to real tumors. This result also implies that manual efforts for\nannotating tumors voxel by voxel (which took years to create) can be\nsignificantly reduced in the future. Moreover, our synthetic tumors can\nautomatically generate many examples of small (or even tiny) synthetic tumors\nand have the potential to improve the success rate of detecting small liver\ntumors, which is critical for detecting the early stages of cancer. In addition\nto enriching the training data, our synthesizing strategy also enables us to\nrigorously assess the AI robustness.\n","authors":["Qixin Hu","Yixiong Chen","Junfei Xiao","Shuwen Sun","Jieneng Chen","Alan Yuille","Zongwei Zhou"],"pdf_url":"https://arxiv.org/pdf/2303.14869v1.pdf","comment":"CVPR 2023"},{"id":"http://arxiv.org/abs/2303.14868v1","updated":"2023-03-27T01:21:31Z","published":"2023-03-27T01:21:31Z","title":"The Resource Problem of Using Linear Layer Leakage Attack in Federated\n  Learning","summary":"  Secure aggregation promises a heightened level of privacy in federated\nlearning, maintaining that a server only has access to a decrypted aggregate\nupdate. Within this setting, linear layer leakage methods are the only data\nreconstruction attacks able to scale and achieve a high leakage rate regardless\nof the number of clients or batch size. This is done through increasing the\nsize of an injected fully-connected (FC) layer. However, this results in a\nresource overhead which grows larger with an increasing number of clients. We\nshow that this resource overhead is caused by an incorrect perspective in all\nprior work that treats an attack on an aggregate update in the same way as an\nindividual update with a larger batch size. Instead, by attacking the update\nfrom the perspective that aggregation is combining multiple individual updates,\nthis allows the application of sparsity to alleviate resource overhead. We show\nthat the use of sparsity can decrease the model size overhead by over\n327$\\times$ and the computation time by 3.34$\\times$ compared to SOTA while\nmaintaining equivalent total leakage rate, 77% even with $1000$ clients in\naggregation.\n","authors":["Joshua C. Zhao","Ahmed Roushdy Elkordy","Atul Sharma","Yahya H. Ezzeldin","Salman Avestimehr","Saurabh Bagchi"],"pdf_url":"https://arxiv.org/pdf/2303.14868v1.pdf","comment":"Accepted to CVPR 2023"},{"id":"http://arxiv.org/abs/2210.11620v2","updated":"2023-03-27T01:19:33Z","published":"2022-10-20T22:31:26Z","title":"LOT: Layer-wise Orthogonal Training on Improving $\\ell_2$ Certified\n  Robustness","summary":"  Recent studies show that training deep neural networks (DNNs) with Lipschitz\nconstraints are able to enhance adversarial robustness and other model\nproperties such as stability. In this paper, we propose a layer-wise orthogonal\ntraining method (LOT) to effectively train 1-Lipschitz convolution layers via\nparametrizing an orthogonal matrix with an unconstrained matrix. We then\nefficiently compute the inverse square root of a convolution kernel by\ntransforming the input domain to the Fourier frequency domain. On the other\nhand, as existing works show that semi-supervised training helps improve\nempirical robustness, we aim to bridge the gap and prove that semi-supervised\nlearning also improves the certified robustness of Lipschitz-bounded models. We\nconduct comprehensive evaluations for LOT under different settings. We show\nthat LOT significantly outperforms baselines regarding deterministic l2\ncertified robustness, and scales to deeper neural networks. Under the\nsupervised scenario, we improve the state-of-the-art certified robustness for\nall architectures (e.g. from 59.04% to 63.50% on CIFAR-10 and from 32.57% to\n34.59% on CIFAR-100 at radius rho = 36/255 for 40-layer networks). With\nsemi-supervised learning over unlabelled data, we are able to improve\nstate-of-the-art certified robustness on CIFAR-10 at rho = 108/255 from 36.04%\nto 42.39%. In addition, LOT consistently outperforms baselines on different\nmodel architectures with only 1/3 evaluation time.\n","authors":["Xiaojun Xu","Linyi Li","Bo Li"],"pdf_url":"https://arxiv.org/pdf/2210.11620v2.pdf","comment":"NeurIPS 2022"},{"id":"http://arxiv.org/abs/2211.05274v2","updated":"2023-03-27T01:04:10Z","published":"2022-11-10T00:40:37Z","title":"Average-Case Complexity of Tensor Decomposition for Low-Degree\n  Polynomials","summary":"  Suppose we are given an $n$-dimensional order-3 symmetric tensor $T \\in\n(\\mathbb{R}^n)^{\\otimes 3}$ that is the sum of $r$ random rank-1 terms. The\nproblem of recovering the rank-1 components is possible in principle when $r\n\\lesssim n^2$ but polynomial-time algorithms are only known in the regime $r\n\\ll n^{3/2}$. Similar \"statistical-computational gaps\" occur in many\nhigh-dimensional inference tasks, and in recent years there has been a flurry\nof work on explaining the apparent computational hardness in these problems by\nproving lower bounds against restricted (yet powerful) models of computation\nsuch as statistical queries (SQ), sum-of-squares (SoS), and low-degree\npolynomials (LDP). However, no such prior work exists for tensor decomposition,\nlargely because its hardness does not appear to be explained by a \"planted\nversus null\" testing problem.\n  We consider a model for random order-3 tensor decomposition where one\ncomponent is slightly larger in norm than the rest (to break symmetry), and the\ncomponents are drawn uniformly from the hypercube. We resolve the computational\ncomplexity in the LDP model: $O(\\log n)$-degree polynomial functions of the\ntensor entries can accurately estimate the largest component when $r \\ll\nn^{3/2}$ but fail to do so when $r \\gg n^{3/2}$. This provides rigorous\nevidence suggesting that the best known algorithms for tensor decomposition\ncannot be improved, at least by known approaches. A natural extension of the\nresult holds for tensors of any fixed order $k \\ge 3$, in which case the LDP\nthreshold is $r \\sim n^{k/2}$.\n","authors":["Alexander S. Wein"],"pdf_url":"https://arxiv.org/pdf/2211.05274v2.pdf","comment":"42 pages; STOC 2023"},{"id":"http://arxiv.org/abs/2302.14383v2","updated":"2023-03-27T01:02:17Z","published":"2023-02-28T08:11:56Z","title":"Linear Spaces of Meanings: Compositional Structures in Vision-Language\n  Models","summary":"  We investigate compositional structures in data embeddings from pre-trained\nvision-language models (VLMs). Traditionally, compositionality has been\nassociated with algebraic operations on embeddings of words from a pre-existing\nvocabulary. In contrast, we seek to approximate representations from an encoder\nas combinations of a smaller set of vectors in the embedding space. These\nvectors can be seen as \"ideal words\" for generating concepts directly within\nthe embedding space of the model. We first present a framework for\nunderstanding compositional structures from a geometric perspective. We then\nexplain what these compositional structures entail probabilistically in the\ncase of VLM embeddings, providing intuitions for why they arise in practice.\nFinally, we empirically explore these structures in CLIP's embeddings and we\nevaluate their usefulness for solving different vision-language tasks such as\nclassification, debiasing, and retrieval. Our results show that simple linear\nalgebraic operations on embedding vectors can be used as compositional and\ninterpretable methods for regulating the behavior of VLMs.\n","authors":["Matthew Trager","Pramuditha Perera","Luca Zancato","Alessandro Achille","Parminder Bhatia","Stefano Soatto"],"pdf_url":"https://arxiv.org/pdf/2302.14383v2.pdf","comment":"18 pages, 9 figures, 7 tables"},{"id":"http://arxiv.org/abs/2303.14863v1","updated":"2023-03-27T00:40:52Z","published":"2023-03-27T00:40:52Z","title":"DiffTAD: Temporal Action Detection with Proposal Denoising Diffusion","summary":"  We propose a new formulation of temporal action detection (TAD) with\ndenoising diffusion, DiffTAD in short. Taking as input random temporal\nproposals, it can yield action proposals accurately given an untrimmed long\nvideo. This presents a generative modeling perspective, against previous\ndiscriminative learning manners. This capability is achieved by first diffusing\nthe ground-truth proposals to random ones (i.e., the forward/noising process)\nand then learning to reverse the noising process (i.e., the backward/denoising\nprocess). Concretely, we establish the denoising process in the Transformer\ndecoder (e.g., DETR) by introducing a temporal location query design with\nfaster convergence in training. We further propose a cross-step selective\nconditioning algorithm for inference acceleration. Extensive evaluations on\nActivityNet and THUMOS show that our DiffTAD achieves top performance compared\nto previous art alternatives. The code will be made available at\nhttps://github.com/sauradip/DiffusionTAD.\n","authors":["Sauradip Nag","Xiatian Zhu","Jiankang Deng","Yi-Zhe Song","Tao Xiang"],"pdf_url":"https://arxiv.org/pdf/2303.14863v1.pdf","comment":"Technical Report"},{"id":"http://arxiv.org/abs/2303.14859v1","updated":"2023-03-27T00:08:45Z","published":"2023-03-27T00:08:45Z","title":"Mind the Label Shift of Augmentation-based Graph OOD Generalization","summary":"  Out-of-distribution (OOD) generalization is an important issue for Graph\nNeural Networks (GNNs). Recent works employ different graph editions to\ngenerate augmented environments and learn an invariant GNN for generalization.\nHowever, the label shift usually occurs in augmentation since graph structural\nedition inevitably alters the graph label. This brings inconsistent predictive\nrelationships among augmented environments, which is harmful to generalization.\nTo address this issue, we propose \\textbf{LiSA}, which generates\nlabel-invariant augmentations to facilitate graph OOD generalization. Instead\nof resorting to graph editions, LiSA exploits \\textbf{L}abel-\\textbf{i}nvariant\n\\textbf{S}ubgraphs of the training graphs to construct \\textbf{A}ugmented\nenvironments. Specifically, LiSA first designs the variational subgraph\ngenerators to extract locally predictive patterns and construct multiple\nlabel-invariant subgraphs efficiently. Then, the subgraphs produced by\ndifferent generators are collected to build different augmented environments.\nTo promote diversity among augmented environments, LiSA further introduces a\ntractable energy-based regularization to enlarge pair-wise distances between\nthe distributions of environments. In this manner, LiSA generates diverse\naugmented environments with a consistent predictive relationship and\nfacilitates learning an invariant GNN. Extensive experiments on node-level and\ngraph-level OOD benchmarks show that LiSA achieves impressive generalization\nperformance with different GNN backbones. Code is available on\n\\url{https://github.com/Samyu0304/LiSA}.\n","authors":["Junchi Yu","Jian Liang","Ran He"],"pdf_url":"https://arxiv.org/pdf/2303.14859v1.pdf","comment":"Accepted to CVPR 2023. arXiv admin note: text overlap with\n  arXiv:2206.09345"},{"id":"http://arxiv.org/abs/2204.08504v3","updated":"2023-03-27T00:03:08Z","published":"2022-04-05T17:34:47Z","title":"CGC: Contrastive Graph Clustering for Community Detection and Tracking","summary":"  Given entities and their interactions in the web data, which may have\noccurred at different time, how can we find communities of entities and track\ntheir evolution? In this paper, we approach this important task from graph\nclustering perspective. Recently, state-of-the-art clustering performance in\nvarious domains has been achieved by deep clustering methods. Especially, deep\ngraph clustering (DGC) methods have successfully extended deep clustering to\ngraph-structured data by learning node representations and cluster assignments\nin a joint optimization framework. Despite some differences in modeling choices\n(e.g., encoder architectures), existing DGC methods are mainly based on\nautoencoders and use the same clustering objective with relatively minor\nadaptations. Also, while many real-world graphs are dynamic, previous DGC\nmethods considered only static graphs. In this work, we develop CGC, a novel\nend-to-end framework for graph clustering, which fundamentally differs from\nexisting methods. CGC learns node embeddings and cluster assignments in a\ncontrastive graph learning framework, where positive and negative samples are\ncarefully selected in a multi-level scheme such that they reflect hierarchical\ncommunity structures and network homophily. Also, we extend CGC for\ntime-evolving data, where temporal graph clustering is performed in an\nincremental learning fashion, with the ability to detect change points.\nExtensive evaluation on real-world graphs demonstrates that the proposed CGC\nconsistently outperforms existing methods.\n","authors":["Namyong Park","Ryan Rossi","Eunyee Koh","Iftikhar Ahamath Burhanuddin","Sungchul Kim","Fan Du","Nesreen Ahmed","Christos Faloutsos"],"pdf_url":"https://arxiv.org/pdf/2204.08504v3.pdf","comment":"TheWebConf 2022 Research Track"},{"id":"http://arxiv.org/abs/2303.15634v1","updated":"2023-03-27T23:29:02Z","published":"2023-03-27T23:29:02Z","title":"Learning Rate Schedules in the Presence of Distribution Shift","summary":"  We design learning rate schedules that minimize regret for SGD-based online\nlearning in the presence of a changing data distribution. We fully characterize\nthe optimal learning rate schedule for online linear regression via a novel\nanalysis with stochastic differential equations. For general convex loss\nfunctions, we propose new learning rate schedules that are robust to\ndistribution shift, and we give upper and lower bounds for the regret that only\ndiffer by constants. For non-convex loss functions, we define a notion of\nregret based on the gradient norm of the estimated models and propose a\nlearning schedule that minimizes an upper bound on the total expected regret.\nIntuitively, one expects changing loss landscapes to require more exploration,\nand we confirm that optimal learning rate schedules typically increase in the\npresence of distribution shift. Finally, we provide experiments for\nhigh-dimensional regression models and neural networks to illustrate these\nlearning rate schedules and their cumulative regret.\n","authors":["Matthew Fahrbach","Adel Javanmard","Vahab Mirrokni","Pratik Worah"],"pdf_url":"https://arxiv.org/pdf/2303.15634v1.pdf","comment":"33 pages, 6 figures"},{"id":"http://arxiv.org/abs/2303.15631v1","updated":"2023-03-27T23:05:21Z","published":"2023-03-27T23:05:21Z","title":"Multiphysics discovery with moving boundaries using Ensemble SINDy and\n  Peridynamic Differential Operator","summary":"  This study proposes a novel framework for learning the underlying physics of\nphenomena with moving boundaries. The proposed approach combines Ensemble SINDy\nand Peridynamic Differential Operator (PDDO) and imposes an inductive bias\nassuming the moving boundary physics evolve in its own corotational coordinate\nsystem. The robustness of the approach is demonstrated by considering various\nlevels of noise in the measured data using the 2D Fisher-Stefan model. The\nconfidence intervals of recovered coefficients are listed, and the\nuncertainties of the moving boundary positions are depicted by obtaining the\nsolutions with the recovered coefficients. Although the main focus of this\nstudy is the Fisher-Stefan model, the proposed approach is applicable to any\ntype of moving boundary problem with a smooth moving boundary front without a\nmushy region. The code and data for this framework is available at:\nhttps://github.com/alicanbekar/MB_PDDO-SINDy.\n","authors":["A. C. Bekar","E. Haghighat","E. Madenci"],"pdf_url":"https://arxiv.org/pdf/2303.15631v1.pdf","comment":"26 pages, 22 figures, submitted to Proceedings of the Royal Society A"},{"id":"http://arxiv.org/abs/2204.04348v2","updated":"2023-03-27T23:03:13Z","published":"2022-04-09T01:48:41Z","title":"Neuronal diversity can improve machine learning for physics and beyond","summary":"  Diversity conveys advantages in nature, yet homogeneous neurons typically\ncomprise the layers of artificial neural networks. Here we construct neural\nnetworks from neurons that learn their own activation functions, quickly\ndiversify, and subsequently outperform their homogeneous counterparts on image\nclassification and nonlinear regression tasks. Sub-networks instantiate the\nneurons, which meta-learn especially efficient sets of nonlinear responses.\nExamples include conventional neural networks classifying digits and\nforecasting a van der Pol oscillator and a physics-informed Hamiltonian neural\nnetwork learning H\\'enon-Heiles orbits. Such learned diversity provides\nexamples of dynamical systems selecting diversity over uniformity and\nelucidates the role of diversity in natural and artificial systems.\n","authors":["Anshul Choudhary","Anil Radhakrishnan","John F. Lindner","Sudeshna Sinha","William L. Ditto"],"pdf_url":"https://arxiv.org/pdf/2204.04348v2.pdf","comment":"12 pages, 8 figures"},{"id":"http://arxiv.org/abs/2303.15626v1","updated":"2023-03-27T22:48:28Z","published":"2023-03-27T22:48:28Z","title":"A Framework for Demonstrating Practical Quantum Advantage: Racing\n  Quantum against Classical Generative Models","summary":"  Generative modeling has seen a rising interest in both classical and quantum\nmachine learning, and it represents a promising candidate to obtain a practical\nquantum advantage in the near term. In this study, we build over a proposed\nframework for evaluating the generalization performance of generative models,\nand we establish the first quantitative comparative race towards practical\nquantum advantage (PQA) between classical and quantum generative models, namely\nQuantum Circuit Born Machines (QCBMs), Transformers (TFs), Recurrent Neural\nNetworks (RNNs), Variational Autoencoders (VAEs), and Wasserstein Generative\nAdversarial Networks (WGANs). After defining four types of PQAs scenarios, we\nfocus on what we refer to as potential PQA, aiming to compare quantum models\nwith the best-known classical algorithms for the task at hand. We let the\nmodels race on a well-defined and application-relevant competition setting,\nwhere we illustrate and demonstrate our framework on 20 variables (qubits)\ngenerative modeling task. Our results suggest that QCBMs are more efficient in\nthe data-limited regime than the other state-of-the-art classical generative\nmodels. Such a feature is highly desirable in a wide range of real-world\napplications where the available data is scarce.\n","authors":["Mohamed Hibat-Allah","Marta Mauri","Juan Carrasquilla","Alejandro Perdomo-Ortiz"],"pdf_url":"https://arxiv.org/pdf/2303.15626v1.pdf","comment":"17 pages, 5 figures, 3 tables"},{"id":"http://arxiv.org/abs/2303.15617v1","updated":"2023-03-27T22:08:05Z","published":"2023-03-27T22:08:05Z","title":"Online Learning for Incentive-Based Demand Response","summary":"  In this paper, we consider the problem of learning online to manage Demand\nResponse (DR) resources. A typical DR mechanism requires the DR manager to\nassign a baseline to the participating consumer, where the baseline is an\nestimate of the counterfactual consumption of the consumer had it not been\ncalled to provide the DR service. A challenge in estimating baseline is the\nincentive the consumer has to inflate the baseline estimate. We consider the\nproblem of learning online to estimate the baseline and to optimize the\noperating costs over a period of time under such incentives. We propose an\nonline learning scheme that employs least-squares for estimation with a\nperturbation to the reward price (for the DR services or load curtailment) that\nis designed to balance the exploration and exploitation trade-off that arises\nwith online learning. We show that, our proposed scheme is able to achieve a\nvery low regret of $\\mathcal{O}\\left((\\log{T})^2\\right)$ with respect to the\noptimal operating cost over $T$ days of the DR program with full knowledge of\nthe baseline, and is individually rational for the consumers to participate.\nOur scheme is significantly better than the averaging type approach, which only\nfetches $\\mathcal{O}(T^{1/3})$ regret.\n","authors":["Deepan Muthirayan","Pramod P. Khargonekar"],"pdf_url":"https://arxiv.org/pdf/2303.15617v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2301.05785v2","updated":"2023-03-27T21:35:49Z","published":"2023-01-13T23:11:14Z","title":"Efficient Activation Function Optimization through Surrogate Modeling","summary":"  Carefully designed activation functions can improve the performance of neural\nnetworks in many machine learning tasks. However, it is difficult for humans to\nconstruct optimal activation functions, and current activation function search\nalgorithms are prohibitively expensive. This paper aims to improve the state of\nthe art through three steps: First, the benchmark datasets Act-Bench-CNN,\nAct-Bench-ResNet, and Act-Bench-ViT were created by training convolutional,\nresidual, and vision transformer architectures from scratch with 2,913\nsystematically generated activation functions. Second, a characterization of\nthe benchmark space was developed, leading to a new surrogate-based method for\noptimization. More specifically, the spectrum of the Fisher information matrix\nassociated with the model's predictive distribution at initialization and the\nactivation function's output distribution were found to be highly predictive of\nperformance. Third, the surrogate was used to discover improved activation\nfunctions in CIFAR-100 and ImageNet tasks. Each of these steps is a\ncontribution in its own right; together they serve as a practical and\ntheoretical foundation for further research on activation function\noptimization. Code is available at\nhttps://github.com/cognizant-ai-labs/aquasurf, and the benchmark datasets are\nat https://github.com/cognizant-ai-labs/act-bench.\n","authors":["Garrett Bingham","Risto Miikkulainen"],"pdf_url":"https://arxiv.org/pdf/2301.05785v2.pdf","comment":"18 pages, 10 figures, 4 tables"},{"id":"http://arxiv.org/abs/2210.10855v2","updated":"2023-03-27T21:28:27Z","published":"2022-10-19T19:35:50Z","title":"Dictionary Learning for the Almost-Linear Sparsity Regime","summary":"  Dictionary learning, the problem of recovering a sparsely used matrix\n$\\mathbf{D} \\in \\mathbb{R}^{M \\times K}$ and $N$ $s$-sparse vectors\n$\\mathbf{x}_i \\in \\mathbb{R}^{K}$ from samples of the form $\\mathbf{y}_i =\n\\mathbf{D}\\mathbf{x}_i$, is of increasing importance to applications in signal\nprocessing and data science. When the dictionary is known, recovery of\n$\\mathbf{x}_i$ is possible even for sparsity linear in dimension $M$, yet to\ndate, the only algorithms which provably succeed in the linear sparsity regime\nare Riemannian trust-region methods, which are limited to orthogonal\ndictionaries, and methods based on the sum-of-squares hierarchy, which requires\nsuper-polynomial time in order to obtain an error which decays in $M$. In this\nwork, we introduce SPORADIC (SPectral ORAcle DICtionary Learning), an efficient\nspectral method on family of reweighted covariance matrices. We prove that in\nhigh enough dimensions, SPORADIC can recover overcomplete ($K > M$)\ndictionaries satisfying the well-known restricted isometry property (RIP) even\nwhen sparsity is linear in dimension up to logarithmic factors. Moreover, these\naccuracy guarantees have an ``oracle property\" that the support and signs of\nthe unknown sparse vectors $\\mathbf{x}_i$ can be recovered exactly with high\nprobability, allowing for arbitrarily close estimation of $\\mathbf{D}$ with\nenough samples in polynomial time. To the author's knowledge, SPORADIC is the\nfirst polynomial-time algorithm which provably enjoys such convergence\nguarantees for overcomplete RIP matrices in the near-linear sparsity regime.\n","authors":["Alexei Novikov","Stephen White"],"pdf_url":"https://arxiv.org/pdf/2210.10855v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.15604v1","updated":"2023-03-27T21:21:46Z","published":"2023-03-27T21:21:46Z","title":"HD-Bind: Encoding of Molecular Structure with Low Precision,\n  Hyperdimensional Binary Representations","summary":"  Publicly available collections of drug-like molecules have grown to comprise\n10s of billions of possibilities in recent history due to advances in chemical\nsynthesis. Traditional methods for identifying ``hit'' molecules from a large\ncollection of potential drug-like candidates have relied on biophysical theory\nto compute approximations to the Gibbs free energy of the binding interaction\nbetween the drug to its protein target. A major drawback of the approaches is\nthat they require exceptional computing capabilities to consider for even\nrelatively small collections of molecules.\n  Hyperdimensional Computing (HDC) is a recently proposed learning paradigm\nthat is able to leverage low-precision binary vector arithmetic to build\nefficient representations of the data that can be obtained without the need for\ngradient-based optimization approaches that are required in many conventional\nmachine learning and deep learning approaches. This algorithmic simplicity\nallows for acceleration in hardware that has been previously demonstrated for a\nrange of application areas. We consider existing HDC approaches for molecular\nproperty classification and introduce two novel encoding algorithms that\nleverage the extended connectivity fingerprint (ECFP) algorithm.\n  We show that HDC-based inference methods are as much as 90 times more\nefficient than more complex representative machine learning methods and achieve\nan acceleration of nearly 9 orders of magnitude as compared to inference with\nmolecular docking. We demonstrate multiple approaches for the encoding of\nmolecular data for HDC and examine their relative performance on a range of\nchallenging molecular property prediction and drug-protein binding\nclassification tasks. Our work thus motivates further investigation into\nmolecular representation learning to develop ultra-efficient pre-screening\ntools.\n","authors":["Derek Jones","Jonathan E. Allen","Xiaohua Zhang","Behnam Khaleghi","Jaeyoung Kang","Weihong Xu","Niema Moshiri","Tajana S. Rosing"],"pdf_url":"https://arxiv.org/pdf/2303.15604v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2209.05954v2","updated":"2023-03-27T21:18:14Z","published":"2022-09-09T23:18:31Z","title":"Automatically Score Tissue Images Like a Pathologist by Transfer\n  Learning","summary":"  Cancer is the second leading cause of death in the world. Diagnosing cancer\nearly on can save many lives. Pathologists have to look at tissue microarray\n(TMA) images manually to identify tumors, which can be time-consuming,\ninconsistent and subjective. Existing algorithms that automatically detect\ntumors have either not achieved the accuracy level of a pathologist or require\nsubstantial human involvements. A major challenge is that TMA images with\ndifferent shapes, sizes, and locations can have the same score. Learning\nstaining patterns in TMA images requires a huge number of images, which are\nseverely limited due to privacy concerns and regulations in medical\norganizations. TMA images from different cancer types may have common\ncharacteristics that could provide valuable information, but using them\ndirectly harms the accuracy. By selective transfer learning from multiple small\nauxiliary sets, the proposed algorithm is able to extract knowledge from tissue\nimages showing a ``similar\" scoring pattern but with different cancer types.\nRemarkably, transfer learning has made it possible for the algorithm to break\nthe critical accuracy barrier -- the proposed algorithm reports an accuracy of\n75.9% on breast cancer TMA images from the Stanford Tissue Microarray Database,\nachieving the 75\\% accuracy level of pathologists. This will allow pathologists\nto confidently use automatic algorithms to assist them in recognizing tumors\nconsistently with a higher accuracy in real time.\n","authors":["Iris Yan"],"pdf_url":"https://arxiv.org/pdf/2209.05954v2.pdf","comment":"19 pages, 6 figures"},{"id":"http://arxiv.org/abs/2103.07016v2","updated":"2023-03-27T21:15:57Z","published":"2021-03-12T00:31:33Z","title":"On the Equivalence Between Temporal and Static Graph Representations for\n  Observational Predictions","summary":"  This work formalizes the associational task of predicting node attribute\nevolution in temporal graphs from the perspective of learning equivariant\nrepresentations. We show that node representations in temporal graphs can be\ncast into two distinct frameworks: (a) The most popular approach, which we\ndenote as time-and-graph, where equivariant graph (e.g., GNN) and sequence\n(e.g., RNN) representations are intertwined to represent the temporal evolution\nof node attributes in the graph; and (b) an approach that we denote as\ntime-then-graph, where the sequences describing the node and edge dynamics are\nrepresented first, then fed as node and edge attributes into a static\nequivariant graph representation that comes after. Interestingly, we show that\ntime-then-graph representations have an expressivity advantage over\ntime-and-graph representations when both use component GNNs that are not\nmost-expressive (e.g., 1-Weisfeiler-Lehman GNNs). Moreover, while our goal is\nnot necessarily to obtain state-of-the-art results, our experiments show that\ntime-then-graph methods are capable of achieving better performance and\nefficiency than state-of-the-art time-and-graph methods in some real-world\ntasks, thereby showcasing that the time-then-graph framework is a worthy\naddition to the graph ML toolbox.\n","authors":["Jianfei Gao","Bruno Ribeiro"],"pdf_url":"https://arxiv.org/pdf/2103.07016v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2205.05749v2","updated":"2023-03-27T21:07:47Z","published":"2022-05-11T19:40:24Z","title":"Are Metrics Enough? Guidelines for Communicating and Visualizing\n  Predictive Models to Subject Matter Experts","summary":"  Presenting a predictive model's performance is a communication bottleneck\nthat threatens collaborations between data scientists and subject matter\nexperts. Accuracy and error metrics alone fail to tell the whole story of a\nmodel - its risks, strengths, and limitations - making it difficult for subject\nmatter experts to feel confident in their decision to use a model. As a result,\nmodels may fail in unexpected ways or go entirely unused, as subject matter\nexperts disregard poorly presented models in favor of familiar, yet arguably\nsubstandard methods. In this paper, we describe an iterative study conducted\nwith both subject matter experts and data scientists to understand the gaps in\ncommunication between these two groups. We find that, while the two groups\nshare common goals of understanding the data and predictions of the model,\nfriction can stem from unfamiliar terms, metrics, and visualizations - limiting\nthe transfer of knowledge to SMEs and discouraging clarifying questions being\nasked during presentations. Based on our findings, we derive a set of\ncommunication guidelines that use visualization as a common medium for\ncommunicating the strengths and weaknesses of a model. We provide a\ndemonstration of our guidelines in a regression modeling scenario and elicit\nfeedback on their use from subject matter experts. From our demonstration,\nsubject matter experts were more comfortable discussing a model's performance,\nmore aware of the trade-offs for the presented model, and better equipped to\nassess the model's risks - ultimately informing and contextualizing the model's\nuse beyond text and numbers.\n","authors":["Ashley Suh","Gabriel Appleby","Erik W. Anderson","Luca Finelli","Remco Chang","Dylan Cashman"],"pdf_url":"https://arxiv.org/pdf/2205.05749v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2302.09235v2","updated":"2023-03-27T21:00:54Z","published":"2023-02-18T05:06:15Z","title":"Generalization and Stability of Interpolating Neural Networks with\n  Minimal Width","summary":"  We investigate the generalization and optimization properties of shallow\nneural-network classifiers trained by gradient descent in the interpolating\nregime. Specifically, in a realizable scenario where model weights can achieve\narbitrarily small training error $\\epsilon$ and their distance from\ninitialization is $g(\\epsilon)$, we demonstrate that gradient descent with $n$\ntraining data achieves training error $O(g(1/T)^2 /T)$ and generalization error\n$O(g(1/T)^2 /n)$ at iteration $T$, provided there are at least\n$m=\\Omega(g(1/T)^4)$ hidden neurons. We then show that our realizable setting\nencompasses a special case where data are separable by the model's neural\ntangent kernel. For this and logistic-loss minimization, we prove the training\nloss decays at a rate of $\\tilde O(1/ T)$ given polylogarithmic number of\nneurons $m=\\Omega(\\log^4 (T))$. Moreover, with $m=\\Omega(\\log^{4} (n))$ neurons\nand $T\\approx n$ iterations, we bound the test loss by $\\tilde{O}(1/n)$. Our\nresults differ from existing generalization outcomes using the\nalgorithmic-stability framework, which necessitate polynomial width and yield\nsuboptimal generalization rates. Central to our analysis is the use of a new\nself-bounded weak-convexity property, which leads to a generalized local\nquasi-convexity property for sufficiently parameterized neural-network\nclassifiers. Eventually, despite the objective's non-convexity, this leads to\nconvergence and generalization-gap bounds that resemble those found in the\nconvex setting of linear logistic regression.\n","authors":["Hossein Taheri","Christos Thrampoulidis"],"pdf_url":"https://arxiv.org/pdf/2302.09235v2.pdf","comment":"With significant changes: Stating results without homogeneity\n  assumption, Discussing results under NTK-separability in Section 4"},{"id":"http://arxiv.org/abs/2303.15592v1","updated":"2023-03-27T20:49:42Z","published":"2023-03-27T20:49:42Z","title":"Uncovering Bias in Personal Informatics","summary":"  Personal informatics (PI) systems, powered by smartphones and wearables,\nenable people to lead healthier lifestyles by providing meaningful and\nactionable insights that break down barriers between users and their health\ninformation. Today, such systems are used by billions of users for monitoring\nnot only physical activity and sleep but also vital signs and women's and heart\nhealth, among others. %Despite their widespread usage, the processing of\nparticularly sensitive personal data, and their proximity to domains known to\nbe susceptible to bias, such as healthcare, bias in PI has not been\ninvestigated systematically. Despite their widespread usage, the processing of\nsensitive PI data may suffer from biases, which may entail practical and\nethical implications. In this work, we present the first comprehensive\nempirical and analytical study of bias in PI systems, including biases in raw\ndata and in the entire machine learning life cycle. We use the most detailed\nframework to date for exploring the different sources of bias and find that\nbiases exist both in the data generation and the model learning and\nimplementation streams. According to our results, the most affected minority\ngroups are users with health issues, such as diabetes, joint issues, and\nhypertension, and female users, whose data biases are propagated or even\namplified by learning models, while intersectional biases can also be observed.\n","authors":["Sofia Yfantidou","Pavlos Sermpezis","Athena Vakali","Ricardo Baeza-Yates"],"pdf_url":"https://arxiv.org/pdf/2303.15592v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.15585v1","updated":"2023-03-27T20:28:26Z","published":"2023-03-27T20:28:26Z","title":"Beyond Accuracy: A Critical Review of Fairness in Machine Learning for\n  Mobile and Wearable Computing","summary":"  The field of mobile, wearable, and ubiquitous computing (UbiComp) is\nundergoing a revolutionary integration of machine learning. Devices can now\ndiagnose diseases, predict heart irregularities, and unlock the full potential\nof human cognition. However, the underlying algorithms are not immune to biases\nwith respect to sensitive attributes (e.g., gender, race), leading to\ndiscriminatory outcomes. The research communities of HCI and AI-Ethics have\nrecently started to explore ways of reporting information about datasets to\nsurface and, eventually, counter those biases. The goal of this work is to\nexplore the extent to which the UbiComp community has adopted such ways of\nreporting and highlight potential shortcomings. Through a systematic review of\npapers published in the Proceedings of the ACM Interactive, Mobile, Wearable\nand Ubiquitous Technologies (IMWUT) journal over the past 5 years (2018-2022),\nwe found that progress on algorithmic fairness within the UbiComp community\nlags behind. Our findings show that only a small portion (5%) of published\npapers adheres to modern fairness reporting, while the overwhelming majority\nthereof focuses on accuracy or error metrics. In light of these findings, our\nwork provides practical guidelines for the design and development of ubiquitous\ntechnologies that not only strive for accuracy but also for fairness.\n","authors":["Sofia Yfantidou","Marios Constantinides","Dimitris Spathis","Athena Vakali","Daniele Quercia","Fahim Kawsar"],"pdf_url":"https://arxiv.org/pdf/2303.15585v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.12748v2","updated":"2023-03-27T20:25:03Z","published":"2023-03-11T17:14:04Z","title":"Enabling Calibration In The Zero-Shot Inference of Large Vision-Language\n  Models","summary":"  Calibration of deep learning models is crucial to their trustworthiness and\nsafe usage, and as such, has been extensively studied in supervised\nclassification models, with methods crafted to decrease miscalibration.\nHowever, there has yet to be a comprehensive study of the calibration of\nvision-language models that are used for zero-shot inference, like CLIP. We\nmeasure calibration across relevant variables like prompt, dataset, and\narchitecture, and find that zero-shot inference with CLIP is miscalibrated.\nFurthermore, we propose a modified version of temperature scaling that is\naligned with the common use cases of CLIP as a zero-shot inference model, and\nshow that a single learned temperature generalizes for each specific CLIP model\n(defined by a chosen pre-training dataset and architecture) across inference\ndataset and prompt choice.\n","authors":["Will LeVine","Benjamin Pikus","Pranav Raj","Fernando Amat Gil"],"pdf_url":"https://arxiv.org/pdf/2303.12748v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.15579v1","updated":"2023-03-27T20:16:11Z","published":"2023-03-27T20:16:11Z","title":"Adjusted Wasserstein Distributionally Robust Estimator in Statistical\n  Learning","summary":"  We propose an adjusted Wasserstein distributionally robust estimator -- based\non a nonlinear transformation of the Wasserstein distributionally robust (WDRO)\nestimator in statistical learning. This transformation will improve the\nstatistical performance of WDRO because the adjusted WDRO estimator is\nasymptotically unbiased and has an asymptotically smaller mean squared error.\nThe adjusted WDRO will not mitigate the out-of-sample performance guarantee of\nWDRO. Sufficient conditions for the existence of the adjusted WDRO estimator\nare presented, and the procedure for the computation of the adjusted WDRO\nestimator is given. Specifically, we will show how the adjusted WDRO estimator\nis developed in the generalized linear model. Numerical experiments demonstrate\nthe favorable practical performance of the adjusted estimator over the classic\none.\n","authors":["Yiling Xie","Xiaoming Huo"],"pdf_url":"https://arxiv.org/pdf/2303.15579v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.15570v1","updated":"2023-03-27T19:37:53Z","published":"2023-03-27T19:37:53Z","title":"Online Non-Destructive Moisture Content Estimation of Filter Media\n  During Drying Using Artificial Neural Networks","summary":"  Moisture content (MC) estimation is important in the manufacturing process of\ndrying bulky filter media products as it is the prerequisite for drying\noptimization. In this study, a dataset collected by performing 161 drying\nindustrial experiments is described and a methodology for MC estimation in an\nnon-destructive and online manner during industrial drying is presented. An\nartificial neural network (ANN) based method is compared to state-of-the-art MC\nestimation methods reported in the literature. Results of model fitting and\ntraining show that a three-layer Perceptron achieves the lowest error.\nExperimental results show that ANNs combined with oven settings data, drying\ntime and product temperature can be used to reliably estimate the MC of bulky\nfilter media products.\n","authors":["Christian Remi Wewer","Alexandros Iosifidis"],"pdf_url":"https://arxiv.org/pdf/2303.15570v1.pdf","comment":"11 pages, 9 figures"},{"id":"http://arxiv.org/abs/2303.15569v1","updated":"2023-03-27T19:36:41Z","published":"2023-03-27T19:36:41Z","title":"Core-Periphery Principle Guided Redesign of Self-Attention in\n  Transformers","summary":"  Designing more efficient, reliable, and explainable neural network\narchitectures is critical to studies that are based on artificial intelligence\n(AI) techniques. Previous studies, by post-hoc analysis, have found that the\nbest-performing ANNs surprisingly resemble biological neural networks (BNN),\nwhich indicates that ANNs and BNNs may share some common principles to achieve\noptimal performance in either machine learning or cognitive/behavior tasks.\nInspired by this phenomenon, we proactively instill organizational principles\nof BNNs to guide the redesign of ANNs. We leverage the Core-Periphery (CP)\norganization, which is widely found in human brain networks, to guide the\ninformation communication mechanism in the self-attention of vision transformer\n(ViT) and name this novel framework as CP-ViT. In CP-ViT, the attention\noperation between nodes is defined by a sparse graph with a Core-Periphery\nstructure (CP graph), where the core nodes are redesigned and reorganized to\nplay an integrative role and serve as a center for other periphery nodes to\nexchange information. We evaluated the proposed CP-ViT on multiple public\ndatasets, including medical image datasets (INbreast) and natural image\ndatasets. Interestingly, by incorporating the BNN-derived principle (CP\nstructure) into the redesign of ViT, our CP-ViT outperforms other\nstate-of-the-art ANNs. In general, our work advances the state of the art in\nthree aspects: 1) This work provides novel insights for brain-inspired AI: we\ncan utilize the principles found in BNNs to guide and improve our ANN\narchitecture design; 2) We show that there exist sweet spots of CP graphs that\nlead to CP-ViTs with significantly improved performance; and 3) The core nodes\nin CP-ViT correspond to task-related meaningful and important image patches,\nwhich can significantly enhance the interpretability of the trained deep model.\n","authors":["Xiaowei Yu","Lu Zhang","Haixing Dai","Yanjun Lyu","Lin Zhao","Zihao Wu","David Liu","Tianming Liu","Dajiang Zhu"],"pdf_url":"https://arxiv.org/pdf/2303.15569v1.pdf","comment":"Core-periphery, functional brain networks, ViT"},{"id":"http://arxiv.org/abs/2303.13489v2","updated":"2023-03-27T19:25:01Z","published":"2023-03-23T17:53:44Z","title":"Boosting Reinforcement Learning and Planning with Demonstrations: A\n  Survey","summary":"  Although reinforcement learning has seen tremendous success recently, this\nkind of trial-and-error learning can be impractical or inefficient in complex\nenvironments. The use of demonstrations, on the other hand, enables agents to\nbenefit from expert knowledge rather than having to discover the best action to\ntake through exploration. In this survey, we discuss the advantages of using\ndemonstrations in sequential decision making, various ways to apply\ndemonstrations in learning-based decision making paradigms (for example,\nreinforcement learning and planning in the learned models), and how to collect\nthe demonstrations in various scenarios. Additionally, we exemplify a practical\npipeline for generating and utilizing demonstrations in the recently proposed\nManiSkill robot learning benchmark.\n","authors":["Tongzhou Mu","Hao Su"],"pdf_url":"https://arxiv.org/pdf/2303.13489v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.15564v1","updated":"2023-03-27T19:23:33Z","published":"2023-03-27T19:23:33Z","title":"Mask and Restore: Blind Backdoor Defense at Test Time with Masked\n  Autoencoder","summary":"  Deep neural networks are vulnerable to backdoor attacks, where an adversary\nmaliciously manipulates the model behavior through overlaying images with\nspecial triggers. Existing backdoor defense methods often require accessing a\nfew validation data and model parameters, which are impractical in many\nreal-world applications, e.g., when the model is provided as a cloud service.\nIn this paper, we address the practical task of blind backdoor defense at test\ntime, in particular for black-box models. The true label of every test image\nneeds to be recovered on the fly from the hard label predictions of a\nsuspicious model. The heuristic trigger search in image space, however, is not\nscalable to complex triggers or high image resolution. We circumvent such\nbarrier by leveraging generic image generation models, and propose a framework\nof Blind Defense with Masked AutoEncoder (BDMAE). It uses the image structural\nsimilarity and label consistency between the test image and MAE restorations to\ndetect possible triggers. The detection result is refined by considering the\ntopology of triggers. We obtain a purified test image from restorations for\nmaking prediction. Our approach is blind to the model architectures, trigger\npatterns or image benignity. Extensive experiments on multiple datasets with\ndifferent backdoor attacks validate its effectiveness and generalizability.\nCode is available at https://github.com/tsun/BDMAE.\n","authors":["Tao Sun","Lu Pang","Chao Chen","Haibin Ling"],"pdf_url":"https://arxiv.org/pdf/2303.15564v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.15563v1","updated":"2023-03-27T19:20:51Z","published":"2023-03-27T19:20:51Z","title":"Privacy-preserving machine learning for healthcare: open challenges and\n  future perspectives","summary":"  Machine Learning (ML) has recently shown tremendous success in modeling\nvarious healthcare prediction tasks, ranging from disease diagnosis and\nprognosis to patient treatment. Due to the sensitive nature of medical data,\nprivacy must be considered along the entire ML pipeline, from model training to\ninference. In this paper, we conduct a review of recent literature concerning\nPrivacy-Preserving Machine Learning (PPML) for healthcare. We primarily focus\non privacy-preserving training and inference-as-a-service, and perform a\ncomprehensive review of existing trends, identify challenges, and discuss\nopportunities for future research directions. The aim of this review is to\nguide the development of private and efficient ML models in healthcare, with\nthe prospects of translating research efforts into real-world settings.\n","authors":["Alejandro Guerra-Manzanares","L. Julian Lechuga Lopez","Michail Maniatakos","Farah E. Shamout"],"pdf_url":"https://arxiv.org/pdf/2303.15563v1.pdf","comment":"ICLR 2023 Workshop on Trustworthy Machine Learning for Healthcare\n  (TML4H)"},{"id":"http://arxiv.org/abs/2303.15544v1","updated":"2023-03-27T18:49:47Z","published":"2023-03-27T18:49:47Z","title":"Multi-Flow Transmission in Wireless Interference Networks: A Convergent\n  Graph Learning Approach","summary":"  We consider the problem of of multi-flow transmission in wireless networks,\nwhere data signals from different flows can interfere with each other due to\nmutual interference between links along their routes, resulting in reduced link\ncapacities. The objective is to develop a multi-flow transmission strategy that\nroutes flows across the wireless interference network to maximize the network\nutility. However, obtaining an optimal solution is computationally expensive\ndue to the large state and action spaces involved. To tackle this challenge, we\nintroduce a novel algorithm called Dual-stage Interference-Aware Multi-flow\nOptimization of Network Data-signals (DIAMOND). The design of DIAMOND allows\nfor a hybrid centralized-distributed implementation, which is a characteristic\nof 5G and beyond technologies with centralized unit deployments. A centralized\nstage computes the multi-flow transmission strategy using a novel design of\ngraph neural network (GNN) reinforcement learning (RL) routing agent. Then, a\ndistributed stage improves the performance based on a novel design of\ndistributed learning updates. We provide a theoretical analysis of DIAMOND and\nprove that it converges to the optimal multi-flow transmission strategy as time\nincreases. We also present extensive simulation results over various network\ntopologies (random deployment, NSFNET, GEANT2), demonstrating the superior\nperformance of DIAMOND compared to existing methods.\n","authors":["Raz Paul","Kobi Cohen","Gil Kedar"],"pdf_url":"https://arxiv.org/pdf/2303.15544v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2002.11436v2","updated":"2023-03-27T18:48:56Z","published":"2020-02-26T12:37:11Z","title":"Nonlinear classifiers for ranking problems based on kernelized SVM","summary":"  Many classification problems focus on maximizing the performance only on the\nsamples with the highest relevance instead of all samples. As an example, we\ncan mention ranking problems, accuracy at the top or search engines where only\nthe top few queries matter. In our previous work, we derived a general\nframework including several classes of these linear classification problems. In\nthis paper, we extend the framework to nonlinear classifiers. Utilizing a\nsimilarity to SVM, we dualize the problems, add kernels and propose a\ncomponentwise dual ascent method.\n","authors":["Václav Mácha","Lukáš Adam","Václav Šmídl"],"pdf_url":"https://arxiv.org/pdf/2002.11436v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.15533v1","updated":"2023-03-27T18:18:15Z","published":"2023-03-27T18:18:15Z","title":"Sequential training of GANs against GAN-classifiers reveals correlated\n  \"knowledge gaps\" present among independently trained GAN instances","summary":"  Modern Generative Adversarial Networks (GANs) generate realistic images\nremarkably well. Previous work has demonstrated the feasibility of\n\"GAN-classifiers\" that are distinct from the co-trained discriminator, and\noperate on images generated from a frozen GAN. That such classifiers work at\nall affirms the existence of \"knowledge gaps\" (out-of-distribution artifacts\nacross samples) present in GAN training. We iteratively train GAN-classifiers\nand train GANs that \"fool\" the classifiers (in an attempt to fill the knowledge\ngaps), and examine the effect on GAN training dynamics, output quality, and\nGAN-classifier generalization. We investigate two settings, a small DCGAN\narchitecture trained on low dimensional images (MNIST), and StyleGAN2, a SOTA\nGAN architecture trained on high dimensional images (FFHQ). We find that the\nDCGAN is unable to effectively fool a held-out GAN-classifier without\ncompromising the output quality. However, StyleGAN2 can fool held-out\nclassifiers with no change in output quality, and this effect persists over\nmultiple rounds of GAN/classifier training which appears to reveal an ordering\nover optima in the generator parameter space. Finally, we study different\nclassifier architectures and show that the architecture of the GAN-classifier\nhas a strong influence on the set of its learned artifacts.\n","authors":["Arkanath Pathak","Nicholas Dufour"],"pdf_url":"https://arxiv.org/pdf/2303.15533v1.pdf","comment":null}],"Multimedia":[{"id":"http://arxiv.org/abs/2204.07841v3","updated":"2023-03-27T15:40:57Z","published":"2022-04-16T16:45:06Z","title":"Multi-Modal Few-Shot Object Detection with Meta-Learning-Based\n  Cross-Modal Prompting","summary":"  We study multi-modal few-shot object detection (FSOD) in this paper, using\nboth few-shot visual examples and class semantic information for detection,\nwhich are complementary to each other by definition. Most of the previous works\non multi-modal FSOD are fine-tuning-based which are inefficient for online\napplications. Moreover, these methods usually require expertise like class\nnames to extract class semantic embedding, which are hard to get for rare\nclasses. Our approach is motivated by the high-level conceptual similarity of\n(metric-based) meta-learning and prompt-based learning to learn generalizable\nfew-shot and zero-shot object detection models respectively without\nfine-tuning. Specifically, we combine the few-shot visual classifier and text\nclassifier learned via meta-learning and prompt-based learning respectively to\nbuild the multi-modal classifier and detection models. In addition, to fully\nexploit the pre-trained language models, we propose meta-learning-based\ncross-modal prompting to generate soft prompts for novel classes present in\nfew-shot visual examples, which are then used to learn the text classifier.\nKnowledge distillation is introduced to learn the soft prompt generator without\nusing human prior knowledge of class names, which may not be available for rare\nclasses. Our insight is that the few-shot support images naturally include\nrelated context information and semantics of the class. We comprehensively\nevaluate the proposed multi-modal FSOD models on multiple few-shot object\ndetection benchmarks, achieving promising results.\n","authors":["Guangxing Han","Long Chen","Jiawei Ma","Shiyuan Huang","Rama Chellappa","Shih-Fu Chang"],"pdf_url":"https://arxiv.org/pdf/2204.07841v3.pdf","comment":"17 pages"},{"id":"http://arxiv.org/abs/2303.15016v1","updated":"2023-03-27T08:59:55Z","published":"2023-03-27T08:59:55Z","title":"Borrowing Human Senses: Comment-Aware Self-Training for Social Media\n  Multimodal Classification","summary":"  Social media is daily creating massive multimedia content with paired image\nand text, presenting the pressing need to automate the vision and language\nunderstanding for various multimodal classification tasks. Compared to the\ncommonly researched visual-lingual data, social media posts tend to exhibit\nmore implicit image-text relations. To better glue the cross-modal semantics\ntherein, we capture hinting features from user comments, which are retrieved\nvia jointly leveraging visual and lingual similarity. Afterwards, the\nclassification tasks are explored via self-training in a teacher-student\nframework, motivated by the usually limited labeled data scales in existing\nbenchmarks. Substantial experiments are conducted on four multimodal social\nmedia benchmarks for image text relation classification, sarcasm detection,\nsentiment classification, and hate speech detection. The results show that our\nmethod further advances the performance of previous state-of-the-art models,\nwhich do not employ comment modeling or self-training.\n","authors":["Chunpu Xu","Jing Li"],"pdf_url":"https://arxiv.org/pdf/2303.15016v1.pdf","comment":"accepted to EMNLP 2022"},{"id":"http://arxiv.org/abs/2211.14905v2","updated":"2023-03-27T08:39:13Z","published":"2022-11-27T18:13:05Z","title":"Multi-Modal Few-Shot Temporal Action Detection","summary":"  Few-shot (FS) and zero-shot (ZS) learning are two different approaches for\nscaling temporal action detection (TAD) to new classes. The former adapts a\npretrained vision model to a new task represented by as few as a single video\nper class, whilst the latter requires no training examples by exploiting a\nsemantic description of the new class. In this work, we introduce a new\nmulti-modality few-shot (MMFS) TAD problem, which can be considered as a\nmarriage of FS-TAD and ZS-TAD by leveraging few-shot support videos and new\nclass names jointly. To tackle this problem, we further introduce a novel\nMUlti-modality PromPt mETa-learning (MUPPET) method. This is enabled by\nefficiently bridging pretrained vision and language models whilst maximally\nreusing already learned capacity. Concretely, we construct multi-modal prompts\nby mapping support videos into the textual token space of a vision-language\nmodel using a meta-learned adapter-equipped visual semantics tokenizer. To\ntackle large intra-class variation, we further design a query feature\nregulation scheme. Extensive experiments on ActivityNetv1.3 and THUMOS14\ndemonstrate that our MUPPET outperforms state-of-the-art alternative methods,\noften by a large margin. We also show that our MUPPET can be easily extended to\ntackle the few-shot object detection problem and again achieves the\nstate-of-the-art performance on MS-COCO dataset. The code will be available in\nhttps://github.com/sauradip/MUPPET\n","authors":["Sauradip Nag","Mengmeng Xu","Xiatian Zhu","Juan-Manuel Perez-Rua","Bernard Ghanem","Yi-Zhe Song","Tao Xiang"],"pdf_url":"https://arxiv.org/pdf/2211.14905v2.pdf","comment":"Technical Report"},{"id":"http://arxiv.org/abs/2210.15518v3","updated":"2023-03-27T02:08:57Z","published":"2022-10-27T14:57:14Z","title":"LongShortNet: Exploring Temporal and Semantic Features Fusion in\n  Streaming Perception","summary":"  Streaming perception is a fundamental task in autonomous driving that\nrequires a careful balance between the latency and accuracy of the autopilot\nsystem. However, current methods for streaming perception are limited as they\nrely only on the current and adjacent two frames to learn movement patterns,\nwhich restricts their ability to model complex scenes, often leading to poor\ndetection results. To address this limitation, we propose LongShortNet, a novel\ndual-path network that captures long-term temporal motion and integrates it\nwith short-term spatial semantics for real-time perception. Our proposed\nLongShortNet is notable as it is the first work to extend long-term temporal\nmodeling to streaming perception, enabling spatiotemporal feature fusion. We\nevaluate LongShortNet on the challenging Argoverse-HD dataset and demonstrate\nthat it outperforms existing state-of-the-art methods with almost no additional\ncomputational cost.\n","authors":["Chenyang Li","Zhi-Qi Cheng","Jun-Yan He","Pengyu Li","Bin Luo","Han-Yuan Chen","Yifeng Geng","Jin-Peng Lan","Xuansong Xie"],"pdf_url":"https://arxiv.org/pdf/2210.15518v3.pdf","comment":"Accepted at ICASSP 2023, source code is at\n  https://github.com/zhiqic/LongShortNet"},{"id":"http://arxiv.org/abs/2210.15511v3","updated":"2023-03-27T02:02:43Z","published":"2022-10-27T14:47:19Z","title":"ProContEXT: Exploring Progressive Context Transformer for Tracking","summary":"  Existing Visual Object Tracking (VOT) only takes the target area in the first\nframe as a template. This causes tracking to inevitably fail in fast-changing\nand crowded scenes, as it cannot account for changes in object appearance\nbetween frames. To this end, we revamped the tracking framework with\nProgressive Context Encoding Transformer Tracker (ProContEXT), which coherently\nexploits spatial and temporal contexts to predict object motion trajectories.\nSpecifically, ProContEXT leverages a context-aware self-attention module to\nencode the spatial and temporal context, refining and updating the multi-scale\nstatic and dynamic templates to progressively perform accurate tracking. It\nexplores the complementary between spatial and temporal context, raising a new\npathway to multi-context modeling for transformer-based trackers. In addition,\nProContEXT revised the token pruning technique to reduce computational\ncomplexity. Extensive experiments on popular benchmark datasets such as GOT-10k\nand TrackingNet demonstrate that the proposed ProContEXT achieves\nstate-of-the-art performance.\n","authors":["Jin-Peng Lan","Zhi-Qi Cheng","Jun-Yan He","Chenyang Li","Bin Luo","Xu Bao","Wangmeng Xiang","Yifeng Geng","Xuansong Xie"],"pdf_url":"https://arxiv.org/pdf/2210.15511v3.pdf","comment":"Accepted at ICASSP 2023, source code is at\n  https://github.com/zhiqic/ProContEXT"},{"id":"http://arxiv.org/abs/2303.12337v2","updated":"2023-03-27T01:59:41Z","published":"2023-03-22T06:26:56Z","title":"Music-Driven Group Choreography","summary":"  Music-driven choreography is a challenging problem with a wide variety of\nindustrial applications. Recently, many methods have been proposed to\nsynthesize dance motions from music for a single dancer. However, generating\ndance motion for a group remains an open problem. In this paper, we present\n$\\rm AIOZ-GDANCE$, a new large-scale dataset for music-driven group dance\ngeneration. Unlike existing datasets that only support single dance, our new\ndataset contains group dance videos, hence supporting the study of group\nchoreography. We propose a semi-autonomous labeling method with humans in the\nloop to obtain the 3D ground truth for our dataset. The proposed dataset\nconsists of 16.7 hours of paired music and 3D motion from in-the-wild videos,\ncovering 7 dance styles and 16 music genres. We show that naively applying\nsingle dance generation technique to creating group dance motion may lead to\nunsatisfactory results, such as inconsistent movements and collisions between\ndancers. Based on our new dataset, we propose a new method that takes an input\nmusic sequence and a set of 3D positions of dancers to efficiently produce\nmultiple group-coherent choreographies. We propose new evaluation metrics for\nmeasuring group dance quality and perform intensive experiments to demonstrate\nthe effectiveness of our method. Our project facilitates future research on\ngroup dance generation and is available at:\nhttps://aioz-ai.github.io/AIOZ-GDANCE/\n","authors":["Nhat Le","Thang Pham","Tuong Do","Erman Tjiputra","Quang D. Tran","Anh Nguyen"],"pdf_url":"https://arxiv.org/pdf/2303.12337v2.pdf","comment":"accepted in CVPR 2023"},{"id":"http://arxiv.org/abs/2303.14863v1","updated":"2023-03-27T00:40:52Z","published":"2023-03-27T00:40:52Z","title":"DiffTAD: Temporal Action Detection with Proposal Denoising Diffusion","summary":"  We propose a new formulation of temporal action detection (TAD) with\ndenoising diffusion, DiffTAD in short. Taking as input random temporal\nproposals, it can yield action proposals accurately given an untrimmed long\nvideo. This presents a generative modeling perspective, against previous\ndiscriminative learning manners. This capability is achieved by first diffusing\nthe ground-truth proposals to random ones (i.e., the forward/noising process)\nand then learning to reverse the noising process (i.e., the backward/denoising\nprocess). Concretely, we establish the denoising process in the Transformer\ndecoder (e.g., DETR) by introducing a temporal location query design with\nfaster convergence in training. We further propose a cross-step selective\nconditioning algorithm for inference acceleration. Extensive evaluations on\nActivityNet and THUMOS show that our DiffTAD achieves top performance compared\nto previous art alternatives. The code will be made available at\nhttps://github.com/sauradip/DiffusionTAD.\n","authors":["Sauradip Nag","Xiatian Zhu","Jiankang Deng","Yi-Zhe Song","Tao Xiang"],"pdf_url":"https://arxiv.org/pdf/2303.14863v1.pdf","comment":"Technical Report"}]},"2023-03-26T00:00:00Z":{"Computation and Language":[{"id":"http://arxiv.org/abs/2303.14814v1","updated":"2023-03-26T20:41:21Z","published":"2023-03-26T20:41:21Z","title":"WinCLIP: Zero-/Few-Shot Anomaly Classification and Segmentation","summary":"  Visual anomaly classification and segmentation are vital for automating\nindustrial quality inspection. The focus of prior research in the field has\nbeen on training custom models for each quality inspection task, which requires\ntask-specific images and annotation. In this paper we move away from this\nregime, addressing zero-shot and few-normal-shot anomaly classification and\nsegmentation. Recently CLIP, a vision-language model, has shown revolutionary\ngenerality with competitive zero-/few-shot performance in comparison to\nfull-supervision. But CLIP falls short on anomaly classification and\nsegmentation tasks. Hence, we propose window-based CLIP (WinCLIP) with (1) a\ncompositional ensemble on state words and prompt templates and (2) efficient\nextraction and aggregation of window/patch/image-level features aligned with\ntext. We also propose its few-normal-shot extension WinCLIP+, which uses\ncomplementary information from normal images. In MVTec-AD (and VisA), without\nfurther tuning, WinCLIP achieves 91.8%/85.1% (78.1%/79.6%) AUROC in zero-shot\nanomaly classification and segmentation while WinCLIP+ does 93.1%/95.2%\n(83.8%/96.4%) in 1-normal-shot, surpassing state-of-the-art by large margins.\n","authors":["Jongheon Jeong","Yang Zou","Taewan Kim","Dongqing Zhang","Avinash Ravichandran","Onkar Dabeer"],"pdf_url":"https://arxiv.org/pdf/2303.14814v1.pdf","comment":"Accepted to Conference on Computer Vision and Pattern Recognition\n  (CVPR) 2023"},{"id":"http://arxiv.org/abs/2303.02841v2","updated":"2023-03-26T19:45:25Z","published":"2023-03-06T02:24:48Z","title":"Model-Agnostic Meta-Learning for Natural Language Understanding Tasks in\n  Finance","summary":"  Natural language understanding(NLU) is challenging for finance due to the\nlack of annotated data and the specialized language in that domain. As a\nresult, researchers have proposed to use pre-trained language model and\nmulti-task learning to learn robust representations. However, aggressive\nfine-tuning often causes over-fitting and multi-task learning may favor tasks\nwith significantly larger amounts data, etc. To address these problems, in this\npaper, we investigate model-agnostic meta-learning algorithm(MAML) in\nlow-resource financial NLU tasks. Our contribution includes: 1. we explore the\nperformance of MAML method with multiple types of tasks: GLUE datasets, SNLI,\nSci-Tail and Financial PhraseBank; 2. we study the performance of MAML method\nwith multiple single-type tasks: a real scenario stock price prediction problem\nwith twitter text data. Our models achieve the state-of-the-art performance\naccording to the experimental results, which demonstrate that our method can\nadapt fast and well to low-resource situations.\n","authors":["Bixing Yan","Shaoling Chen","Yuxuan He","Zhihan Li"],"pdf_url":"https://arxiv.org/pdf/2303.02841v2.pdf","comment":"13 pages, 6 figures, 8 tables"},{"id":"http://arxiv.org/abs/2303.14770v1","updated":"2023-03-26T16:29:18Z","published":"2023-03-26T16:29:18Z","title":"Koala: An Index for Quantifying Overlaps with Pre-training Corpora","summary":"  In very recent years more attention has been placed on probing the role of\npre-training data in Large Language Models (LLMs) downstream behaviour. Despite\nthe importance, there is no public tool that supports such analysis of\npre-training corpora at large scale. To help research in this space, we launch\nKoala, a searchable index over large pre-training corpora using compressed\nsuffix arrays with highly efficient compression rate and search support. In its\nfirst release we index the public proportion of OPT 175B pre-training data.\nKoala provides a framework to do forensic analysis on the current and future\nbenchmarks as well as to assess the degree of memorization in the output from\nthe LLMs. Koala is available for public use at\nhttps://koala-index.erc.monash.edu/.\n","authors":["Thuy-Trang Vu","Xuanli He","Gholamreza Haffari","Ehsan Shareghi"],"pdf_url":"https://arxiv.org/pdf/2303.14770v1.pdf","comment":"Available here: https://koala-index.erc.monash.edu/"},{"id":"http://arxiv.org/abs/2303.14742v1","updated":"2023-03-26T14:49:37Z","published":"2023-03-26T14:49:37Z","title":"Exploring the Impact of Instruction Data Scaling on Large Language\n  Models: An Empirical Study on Real-World Use Cases","summary":"  The success of ChatGPT has recently attracted numerous efforts to replicate\nit, with instruction-tuning strategies being a key factor in achieving\nremarkable results. Instruction-tuning not only significantly enhances the\nmodel's performance and generalization but also makes the model's generated\nresults more consistent with human speech patterns. However current research\nrarely studies the impact of different amounts of instruction data on model\nperformance, especially in the real-world use cases. In this paper we explore\nthe performance of large language models based on instruction tuning across\ndifferent scales of instruction data. An evaluation dataset consisting of 12\nmajor online use cases is constructed in the experiment. With Bloomz-7B1-mt as\nthe base model, the results show that 1) merely increasing the amount of\ninstruction data leads to continuous improvement in tasks such as open-ended\ngeneration, 2) in tasks such as math and code, the model performance curve\nremains quite flat while increasing data size. We further analyze the possible\ncauses of these phenomena and propose potential future research directions such\nas effectively selecting high-quality training data, scaling base models and\ntraining methods specialized for hard tasks. We will release our training and\nevaluation datasets, as well as model checkpoints.\n","authors":["Yunjie Ji","Yong Deng","Yan Gong","Yiping Peng","Qiang Niu","Lei Zhang","Baochang Ma","Xiangang Li"],"pdf_url":"https://arxiv.org/pdf/2303.14742v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2211.13437v2","updated":"2023-03-26T13:59:36Z","published":"2022-11-24T06:39:16Z","title":"Seeing What You Miss: Vision-Language Pre-training with Semantic\n  Completion Learning","summary":"  Cross-modal alignment is essential for vision-language pre-training (VLP)\nmodels to learn the correct corresponding information across different\nmodalities. For this purpose, inspired by the success of masked language\nmodeling (MLM) tasks in the NLP pre-training area, numerous masked modeling\ntasks have been proposed for VLP to further promote cross-modal interactions.\nThe core idea of previous masked modeling tasks is to focus on reconstructing\nthe masked tokens based on visible context for learning local-to-local\nalignment. However, most of them pay little attention to the global semantic\nfeatures generated for the masked data, resulting in a limited cross-modal\nalignment ability of global representations. Therefore, in this paper, we\npropose a novel Semantic Completion Learning (SCL) task, complementary to\nexisting masked modeling tasks, to facilitate global-to-local alignment.\nSpecifically, the SCL task complements the missing semantics of masked data by\ncapturing the corresponding information from the other modality, promoting\nlearning more representative global features which have a great impact on the\nperformance of downstream tasks. Moreover, we present a flexible vision\nencoder, which enables our model to perform image-text and video-text\nmultimodal tasks simultaneously. Experimental results show that our proposed\nmethod obtains state-of-the-art performance on various vision-language\nbenchmarks, such as visual question answering, image-text retrieval, and\nvideo-text retrieval.\n","authors":["Yatai Ji","Rongcheng Tu","Jie Jiang","Weijie Kong","Chengfei Cai","Wenzhe Zhao","Hongfa Wang","Yujiu Yang","Wei Liu"],"pdf_url":"https://arxiv.org/pdf/2211.13437v2.pdf","comment":"CVPR 2023 accept"},{"id":"http://arxiv.org/abs/2303.14725v1","updated":"2023-03-26T13:44:18Z","published":"2023-03-26T13:44:18Z","title":"Nature Language Reasoning, A Survey","summary":"  This survey paper proposes a clearer view of natural language reasoning in\nthe field of Natural Language Processing (NLP), both conceptually and\npractically. Conceptually, we provide a distinct definition for natural\nlanguage reasoning in NLP, based on both philosophy and NLP scenarios, discuss\nwhat types of tasks require reasoning, and introduce a taxonomy of reasoning.\nPractically, we conduct a comprehensive literature review on natural language\nreasoning in NLP, mainly covering classical logical reasoning, natural language\ninference, multi-hop question answering, and commonsense reasoning. The paper\nalso identifies and views backward reasoning, a powerful paradigm for\nmulti-step reasoning, and introduces defeasible reasoning as one of the most\nimportant future directions in natural language reasoning research. We focus on\nsingle-modality unstructured natural language text, excluding neuro-symbolic\ntechniques and mathematical reasoning.\n","authors":["Fei Yu","Hongbo Zhang","Benyou Wang"],"pdf_url":"https://arxiv.org/pdf/2303.14725v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.09075v2","updated":"2023-03-26T13:43:06Z","published":"2023-03-16T04:21:19Z","title":"Self-Consistent Learning: Cooperation between Generators and\n  Discriminators","summary":"  Using generated data to improve the performance of downstream discriminative\nmodels has recently gained popularity due to the great development of\npre-trained language models. In most previous studies, generative models and\ndiscriminative models are trained separately and thus could not adapt to any\nchanges in each other. As a result, the generated samples can easily deviate\nfrom the real data distribution, while the improvement of the discriminative\nmodel quickly reaches saturation. Generative adversarial networks (GANs) train\ngenerative models via an adversarial process with discriminative models to\nachieve joint training. However, the training of standard GANs is notoriously\nunstable and often falls short of convergence. In this paper, to address these\nissues, we propose a $\\textit{self-consistent learning}$ framework, in which a\ndiscriminator and a generator are cooperatively trained in a closed-loop form.\nThe discriminator and the generator enhance each other during multiple rounds\nof alternating training until a scoring consensus is reached. This framework\nproves to be easy to train and free from instabilities such as mode collapse\nand non-convergence. Extensive experiments on sentence semantic matching\ndemonstrate the effectiveness of the proposed framework: the discriminator\nachieves 10+ AP of improvement on the zero-shot setting and new\nstate-of-the-art performance on the full-data setting.\n","authors":["Tong Wu","Hao Wang","Zhongshen Zeng","Wei Wang","Hai-Tao Zheng","Jiaxing Zhang"],"pdf_url":"https://arxiv.org/pdf/2303.09075v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.14704v1","updated":"2023-03-26T12:18:00Z","published":"2023-03-26T12:18:00Z","title":"Task-oriented Memory-efficient Pruning-Adapter","summary":"  The Outstanding performance and growing size of Large Language Models has led\nto increased attention in parameter efficient learning. The two predominant\napproaches are Adapters and Pruning. Adapters are to freeze the model and give\nit a new weight matrix on the side, which can significantly reduce the time and\nmemory of training, but the cost is that the evaluation and testing will\nincrease the time and memory consumption. Pruning is to cut off some weight and\nre-distribute the remaining weight, which sacrifices the complexity of training\nat the cost of extremely high memory and training time, making the cost of\nevaluation and testing relatively low. So efficiency of training and inference\ncan't be obtained in the same time. In this work, we propose a task-oriented\nPruning-Adapter method that achieve a high memory efficiency of training and\nmemory, and speeds up training time and ensures no significant decrease in\naccuracy in GLUE tasks, achieving training and inference efficiency at the same\ntime.\n","authors":["Guorun Wang","Qingqing Cao","Jun Yang","Yaoru Sun"],"pdf_url":"https://arxiv.org/pdf/2303.14704v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2212.01117v3","updated":"2023-03-26T11:59:01Z","published":"2022-12-02T12:04:48Z","title":"Zero-Shot Rumor Detection with Propagation Structure via Prompt Learning","summary":"  The spread of rumors along with breaking events seriously hinders the truth\nin the era of social media. Previous studies reveal that due to the lack of\nannotated resources, rumors presented in minority languages are hard to be\ndetected. Furthermore, the unforeseen breaking events not involved in\nyesterday's news exacerbate the scarcity of data resources. In this work, we\npropose a novel zero-shot framework based on prompt learning to detect rumors\nfalling in different domains or presented in different languages. More\nspecifically, we firstly represent rumor circulated on social media as diverse\npropagation threads, then design a hierarchical prompt encoding mechanism to\nlearn language-agnostic contextual representations for both prompts and rumor\ndata. To further enhance domain adaptation, we model the domain-invariant\nstructural features from the propagation threads, to incorporate structural\nposition representations of influential community response. In addition, a new\nvirtual response augmentation method is used to improve model training.\nExtensive experiments conducted on three real-world datasets demonstrate that\nour proposed model achieves much better performance than state-of-the-art\nmethods and exhibits a superior capacity for detecting rumors at early stages.\n","authors":["Hongzhan Lin","Pengyao Yi","Jing Ma","Haiyun Jiang","Ziyang Luo","Shuming Shi","Ruifang Liu"],"pdf_url":"https://arxiv.org/pdf/2212.01117v3.pdf","comment":"AAAI 2023"},{"id":"http://arxiv.org/abs/2303.05063v2","updated":"2023-03-26T11:56:34Z","published":"2023-03-09T06:24:50Z","title":"ICL-D3IE: In-Context Learning with Diverse Demonstrations Updating for\n  Document Information Extraction","summary":"  Large language models (LLMs), such as GPT-3 and ChatGPT, have demonstrated\nremarkable results in various natural language processing (NLP) tasks with\nin-context learning, which involves inference based on a few demonstration\nexamples. Despite their successes in NLP tasks, no investigation has been\nconducted to assess the ability of LLMs to perform document information\nextraction (DIE) using in-context learning. Applying LLMs to DIE poses two\nchallenges: the modality and task gap. To this end, we propose a simple but\neffective in-context learning framework called ICL-D3IE, which enables LLMs to\nperform DIE with different types of demonstration examples. Specifically, we\nextract the most difficult and distinct segments from hard training documents\nas hard demonstrations for benefiting all test instances. We design\ndemonstrations describing relationships that enable LLMs to understand\npositional relationships. We introduce formatting demonstrations for easy\nanswer extraction. Additionally, the framework improves diverse demonstrations\nby updating them iteratively. Our experiments on three widely used benchmark\ndatasets demonstrate that the ICL-D3IE framework enables GPT-3/ChatGPT to\nachieve superior performance when compared to previous pre-trained methods\nfine-tuned with full training in both the in-distribution (ID) setting and in\nthe out-of-distribution (OOD) setting.\n","authors":["Jiabang He","Lei Wang","Yi Hu","Ning Liu","Hui Liu","Xing Xu","Heng Tao Shen"],"pdf_url":"https://arxiv.org/pdf/2303.05063v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.14655v1","updated":"2023-03-26T08:43:36Z","published":"2023-03-26T08:43:36Z","title":"GOAL: A Challenging Knowledge-grounded Video Captioning Benchmark for\n  Real-time Soccer Commentary Generation","summary":"  Despite the recent emergence of video captioning models, how to generate\nvivid, fine-grained video descriptions based on the background knowledge (i.e.,\nlong and informative commentary about the domain-specific scenes with\nappropriate reasoning) is still far from being solved, which however has great\napplications such as automatic sports narrative. In this paper, we present\nGOAL, a benchmark of over 8.9k soccer video clips, 22k sentences, and 42k\nknowledge triples for proposing a challenging new task setting as\nKnowledge-grounded Video Captioning (KGVC). Moreover, we conduct experimental\nadaption of existing methods to show the difficulty and potential directions\nfor solving this valuable and applicable task.\n","authors":["Ji Qi","Jifan Yu","Teng Tu","Kunyu Gao","Yifan Xu","Xinyu Guan","Xiaozhi Wang","Yuxiao Dong","Bin Xu","Lei Hou","Juanzi Li","Jie Tang","Weidong Guo","Hui Liu","Yu Xu"],"pdf_url":"https://arxiv.org/pdf/2303.14655v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.14647v1","updated":"2023-03-26T07:41:26Z","published":"2023-03-26T07:41:26Z","title":"Farspredict: A benchmark dataset for link prediction","summary":"  Link prediction with knowledge graph embedding (KGE) is a popular method for\nknowledge graph completion. Furthermore, training KGEs on non-English knowledge\ngraph promote knowledge extraction and knowledge graph reasoning in the context\nof these languages. However, many challenges in non-English KGEs pose to\nlearning a low-dimensional representation of a knowledge graph's entities and\nrelations. This paper proposes \"Farspredict\" a Persian knowledge graph based on\nFarsbase (the most comprehensive knowledge graph in Persian). It also explains\nhow the knowledge graph structure affects link prediction accuracy in KGE. To\nevaluate Farspredict, we implemented the popular models of KGE on it and\ncompared the results with Freebase. Given the analysis results, some\noptimizations on the knowledge graph are carried out to improve its\nfunctionality in the KGE. As a result, a new Persian knowledge graph is\nachieved. Implementation results in the KGE models on Farspredict outperforming\nFreebases in many cases. At last, we discuss what improvements could be\neffective in enhancing the quality of Farspredict and how much it improves.\n","authors":["Najmeh Torabian","Behrouz Minaei-Bidgoli","Mohsen Jahanshahi"],"pdf_url":"https://arxiv.org/pdf/2303.14647v1.pdf","comment":"13 pages, 3 figures, 1 algorithm and 5 tables"},{"id":"http://arxiv.org/abs/2210.05335v2","updated":"2023-03-26T04:54:25Z","published":"2022-10-11T10:54:54Z","title":"MAP: Multimodal Uncertainty-Aware Vision-Language Pre-training Model","summary":"  Multimodal semantic understanding often has to deal with uncertainty, which\nmeans the obtained messages tend to refer to multiple targets. Such uncertainty\nis problematic for our interpretation, including inter- and intra-modal\nuncertainty. Little effort has studied the modeling of this uncertainty,\nparticularly in pre-training on unlabeled datasets and fine-tuning in\ntask-specific downstream datasets. In this paper, we project the\nrepresentations of all modalities as probabilistic distributions via a\nProbability Distribution Encoder (PDE) by utilizing sequence-level\ninteractions. Compared to the existing deterministic methods, such uncertainty\nmodeling can convey richer multimodal semantic information and more complex\nrelationships. Furthermore, we integrate uncertainty modeling with popular\npre-training frameworks and propose suitable pre-training tasks:\nDistribution-based Vision-Language Contrastive learning (D-VLC),\nDistribution-based Masked Language Modeling (D-MLM), and Distribution-based\nImage-Text Matching (D-ITM). The fine-tuned models are applied to challenging\ndownstream tasks, including image-text retrieval, visual question answering,\nvisual reasoning, and visual entailment, and achieve state-of-the-art results.\n","authors":["Yatai Ji","Junjie Wang","Yuan Gong","Lin Zhang","Yanru Zhu","Hongfa Wang","Jiaxing Zhang","Tetsuya Sakai","Yujiu Yang"],"pdf_url":"https://arxiv.org/pdf/2210.05335v2.pdf","comment":"CVPR 2023 accept"},{"id":"http://arxiv.org/abs/2303.14589v1","updated":"2023-03-26T00:02:25Z","published":"2023-03-26T00:02:25Z","title":"SASS: Data and Methods for Subject Aware Sentence Simplification","summary":"  Sentence simplification tends to focus on the generic simplification of\nsentences by making them more readable and easier to understand. This paper\nprovides a dataset aimed at training models that perform subject aware sentence\nsimplifications rather than simplifying sentences as a whole. We also test\nmodels on that dataset which are inspired by model architecture used in\nabstractive summarization. We hand generated portions of the data and augment\nthe dataset by further manipulating those hand written simplifications. Our\nresults show that data-augmentation, data-masking, and model architecture\nchoices used in summarization provide a solid baseline for comparison on\nsubject aware simplification.\n","authors":["Brad Windsor","Luke Martin","Anand Tyagi"],"pdf_url":"https://arxiv.org/pdf/2303.14589v1.pdf","comment":null}],"Information Retrieval":[{"id":"http://arxiv.org/abs/2211.15743v2","updated":"2023-03-26T08:28:44Z","published":"2022-11-28T19:49:02Z","title":"Towards Reliable Item Sampling for Recommendation Evaluation","summary":"  Since Rendle and Krichene argued that commonly used sampling-based evaluation\nmetrics are \"inconsistent\" with respect to the global metrics (even in\nexpectation), there have been a few studies on the sampling-based recommender\nsystem evaluation. Existing methods try either mapping the sampling-based\nmetrics to their global counterparts or more generally, learning the empirical\nrank distribution to estimate the top-$K$ metrics. However, despite existing\nefforts, there is still a lack of rigorous theoretical understanding of the\nproposed metric estimators, and the basic item sampling also suffers from the\n\"blind spot\" issue, i.e., estimation accuracy to recover the top-$K$ metrics\nwhen $K$ is small can still be rather substantial. In this paper, we provide an\nin-depth investigation into these problems and make two innovative\ncontributions. First, we propose a new item-sampling estimator that explicitly\noptimizes the error with respect to the ground truth, and theoretically\nhighlight its subtle difference against prior work. Second, we propose a new\nadaptive sampling method which aims to deal with the \"blind spot\" problem and\nalso demonstrate the expectation-maximization (EM) algorithm can be generalized\nfor such a setting. Our experimental results confirm our statistical analysis\nand the superiority of the proposed works. This study helps lay the theoretical\nfoundation for adopting item sampling metrics for recommendation evaluation,\nand provides strong evidence towards making item sampling a powerful and\nreliable tool for recommendation evaluation.\n","authors":["Dong Li","Ruoming Jin","Zhenming Liu","Bin Ren","Jing Gao","Zhi Liu"],"pdf_url":"https://arxiv.org/pdf/2211.15743v2.pdf","comment":"aaai2023"},{"id":"http://arxiv.org/abs/2303.14601v1","updated":"2023-03-26T01:38:11Z","published":"2023-03-26T01:38:11Z","title":"PORE: Provably Robust Recommender Systems against Data Poisoning Attacks","summary":"  Data poisoning attacks spoof a recommender system to make arbitrary,\nattacker-desired recommendations via injecting fake users with carefully\ncrafted rating scores into the recommender system. We envision a cat-and-mouse\ngame for such data poisoning attacks and their defenses, i.e., new defenses are\ndesigned to defend against existing attacks and new attacks are designed to\nbreak them. To prevent such a cat-and-mouse game, we propose PORE, the first\nframework to build provably robust recommender systems in this work. PORE can\ntransform any existing recommender system to be provably robust against any\nuntargeted data poisoning attacks, which aim to reduce the overall performance\nof a recommender system. Suppose PORE recommends top-$N$ items to a user when\nthere is no attack. We prove that PORE still recommends at least $r$ of the $N$\nitems to the user under any data poisoning attack, where $r$ is a function of\nthe number of fake users in the attack. Moreover, we design an efficient\nalgorithm to compute $r$ for each user. We empirically evaluate PORE on popular\nbenchmark datasets.\n","authors":["Jinyuan Jia","Yupei Liu","Yuepeng Hu","Neil Zhenqiang Gong"],"pdf_url":"https://arxiv.org/pdf/2303.14601v1.pdf","comment":"To appear in USENIX Security Symposium, 2023"}],"Machine Learning":[{"id":"http://arxiv.org/abs/2303.14844v1","updated":"2023-03-26T22:58:06Z","published":"2023-03-26T22:58:06Z","title":"Analyzing Convergence in Quantum Neural Networks: Deviations from Neural\n  Tangent Kernels","summary":"  A quantum neural network (QNN) is a parameterized mapping efficiently\nimplementable on near-term Noisy Intermediate-Scale Quantum (NISQ) computers.\nIt can be used for supervised learning when combined with classical\ngradient-based optimizers. Despite the existing empirical and theoretical\ninvestigations, the convergence of QNN training is not fully understood.\nInspired by the success of the neural tangent kernels (NTKs) in probing into\nthe dynamics of classical neural networks, a recent line of works proposes to\nstudy over-parameterized QNNs by examining a quantum version of tangent\nkernels. In this work, we study the dynamics of QNNs and show that contrary to\npopular belief it is qualitatively different from that of any kernel\nregression: due to the unitarity of quantum operations, there is a\nnon-negligible deviation from the tangent kernel regression derived at the\nrandom initialization. As a result of the deviation, we prove the at-most\nsublinear convergence for QNNs with Pauli measurements, which is beyond the\nexplanatory power of any kernel regression dynamics. We then present the actual\ndynamics of QNNs in the limit of over-parameterization. The new dynamics\ncapture the change of convergence rate during training and implies that the\nrange of measurements is crucial to the fast QNN convergence.\n","authors":["Xuchen You","Shouvanik Chakrabarti","Boyang Chen","Xiaodi Wu"],"pdf_url":"https://arxiv.org/pdf/2303.14844v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.14836v1","updated":"2023-03-26T22:20:17Z","published":"2023-03-26T22:20:17Z","title":"Illuminati: Towards Explaining Graph Neural Networks for Cybersecurity\n  Analysis","summary":"  Graph neural networks (GNNs) have been utilized to create multi-layer graph\nmodels for a number of cybersecurity applications from fraud detection to\nsoftware vulnerability analysis. Unfortunately, like traditional neural\nnetworks, GNNs also suffer from a lack of transparency, that is, it is\nchallenging to interpret the model predictions. Prior works focused on specific\nfactor explanations for a GNN model. In this work, we have designed and\nimplemented Illuminati, a comprehensive and accurate explanation framework for\ncybersecurity applications using GNN models. Given a graph and a pre-trained\nGNN model, Illuminati is able to identify the important nodes, edges, and\nattributes that are contributing to the prediction while requiring no prior\nknowledge of GNN models. We evaluate Illuminati in two cybersecurity\napplications, i.e., code vulnerability detection and smart contract\nvulnerability detection. The experiments show that Illuminati achieves more\naccurate explanation results than state-of-the-art methods, specifically, 87.6%\nof subgraphs identified by Illuminati are able to retain their original\nprediction, an improvement of 10.3% over others at 77.3%. Furthermore, the\nexplanation of Illuminati can be easily understood by the domain experts,\nsuggesting the significant usefulness for the development of cybersecurity\napplications.\n","authors":["Haoyu He","Yuede Ji","H. Howie Huang"],"pdf_url":"https://arxiv.org/pdf/2303.14836v1.pdf","comment":"EuroS&P 2022"},{"id":"http://arxiv.org/abs/2303.14822v1","updated":"2023-03-26T21:12:36Z","published":"2023-03-26T21:12:36Z","title":"MGTBench: Benchmarking Machine-Generated Text Detection","summary":"  Nowadays large language models (LLMs) have shown revolutionary power in a\nvariety of natural language processing (NLP) tasks such as text classification,\nsentiment analysis, language translation, and question-answering. In this way,\ndetecting machine-generated texts (MGTs) is becoming increasingly important as\nLLMs become more advanced and prevalent. These models can generate human-like\nlanguage that can be difficult to distinguish from text written by a human,\nwhich raises concerns about authenticity, accountability, and potential bias.\nHowever, existing detection methods against MGTs are evaluated under different\nmodel architectures, datasets, and experimental settings, resulting in a lack\nof a comprehensive evaluation framework across different methodologies\n  In this paper, we fill this gap by proposing the first benchmark framework\nfor MGT detection, named MGTBench. Extensive evaluations on public datasets\nwith curated answers generated by ChatGPT (the most representative and powerful\nLLMs thus far) show that most of the current detection methods perform less\nsatisfactorily against MGTs. An exceptional case is ChatGPT Detector, which is\ntrained with ChatGPT-generated texts and shows great performance in detecting\nMGTs. Nonetheless, we note that only a small fraction of adversarial-crafted\nperturbations on MGTs can evade the ChatGPT Detector, thus highlighting the\nneed for more robust MGT detection methods. We envision that MGTBench will\nserve as a benchmark tool to accelerate future investigations involving the\nevaluation of state-of-the-art MGT detection methods on their respective\ndatasets and the development of more advanced MGT detection methods. Our source\ncode and datasets are available at https://github.com/xinleihe/MGTBench.\n","authors":["Xinlei He","Xinyue Shen","Zeyuan Chen","Michael Backes","Yang Zhang"],"pdf_url":"https://arxiv.org/pdf/2303.14822v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.14817v1","updated":"2023-03-26T20:51:35Z","published":"2023-03-26T20:51:35Z","title":"Frame Flexible Network","summary":"  Existing video recognition algorithms always conduct different training\npipelines for inputs with different frame numbers, which requires repetitive\ntraining operations and multiplying storage costs. If we evaluate the model\nusing other frames which are not used in training, we observe the performance\nwill drop significantly (see Fig.1), which is summarized as Temporal Frequency\nDeviation phenomenon. To fix this issue, we propose a general framework, named\nFrame Flexible Network (FFN), which not only enables the model to be evaluated\nat different frames to adjust its computation, but also reduces the memory\ncosts of storing multiple models significantly. Concretely, FFN integrates\nseveral sets of training sequences, involves Multi-Frequency Alignment (MFAL)\nto learn temporal frequency invariant representations, and leverages\nMulti-Frequency Adaptation (MFAD) to further strengthen the representation\nabilities. Comprehensive empirical validations using various architectures and\npopular benchmarks solidly demonstrate the effectiveness and generalization of\nFFN (e.g., 7.08/5.15/2.17% performance gain at Frame 4/8/16 on\nSomething-Something V1 dataset over Uniformer). Code is available at\nhttps://github.com/BeSpontaneous/FFN.\n","authors":["Yitian Zhang","Yue Bai","Chang Liu","Huan Wang","Sheng Li","Yun Fu"],"pdf_url":"https://arxiv.org/pdf/2303.14817v1.pdf","comment":"Accepted by CVPR2023"},{"id":"http://arxiv.org/abs/2303.14811v1","updated":"2023-03-26T20:33:44Z","published":"2023-03-26T20:33:44Z","title":"Learning Generative Models with Goal-conditioned Reinforcement Learning","summary":"  We present a novel, alternative framework for learning generative models with\ngoal-conditioned reinforcement learning. We define two agents, a goal\nconditioned agent (GC-agent) and a supervised agent (S-agent). Given a\nuser-input initial state, the GC-agent learns to reconstruct the training set.\nIn this context, elements in the training set are the goals. During training,\nthe S-agent learns to imitate the GC-agent while remaining agnostic of the\ngoals. At inference we generate new samples with the S-agent. Following a\nsimilar route as in variational auto-encoders, we derive an upper bound on the\nnegative log-likelihood that consists of a reconstruction term and a divergence\nbetween the GC-agent policy and the (goal-agnostic) S-agent policy. We\nempirically demonstrate that our method is able to generate diverse and high\nquality samples in the task of image synthesis.\n","authors":["Mariana Vargas Vieyra","Pierre Ménard"],"pdf_url":"https://arxiv.org/pdf/2303.14811v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2212.05189v2","updated":"2023-03-26T19:51:15Z","published":"2022-12-10T03:42:27Z","title":"Expanding Knowledge Graphs with Humans in the Loop","summary":"  Curated knowledge graphs encode domain expertise and improve the performance\nof recommendation, segmentation, ad targeting, and other machine learning\nsystems in several domains. As new concepts emerge in a domain, knowledge\ngraphs must be expanded to preserve machine learning performance. Manually\nexpanding knowledge graphs, however, is infeasible at scale. In this work, we\npropose a method for knowledge graph expansion with humans-in-the-loop.\nConcretely, given a knowledge graph, our method predicts the \"parents\" of new\nconcepts to be added to this graph for further verification by human experts.\nWe show that our method is both accurate and provably \"human-friendly\".\nSpecifically, we prove that our method predicts parents that are \"near\"\nconcepts' true parents in the knowledge graph, even when the predictions are\nincorrect. We then show, with a controlled experiment, that satisfying this\nproperty increases both the speed and the accuracy of the human-algorithm\ncollaboration. We further evaluate our method on a knowledge graph from\nPinterest and show that it outperforms competing methods on both accuracy and\nhuman-friendliness. Upon deployment in production at Pinterest, our method\nreduced the time needed for knowledge graph expansion by ~400% (compared to\nmanual expansion), and contributed to a subsequent increase in ad revenue of\n20%.\n","authors":["Emaad Manzoor","Jordan Tong","Sriniketh Vijayaraghavan","Rui Li"],"pdf_url":"https://arxiv.org/pdf/2212.05189v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.02841v2","updated":"2023-03-26T19:45:25Z","published":"2023-03-06T02:24:48Z","title":"Model-Agnostic Meta-Learning for Natural Language Understanding Tasks in\n  Finance","summary":"  Natural language understanding(NLU) is challenging for finance due to the\nlack of annotated data and the specialized language in that domain. As a\nresult, researchers have proposed to use pre-trained language model and\nmulti-task learning to learn robust representations. However, aggressive\nfine-tuning often causes over-fitting and multi-task learning may favor tasks\nwith significantly larger amounts data, etc. To address these problems, in this\npaper, we investigate model-agnostic meta-learning algorithm(MAML) in\nlow-resource financial NLU tasks. Our contribution includes: 1. we explore the\nperformance of MAML method with multiple types of tasks: GLUE datasets, SNLI,\nSci-Tail and Financial PhraseBank; 2. we study the performance of MAML method\nwith multiple single-type tasks: a real scenario stock price prediction problem\nwith twitter text data. Our models achieve the state-of-the-art performance\naccording to the experimental results, which demonstrate that our method can\nadapt fast and well to low-resource situations.\n","authors":["Bixing Yan","Shaoling Chen","Yuxuan He","Zhihan Li"],"pdf_url":"https://arxiv.org/pdf/2303.02841v2.pdf","comment":"13 pages, 6 figures, 8 tables"},{"id":"http://arxiv.org/abs/2302.05971v2","updated":"2023-03-26T19:39:59Z","published":"2023-02-12T18:29:20Z","title":"Review of Extreme Multilabel Classification","summary":"  Extreme multilabel classification or XML, is an active area of interest in\nmachine learning. Compared to traditional multilabel classification, here the\nnumber of labels is extremely large, hence, the name extreme multilabel\nclassification. Using classical one versus all classification wont scale in\nthis case due to large number of labels, same is true for any other\nclassifiers. Embedding of labels as well as features into smaller label space\nis an essential first step. Moreover, other issues include existence of head\nand tail labels, where tail labels are labels which exist in relatively smaller\nnumber of given samples. The existence of tail labels creates issues during\nembedding. This area has invited application of wide range of approaches\nranging from bit compression motivated from compressed sensing, tree based\nembeddings, deep learning based latent space embedding including using\nattention weights, linear algebra based embeddings such as SVD, clustering,\nhashing, to name a few. The community has come up with a useful set of metrics\nto identify correctly the prediction for head or tail labels.\n","authors":["Arpan Dasgupta","Siddhant Katyan","Shrutimoy Das","Pawan Kumar"],"pdf_url":"https://arxiv.org/pdf/2302.05971v2.pdf","comment":"46 pages, 13 figures"},{"id":"http://arxiv.org/abs/2303.11207v2","updated":"2023-03-26T19:36:25Z","published":"2023-03-20T15:40:28Z","title":"Investigating Topological Order using Recurrent Neural Networks","summary":"  Recurrent neural networks (RNNs), originally developed for natural language\nprocessing, hold great promise for accurately describing strongly correlated\nquantum many-body systems. Here, we employ 2D RNNs to investigate two\nprototypical quantum many-body Hamiltonians exhibiting topological order.\nSpecifically, we demonstrate that RNN wave functions can effectively capture\nthe topological order of the toric code and a Bose-Hubbard spin liquid on the\nkagome lattice by estimating their topological entanglement entropies. We also\nfind that RNNs favor coherent superpositions of minimally-entangled states over\nminimally-entangled states themselves. Overall, our findings demonstrate that\nRNN wave functions constitute a powerful tool to study phases of matter beyond\nLandau's symmetry-breaking paradigm.\n","authors":["Mohamed Hibat-Allah","Roger G. Melko","Juan Carrasquilla"],"pdf_url":"https://arxiv.org/pdf/2303.11207v2.pdf","comment":"14 pages, 7 figures, 1 table. A version with new corrections"},{"id":"http://arxiv.org/abs/2202.10574v4","updated":"2023-03-26T18:52:17Z","published":"2022-02-21T23:36:40Z","title":"A Multi-Agent Reinforcement Learning Framework for Off-Policy Evaluation\n  in Two-sided Markets","summary":"  The two-sided markets such as ride-sharing companies often involve a group of\nsubjects who are making sequential decisions across time and/or location. With\nthe rapid development of smart phones and internet of things, they have\nsubstantially transformed the transportation landscape of human beings. In this\npaper we consider large-scale fleet management in ride-sharing companies that\ninvolve multiple units in different areas receiving sequences of products (or\ntreatments) over time. Major technical challenges, such as policy evaluation,\narise in those studies because (i) spatial and temporal proximities induce\ninterference between locations and times; and (ii) the large number of\nlocations results in the curse of dimensionality. To address both challenges\nsimultaneously, we introduce a multi-agent reinforcement learning (MARL)\nframework for carrying policy evaluation in these studies. We propose novel\nestimators for mean outcomes under different products that are consistent\ndespite the high-dimensionality of state-action space. The proposed estimator\nworks favorably in simulation experiments. We further illustrate our method\nusing a real dataset obtained from a two-sided marketplace company to evaluate\nthe effects of applying different subsidizing policies. A Python implementation\nof our proposed method is available at\nhttps://github.com/RunzheStat/CausalMARL.\n","authors":["Chengchun Shi","Runzhe Wan","Ge Song","Shikai Luo","Rui Song","Hongtu Zhu"],"pdf_url":"https://arxiv.org/pdf/2202.10574v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2302.06086v2","updated":"2023-03-26T18:42:57Z","published":"2023-02-13T04:09:34Z","title":"Reliability Assurance for Deep Neural Network Architectures Against\n  Numerical Defects","summary":"  With the widespread deployment of deep neural networks (DNNs), ensuring the\nreliability of DNN-based systems is of great importance. Serious reliability\nissues such as system failures can be caused by numerical defects, one of the\nmost frequent defects in DNNs. To assure high reliability against numerical\ndefects, in this paper, we propose the RANUM approach including novel\ntechniques for three reliability assurance tasks: detection of potential\nnumerical defects, confirmation of potential-defect feasibility, and suggestion\nof defect fixes. To the best of our knowledge, RANUM is the first approach that\nconfirms potential-defect feasibility with failure-exhibiting tests and\nsuggests fixes automatically. Extensive experiments on the benchmarks of 63\nreal-world DNN architectures show that RANUM outperforms state-of-the-art\napproaches across the three reliability assurance tasks. In addition, when the\nRANUM-generated fixes are compared with developers' fixes on open-source\nprojects, in 37 out of 40 cases, RANUM-generated fixes are equivalent to or\neven better than human fixes.\n","authors":["Linyi Li","Yuhao Zhang","Luyao Ren","Yingfei Xiong","Tao Xie"],"pdf_url":"https://arxiv.org/pdf/2302.06086v2.pdf","comment":"To appear at 45th International Conference on Software Engineering\n  (ICSE 2023), camera-ready version"},{"id":"http://arxiv.org/abs/2205.13832v3","updated":"2023-03-26T17:14:08Z","published":"2022-05-27T08:51:07Z","title":"Counterfactual Analysis in Dynamic Latent State Models","summary":"  We provide an optimization-based framework to perform counterfactual analysis\nin a dynamic model with hidden states. Our framework is grounded in the\n``abduction, action, and prediction'' approach to answer counterfactual queries\nand handles two key challenges where (1) the states are hidden and (2) the\nmodel is dynamic. Recognizing the lack of knowledge on the underlying causal\nmechanism and the possibility of infinitely many such mechanisms, we optimize\nover this space and compute upper and lower bounds on the counterfactual\nquantity of interest. Our work brings together ideas from causality,\nstate-space models, simulation, and optimization, and we apply it on a breast\ncancer case study. To the best of our knowledge, we are the first to compute\nlower and upper bounds on a counterfactual query in a dynamic latent-state\nmodel.\n","authors":["Martin Haugh","Raghav Singal"],"pdf_url":"https://arxiv.org/pdf/2205.13832v3.pdf","comment":"29 pages, 8 figures"},{"id":"http://arxiv.org/abs/2303.14773v1","updated":"2023-03-26T16:42:05Z","published":"2023-03-26T16:42:05Z","title":"BlackVIP: Black-Box Visual Prompting for Robust Transfer Learning","summary":"  With the surge of large-scale pre-trained models (PTMs), fine-tuning these\nmodels to numerous downstream tasks becomes a crucial problem. Consequently,\nparameter efficient transfer learning (PETL) of large models has grasped huge\nattention. While recent PETL methods showcase impressive performance, they rely\non optimistic assumptions: 1) the entire parameter set of a PTM is available,\nand 2) a sufficiently large memory capacity for the fine-tuning is equipped.\nHowever, in most real-world applications, PTMs are served as a black-box API or\nproprietary software without explicit parameter accessibility. Besides, it is\nhard to meet a large memory requirement for modern PTMs. In this work, we\npropose black-box visual prompting (BlackVIP), which efficiently adapts the\nPTMs without knowledge about model architectures and parameters. BlackVIP has\ntwo components; 1) Coordinator and 2) simultaneous perturbation stochastic\napproximation with gradient correction (SPSA-GC). The Coordinator designs\ninput-dependent image-shaped visual prompts, which improves few-shot adaptation\nand robustness on distribution/location shift. SPSA-GC efficiently estimates\nthe gradient of a target model to update Coordinator. Extensive experiments on\n16 datasets demonstrate that BlackVIP enables robust adaptation to diverse\ndomains without accessing PTMs' parameters, with minimal memory requirements.\nCode: \\url{https://github.com/changdaeoh/BlackVIP}\n","authors":["Changdae Oh","Hyeji Hwang","Hee-young Lee","YongTaek Lim","Geunyoung Jung","Jiyoung Jung","Hosik Choi","Kyungwoo Song"],"pdf_url":"https://arxiv.org/pdf/2303.14773v1.pdf","comment":"Accepted to CVPR 2023"},{"id":"http://arxiv.org/abs/2303.14771v1","updated":"2023-03-26T16:35:45Z","published":"2023-03-26T16:35:45Z","title":"Prototype-Sample Relation Distillation: Towards Replay-Free Continual\n  Learning","summary":"  In Continual learning (CL) balancing effective adaptation while combating\ncatastrophic forgetting is a central challenge. Many of the recent\nbest-performing methods utilize various forms of prior task data, e.g. a replay\nbuffer, to tackle the catastrophic forgetting problem. Having access to\nprevious task data can be restrictive in many real-world scenarios, for example\nwhen task data is sensitive or proprietary. To overcome the necessity of using\nprevious tasks data, in this work, we start with strong representation learning\nmethods that have been shown to be less prone to forgetting. We propose a\nholistic approach to jointly learn the representation and class prototypes\nwhile maintaining the relevance of old class prototypes and their embedded\nsimilarities. Specifically, samples are mapped to an embedding space where the\nrepresentations are learned using a supervised contrastive loss. Class\nprototypes are evolved continually in the same latent space, enabling learning\nand prediction at any point. To continually adapt the prototypes without\nkeeping any prior task data, we propose a novel distillation loss that\nconstrains class prototypes to maintain relative similarities as compared to\nnew task data. This method yields state-of-the-art performance in the\ntask-incremental setting where we are able to outperform other methods that\nboth use no data as well as approaches relying on large amounts of data. Our\nmethod is also shown to provide strong performance in the class-incremental\nsetting without using any stored data points.\n","authors":["Nader Asadi","MohammadReza Davar","Sudhir Mudur","Rahaf Aljundi","Eugene Belilovsky"],"pdf_url":"https://arxiv.org/pdf/2303.14771v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.14770v1","updated":"2023-03-26T16:29:18Z","published":"2023-03-26T16:29:18Z","title":"Koala: An Index for Quantifying Overlaps with Pre-training Corpora","summary":"  In very recent years more attention has been placed on probing the role of\npre-training data in Large Language Models (LLMs) downstream behaviour. Despite\nthe importance, there is no public tool that supports such analysis of\npre-training corpora at large scale. To help research in this space, we launch\nKoala, a searchable index over large pre-training corpora using compressed\nsuffix arrays with highly efficient compression rate and search support. In its\nfirst release we index the public proportion of OPT 175B pre-training data.\nKoala provides a framework to do forensic analysis on the current and future\nbenchmarks as well as to assess the degree of memorization in the output from\nthe LLMs. Koala is available for public use at\nhttps://koala-index.erc.monash.edu/.\n","authors":["Thuy-Trang Vu","Xuanli He","Gholamreza Haffari","Ehsan Shareghi"],"pdf_url":"https://arxiv.org/pdf/2303.14770v1.pdf","comment":"Available here: https://koala-index.erc.monash.edu/"}],"Multimedia":[{"id":"http://arxiv.org/abs/2211.13437v2","updated":"2023-03-26T13:59:36Z","published":"2022-11-24T06:39:16Z","title":"Seeing What You Miss: Vision-Language Pre-training with Semantic\n  Completion Learning","summary":"  Cross-modal alignment is essential for vision-language pre-training (VLP)\nmodels to learn the correct corresponding information across different\nmodalities. For this purpose, inspired by the success of masked language\nmodeling (MLM) tasks in the NLP pre-training area, numerous masked modeling\ntasks have been proposed for VLP to further promote cross-modal interactions.\nThe core idea of previous masked modeling tasks is to focus on reconstructing\nthe masked tokens based on visible context for learning local-to-local\nalignment. However, most of them pay little attention to the global semantic\nfeatures generated for the masked data, resulting in a limited cross-modal\nalignment ability of global representations. Therefore, in this paper, we\npropose a novel Semantic Completion Learning (SCL) task, complementary to\nexisting masked modeling tasks, to facilitate global-to-local alignment.\nSpecifically, the SCL task complements the missing semantics of masked data by\ncapturing the corresponding information from the other modality, promoting\nlearning more representative global features which have a great impact on the\nperformance of downstream tasks. Moreover, we present a flexible vision\nencoder, which enables our model to perform image-text and video-text\nmultimodal tasks simultaneously. Experimental results show that our proposed\nmethod obtains state-of-the-art performance on various vision-language\nbenchmarks, such as visual question answering, image-text retrieval, and\nvideo-text retrieval.\n","authors":["Yatai Ji","Rongcheng Tu","Jie Jiang","Weijie Kong","Chengfei Cai","Wenzhe Zhao","Hongfa Wang","Yujiu Yang","Wei Liu"],"pdf_url":"https://arxiv.org/pdf/2211.13437v2.pdf","comment":"CVPR 2023 accept"},{"id":"http://arxiv.org/abs/2303.14708v1","updated":"2023-03-26T12:34:01Z","published":"2023-03-26T12:34:01Z","title":"Exploring Multimodal Sentiment Analysis via CBAM Attention and\n  Double-layer BiLSTM Architecture","summary":"  Because multimodal data contains more modal information, multimodal sentiment\nanalysis has become a recent research hotspot. However, redundant information\nis easily involved in feature fusion after feature extraction, which has a\ncertain impact on the feature representation after fusion. Therefore, in this\npapaer, we propose a new multimodal sentiment analysis model. In our model, we\nuse BERT + BiLSTM as new feature extractor to capture the long-distance\ndependencies in sentences and consider the position information of input\nsequences to obtain richer text features. To remove redundant information and\nmake the network pay more attention to the correlation between image and text\nfeatures, CNN and CBAM attention are added after splicing text features and\npicture features, to improve the feature representation ability. On the\nMVSA-single dataset and HFM dataset, compared with the baseline model, the ACC\nof our model is improved by 1.78% and 1.91%, and the F1 value is enhanced by\n3.09% and 2.0%, respectively. The experimental results show that our model\nachieves a sound effect, similar to the advanced model.\n","authors":["Huiru Wang","Xiuhong Li","Zenyu Ren","Dan Yang","chunming Ma"],"pdf_url":"https://arxiv.org/pdf/2303.14708v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.14637v1","updated":"2023-03-26T06:09:53Z","published":"2023-03-26T06:09:53Z","title":"Improved Nonlinear Transform Source-Channel Coding to Catalyze Semantic\n  Communications","summary":"  Recent deep learning methods have led to increased interest in solving\nhigh-efficiency end-to-end transmission problems. These methods, we call\nnonlinear transform source-channel coding (NTSCC), extract the semantic latent\nfeatures of source signal, and learn entropy model to guide the joint\nsource-channel coding with variable rate to transmit latent features over\nwireless channels. In this paper, we propose a comprehensive framework for\nimproving NTSCC, thereby higher system coding gain, better model versatility,\nand more flexible adaptation strategy aligned with semantic guidance are all\nachieved. This new sophisticated NTSCC model is now ready to support large-size\ndata interaction in emerging XR, which catalyzes the application of semantic\ncommunications. Specifically, we propose three useful improvement approaches.\nFirst, we introduce a contextual entropy model to better capture the spatial\ncorrelations among the semantic latent features, thereby more accurate rate\nallocation and contextual joint source-channel coding are developed accordingly\nto enable higher coding gain. On that basis, we further propose response\nnetwork architectures to formulate versatile NTSCC, i.e., once-trained model\nsupports various rates and channel states that benefits the practical\ndeployment. Following this, we propose an online latent feature editing method\nto enable more flexible coding rate control aligned with some specific semantic\nguidance. By comprehensively applying the above three improvement methods for\nNTSCC, a deployment-friendly semantic coded transmission system stands out\nfinally. Our improved NTSCC system has been experimentally verified to achieve\n16.35% channel bandwidth saving versus the state-of-the-art engineered VTM + 5G\nLDPC coded transmission system with lower processing latency.\n","authors":["Sixian Wang","Jincheng Dai","Xiaoqi Qin","Zhongwei Si","Kai Niu","Ping Zhang"],"pdf_url":"https://arxiv.org/pdf/2303.14637v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2210.05335v2","updated":"2023-03-26T04:54:25Z","published":"2022-10-11T10:54:54Z","title":"MAP: Multimodal Uncertainty-Aware Vision-Language Pre-training Model","summary":"  Multimodal semantic understanding often has to deal with uncertainty, which\nmeans the obtained messages tend to refer to multiple targets. Such uncertainty\nis problematic for our interpretation, including inter- and intra-modal\nuncertainty. Little effort has studied the modeling of this uncertainty,\nparticularly in pre-training on unlabeled datasets and fine-tuning in\ntask-specific downstream datasets. In this paper, we project the\nrepresentations of all modalities as probabilistic distributions via a\nProbability Distribution Encoder (PDE) by utilizing sequence-level\ninteractions. Compared to the existing deterministic methods, such uncertainty\nmodeling can convey richer multimodal semantic information and more complex\nrelationships. Furthermore, we integrate uncertainty modeling with popular\npre-training frameworks and propose suitable pre-training tasks:\nDistribution-based Vision-Language Contrastive learning (D-VLC),\nDistribution-based Masked Language Modeling (D-MLM), and Distribution-based\nImage-Text Matching (D-ITM). The fine-tuned models are applied to challenging\ndownstream tasks, including image-text retrieval, visual question answering,\nvisual reasoning, and visual entailment, and achieve state-of-the-art results.\n","authors":["Yatai Ji","Junjie Wang","Yuan Gong","Lin Zhang","Yanru Zhu","Hongfa Wang","Jiaxing Zhang","Tetsuya Sakai","Yujiu Yang"],"pdf_url":"https://arxiv.org/pdf/2210.05335v2.pdf","comment":"CVPR 2023 accept"},{"id":"http://arxiv.org/abs/2208.00339v3","updated":"2023-03-26T03:32:28Z","published":"2022-07-31T02:23:24Z","title":"GraphMFT: A Graph Network based Multimodal Fusion Technique for Emotion\n  Recognition in Conversation","summary":"  Multimodal machine learning is an emerging area of research, which has\nreceived a great deal of scholarly attention in recent years. Up to now, there\nare few studies on multimodal conversational emotion recognition. Since Graph\nNeural Networks (GNNs) possess the powerful capacity of relational modeling,\nthey have an inherent advantage in the field of multimodal learning. GNNs\nleverage the graph constructed from multimodal data to perform intra- and\ninter-modal information interaction, which effectively facilitates the\nintegration and complementation of multimodal data. In this work, we propose a\nnovel Graph network based Multimodal Fusion Technique (GraphMFT) for emotion\nrecognition in conversation. Multimodal data can be modeled as a graph, where\neach data object is regarded as a node, and both intra- and inter-modal\ndependencies existing between data objects can be regarded as edges. GraphMFT\nutilizes multiple improved graph attention networks to capture intra-modal\ncontextual information and inter-modal complementary information. In addition,\nthe proposed GraphMFT attempts to address the challenges of existing\ngraph-based multimodal Emotion Recognition in Conversation (ERC) models such as\nMMGCN. Empirical results on two public multimodal datasets reveal that our\nmodel outperforms the State-Of-The-Art (SOTA) approaches with the accuracy of\n67.90% and 61.30%.\n","authors":["Jiang Li","Xiaoping Wang","Guoqing Lv","Zhigang Zeng"],"pdf_url":"https://arxiv.org/pdf/2208.00339v3.pdf","comment":"12 pages"}]},"2023-03-25T00:00:00Z":{"Computation and Language":[{"id":"http://arxiv.org/abs/2303.14588v1","updated":"2023-03-25T23:41:33Z","published":"2023-03-25T23:41:33Z","title":"Fine-Tashkeel: Finetuning Byte-Level Models for Accurate Arabic Text\n  Diacritization","summary":"  Most of previous work on learning diacritization of the Arabic language\nrelied on training models from scratch. In this paper, we investigate how to\nleverage pre-trained language models to learn diacritization. We finetune\ntoken-free pre-trained multilingual models (ByT5) to learn to predict and\ninsert missing diacritics in Arabic text, a complex task that requires\nunderstanding the sentence semantics and the morphological structure of the\ntokens. We show that we can achieve state-of-the-art on the diacritization task\nwith minimal amount of training and no feature engineering, reducing WER by\n40%. We release our finetuned models for the greater benefit of the researchers\nin the community.\n","authors":["Bashar Al-Rfooh","Gheith Abandah","Rami Al-Rfou"],"pdf_url":"https://arxiv.org/pdf/2303.14588v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.14582v1","updated":"2023-03-25T23:16:11Z","published":"2023-03-25T23:16:11Z","title":"Identification of Negative Transfers in Multitask Learning Using\n  Surrogate Models","summary":"  Multitask learning is widely used in practice to train a low-resource target\ntask by augmenting it with multiple related source tasks. Yet, naively\ncombining all the source tasks with a target task does not always improve the\nprediction performance for the target task due to negative transfers. Thus, a\ncritical problem in multitask learning is identifying subsets of source tasks\nthat would benefit the target task. This problem is computationally challenging\nsince the number of subsets grows exponentially with the number of source\ntasks; efficient heuristics for subset selection does not always capture the\nrelationship between task subsets and multitask learning performances. In this\npaper, we introduce an efficient procedure to address this problem via\nsurrogate modeling. In surrogate modeling, we sample (random) subsets of source\ntasks and precompute their multitask learning performances; Then, we\napproximate the precomputed performances with a linear regression model that\ncan also be used to predict the multitask performance of unseen task subsets.\nWe show theoretically and empirically that fitting this model only requires\nsampling linearly many subsets in the number of source tasks. The fitted model\nprovides a relevance score between each source task and the target task; We use\nthe relevance scores to perform subset selection for multitask learning by\nthresholding. Through extensive experiments, we show that our approach predicts\nnegative transfers from multiple source tasks to target tasks much more\naccurately than existing task affinity measures. Additionally, we demonstrate\nthat for five weak supervision datasets, our approach consistently improves\nupon existing optimization methods for multi-task learning.\n","authors":["Dongyue Li","Huy L. Nguyen","Hongyang R. Zhang"],"pdf_url":"https://arxiv.org/pdf/2303.14582v1.pdf","comment":"30 pages, 7 figures, 7 tables. Published in Transactions on Machine\n  Learning Research, March 2023"},{"id":"http://arxiv.org/abs/2302.02676v6","updated":"2023-03-25T23:02:25Z","published":"2023-02-06T10:28:16Z","title":"Chain of Hindsight Aligns Language Models with Feedback","summary":"  Learning from human preferences is important for language models to be\nhelpful and useful for humans, and to align with human and social values. Prior\nwork have achieved remarkable successes by learning from human feedback to\nunderstand and follow instructions. Nonetheless, these methods are either\nfounded on hand-picked model generations that are favored by human annotators,\nrendering them ineffective in terms of data utilization and challenging to\napply in general, or they depend on reward functions and reinforcement\nlearning, which are prone to imperfect reward function and extremely\nchallenging to optimize. In this work, we propose a novel technique, Chain of\nHindsight, that is easy to optimize and can learn from any form of feedback,\nregardless of its polarity. Our idea is inspired by how humans learn from\nextensive feedback presented in the form of languages. We convert all types of\nfeedback into sentences, which are then used to fine-tune the model, allowing\nus to take advantage of the language comprehension capabilities of language\nmodels. We condition the model on a sequence of model generations paired with\nfeedback. By doing so, models are trained to generate outputs based on\nfeedback, and models can learn to identify and correct negative attributes or\nerrors. Applying our method to large language models, we observed that Chain of\nHindsight significantly surpasses previous methods in aligning language models\nwith human preferences. We observed significant improvements on summarization\nand dialogue tasks and our approach is markedly preferred in human evaluations.\n","authors":["Hao Liu","Carmelo Sferrazza","Pieter Abbeel"],"pdf_url":"https://arxiv.org/pdf/2302.02676v6.pdf","comment":"included new results"},{"id":"http://arxiv.org/abs/2303.14576v1","updated":"2023-03-25T22:45:54Z","published":"2023-03-25T22:45:54Z","title":"Automatic Generation of Multiple-Choice Questions","summary":"  Creating multiple-choice questions to assess reading comprehension of a given\narticle involves generating question-answer pairs (QAPs) and adequate\ndistractors. We present two methods to tackle the challenge of QAP generations:\n(1) A deep-learning-based end-to-end question generation system based on T5\nTransformer with Preprocessing and Postprocessing Pipelines (TP3). We use the\nfinetuned T5 model for our downstream task of question generation and improve\naccuracy using a combination of various NLP tools and algorithms in\npreprocessing and postprocessing to select appropriate answers and filter\nundesirable questions. (2) A sequence-learning-based scheme to generate\nadequate QAPs via meta-sequence representations of sentences. A meta-sequence\nis a sequence of vectors comprising semantic and syntactic tags. we devise a\nscheme called MetaQA to learn meta sequences from training data to form pairs\nof a meta sequence for a declarative sentence and a corresponding interrogative\nsentence. The TP3 works well on unseen data, which is complemented by MetaQA.\nBoth methods can generate well-formed and grammatically correct questions.\nMoreover, we present a novel approach to automatically generate adequate\ndistractors for a given QAP. The method is a combination of part-of-speech\ntagging, named-entity tagging, semantic-role labeling, regular expressions,\ndomain knowledge bases, word embeddings, word edit distance, WordNet, and other\nalgorithms.\n","authors":["Cheng Zhang"],"pdf_url":"https://arxiv.org/pdf/2303.14576v1.pdf","comment":"thesis"},{"id":"http://arxiv.org/abs/2303.14537v1","updated":"2023-03-25T19:03:57Z","published":"2023-03-25T19:03:57Z","title":"Deep Augmentation: Enhancing Self-Supervised Learning through\n  Transformations in Higher Activation Space","summary":"  We introduce Deep Augmentation, an approach to data augmentation using\ndropout to dynamically transform a targeted layer within a neural network, with\nthe option to use the stop-gradient operation, offering significant\nimprovements in model performance and generalization. We demonstrate the\nefficacy of Deep Augmentation through extensive experiments on contrastive\nlearning tasks in computer vision and NLP domains, where we observe substantial\nperformance gains with ResNets and Transformers as the underlying models. Our\nexperimentation reveals that targeting deeper layers with Deep Augmentation\noutperforms augmenting the input data, and the simple network- and\ndata-agnostic nature of this approach enables its seamless integration into\ncomputer vision and NLP pipelines.\n","authors":["Rickard Brüel-Gabrielsson","Tongzhou Wang","Manel Baradad","Justin Solomon"],"pdf_url":"https://arxiv.org/pdf/2303.14537v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.14524v1","updated":"2023-03-25T17:37:43Z","published":"2023-03-25T17:37:43Z","title":"Chat-REC: Towards Interactive and Explainable LLMs-Augmented Recommender\n  System","summary":"  Large language models (LLMs) have demonstrated their significant potential to\nbe applied for addressing various application tasks. However, traditional\nrecommender systems continue to face great challenges such as poor\ninteractivity and explainability, which actually also hinder their broad\ndeployment in real-world systems. To address these limitations, this paper\nproposes a novel paradigm called Chat-Rec (ChatGPT Augmented Recommender\nSystem) that innovatively augments LLMs for building conversational recommender\nsystems by converting user profiles and historical interactions into prompts.\nChat-Rec is demonstrated to be effective in learning user preferences and\nestablishing connections between users and products through in-context\nlearning, which also makes the recommendation process more interactive and\nexplainable. What's more, within the Chat-Rec framework, user's preferences can\ntransfer to different products for cross-domain recommendations, and\nprompt-based injection of information into LLMs can also handle the cold-start\nscenarios with new items. In our experiments, Chat-Rec effectively improve the\nresults of top-k recommendations and performs better in zero-shot rating\nprediction task. Chat-Rec offers a novel approach to improving recommender\nsystems and presents new practical scenarios for the implementation of AIGC (AI\ngenerated content) in recommender system studies.\n","authors":["Yunfan Gao","Tao Sheng","Youlin Xiang","Yun Xiong","Haofen Wang","Jiawei Zhang"],"pdf_url":"https://arxiv.org/pdf/2303.14524v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.11525v2","updated":"2023-03-25T15:35:03Z","published":"2023-03-21T01:06:37Z","title":"Sparse Iso-FLOP Transformations for Maximizing Training Efficiency","summary":"  Recent works have explored the use of weight sparsity to improve the training\nefficiency (test accuracy w.r.t training FLOPs) of deep neural networks (DNNs).\nThese works aim to reduce training FLOPs but training with sparse weights often\nleads to accuracy loss or requires longer training schedules, making the\nresulting training efficiency less clear. In contrast, we focus on using\nsparsity to increase accuracy while using the same FLOPs as the dense model and\nshow training efficiency gains through higher accuracy. In this work, we\nintroduce Sparse-IFT, a family of Sparse Iso-FLOP Transformations which are\nused as drop-in replacements for dense layers to improve their representational\ncapacity and FLOP efficiency. Each transformation is parameterized by a single\nhyperparameter (sparsity level) and provides a larger search space to find\noptimal sparse masks. Without changing any training hyperparameters, replacing\ndense layers with Sparse-IFT leads to significant improvements across computer\nvision (CV) and natural language processing (NLP) tasks, including ResNet-18 on\nImageNet (+3.5%) and GPT-3 Small on WikiText-103 (-0.4 PPL), both matching\nlarger dense model variants that use 2x or more FLOPs. To our knowledge, this\nis the first work to demonstrate the use of sparsity for improving the accuracy\nof dense models via a simple-to-use set of sparse transformations. Code is\navailable at: https://github.com/CerebrasResearch/Sparse-IFT.\n","authors":["Shreyas Saxena","Vithursan Thangarasa","Abhay Gupta","Sean Lie"],"pdf_url":"https://arxiv.org/pdf/2303.11525v2.pdf","comment":"Code available from Cerebras Systems:\n  https://github.com/CerebrasResearch/Sparse-IFT"},{"id":"http://arxiv.org/abs/2303.14480v1","updated":"2023-03-25T14:24:50Z","published":"2023-03-25T14:24:50Z","title":"GANTEE: Generative Adversatial Network for Taxonomy Entering Evaluation","summary":"  Taxonomy is formulated as directed acyclic concepts graphs or trees that\nsupport many downstream tasks. Many new coming concepts need to be added to an\nexisting taxonomy. The traditional taxonomy expansion task aims only at finding\nthe best position for new coming concepts in the existing taxonomy. However,\nthey have two drawbacks when being applied to the real-scenarios. The previous\nmethods suffer from low-efficiency since they waste much time when most of the\nnew coming concepts are indeed noisy concepts. They also suffer from\nlow-effectiveness since they collect training samples only from the existing\ntaxonomy, which limits the ability of the model to mine more hypernym-hyponym\nrelationships among real concepts. This paper proposes a pluggable framework\ncalled Generative Adversarial Network for Taxonomy Entering Evaluation (GANTEE)\nto alleviate these drawbacks. A generative adversarial network is designed in\nthis framework by discriminative models to alleviate the first drawback and the\ngenerative model to alleviate the second drawback. Two discriminators are used\nin GANTEE to provide long-term and short-term rewards, respectively. Moreover,\nto further improve the efficiency, pre-trained language models are used to\nretrieve the representation of the concepts quickly. The experiments on three\nreal-world large-scale datasets with two different languages show that GANTEE\nimproves the performance of the existing taxonomy expansion methods in both\neffectiveness and efficiency.\n","authors":["Zhouhong Gu","Sihang Jiang","Jingping Liu","Yanghua Xiao","Hongwei Feng","Zhixu Li","Jiaqing Liang","Jian Zhong"],"pdf_url":"https://arxiv.org/pdf/2303.14480v1.pdf","comment":"Accepted by AAAI 2023"},{"id":"http://arxiv.org/abs/2303.14475v1","updated":"2023-03-25T14:08:21Z","published":"2023-03-25T14:08:21Z","title":"Informed Machine Learning, Centrality, CNN, Relevant Document Detection,\n  Repatriation of Indigenous Human Remains","summary":"  Among the pressing issues facing Australian and other First Nations peoples\nis the repatriation of the bodily remains of their ancestors, which are\ncurrently held in Western scientific institutions. The success of securing the\nreturn of these remains to their communities for reburial depends largely on\nlocating information within scientific and other literature published between\n1790 and 1970 documenting their theft, donation, sale, or exchange between\ninstitutions. This article reports on collaborative research by data scientists\nand social science researchers in the Research, Reconcile, Renew Network (RRR)\nto develop and apply text mining techniques to identify this vital information.\nWe describe our work to date on developing a machine learning-based solution to\nautomate the process of finding and semantically analysing relevant texts.\nClassification models, particularly deep learning-based models, are known to\nhave low accuracy when trained with small amounts of labelled (i.e.\nrelevant/non-relevant) documents. To improve the accuracy of our detection\nmodel, we explore the use of an Informed Neural Network (INN) model that\ndescribes documentary content using expert-informed contextual knowledge. Only\na few labelled documents are used to provide specificity to the model, using\nconceptually related keywords identified by RRR experts in provenance research.\nThe results confirm the value of using an INN network model for identifying\nrelevant documents related to the investigation of the global commercial trade\nin Indigenous human remains. Empirical analysis suggests that this INN model\ncan be generalized for use by other researchers in the social sciences and\nhumanities who want to extract relevant information from large textual corpora.\n","authors":["Md Abul Bashar","Richi Nayak","Gareth Knapman","Paul Turnbull","Cressida Fforde"],"pdf_url":"https://arxiv.org/pdf/2303.14475v1.pdf","comment":"Accepted Version of the Journal Article"},{"id":"http://arxiv.org/abs/2303.14461v1","updated":"2023-03-25T13:05:54Z","published":"2023-03-25T13:05:54Z","title":"Indian Language Summarization using Pretrained Sequence-to-Sequence\n  Models","summary":"  The ILSUM shared task focuses on text summarization for two major Indian\nlanguages- Hindi and Gujarati, along with English. In this task, we experiment\nwith various pretrained sequence-to-sequence models to find out the best model\nfor each of the languages. We present a detailed overview of the models and our\napproaches in this paper. We secure the first rank across all three sub-tasks\n(English, Hindi and Gujarati). This paper also extensively analyzes the impact\nof k-fold cross-validation while experimenting with limited data size, and we\nalso perform various experiments with a combination of the original and a\nfiltered version of the data to determine the efficacy of the pretrained\nmodels.\n","authors":["Ashok Urlana","Sahil Manoj Bhatt","Nirmal Surange","Manish Shrivastava"],"pdf_url":"https://arxiv.org/pdf/2303.14461v1.pdf","comment":"Accepted at FIRE-2022, Indian Language Summarization (ILSUM) track"},{"id":"http://arxiv.org/abs/2211.05166v3","updated":"2023-03-25T13:00:31Z","published":"2022-11-09T19:34:38Z","title":"Grammatical Error Correction: A Survey of the State of the Art","summary":"  Grammatical Error Correction (GEC) is the task of automatically detecting and\ncorrecting errors in text. The task not only includes the correction of\ngrammatical errors, such as missing prepositions and mismatched subject-verb\nagreement, but also orthographic and semantic errors, such as misspellings and\nword choice errors respectively. The field has seen significant progress in the\nlast decade, motivated in part by a series of five shared tasks, which drove\nthe development of rule-based methods, statistical classifiers, statistical\nmachine translation, and finally neural machine translation systems which\nrepresent the current dominant state of the art. In this survey paper, we\ncondense the field into a single article and first outline some of the\nlinguistic challenges of the task, introduce the most popular datasets that are\navailable to researchers (for both English and other languages), and summarise\nthe various methods and techniques that have been developed with a particular\nfocus on artificial error generation. We next describe the many different\napproaches to evaluation as well as concerns surrounding metric reliability,\nespecially in relation to subjective human judgements, before concluding with\nan overview of recent progress and suggestions for future work and remaining\nchallenges. We hope that this survey will serve as comprehensive resource for\nresearchers who are new to the field or who want to be kept apprised of recent\ndevelopments.\n","authors":["Christopher Bryant","Zheng Yuan","Muhammad Reza Qorib","Hannan Cao","Hwee Tou Ng","Ted Briscoe"],"pdf_url":"https://arxiv.org/pdf/2211.05166v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2301.10405v4","updated":"2023-03-25T12:44:35Z","published":"2023-01-25T04:45:06Z","title":"Editing Language Model-based Knowledge Graph Embeddings","summary":"  Recently decades have witnessed the empirical success of framing Knowledge\nGraph (KG) embeddings via language models. However, language model-based KG\nembeddings are usually deployed as static artifacts, which are challenging to\nmodify without re-training after deployment. To address this issue, we propose\na new task of editing language model-based KG embeddings in this paper. The\nproposed task aims to enable data-efficient and fast updates to KG embeddings\nwithout damaging the performance of the rest. We build four new datasets:\nE-FB15k237, A-FB15k237, E-WN18RR, and A-WN18RR, and evaluate several knowledge\nediting baselines demonstrating the limited ability of previous models to\nhandle the proposed challenging task. We further propose a simple yet strong\nbaseline dubbed KGEditor, which utilizes additional parametric layers of the\nhyper network to edit/add facts. Comprehensive experimental results demonstrate\nthat KGEditor can perform better when updating specific facts while not\naffecting the rest with low training resources. Code and datasets will be\navailable in https://github.com/zjunlp/PromptKG/tree/main/deltaKG.\n","authors":["Siyuan Cheng","Ningyu Zhang","Bozhong Tian","Zelin Dai","Feiyu Xiong","Wei Guo","Huajun Chen"],"pdf_url":"https://arxiv.org/pdf/2301.10405v4.pdf","comment":"Work in progress and the project website is\n  https://zjunlp.github.io/project/KGE_Editing/"},{"id":"http://arxiv.org/abs/2303.14452v1","updated":"2023-03-25T12:12:30Z","published":"2023-03-25T12:12:30Z","title":"COFFEE: A Contrastive Oracle-Free Framework for Event Extraction","summary":"  Event extraction is a complex information extraction task that involves\nextracting events from unstructured text. Prior classification-based methods\nrequire comprehensive entity annotations for joint training, while newer\ngeneration-based methods rely on heuristic templates containing oracle\ninformation such as event type, which is often unavailable in real-world\nscenarios. In this study, we consider a more realistic setting of this task,\nnamely the Oracle-Free Event Extraction (OFEE) task, where only the input\ncontext is given without any oracle information, including event type, event\nontology and trigger word. To solve this task, we propose a new framework,\ncalled COFFEE, which extracts the events solely based on the document context\nwithout referring to any oracle information. In particular, a contrastive\nselection model is introduced in COFFEE to rectify the generated triggers and\nhandle multi-event instances. The proposed COFFEE outperforms state-of-the-art\napproaches under the oracle-free setting of the event extraction task, as\nevaluated on a public event extraction benchmark ACE05.\n","authors":["Meiru Zhang","Yixuan Su","Zaiqiao Meng","Zihao Fu","Nigel Collier"],"pdf_url":"https://arxiv.org/pdf/2303.14452v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.14425v1","updated":"2023-03-25T10:19:14Z","published":"2023-03-25T10:19:14Z","title":"Sem4SAP: Synonymous Expression Mining From Open Knowledge Graph For\n  Language Model Synonym-Aware Pretraining","summary":"  The model's ability to understand synonymous expression is crucial in many\nkinds of downstream tasks. It will make the model to better understand the\nsimilarity between context, and more robust to the synonym substitution attack.\nHowever, many Pretrained Language Model (PLM) lack synonym knowledge due to\nlimitation of small-scale synsets and PLM's pretraining objectives. In this\npaper, we propose a framework called Sem4SAP to mine synsets from Open\nKnowledge Graph (Open-KG) and using the mined synsets to do synonym-aware\npretraining for language models. We propose to coarsly filter the content in\nOpen-KG and use the frequency information to better help the clustering process\nunder low-resource unsupervised conditions. We expand the mined synsets by\nmigrating core semantics between synonymous expressions.We also propose two\nnovel and effective synonym-aware pre-training methods for injecting synonym\nknowledge into PLMs.Extensive experiments demonstrate that Sem4SAP can\ndramatically outperform the original PLMs and other baselines on ten different\ntasks.\n","authors":["Zhouhong Gu","Sihang Jiang","Wenhao Huang","Jiaqing Liang","Hongwei Feng","Yanghua Xiao"],"pdf_url":"https://arxiv.org/pdf/2303.14425v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.14406v1","updated":"2023-03-25T09:04:29Z","published":"2023-03-25T09:04:29Z","title":"Natural Language Processing in Ethiopian Languages: Current State,\n  Challenges, and Opportunities","summary":"  This survey delves into the current state of natural language processing\n(NLP) for four Ethiopian languages: Amharic, Afaan Oromo, Tigrinya, and\nWolaytta. Through this paper, we identify key challenges and opportunities for\nNLP research in Ethiopia. Furthermore, we provide a centralized repository on\nGitHub that contains publicly available resources for various NLP tasks in\nthese languages. This repository can be updated periodically with contributions\nfrom other researchers. Our objective is to identify research gaps and\ndisseminate the information to NLP researchers interested in Ethiopian\nlanguages and encourage future research in this domain.\n","authors":["Atnafu Lambebo Tonja","Tadesse Destaw Belay","Israel Abebe Azime","Abinew Ali Ayele","Moges Ahmed Mehamed","Olga Kolesnikova","Seid Muhie Yimam"],"pdf_url":"https://arxiv.org/pdf/2303.14406v1.pdf","comment":"Accepted to Fourth workshop on Resources for African Indigenous\n  Languages (RAIL), EACL2023"},{"id":"http://arxiv.org/abs/2211.12054v2","updated":"2023-03-25T07:16:48Z","published":"2022-11-22T07:00:16Z","title":"Visually Grounded Commonsense Knowledge Acquisition","summary":"  Large-scale commonsense knowledge bases empower a broad range of AI\napplications, where the automatic extraction of commonsense knowledge (CKE) is\na fundamental and challenging problem. CKE from text is known for suffering\nfrom the inherent sparsity and reporting bias of commonsense in text. Visual\nperception, on the other hand, contains rich commonsense knowledge about\nreal-world entities, e.g., (person, can_hold, bottle), which can serve as\npromising sources for acquiring grounded commonsense knowledge. In this work,\nwe present CLEVER, which formulates CKE as a distantly supervised\nmulti-instance learning problem, where models learn to summarize commonsense\nrelations from a bag of images about an entity pair without any human\nannotation on image instances. To address the problem, CLEVER leverages\nvision-language pre-training models for deep understanding of each image in the\nbag, and selects informative instances from the bag to summarize commonsense\nentity relations via a novel contrastive attention mechanism. Comprehensive\nexperimental results in held-out and human evaluation show that CLEVER can\nextract commonsense knowledge in promising quality, outperforming pre-trained\nlanguage model-based methods by 3.9 AUC and 6.4 mAUC points. The predicted\ncommonsense scores show strong correlation with human judgment with a 0.78\nSpearman coefficient. Moreover, the extracted commonsense can also be grounded\ninto images with reasonable interpretability. The data and codes can be\nobtained at https://github.com/thunlp/CLEVER.\n","authors":["Yuan Yao","Tianyu Yu","Ao Zhang","Mengdi Li","Ruobing Xie","Cornelius Weber","Zhiyuan Liu","Hai-Tao Zheng","Stefan Wermter","Tat-Seng Chua","Maosong Sun"],"pdf_url":"https://arxiv.org/pdf/2211.12054v2.pdf","comment":"Accepted by AAAI 2023"},{"id":"http://arxiv.org/abs/2211.07717v2","updated":"2023-03-25T06:52:37Z","published":"2022-10-28T18:31:52Z","title":"Deep Temporal Modelling of Clinical Depression through Social Media Text","summary":"  We describe the development of a model to detect user-level clinical\ndepression based on a user's temporal social media posts. Our model uses a\nDepression Symptoms Detection (DSD) classifier, which is trained on the largest\nexisting samples of clinician annotated tweets for clinical depression\nsymptoms. We subsequently use our DSD model to extract clinically relevant\nfeatures, e.g., depression scores and their consequent temporal patterns, as\nwell as user posting activity patterns, e.g., quantifying their ``no activity''\nor ``silence.'' Furthermore, to evaluate the efficacy of these extracted\nfeatures, we create three kinds of datasets including a test dataset, from two\nexisting well-known benchmark datasets for user-level depression detection. We\nthen provide accuracy measures based on single features, baseline features and\nfeature ablation tests, at several different levels of temporal granularity.\nThe relevant data distributions and clinical depression detection related\nsettings can be exploited to draw a complete picture of the impact of different\nfeatures across our created datasets. Finally, we show that, in general, only\nsemantic oriented representation models perform well. However, clinical\nfeatures may enhance overall performance provided that the training and testing\ndistribution is similar, and there is more data in a user's timeline. The\nconsequence is that the predictive capability of depression scores increase\nsignificantly while used in a more sensitive clinical depression detection\nsettings.\n","authors":["Nawshad Farruque","Randy Goebel","Sudhakar Sivapalan","Osmar R. Zaïane"],"pdf_url":"https://arxiv.org/pdf/2211.07717v2.pdf","comment":"Typos and minor errors fixed from earlier versions and corresponding\n  PhD thesis chapter"},{"id":"http://arxiv.org/abs/2303.14375v1","updated":"2023-03-25T06:41:19Z","published":"2023-03-25T06:41:19Z","title":"Knowledge-augmented Frame Semantic Parsing with Hybrid Prompt-tuning","summary":"  Frame semantics-based approaches have been widely used in semantic parsing\ntasks and have become mainstream. It remains challenging to disambiguate frame\nrepresentations evoked by target lexical units under different contexts.\nPre-trained Language Models (PLMs) have been used in semantic parsing and\nsignificantly improve the accuracy of neural parsers. However, the PLMs-based\napproaches tend to favor collocated patterns presented in the training data,\nleading to inaccurate outcomes. The intuition here is to design a mechanism to\noptimally use knowledge captured in semantic frames in conjunction with PLMs to\ndisambiguate frames. We propose a novel Knowledge-Augmented Frame Semantic\nParsing Architecture (KAF-SPA) to enhance semantic representation by\nincorporating accurate frame knowledge into PLMs during frame semantic parsing.\nSpecifically, a Memory-based Knowledge Extraction Module (MKEM) is devised to\nselect accurate frame knowledge and construct the continuous templates in the\nhigh dimensional vector space. Moreover, we design a Task-oriented Knowledge\nProbing Module (TKPM) using hybrid prompts (in terms of continuous and discrete\nprompts) to incorporate the selected knowledge into the PLMs and adapt PLMs to\nthe tasks of frame and argument identification. Experimental results on two\npublic FrameNet datasets demonstrate that our method significantly outperforms\nstrong baselines (by more than +3$\\%$ in F1), achieving state-of-art results on\nthe current benchmark. Ablation studies verify the effectiveness of KAF-SPA.\n","authors":["Rui Zhang","Yajing Sun","Jingyuan Yang","Wei Peng"],"pdf_url":"https://arxiv.org/pdf/2303.14375v1.pdf","comment":"accepted by ICASSP 2023"},{"id":"http://arxiv.org/abs/2303.13217v2","updated":"2023-03-25T04:56:38Z","published":"2023-03-23T12:28:25Z","title":"Fairness-guided Few-shot Prompting for Large Language Models","summary":"  Large language models have demonstrated surprising ability to perform\nin-context learning, i.e., these models can be directly applied to solve\nnumerous downstream tasks by conditioning on a prompt constructed by a few\ninput-output examples. However, prior research has shown that in-context\nlearning can suffer from high instability due to variations in training\nexamples, example order, and prompt formats. Therefore, the construction of an\nappropriate prompt is essential for improving the performance of in-context\nlearning. In this paper, we revisit this problem from the view of predictive\nbias. Specifically, we introduce a metric to evaluate the predictive bias of a\nfixed prompt against labels or a given attributes. Then we empirically show\nthat prompts with higher bias always lead to unsatisfactory predictive quality.\nBased on this observation, we propose a novel search strategy based on the\ngreedy search to identify the near-optimal prompt for improving the performance\nof in-context learning. We perform comprehensive experiments with\nstate-of-the-art mainstream models such as GPT-3 on various downstream tasks.\nOur results indicate that our method can enhance the model's in-context\nlearning performance in an effective and interpretable manner.\n","authors":["Huan Ma","Changqing Zhang","Yatao Bian","Lemao Liu","Zhirui Zhang","Peilin Zhao","Shu Zhang","Huazhu Fu","Qinghua Hu","Bingzhe Wu"],"pdf_url":"https://arxiv.org/pdf/2303.13217v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2210.10260v3","updated":"2023-03-25T04:41:11Z","published":"2022-10-19T02:42:46Z","title":"End-to-End Entity Detection with Proposer and Regressor","summary":"  Named entity recognition is a traditional task in natural language\nprocessing. In particular, nested entity recognition receives extensive\nattention for the widespread existence of the nesting scenario. The latest\nresearch migrates the well-established paradigm of set prediction in object\ndetection to cope with entity nesting. However, the manual creation of query\nvectors, which fail to adapt to the rich semantic information in the context,\nlimits these approaches. An end-to-end entity detection approach with proposer\nand regressor is presented in this paper to tackle the issues. First, the\nproposer utilizes the feature pyramid network to generate high-quality entity\nproposals. Then, the regressor refines the proposals for generating the final\nprediction. The model adopts encoder-only architecture and thus obtains the\nadvantages of the richness of query semantics, high precision of entity\nlocalization, and easiness of model training. Moreover, we introduce the novel\nspatially modulated attention and progressive refinement for further\nimprovement. Extensive experiments demonstrate that our model achieves advanced\nperformance in flat and nested NER, achieving a new state-of-the-art F1 score\nof 80.74 on the GENIA dataset and 72.38 on the WeiboNER dataset.\n","authors":["Xueru Wen","Changjiang Zhou","Haotian Tang","Luguang Liang","Yu Jiang","Hong Qi"],"pdf_url":"https://arxiv.org/pdf/2210.10260v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.14342v1","updated":"2023-03-25T03:08:49Z","published":"2023-03-25T03:08:49Z","title":"An Analysis of GPT-3's Performance in Grammatical Error Correction","summary":"  GPT-3 models are very powerful, achieving high performance on a variety of\nnatural language processing tasks. However, there is a relative lack of\ndetailed published analysis on how well they perform on the task of grammatical\nerror correction (GEC). To address this, we perform experiments testing the\ncapabilities of a GPT-3 model (text-davinci-003) against major GEC benchmarks,\ncomparing the performance of several different prompts, including a comparison\nof zero-shot and few-shot settings. We analyze intriguing or problematic\noutputs encountered with different prompt formats. We report the performance of\nour best prompt on the BEA-2019 and JFLEG datasets using a combination of\nautomatic metrics and human evaluations, revealing interesting differences\nbetween the preferences of human raters and the reference-based automatic\nmetrics.\n","authors":["Steven Coyne","Keisuke Sakaguchi"],"pdf_url":"https://arxiv.org/pdf/2303.14342v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.14337v1","updated":"2023-03-25T03:03:00Z","published":"2023-03-25T03:03:00Z","title":"SmartBook: AI-Assisted Situation Report Generation","summary":"  Emerging events, such as the COVID pandemic and the Ukraine Crisis, require a\ntime-sensitive comprehensive understanding of the situation to allow for\nappropriate decision-making and effective action response. Automated generation\nof situation reports can significantly reduce the time, effort, and cost for\ndomain experts when preparing their official human-curated reports. However, AI\nresearch toward this goal has been very limited, and no successful trials have\nyet been conducted to automate such report generation. We propose SmartBook, a\nnovel task formulation targeting situation report generation, which consumes\nlarge volumes of news data to produce a structured situation report with\nmultiple hypotheses (claims) summarized and grounded with rich links to factual\nevidence. We realize SmartBook for the Ukraine-Russia crisis by automatically\ngenerating intelligence analysis reports to assist expert analysts. The\nmachine-generated reports are structured in the form of timelines, with each\ntimeline organized by major events (or chapters), corresponding strategic\nquestions (or sections) and their grounded summaries (or section content). Our\nproposed framework automatically detects real-time event-related strategic\nquestions, which are more directed than manually-crafted analyst questions,\nwhich tend to be too complex, hard to parse, vague and high-level. Results from\nthorough qualitative evaluations show that roughly 82% of the questions in\nSmartbook have strategic importance, with at least 93% of the sections in the\nreport being tactically useful. Further, experiments show that expert analysts\ntend to add more information into the SmartBook reports, with only 2.3% of the\nexisting tokens being deleted, meaning SmartBook can serve as a useful\nfoundation for analysts to build upon when creating intelligence reports.\n","authors":["Revanth Gangi Reddy","Yi R. Fung","Qi Zeng","Manling Li","Ziqi Wang","Paul Sullivan","Heng J"],"pdf_url":"https://arxiv.org/pdf/2303.14337v1.pdf","comment":"Preprint"},{"id":"http://arxiv.org/abs/2303.14334v1","updated":"2023-03-25T02:47:09Z","published":"2023-03-25T02:47:09Z","title":"The Semantic Reader Project: Augmenting Scholarly Documents through\n  AI-Powered Interactive Reading Interfaces","summary":"  Scholarly publications are key to the transfer of knowledge from scholars to\nothers. However, research papers are information-dense, and as the volume of\nthe scientific literature grows, the need for new technology to support the\nreading process grows. In contrast to the process of finding papers, which has\nbeen transformed by Internet technology, the experience of reading research\npapers has changed little in decades. The PDF format for sharing research\npapers is widely used due to its portability, but it has significant downsides\nincluding: static content, poor accessibility for low-vision readers, and\ndifficulty reading on mobile devices. This paper explores the question \"Can\nrecent advances in AI and HCI power intelligent, interactive, and accessible\nreading interfaces -- even for legacy PDFs?\" We describe the Semantic Reader\nProject, a collaborative effort across multiple institutions to explore\nautomatic creation of dynamic reading interfaces for research papers. Through\nthis project, we've developed ten research prototype interfaces and conducted\nusability studies with more than 300 participants and real-world users showing\nimproved reading experiences for scholars. We've also released a production\nreading interface for research papers that will incorporate the best features\nas they mature. We structure this paper around challenges scholars and the\npublic face when reading research papers -- Discovery, Efficiency,\nComprehension, Synthesis, and Accessibility -- and present an overview of our\nprogress and remaining open challenges.\n","authors":["Kyle Lo","Joseph Chee Chang","Andrew Head","Jonathan Bragg","Amy X. Zhang","Cassidy Trier","Chloe Anastasiades","Tal August","Russell Authur","Danielle Bragg","Erin Bransom","Isabel Cachola","Stefan Candra","Yoganand Chandrasekhar","Yen-Sung Chen","Evie Yu-Yen Cheng","Yvonne Chou","Doug Downey","Rob Evans","Raymond Fok","Fangzhou Hu","Regan Huff","Dongyeop Kang","Tae Soo Kim","Rodney Kinney","Aniket Kittur","Hyeonsu Kang","Egor Klevak","Bailey Kuehl","Michael Langan","Matt Latzke","Jaron Lochner","Kelsey MacMillan","Eric Marsh","Tyler Murray","Aakanksha Naik","Ngoc-Uyen Nguyen","Srishti Palani","Soya Park","Caroline Paulic","Napol Rachatasumrit","Smita Rao","Paul Sayre","Zejiang Shen","Pao Siangliulue","Luca Soldaini","Huy Tran","Madeleine van Zuylen","Lucy Lu Wang","Christopher Wilhelm","Caroline Wu","Jiangjiang Yang","Angele Zamarron","Marti A. Hearst","Daniel S. Weld"],"pdf_url":"https://arxiv.org/pdf/2303.14334v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.14325v1","updated":"2023-03-25T01:41:54Z","published":"2023-03-25T01:41:54Z","title":"Backdoor Attacks with Input-unique Triggers in NLP","summary":"  Backdoor attack aims at inducing neural models to make incorrect predictions\nfor poison data while keeping predictions on the clean dataset unchanged, which\ncreates a considerable threat to current natural language processing (NLP)\nsystems. Existing backdoor attacking systems face two severe issues:firstly,\nmost backdoor triggers follow a uniform and usually input-independent pattern,\ne.g., insertion of specific trigger words, synonym replacement. This\nsignificantly hinders the stealthiness of the attacking model, leading the\ntrained backdoor model being easily identified as malicious by model probes.\nSecondly, trigger-inserted poisoned sentences are usually disfluent,\nungrammatical, or even change the semantic meaning from the original sentence,\nmaking them being easily filtered in the pre-processing stage. To resolve these\ntwo issues, in this paper, we propose an input-unique backdoor attack(NURA),\nwhere we generate backdoor triggers unique to inputs. IDBA generates\ncontext-related triggers by continuing writing the input with a language model\nlike GPT2. The generated sentence is used as the backdoor trigger. This\nstrategy not only creates input-unique backdoor triggers, but also preserves\nthe semantics of the original input, simultaneously resolving the two issues\nabove. Experimental results show that the IDBA attack is effective for attack\nand difficult to defend: it achieves high attack success rate across all the\nwidely applied benchmarks, while is immune to existing defending methods. In\naddition, it is able to generate fluent, grammatical, and diverse backdoor\ninputs, which can hardly be recognized through human inspection.\n","authors":["Xukun Zhou","Jiwei Li","Tianwei Zhang","Lingjuan Lyu","Muqiao Yang","Jun He"],"pdf_url":"https://arxiv.org/pdf/2303.14325v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.06333v2","updated":"2023-03-25T01:40:34Z","published":"2023-03-11T07:30:52Z","title":"Parachute: Evaluating Interactive Human-LM Co-writing Systems","summary":"  A surge of advances in language models (LMs) has led to significant interest\nin using LMs to build co-writing systems, in which humans and LMs interactively\ncontribute to a shared writing artifact. However, there is a lack of studies\nassessing co-writing systems in interactive settings. We propose a\nhuman-centered evaluation framework, Parachute, for interactive co-writing\nsystems. Parachute showcases an integrative view of interaction evaluation,\nwhere each evaluation aspect consists of categorized practical metrics.\nFurthermore, we present Parachute with a use case to demonstrate how to\nevaluate and compare co-writing systems using Parachute.\n","authors":["Hua Shen","Tongshuang Wu"],"pdf_url":"https://arxiv.org/pdf/2303.06333v2.pdf","comment":"Accepted by CHI'23 In2Writing Workshop"},{"id":"http://arxiv.org/abs/2303.14310v1","updated":"2023-03-25T00:43:41Z","published":"2023-03-25T00:43:41Z","title":"GPT is becoming a Turing machine: Here are some ways to program it","summary":"  We demonstrate that, through appropriate prompting, GPT-3 family of models\ncan be triggered to perform iterative behaviours necessary to execute (rather\nthan just write or recall) programs that involve loops, including several\npopular algorithms found in computer science curricula or software developer\ninterviews. We trigger execution and description of Iterations by Regimenting\nSelf-Attention (IRSA) in one (or a combination) of three ways: 1) Using strong\nrepetitive structure in an example of an execution path of a target program for\none particular input, 2) Prompting with fragments of execution paths, and 3)\nExplicitly forbidding (skipping) self-attention to parts of the generated text.\nOn a dynamic program execution, IRSA leads to larger accuracy gains than\nreplacing the model with the much more powerful GPT-4. IRSA has promising\napplications in education, as the prompts and responses resemble student\nassignments in data structures and algorithms classes. Our findings hold\nimplications for evaluating LLMs, which typically target the in-context\nlearning: We show that prompts that may not even cover one full task example\ncan trigger algorithmic behaviour, allowing solving problems previously thought\nof as hard for LLMs, such as logical puzzles. Consequently, prompt design plays\nan even more critical role in LLM performance than previously recognized.\n","authors":["Ana Jojic","Zhen Wang","Nebojsa Jojic"],"pdf_url":"https://arxiv.org/pdf/2303.14310v1.pdf","comment":"25 pages, 1 figure"}],"Information Retrieval":[{"id":"http://arxiv.org/abs/2303.14524v1","updated":"2023-03-25T17:37:43Z","published":"2023-03-25T17:37:43Z","title":"Chat-REC: Towards Interactive and Explainable LLMs-Augmented Recommender\n  System","summary":"  Large language models (LLMs) have demonstrated their significant potential to\nbe applied for addressing various application tasks. However, traditional\nrecommender systems continue to face great challenges such as poor\ninteractivity and explainability, which actually also hinder their broad\ndeployment in real-world systems. To address these limitations, this paper\nproposes a novel paradigm called Chat-Rec (ChatGPT Augmented Recommender\nSystem) that innovatively augments LLMs for building conversational recommender\nsystems by converting user profiles and historical interactions into prompts.\nChat-Rec is demonstrated to be effective in learning user preferences and\nestablishing connections between users and products through in-context\nlearning, which also makes the recommendation process more interactive and\nexplainable. What's more, within the Chat-Rec framework, user's preferences can\ntransfer to different products for cross-domain recommendations, and\nprompt-based injection of information into LLMs can also handle the cold-start\nscenarios with new items. In our experiments, Chat-Rec effectively improve the\nresults of top-k recommendations and performs better in zero-shot rating\nprediction task. Chat-Rec offers a novel approach to improving recommender\nsystems and presents new practical scenarios for the implementation of AIGC (AI\ngenerated content) in recommender system studies.\n","authors":["Yunfan Gao","Tao Sheng","Youlin Xiang","Yun Xiong","Haofen Wang","Jiawei Zhang"],"pdf_url":"https://arxiv.org/pdf/2303.14524v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.14475v1","updated":"2023-03-25T14:08:21Z","published":"2023-03-25T14:08:21Z","title":"Informed Machine Learning, Centrality, CNN, Relevant Document Detection,\n  Repatriation of Indigenous Human Remains","summary":"  Among the pressing issues facing Australian and other First Nations peoples\nis the repatriation of the bodily remains of their ancestors, which are\ncurrently held in Western scientific institutions. The success of securing the\nreturn of these remains to their communities for reburial depends largely on\nlocating information within scientific and other literature published between\n1790 and 1970 documenting their theft, donation, sale, or exchange between\ninstitutions. This article reports on collaborative research by data scientists\nand social science researchers in the Research, Reconcile, Renew Network (RRR)\nto develop and apply text mining techniques to identify this vital information.\nWe describe our work to date on developing a machine learning-based solution to\nautomate the process of finding and semantically analysing relevant texts.\nClassification models, particularly deep learning-based models, are known to\nhave low accuracy when trained with small amounts of labelled (i.e.\nrelevant/non-relevant) documents. To improve the accuracy of our detection\nmodel, we explore the use of an Informed Neural Network (INN) model that\ndescribes documentary content using expert-informed contextual knowledge. Only\na few labelled documents are used to provide specificity to the model, using\nconceptually related keywords identified by RRR experts in provenance research.\nThe results confirm the value of using an INN network model for identifying\nrelevant documents related to the investigation of the global commercial trade\nin Indigenous human remains. Empirical analysis suggests that this INN model\ncan be generalized for use by other researchers in the social sciences and\nhumanities who want to extract relevant information from large textual corpora.\n","authors":["Md Abul Bashar","Richi Nayak","Gareth Knapman","Paul Turnbull","Cressida Fforde"],"pdf_url":"https://arxiv.org/pdf/2303.14475v1.pdf","comment":"Accepted Version of the Journal Article"},{"id":"http://arxiv.org/abs/2301.10405v4","updated":"2023-03-25T12:44:35Z","published":"2023-01-25T04:45:06Z","title":"Editing Language Model-based Knowledge Graph Embeddings","summary":"  Recently decades have witnessed the empirical success of framing Knowledge\nGraph (KG) embeddings via language models. However, language model-based KG\nembeddings are usually deployed as static artifacts, which are challenging to\nmodify without re-training after deployment. To address this issue, we propose\na new task of editing language model-based KG embeddings in this paper. The\nproposed task aims to enable data-efficient and fast updates to KG embeddings\nwithout damaging the performance of the rest. We build four new datasets:\nE-FB15k237, A-FB15k237, E-WN18RR, and A-WN18RR, and evaluate several knowledge\nediting baselines demonstrating the limited ability of previous models to\nhandle the proposed challenging task. We further propose a simple yet strong\nbaseline dubbed KGEditor, which utilizes additional parametric layers of the\nhyper network to edit/add facts. Comprehensive experimental results demonstrate\nthat KGEditor can perform better when updating specific facts while not\naffecting the rest with low training resources. Code and datasets will be\navailable in https://github.com/zjunlp/PromptKG/tree/main/deltaKG.\n","authors":["Siyuan Cheng","Ningyu Zhang","Bozhong Tian","Zelin Dai","Feiyu Xiong","Wei Guo","Huajun Chen"],"pdf_url":"https://arxiv.org/pdf/2301.10405v4.pdf","comment":"Work in progress and the project website is\n  https://zjunlp.github.io/project/KGE_Editing/"},{"id":"http://arxiv.org/abs/2303.14419v1","updated":"2023-03-25T10:07:59Z","published":"2023-03-25T10:07:59Z","title":"Evolution of the Online Rating Platform Data Structures and its\n  Implications for Recommender Systems","summary":"  Online rating platform represents the new trend of online cultural and\ncommercial goods consumption. The user rating data on such platforms are foods\nfor recommender system algorithms. Understanding the evolution pattern and its\nunderlying mechanism is the key to understand the structures of input data for\nrecommender systems. Prior research on input data analysis for recommender\nsystems is quite limited, with a notable exception in 2018 [6]. In this paper,\nwe take advantage of Poisson Process to analyze the evolution mechanism of the\ninput data structures. We discover that homogeneous Poisson Process could not\ncapture the mechanism of user rating behavior on online rating platforms, and\ninhomogeneous Poisson Process is compatible with the formation process.\n","authors":["Hao Wang"],"pdf_url":"https://arxiv.org/pdf/2303.14419v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.14417v1","updated":"2023-03-25T09:58:19Z","published":"2023-03-25T09:58:19Z","title":"Analysis and Visualization of the Parameter Space of Matrix\n  Factorization-based Recommender Systems","summary":"  Recommender system is the most successful commercial technology in the past\ndecade. Technical mammoth such as Temu, TikTok and Amazon utilize the\ntechnology to generate enormous revenues each year. Although there have been\nenough research literature on accuracy enhancement of the technology,\nexplainable AI is still a new idea to the field. In 2022, the author of this\npaper provides a geometric interpretation of the matrix factorization-based\nmethods and uses geometric approximation to solve the recommendation problem.\nWe continue the research in this direction in this paper, and visualize the\ninner structure of the parameter space of matrix factorization technologies. We\nshow that the parameters of matrix factorization methods are distributed within\na hyper-ball. After further analysis, we prove that the distribution of the\nparameters is not multivariate normal.\n","authors":["Hao Wang"],"pdf_url":"https://arxiv.org/pdf/2303.14417v1.pdf","comment":null}],"Multimedia":[{"id":"http://arxiv.org/abs/2303.14369v1","updated":"2023-03-25T05:47:52Z","published":"2023-03-25T05:47:52Z","title":"Video-Text as Game Players: Hierarchical Banzhaf Interaction for\n  Cross-Modal Representation Learning","summary":"  Contrastive learning-based video-language representation learning approaches,\ne.g., CLIP, have achieved outstanding performance, which pursue semantic\ninteraction upon pre-defined video-text pairs. To clarify this coarse-grained\nglobal interaction and move a step further, we have to encounter challenging\nshell-breaking interactions for fine-grained cross-modal learning. In this\npaper, we creatively model video-text as game players with multivariate\ncooperative game theory to wisely handle the uncertainty during fine-grained\nsemantic interaction with diverse granularity, flexible combination, and vague\nintensity. Concretely, we propose Hierarchical Banzhaf Interaction (HBI) to\nvalue possible correspondence between video frames and text words for sensitive\nand explainable cross-modal contrast. To efficiently realize the cooperative\ngame of multiple video frames and multiple text words, the proposed method\nclusters the original video frames (text words) and computes the Banzhaf\nInteraction between the merged tokens. By stacking token merge modules, we\nachieve cooperative games at different semantic levels. Extensive experiments\non commonly used text-video retrieval and video-question answering benchmarks\nwith superior performances justify the efficacy of our HBI. More encouragingly,\nit can also serve as a visualization tool to promote the understanding of\ncross-modal interaction, which have a far-reaching impact on the community.\nProject page is available at https://jpthu17.github.io/HBI/.\n","authors":["Peng Jin","Jinfa Huang","Pengfei Xiong","Shangxuan Tian","Chang Liu","Xiangyang Ji","Li Yuan","Jie Chen"],"pdf_url":"https://arxiv.org/pdf/2303.14369v1.pdf","comment":"CVPR 2023 Highlight"},{"id":"http://arxiv.org/abs/2303.14309v1","updated":"2023-03-25T00:43:36Z","published":"2023-03-25T00:43:36Z","title":"The Challenges of Studying Misinformation on Video-Sharing Platforms\n  During Crises and Mass-Convergence Events","summary":"  Mis- and disinformation can spread rapidly on video-sharing platforms (VSPs).\nDespite the growing use of VSPs, there has not been a proportional increase in\nour ability to understand this medium and the messages conveyed through it. In\nthis work, we draw on our prior experiences to outline three core challenges\nfaced in studying VSPs in high-stakes and fast-paced settings: (1) navigating\nthe unique affordances of VSPs, (2) understanding VSP content and determining\nits authenticity, and (3) novel user behaviors on VSPs for spreading\nmisinformation. By highlighting these challenges, we hope that researchers can\nreflect on how to adapt existing research methods and tools to these new\ncontexts, or develop entirely new ones.\n","authors":["Sukrit Venkatagiri","Joseph S. Schafer","Stephen Prochaska"],"pdf_url":"https://arxiv.org/pdf/2303.14309v1.pdf","comment":"Accepted to the ACM CHI 2023 Workshop on Building Credibility, Trust,\n  and Safety on Video-Sharing Platforms (https://safevsp.github.io/)"}]},"2023-03-28T00:00:00Z":{"Computation and Language":[{"id":"http://arxiv.org/abs/2303.16199v1","updated":"2023-03-28T17:59:12Z","published":"2023-03-28T17:59:12Z","title":"LLaMA-Adapter: Efficient Fine-tuning of Language Models with Zero-init\n  Attention","summary":"  We present LLaMA-Adapter, a lightweight adaption method to efficiently\nfine-tune LLaMA into an instruction-following model. Using 52K self-instruct\ndemonstrations, LLaMA-Adapter only introduces 1.2M learnable parameters upon\nthe frozen LLaMA 7B model, and costs less than one hour for fine-tuning on 8\nA100 GPUs. Specifically, we adopt a set of learnable adaption prompts, and\nprepend them to the input text tokens at higher transformer layers. Then, a\nzero-init attention mechanism with zero gating is proposed, which adaptively\ninjects the new instructional cues into LLaMA, while effectively preserves its\npre-trained knowledge. With efficient training, LLaMA-Adapter generates\nhigh-quality responses, comparable to Alpaca with fully fine-tuned 7B\nparameters. Furthermore, our approach can be simply extended to multi-modal\ninput, e.g., images, for image-conditioned LLaMA, which achieves superior\nreasoning capacity on ScienceQA. We release our code at\nhttps://github.com/ZrrSkywalker/LLaMA-Adapter.\n","authors":["Renrui Zhang","Jiaming Han","Aojun Zhou","Xiangfei Hu","Shilin Yan","Pan Lu","Hongsheng Li","Peng Gao","Yu Qiao"],"pdf_url":"https://arxiv.org/pdf/2303.16199v1.pdf","comment":"Work in Progress. Code is available at\n  https://github.com/ZrrSkywalker/LLaMA-Adapter"},{"id":"http://arxiv.org/abs/2303.03487v2","updated":"2023-03-28T17:55:52Z","published":"2023-03-06T20:35:51Z","title":"Two-stage Pipeline for Multilingual Dialect Detection","summary":"  Dialect Identification is a crucial task for localizing various Large\nLanguage Models. This paper outlines our approach to the VarDial 2023 shared\ntask. Here we have to identify three or two dialects from three languages each\nwhich results in a 9-way classification for Track-1 and 6-way classification\nfor Track-2 respectively. Our proposed approach consists of a two-stage system\nand outperforms other participants' systems and previous works in this domain.\nWe achieve a score of 58.54% for Track-1 and 85.61% for Track-2. Our codebase\nis available publicly (https://github.com/ankit-vaidya19/EACL_VarDial2023).\n","authors":["Ankit Vaidya","Aditya Kane"],"pdf_url":"https://arxiv.org/pdf/2303.03487v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.16173v1","updated":"2023-03-28T17:34:59Z","published":"2023-03-28T17:34:59Z","title":"Towards Countering Essentialism through Social Bias Reasoning","summary":"  Essentialist beliefs (i.e., believing that members of the same group are\nfundamentally alike) play a central role in social stereotypes and can lead to\nharm when left unchallenged. In our work, we conduct exploratory studies into\nthe task of countering essentialist beliefs (e.g., ``liberals are stupid'').\nDrawing on prior work from psychology and NLP, we construct five types of\ncounterstatements and conduct human studies on the effectiveness of these\ndifferent strategies. Our studies also investigate the role in choosing a\ncounterstatement of the level of explicitness with which an essentialist belief\nis conveyed. We find that statements that broaden the scope of a stereotype\n(e.g., to other groups, as in ``conservatives can also be stupid'') are the\nmost popular countering strategy. We conclude with a discussion of challenges\nand open questions for future work in this area (e.g., improving factuality,\nstudying community-specific variation) and we emphasize the importance of work\nat the intersection of NLP and psychology.\n","authors":["Emily Allaway","Nina Taneja","Sarah-Jane Leslie","Maarten Sap"],"pdf_url":"https://arxiv.org/pdf/2303.16173v1.pdf","comment":"Workshop on NLP for Positive Impact @ EMNLP 2022"},{"id":"http://arxiv.org/abs/2303.16166v1","updated":"2023-03-28T17:28:52Z","published":"2023-03-28T17:28:52Z","title":"Reproducibility is Nothing without Correctness: The Importance of\n  Testing Code in NLP","summary":"  Despite its pivotal role in research experiments, code correctness is often\npresumed only on the basis of the perceived quality of the results. This comes\nwith the risk of erroneous outcomes and potentially misleading findings. To\naddress this issue, we posit that the current focus on result reproducibility\nshould go hand in hand with the emphasis on coding best practices. We bolster\nour call to the NLP community by presenting a case study, in which we identify\n(and correct) three bugs in widely used open-source implementations of the\nstate-of-the-art Conformer architecture. Through comparative experiments on\nautomatic speech recognition and translation in various language settings, we\ndemonstrate that the existence of bugs does not prevent the achievement of good\nand reproducible results and can lead to incorrect conclusions that potentially\nmisguide future research. In response to this, this study is a call to action\ntoward the adoption of coding best practices aimed at fostering correctness and\nimproving the quality of the developed software.\n","authors":["Sara Papi","Marco Gaido","Matteo Negri","Andrea Pilzer"],"pdf_url":"https://arxiv.org/pdf/2303.16166v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.16133v1","updated":"2023-03-28T16:57:12Z","published":"2023-03-28T16:57:12Z","title":"Exposing and Addressing Cross-Task Inconsistency in Unified\n  Vision-Language Models","summary":"  As general purpose vision models get increasingly effective at a wide set of\ntasks, it is imperative that they be consistent across the tasks they support.\nInconsistent AI models are considered brittle and untrustworthy by human users\nand are more challenging to incorporate into larger systems that take\ndependencies on their outputs. Measuring consistency between very heterogeneous\ntasks that might include outputs in different modalities is challenging since\nit is difficult to determine if the predictions are consistent with one\nanother. As a solution, we introduce a benchmark dataset, COCOCON, where we use\ncontrast sets created by modifying test instances for multiple tasks in small\nbut semantically meaningful ways to change the gold label, and outline metrics\nfor measuring if a model is consistent by ranking the original and perturbed\ninstances across tasks. We find that state-of-the-art systems suffer from a\nsurprisingly high degree of inconsistent behavior across tasks, especially for\nmore heterogeneous tasks. Finally, we propose using a rank correlation-based\nauxiliary objective computed over large automatically created cross-task\ncontrast sets to improve the multi-task consistency of large unified models,\nwhile retaining their original accuracy on downstream tasks. Project website\navailable at https://adymaharana.github.io/cococon/\n","authors":["Adyasha Maharana","Amita Kamath","Christopher Clark","Mohit Bansal","Aniruddha Kembhavi"],"pdf_url":"https://arxiv.org/pdf/2303.16133v1.pdf","comment":"Project Website: https://adymaharana.github.io/cococon/"},{"id":"http://arxiv.org/abs/2303.16104v1","updated":"2023-03-28T16:17:59Z","published":"2023-03-28T16:17:59Z","title":"Hallucinations in Large Multilingual Translation Models","summary":"  Large-scale multilingual machine translation systems have demonstrated\nremarkable ability to translate directly between numerous languages, making\nthem increasingly appealing for real-world applications. However, when deployed\nin the wild, these models may generate hallucinated translations which have the\npotential to severely undermine user trust and raise safety concerns. Existing\nresearch on hallucinations has primarily focused on small bilingual models\ntrained on high-resource languages, leaving a gap in our understanding of\nhallucinations in massively multilingual models across diverse translation\nscenarios. In this work, we fill this gap by conducting a comprehensive\nanalysis on both the M2M family of conventional neural machine translation\nmodels and ChatGPT, a general-purpose large language model~(LLM) that can be\nprompted for translation. Our investigation covers a broad spectrum of\nconditions, spanning over 100 translation directions across various resource\nlevels and going beyond English-centric language pairs. We provide key insights\nregarding the prevalence, properties, and mitigation of hallucinations, paving\nthe way towards more responsible and reliable machine translation systems.\n","authors":["Nuno M. Guerreiro","Duarte Alves","Jonas Waldendorf","Barry Haddow","Alexandra Birch","Pierre Colombo","André F. T. Martins"],"pdf_url":"https://arxiv.org/pdf/2303.16104v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.16098v1","updated":"2023-03-28T16:09:40Z","published":"2023-03-28T16:09:40Z","title":"Carolina: a General Corpus of Contemporary Brazilian Portuguese with\n  Provenance, Typology and Versioning Information","summary":"  This paper presents the first publicly available version of the Carolina\nCorpus and discusses its future directions. Carolina is a large open corpus of\nBrazilian Portuguese texts under construction using web-as-corpus methodology\nenhanced with provenance, typology, versioning, and text integrality. The\ncorpus aims at being used both as a reliable source for research in Linguistics\nand as an important resource for Computer Science research on language models,\ncontributing towards removing Portuguese from the set of low-resource\nlanguages. Here we present the construction of the corpus methodology,\ncomparing it with other existing methodologies, as well as the corpus current\nstate: Carolina's first public version has $653,322,577$ tokens, distributed\nover $7$ broad types. Each text is annotated with several different metadata\ncategories in its header, which we developed using TEI annotation standards. We\nalso present ongoing derivative works and invite NLP researchers to contribute\nwith their own.\n","authors":["Maria Clara Ramos Morales Crespo","Maria Lina de Souza Jeannine Rocha","Mariana Lourenço Sturzeneker","Felipe Ribas Serras","Guilherme Lamartine de Mello","Aline Silva Costa","Mayara Feliciano Palma","Renata Morais Mesquita","Raquel de Paula Guets","Mariana Marques da Silva","Marcelo Finger","Maria Clara Paixão de Sousa","Cristiane Namiuti","Vanessa Martins do Monte"],"pdf_url":"https://arxiv.org/pdf/2303.16098v1.pdf","comment":"14 pages, 3 figures, 1 appendix"},{"id":"http://arxiv.org/abs/2303.14337v2","updated":"2023-03-28T15:58:44Z","published":"2023-03-25T03:03:00Z","title":"SmartBook: AI-Assisted Situation Report Generation","summary":"  Emerging events, such as the COVID pandemic and the Ukraine Crisis, require a\ntime-sensitive comprehensive understanding of the situation to allow for\nappropriate decision-making and effective action response. Automated generation\nof situation reports can significantly reduce the time, effort, and cost for\ndomain experts when preparing their official human-curated reports. However, AI\nresearch toward this goal has been very limited, and no successful trials have\nyet been conducted to automate such report generation. We propose SmartBook, a\nnovel task formulation targeting situation report generation, which consumes\nlarge volumes of news data to produce a structured situation report with\nmultiple hypotheses (claims) summarized and grounded with rich links to factual\nevidence. We realize SmartBook for the Ukraine-Russia crisis by automatically\ngenerating intelligence analysis reports to assist expert analysts. The\nmachine-generated reports are structured in the form of timelines, with each\ntimeline organized by major events (or chapters), corresponding strategic\nquestions (or sections) and their grounded summaries (or section content). Our\nproposed framework automatically detects real-time event-related strategic\nquestions, which are more directed than manually-crafted analyst questions,\nwhich tend to be too complex, hard to parse, vague and high-level. Results from\nthorough qualitative evaluations show that roughly 82% of the questions in\nSmartbook have strategic importance, with at least 93% of the sections in the\nreport being tactically useful. Further, experiments show that expert analysts\ntend to add more information into the SmartBook reports, with only 2.3% of the\nexisting tokens being deleted, meaning SmartBook can serve as a useful\nfoundation for analysts to build upon when creating intelligence reports.\n","authors":["Revanth Gangi Reddy","Yi R. Fung","Qi Zeng","Manling Li","Ziqi Wang","Paul Sullivan","Heng Ji"],"pdf_url":"https://arxiv.org/pdf/2303.14337v2.pdf","comment":"Preprint"},{"id":"http://arxiv.org/abs/2303.15078v2","updated":"2023-03-28T15:25:19Z","published":"2023-03-27T10:40:59Z","title":"Large Language Models are Diverse Role-Players for Summarization\n  Evaluation","summary":"  Text summarization has a wide range of applications in many scenarios. The\nevaluation of the quality of the generated text is a complex problem. A big\nchallenge to language evaluation is that there is a clear divergence between\nexisting metrics and human evaluation. For example, the quality of a document\nsummary can be measured by human annotators from both objective aspects, such\nas grammatical and semantic correctness, as well as subjective dimensions, such\nas comprehensiveness, succinctness, and interestingness. Most of the automatic\nevaluation methods like BLUE/ROUGE may be not able to capture the above\ndimensions well. In this paper, we propose a new evaluation framework based on\nLLMs, which provides a comprehensive evaluation framework by comparing\ngenerated text and reference text from both objective and subjective aspects.\nFirst, we propose to model objective and subjective dimensions of generated\ntext based on roleplayers prompting mechanism. Furthermore, we introduce a\ncontext-based prompting mechanism that is able to generate dynamic roleplayer\nprofiles based on input context. Finally, we design a multi-roleplayer\nprompting technology based on batch prompting to integrate multiple evaluation\nresults into evaluation results. Experimental results on two real datasets for\nsummarization show that our model is highly competitive and has a very high\nconsistency with human annotators.\n","authors":["Ning Wu","Ming Gong","Linjun Shou","Shining Liang","Daxin Jiang"],"pdf_url":"https://arxiv.org/pdf/2303.15078v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2301.02057v3","updated":"2023-03-28T14:57:59Z","published":"2023-01-05T13:19:17Z","title":"TextDescriptives: A Python package for calculating a large variety of\n  metrics from text","summary":"  TextDescriptives is a Python package for calculating a large variety of\nmetrics from text. It is built on top of spaCy and can be easily integrated\ninto existing workflows. The package has already been used for analysing the\nlinguistic stability of clinical texts, creating features for predicting\nneuropsychiatric conditions, and analysing linguistic goals of primary school\nstudents. This paper describes the package and its features.\n","authors":["Lasse Hansen","Ludvig Renbo Olsen","Kenneth Enevoldsen"],"pdf_url":"https://arxiv.org/pdf/2301.02057v3.pdf","comment":"3 pages, 0 figures. Submitted to Journal of Open Source Software"},{"id":"http://arxiv.org/abs/2303.16028v1","updated":"2023-03-28T14:55:13Z","published":"2023-03-28T14:55:13Z","title":"Synthetically generated text for supervised text analysis","summary":"  Supervised text models are a valuable tool for political scientists but\npresent several obstacles to their use, including the expense of hand-labeling\ndocuments, the difficulty of retrieving rare relevant documents for annotation,\nand copyright and privacy concerns involved in sharing annotated documents.\nThis article proposes a partial solution to these three issues, in the form of\ncontrolled generation of synthetic text with large language models. I provide a\nconceptual overview of text generation, guidance on when researchers should\nprefer different techniques for generating synthetic text, a discussion of\nethics, and a simple technique for improving the quality of synthetic text. I\ndemonstrate the usefulness of synthetic text with three applications:\ngenerating synthetic tweets describing the fighting in Ukraine, synthetic news\narticles describing specified political events for training an event detection\nsystem, and a multilingual corpus of populist manifesto statements for training\na sentence-level populism classifier.\n","authors":["Andrew Halterman"],"pdf_url":"https://arxiv.org/pdf/2303.16028v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.15987v1","updated":"2023-03-28T14:02:42Z","published":"2023-03-28T14:02:42Z","title":"An Experimental Study on Sentiment Classification of Moroccan dialect\n  texts in the web","summary":"  With the rapid growth of the use of social media websites, obtaining the\nusers' feedback automatically became a crucial task to evaluate their\ntendencies and behaviors online. Despite this great availability of\ninformation, and the increasing number of Arabic users only few research has\nmanaged to treat Arabic dialects. The purpose of this paper is to study the\nopinion and emotion expressed in real Moroccan texts precisely in the YouTube\ncomments using some well-known and commonly used methods for sentiment\nanalysis. In this paper, we present our work of Moroccan dialect comments\nclassification using Machine Learning (ML) models and based on our collected\nand manually annotated YouTube Moroccan dialect dataset. By employing many text\npreprocessing and data representation techniques we aim to compare our\nclassification results utilizing the most commonly used supervised classifiers:\nk-nearest neighbors (KNN), Support Vector Machine (SVM), Naive Bayes (NB), and\ndeep learning (DL) classifiers such as Convolutional Neural Network (CNN) and\nLong Short-Term Memory (LTSM). Experiments were performed using both raw and\npreprocessed data to show the importance of the preprocessing. In fact, the\nexperimental results prove that DL models have a better performance for\nMoroccan Dialect than classical approaches and we achieved an accuracy of 90%.\n","authors":["Mouad Jbel","Imad Hafidi","Abdulmutallib Metrane"],"pdf_url":"https://arxiv.org/pdf/2303.15987v1.pdf","comment":"13 pages, 5 tables, 2 figures"},{"id":"http://arxiv.org/abs/2303.12314v2","updated":"2023-03-28T13:56:07Z","published":"2023-03-22T05:04:21Z","title":"Meta-augmented Prompt Tuning for Better Few-shot Learning","summary":"  Prompt tuning is a parameter-efficient method, which freezes all PLM\nparameters and only prepends some additional tunable tokens called soft prompts\nto the input text. However, soft prompts heavily rely on a better\ninitialization and may easily result in overfitting under few-shot settings,\nwhich causes prompt-tuning performing much worse than fine-tuning. To address\nthe above issues, this paper proposes a novel Self-sUpervised Meta-prompt\nlearning framework with MEtagradient Regularization for few shot generalization\n(SUMMER). We leverage self-supervised meta-learning to better initialize soft\nprompts and curriculum-based task augmentation is further proposed to enrich\nthe meta-task distribution. Besides, a novel meta-gradient regularization\nmethod is integrated into the meta-prompt learning framework, which meta-learns\nto transform the raw gradient during few-shot learning into a\ndomain-generalizable direction, thus alleviating the problem of overfitting.\nExtensive experiments show that SUMMER achieves better performance for\ndifferent few-shot downstream tasks, and also exhibits a stronger domain\ngeneralization ability.\n","authors":["Kaihang Pan","Juncheng Li","Hongye Song","Jun Lin","Xiaozhong Liu","Siliang Tang"],"pdf_url":"https://arxiv.org/pdf/2303.12314v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.15973v1","updated":"2023-03-28T13:45:39Z","published":"2023-03-28T13:45:39Z","title":"Do Neural Topic Models Really Need Dropout? Analysis of the Effect of\n  Dropout in Topic Modeling","summary":"  Dropout is a widely used regularization trick to resolve the overfitting\nissue in large feedforward neural networks trained on a small dataset, which\nperforms poorly on the held-out test subset. Although the effectiveness of this\nregularization trick has been extensively studied for convolutional neural\nnetworks, there is a lack of analysis of it for unsupervised models and in\nparticular, VAE-based neural topic models. In this paper, we have analyzed the\nconsequences of dropout in the encoder as well as in the decoder of the VAE\narchitecture in three widely used neural topic models, namely, contextualized\ntopic model (CTM), ProdLDA, and embedded topic model (ETM) using four publicly\navailable datasets. We characterize the dropout effect on these models in terms\nof the quality and predictive performance of the generated topics.\n","authors":["Suman Adhya","Avishek Lahiri","Debarshi Kumar Sanyal"],"pdf_url":"https://arxiv.org/pdf/2303.15973v1.pdf","comment":"Accepted at EACL 2023"},{"id":"http://arxiv.org/abs/2302.07027v3","updated":"2023-03-28T13:37:54Z","published":"2023-02-14T13:09:23Z","title":"AdapterSoup: Weight Averaging to Improve Generalization of Pretrained\n  Language Models","summary":"  Pretrained language models (PLMs) are trained on massive corpora, but often\nneed to specialize to specific domains. A parameter-efficient adaptation method\nsuggests training an adapter for each domain on the task of language modeling.\nThis leads to good in-domain scores but can be impractical for domain- or\nresource-restricted settings. A solution is to use a related-domain adapter for\nthe novel domain at test time. In this paper, we introduce AdapterSoup, an\napproach that performs weight-space averaging of adapters trained on different\ndomains. Our approach is embarrassingly parallel: first, we train a set of\ndomain-specific adapters; then, for each novel domain, we determine which\nadapters should be averaged at test time. We present extensive experiments\nshowing that AdapterSoup consistently improves performance to new domains\nwithout extra training. We also explore weight averaging of adapters trained on\nthe same domain with different hyper-parameters, and show that it preserves the\nperformance of a PLM on new domains while obtaining strong in-domain results.\nWe explore various approaches for choosing which adapters to combine, such as\ntext clustering and semantic similarity. We find that using clustering leads to\nthe most competitive results on novel domains.\n","authors":["Alexandra Chronopoulou","Matthew E. Peters","Alexander Fraser","Jesse Dodge"],"pdf_url":"https://arxiv.org/pdf/2302.07027v3.pdf","comment":"Accepted at EACL 2023; camera-ready version; fixed typo in related\n  work"},{"id":"http://arxiv.org/abs/2302.00412v2","updated":"2023-03-28T12:01:21Z","published":"2023-02-01T12:53:31Z","title":"KNNs of Semantic Encodings for Rating Prediction","summary":"  This paper explores a novel application of textual semantic similarity to\nuser-preference representation for rating prediction. The approach represents a\nuser's preferences as a graph of textual snippets from review text, where the\nedges are defined by semantic similarity. This textual, memory-based approach\nto rating prediction enables review-based explanations for recommendations. The\nmethod is evaluated quantitatively, highlighting that leveraging text in this\nway outperforms both strong memory-based and model-based collaborative\nfiltering baselines.\n","authors":["Léo Laugier","Raghuram Vadapalli","Thomas Bonald","Lucas Dixon"],"pdf_url":"https://arxiv.org/pdf/2302.00412v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2210.16771v2","updated":"2023-03-28T11:36:36Z","published":"2022-10-30T08:29:20Z","title":"Parameter-Efficient Tuning Makes a Good Classification Head","summary":"  In recent years, pretrained models revolutionized the paradigm of natural\nlanguage understanding (NLU), where we append a randomly initialized\nclassification head after the pretrained backbone, e.g. BERT, and finetune the\nwhole model. As the pretrained backbone makes a major contribution to the\nimprovement, we naturally expect a good pretrained classification head can also\nbenefit the training. However, the final-layer output of the backbone, i.e. the\ninput of the classification head, will change greatly during finetuning, making\nthe usual head-only pretraining (LP-FT) ineffective. In this paper, we find\nthat parameter-efficient tuning makes a good classification head, with which we\ncan simply replace the randomly initialized heads for a stable performance\ngain. Our experiments demonstrate that the classification head jointly\npretrained with parameter-efficient tuning consistently improves the\nperformance on 9 tasks in GLUE and SuperGLUE.\n","authors":["Zhuoyi Yang","Ming Ding","Yanhui Guo","Qingsong Lv","Jie Tang"],"pdf_url":"https://arxiv.org/pdf/2210.16771v2.pdf","comment":"Accepted as a long paper to EMNLP 2022 Main Conference"},{"id":"http://arxiv.org/abs/2211.09699v3","updated":"2023-03-28T11:14:23Z","published":"2022-11-15T19:07:53Z","title":"PromptCap: Prompt-Guided Task-Aware Image Captioning","summary":"  Knowledge-based visual question answering (VQA) involves questions that\nrequire world knowledge beyond the image to yield the correct answer. Large\nlanguage models (LMs) like GPT-3 are particularly helpful for this task because\nof their strong knowledge retrieval and reasoning capabilities. To enable LM to\nunderstand images, prior work uses a captioning model to convert images into\ntext. However, when summarizing an image in a single caption sentence, which\nvisual entities to describe are often underspecified. Generic image captions\noften miss visual details essential for the LM to answer visual questions\ncorrectly. To address this challenge, we propose PromptCap (Prompt-guided image\nCaptioning), a captioning model designed to serve as a better connector between\nimages and black-box LMs. Different from generic captions, PromptCap takes a\nnatural-language prompt to control the visual entities to describe in the\ngenerated caption. The prompt contains a question that the caption should aid\nin answering. To avoid extra annotation, PromptCap is trained by examples\nsynthesized with GPT-3 and existing datasets. We demonstrate PromptCap's\neffectiveness on an existing pipeline in which GPT-3 is prompted with image\ncaptions to carry out VQA. PromptCap outperforms generic captions by a large\nmargin and achieves state-of-the-art accuracy on knowledge-based VQA tasks\n(60.4% on OK-VQA and 59.6% on A-OKVQA). Zero-shot results on WebQA show that\nPromptCap generalizes well to unseen domains.\n","authors":["Yushi Hu","Hang Hua","Zhengyuan Yang","Weijia Shi","Noah A. Smith","Jiebo Luo"],"pdf_url":"https://arxiv.org/pdf/2211.09699v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.15870v1","updated":"2023-03-28T10:25:17Z","published":"2023-03-28T10:25:17Z","title":"A Multi-Granularity Matching Attention Network for Query Intent\n  Classification in E-commerce Retrieval","summary":"  Query intent classification, which aims at assisting customers to find\ndesired products, has become an essential component of the e-commerce search.\nExisting query intent classification models either design more exquisite models\nto enhance the representation learning of queries or explore label-graph and\nmulti-task to facilitate models to learn external information. However, these\nmodels cannot capture multi-granularity matching features from queries and\ncategories, which makes them hard to mitigate the gap in the expression between\ninformal queries and categories.\n  This paper proposes a Multi-granularity Matching Attention Network (MMAN),\nwhich contains three modules: a self-matching module, a char-level matching\nmodule, and a semantic-level matching module to comprehensively extract\nfeatures from the query and a query-category interaction matrix. In this way,\nthe model can eliminate the difference in expression between queries and\ncategories for query intent classification. We conduct extensive offline and\nonline A/B experiments, and the results show that the MMAN significantly\noutperforms the strong baselines, which shows the superiority and effectiveness\nof MMAN. MMAN has been deployed in production and brings great commercial value\nfor our company.\n","authors":["Chunyuan Yuan","Yiming Qiu","Mingming Li","Haiqing Hu","Songlin Wang","Sulong Xu"],"pdf_url":"https://arxiv.org/pdf/2303.15870v1.pdf","comment":"Accepted by WWW 2023"},{"id":"http://arxiv.org/abs/2303.13284v3","updated":"2023-03-28T09:48:50Z","published":"2023-03-23T14:06:26Z","title":"GETT-QA: Graph Embedding based T2T Transformer for Knowledge Graph\n  Question Answering","summary":"  In this work, we present an end-to-end Knowledge Graph Question Answering\n(KGQA) system named GETT-QA. GETT-QA uses T5, a popular text-to-text\npre-trained language model. The model takes a question in natural language as\ninput and produces a simpler form of the intended SPARQL query. In the simpler\nform, the model does not directly produce entity and relation IDs. Instead, it\nproduces corresponding entity and relation labels. The labels are grounded to\nKG entity and relation IDs in a subsequent step. To further improve the\nresults, we instruct the model to produce a truncated version of the KG\nembedding for each entity. The truncated KG embedding enables a finer search\nfor disambiguation purposes. We find that T5 is able to learn the truncated KG\nembeddings without any change of loss function, improving KGQA performance. As\na result, we report strong results for LC-QuAD 2.0 and SimpleQuestions-Wikidata\ndatasets on end-to-end KGQA over Wikidata.\n","authors":["Debayan Banerjee","Pranav Ajit Nair","Ricardo Usbeck","Chris Biemann"],"pdf_url":"https://arxiv.org/pdf/2303.13284v3.pdf","comment":"16 pages single column format accepted at ESWC 2023 research track"},{"id":"http://arxiv.org/abs/2303.13351v2","updated":"2023-03-28T09:47:57Z","published":"2023-03-23T15:29:21Z","title":"DBLP-QuAD: A Question Answering Dataset over the DBLP Scholarly\n  Knowledge Graph","summary":"  In this work we create a question answering dataset over the DBLP scholarly\nknowledge graph (KG). DBLP is an on-line reference for bibliographic\ninformation on major computer science publications that indexes over 4.4\nmillion publications published by more than 2.2 million authors. Our dataset\nconsists of 10,000 question answer pairs with the corresponding SPARQL queries\nwhich can be executed over the DBLP KG to fetch the correct answer. DBLP-QuAD\nis the largest scholarly question answering dataset.\n","authors":["Debayan Banerjee","Sushil Awale","Ricardo Usbeck","Chris Biemann"],"pdf_url":"https://arxiv.org/pdf/2303.13351v2.pdf","comment":"12 pages ceur-ws 1 column accepted at International Bibliometric\n  Information Retrieval Workshp @ ECIR 2023"},{"id":"http://arxiv.org/abs/2303.15846v1","updated":"2023-03-28T09:36:53Z","published":"2023-03-28T09:36:53Z","title":"Soft-prompt tuning to predict lung cancer using primary care free-text\n  Dutch medical notes","summary":"  We investigate different natural language processing (NLP) approaches based\non contextualised word representations for the problem of early prediction of\nlung cancer using free-text patient medical notes of Dutch primary care\nphysicians. Because lung cancer has a low prevalence in primary care, we also\naddress the problem of classification under highly imbalanced classes.\nSpecifically, we use large Transformer-based pretrained language models (PLMs)\nand investigate: 1) how \\textit{soft prompt-tuning} -- an NLP technique used to\nadapt PLMs using small amounts of training data -- compares to standard model\nfine-tuning; 2) whether simpler static word embedding models (WEMs) can be more\nrobust compared to PLMs in highly imbalanced settings; and 3) how models fare\nwhen trained on notes from a small number of patients. We find that 1)\nsoft-prompt tuning is an efficient alternative to standard model fine-tuning;\n2) PLMs show better discrimination but worse calibration compared to simpler\nstatic word embedding models as the classification problem becomes more\nimbalanced; and 3) results when training models on small number of patients are\nmixed and show no clear differences between PLMs and WEMs. All our code is\navailable open source in\n\\url{https://bitbucket.org/aumc-kik/prompt_tuning_cancer_prediction/}.\n","authors":["Auke Elfrink","Iacopo Vagliano","Ameen Abu-Hanna","Iacer Calixto"],"pdf_url":"https://arxiv.org/pdf/2303.15846v1.pdf","comment":"A short version of this paper has been published at the 21st\n  International Conference on Artificial Intelligence in Medicine (AIME 2023)"},{"id":"http://arxiv.org/abs/2112.11479v3","updated":"2023-03-28T08:38:01Z","published":"2021-12-10T20:01:44Z","title":"AtteSTNet -- An attention and subword tokenization based approach for\n  code-switched text hate speech detection","summary":"  Recent advancements in technology have led to a boost in social media usage\nwhich has ultimately led to large amounts of user-generated data which also\nincludes hateful and offensive speech. The language used in social media is\noften a combination of English and the native language in the region. In India,\nHindi is used predominantly and is often code-switched with English, giving\nrise to the Hinglish (Hindi+English) language. Various approaches have been\nmade in the past to classify the code-mixed Hinglish hate speech using\ndifferent machine learning and deep learning-based techniques. However, these\ntechniques make use of recurrence on convolution mechanisms which are\ncomputationally expensive and have high memory requirements. Past techniques\nalso make use of complex data processing making the existing techniques very\ncomplex and non-sustainable to change in data. Proposed work gives a much\nsimpler approach which is not only at par with these complex networks but also\nexceeds performance with the use of subword tokenization algorithms like BPE\nand Unigram, along with multi-head attention-based techniques, giving an\naccuracy of 87.41% and an F1 score of 0.851 on standard datasets. Efficient use\nof BPE and Unigram algorithms help handle the nonconventional Hinglish\nvocabulary making the proposed technique simple, efficient and sustainable to\nuse in the real world.\n","authors":["Geet Shingi","Vedangi Wagh","Kishor Wagh","Sharmila Wagh"],"pdf_url":"https://arxiv.org/pdf/2112.11479v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.15727v1","updated":"2023-03-28T04:47:43Z","published":"2023-03-28T04:47:43Z","title":"Evaluation of ChatGPT for NLP-based Mental Health Applications","summary":"  Large language models (LLM) have been successful in several natural language\nunderstanding tasks and could be relevant for natural language processing\n(NLP)-based mental health application research. In this work, we report the\nperformance of LLM-based ChatGPT (with gpt-3.5-turbo backend) in three\ntext-based mental health classification tasks: stress detection (2-class\nclassification), depression detection (2-class classification), and suicidality\ndetection (5-class classification). We obtained annotated social media posts\nfor the three classification tasks from public datasets. Then ChatGPT API\nclassified the social media posts with an input prompt for classification. We\nobtained F1 scores of 0.73, 0.86, and 0.37 for stress detection, depression\ndetection, and suicidality detection, respectively. A baseline model that\nalways predicted the dominant class resulted in F1 scores of 0.35, 0.60, and\n0.19. The zero-shot classification accuracy obtained with ChatGPT indicates a\npotential use of language models for mental health classification tasks.\n","authors":["Bishal Lamichhane"],"pdf_url":"https://arxiv.org/pdf/2303.15727v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2212.06373v3","updated":"2023-03-28T03:55:11Z","published":"2022-12-13T05:12:40Z","title":"InferEM: Inferring the Speaker's Intention for Empathetic Dialogue\n  Generation","summary":"  Current approaches to empathetic response generation typically encode the\nentire dialogue history directly and put the output into a decoder to generate\nfriendly feedback. These methods focus on modelling contextual information but\nneglect capturing the direct intention of the speaker. We argue that the last\nutterance in the dialogue empirically conveys the intention of the speaker.\nConsequently, we propose a novel model named InferEM for empathetic response\ngeneration. We separately encode the last utterance and fuse it with the entire\ndialogue through the multi-head attention based intention fusion module to\ncapture the speaker's intention. Besides, we utilize previous utterances to\npredict the last utterance, which simulates human's psychology to guess what\nthe interlocutor may speak in advance. To balance the optimizing rates of the\nutterance prediction and response generation, a multi-task learning strategy is\ndesigned for InferEM. Experimental results demonstrate the plausibility and\nvalidity of InferEM in improving empathetic expression.\n","authors":["Guoqing Lv","Jiang Li","Xiaoping Wang"],"pdf_url":"https://arxiv.org/pdf/2212.06373v3.pdf","comment":"7 pages"},{"id":"http://arxiv.org/abs/2303.15714v1","updated":"2023-03-28T03:55:03Z","published":"2023-03-28T03:55:03Z","title":"Explicit Planning Helps Language Models in Logical Reasoning","summary":"  Language models have been shown to perform remarkably well on a wide range of\nnatural language processing tasks. In this paper, we propose a novel system\nthat uses language models to perform multi-step logical reasoning. Our system\nincorporates explicit planning into its inference procedure, thus able to make\nmore informed reasoning decisions at each step by looking ahead into their\nfuture effects. In our experiments, our full system significantly outperforms\nother competing systems. On a multiple-choice question answering task, our\nsystem performs competitively compared to GPT-3-davinci despite having only\naround 1.5B parameters. We conduct several ablation studies to demonstrate that\nexplicit planning plays a crucial role in the system's performance.\n","authors":["Hongyu Zhao","Kangrui Wang","Mo Yu","Hongyuan Mei"],"pdf_url":"https://arxiv.org/pdf/2303.15714v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.15708v1","updated":"2023-03-28T03:31:37Z","published":"2023-03-28T03:31:37Z","title":"Bias or Diversity? Unraveling Semantic Discrepancy in U.S. News\n  Headlines","summary":"  There is a broad consensus that news media outlets incorporate ideological\nbiases in their news articles. However, prior studies on measuring the\ndiscrepancies among media outlets and further dissecting the origins of\nsemantic differences suffer from small sample sizes and limited scope. In this\nstudy, we collect a large dataset of 1.8 million news headlines from major U.S.\nmedia outlets spanning from 2014 to 2022 to thoroughly track and dissect the\nsemantic discrepancy in U.S. news media. We employ multiple correspondence\nanalysis (MCA) to quantify the semantic discrepancy relating to four prominent\ntopics - domestic politics, economic issues, social issues, and foreign\naffairs. Additionally, we compare the most frequent n-grams in media headlines\nto provide further qualitative insights into our analysis. Our findings\nindicate that on domestic politics and social issues, the discrepancy can be\nattributed to a certain degree of media bias. Meanwhile, the discrepancy in\nreporting foreign affairs is largely attributed to the diversity in individual\njournalistic styles. Finally, U.S. media outlets show consistency and high\nsimilarity in their coverage of economic issues.\n","authors":["Jinsheng Pan","Weihong Qi","Zichen Wang","Hanjia Lyu","Jiebo Luo"],"pdf_url":"https://arxiv.org/pdf/2303.15708v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.15705v1","updated":"2023-03-28T03:17:59Z","published":"2023-03-28T03:17:59Z","title":"Translate the Beauty in Songs: Jointly Learning to Align Melody and\n  Translate Lyrics","summary":"  Song translation requires both translation of lyrics and alignment of music\nnotes so that the resulting verse can be sung to the accompanying melody, which\nis a challenging problem that has attracted some interests in different aspects\nof the translation process. In this paper, we propose Lyrics-Melody Translation\nwith Adaptive Grouping (LTAG), a holistic solution to automatic song\ntranslation by jointly modeling lyrics translation and lyrics-melody alignment.\nIt is a novel encoder-decoder framework that can simultaneously translate the\nsource lyrics and determine the number of aligned notes at each decoding step\nthrough an adaptive note grouping module. To address data scarcity, we\ncommissioned a small amount of training data annotated specifically for this\ntask and used large amounts of augmented data through back-translation.\nExperiments conducted on an English-Chinese song translation data set show the\neffectiveness of our model in both automatic and human evaluation.\n","authors":["Chengxi Li","Kai Fan","Jiajun Bu","Boxing Chen","Zhongqiang Huang","Zhi Yu"],"pdf_url":"https://arxiv.org/pdf/2303.15705v1.pdf","comment":"13 pages"},{"id":"http://arxiv.org/abs/2303.15697v1","updated":"2023-03-28T03:00:01Z","published":"2023-03-28T03:00:01Z","title":"Model and Evaluation: Towards Fairness in Multilingual Text\n  Classification","summary":"  Recently, more and more research has focused on addressing bias in text\nclassification models. However, existing research mainly focuses on the\nfairness of monolingual text classification models, and research on fairness\nfor multilingual text classification is still very limited. In this paper, we\nfocus on the task of multilingual text classification and propose a debiasing\nframework for multilingual text classification based on contrastive learning.\nOur proposed method does not rely on any external language resources and can be\nextended to any other languages. The model contains four modules: multilingual\ntext representation module, language fusion module, text debiasing module, and\ntext classification module. The multilingual text representation module uses a\nmultilingual pre-trained language model to represent the text, the language\nfusion module makes the semantic spaces of different languages tend to be\nconsistent through contrastive learning, and the text debiasing module uses\ncontrastive learning to make the model unable to identify sensitive attributes'\ninformation. The text classification module completes the basic tasks of\nmultilingual text classification. In addition, the existing research on the\nfairness of multilingual text classification is relatively simple in the\nevaluation mode. The evaluation method of fairness is the same as the\nmonolingual equality difference evaluation method, that is, the evaluation is\nperformed on a single language. We propose a multi-dimensional fairness\nevaluation framework for multilingual text classification, which evaluates the\nmodel's monolingual equality difference, multilingual equality difference,\nmultilingual equality performance difference, and destructiveness of the\nfairness strategy. We hope that our work can provide a more general debiasing\nmethod and a more comprehensive evaluation framework for multilingual text\nfairness tasks.\n","authors":["Nankai Lin","Junheng He","Zhenghang Tang","Dong Zhou","Aimin Yang"],"pdf_url":"https://arxiv.org/pdf/2303.15697v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.15682v1","updated":"2023-03-28T02:10:37Z","published":"2023-03-28T02:10:37Z","title":"Pre-training Transformers for Knowledge Graph Completion","summary":"  Learning transferable representation of knowledge graphs (KGs) is challenging\ndue to the heterogeneous, multi-relational nature of graph structures. Inspired\nby Transformer-based pretrained language models' success on learning\ntransferable representation for texts, we introduce a novel inductive KG\nrepresentation model (iHT) for KG completion by large-scale pre-training. iHT\nconsists of a entity encoder (e.g., BERT) and a neighbor-aware relational\nscoring function both parameterized by Transformers. We first pre-train iHT on\na large KG dataset, Wikidata5M. Our approach achieves new state-of-the-art\nresults on matched evaluations, with a relative improvement of more than 25% in\nmean reciprocal rank over previous SOTA models. When further fine-tuned on\nsmaller KGs with either entity and relational shifts, pre-trained iHT\nrepresentations are shown to be transferable, significantly improving the\nperformance on FB15K-237 and WN18RR.\n","authors":["Sanxing Chen","Hao Cheng","Xiaodong Liu","Jian Jiao","Yangfeng Ji","Jianfeng Gao"],"pdf_url":"https://arxiv.org/pdf/2303.15682v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.15662v1","updated":"2023-03-28T01:07:38Z","published":"2023-03-28T01:07:38Z","title":"ChatGPT4PCG Competition: Character-like Level Generation for Science\n  Birds","summary":"  This paper presents the first ChatGPT4PCG Competition at the 2023 IEEE\nConference on Games. The objective of this competition is for participants to\ncreate effective prompts for ChatGPT--enabling it to generate Science Birds\nlevels with high stability and character-like qualities--fully using their\ncreativity as well as prompt engineering skills. ChatGPT is a conversational\nagent developed by OpenAI. Science Birds is selected as the competition\nplatform because designing an Angry Birds-like level is not a trivial task due\nto the in-game gravity; the playability of the levels is determined by their\nstability. To lower the entry barrier to the competition, we limit the task to\nthe generation of capitalized English alphabetical characters. Here, the\nquality of the generated levels is determined by their stability and similarity\nto the given characters. A sample prompt is provided to participants for their\nreference. An experiment is conducted to determine the effectiveness of its\nmodified versions on level stability and similarity by testing them on several\ncharacters. To the best of our knowledge, we believe that ChatGPT4PCG is the\nfirst competition of its kind and hope to inspire enthusiasm for prompt\nengineering in procedural content generation.\n","authors":["Pittawat Taveekitworachai","Febri Abdullah","Mury F. Dewantoro","Ruck Thawonmas","Julian Togelius","Jochen Renz"],"pdf_url":"https://arxiv.org/pdf/2303.15662v1.pdf","comment":"This paper under review is made available for participants of\n  ChatGPT4PCG Competition (https://chatgpt4pcg.github.io/) and readers\n  interested in relevant areas"},{"id":"http://arxiv.org/abs/2303.15655v1","updated":"2023-03-28T00:42:29Z","published":"2023-03-28T00:42:29Z","title":"Joint embedding in Hierarchical distance and semantic representation\n  learning for link prediction","summary":"  The link prediction task aims to predict missing entities or relations in the\nknowledge graph and is essential for the downstream application. Existing\nwell-known models deal with this task by mainly focusing on representing\nknowledge graph triplets in the distance space or semantic space. However, they\ncan not fully capture the information of head and tail entities, nor even make\ngood use of hierarchical level information. Thus, in this paper, we propose a\nnovel knowledge graph embedding model for the link prediction task, namely,\nHIE, which models each triplet (\\textit{h}, \\textit{r}, \\textit{t}) into\ndistance measurement space and semantic measurement space, simultaneously.\nMoreover, HIE is introduced into hierarchical-aware space to leverage rich\nhierarchical information of entities and relations for better representation\nlearning. Specifically, we apply distance transformation operation on the head\nentity in distance space to obtain the tail entity instead of translation-based\nor rotation-based approaches. Experimental results of HIE on four real-world\ndatasets show that HIE outperforms several existing state-of-the-art knowledge\ngraph embedding methods on the link prediction task and deals with complex\nrelations accurately.\n","authors":["Jin Liu","Jianye Chen","Chongfeng Fan","Fengyu Zhou"],"pdf_url":"https://arxiv.org/pdf/2303.15655v1.pdf","comment":"Submitted to Big Data research one year ago"},{"id":"http://arxiv.org/abs/2303.15647v1","updated":"2023-03-28T00:06:38Z","published":"2023-03-28T00:06:38Z","title":"Scaling Down to Scale Up: A Guide to Parameter-Efficient Fine-Tuning","summary":"  This paper presents a systematic overview and comparison of\nparameter-efficient fine-tuning methods covering over 40 papers published\nbetween February 2019 and February 2023. These methods aim to resolve the\ninfeasibility and impracticality of fine-tuning large language models by only\ntraining a small set of parameters. We provide a taxonomy that covers a broad\nrange of methods and present a detailed method comparison with a specific focus\non real-life efficiency and fine-tuning multibillion-scale language models.\n","authors":["Vladislav Lialin","Vijeta Deshpande","Anna Rumshisky"],"pdf_url":"https://arxiv.org/pdf/2303.15647v1.pdf","comment":null}],"Computer Vision and Pattern Recognition":[{"id":"http://arxiv.org/abs/2303.16203v1","updated":"2023-03-28T17:59:56Z","published":"2023-03-28T17:59:56Z","title":"Your Diffusion Model is Secretly a Zero-Shot Classifier","summary":"  The recent wave of large-scale text-to-image diffusion models has\ndramatically increased our text-based image generation abilities. These models\ncan generate realistic images for a staggering variety of prompts and exhibit\nimpressive compositional generalization abilities. Almost all use cases thus\nfar have solely focused on sampling; however, diffusion models can also provide\nconditional density estimates, which are useful for tasks beyond image\ngeneration. In this paper, we show that the density estimates from large-scale\ntext-to-image diffusion models like Stable Diffusion can be leveraged to\nperform zero-shot classification without any additional training. Our\ngenerative approach to classification attains strong results on a variety of\nbenchmarks and outperforms alternative methods of extracting knowledge from\ndiffusion models. We also find that our diffusion-based approach has stronger\nmultimodal relational reasoning abilities than competing contrastive\napproaches. Finally, we evaluate diffusion models trained on ImageNet and find\nthat they approach the performance of SOTA discriminative classifiers trained\non the same dataset, even with weak augmentations and no regularization.\nResults and visualizations at https://diffusion-classifier.github.io/\n","authors":["Alexander C. Li","Mihir Prabhudesai","Shivam Duggal","Ellis Brown","Deepak Pathak"],"pdf_url":"https://arxiv.org/pdf/2303.16203v1.pdf","comment":"Website at https://diffusion-classifier.github.io/"},{"id":"http://arxiv.org/abs/2303.16202v1","updated":"2023-03-28T17:59:55Z","published":"2023-03-28T17:59:55Z","title":"CCuantuMM: Cycle-Consistent Quantum-Hybrid Matching of Multiple Shapes","summary":"  Jointly matching multiple, non-rigidly deformed 3D shapes is a challenging,\n$\\mathcal{NP}$-hard problem. A perfect matching is necessarily\ncycle-consistent: Following the pairwise point correspondences along several\nshapes must end up at the starting vertex of the original shape. Unfortunately,\nexisting quantum shape-matching methods do not support multiple shapes and even\nless cycle consistency. This paper addresses the open challenges and introduces\nthe first quantum-hybrid approach for 3D shape multi-matching; in addition, it\nis also cycle-consistent. Its iterative formulation is admissible to modern\nadiabatic quantum hardware and scales linearly with the total number of input\nshapes. Both these characteristics are achieved by reducing the $N$-shape case\nto a sequence of three-shape matchings, the derivation of which is our main\ntechnical contribution. Thanks to quantum annealing, high-quality solutions\nwith low energy are retrieved for the intermediate $\\mathcal{NP}$-hard\nobjectives. On benchmark datasets, the proposed approach significantly\noutperforms extensions to multi-shape matching of a previous quantum-hybrid\ntwo-shape matching method and is on-par with classical multi-matching methods.\n","authors":["Harshil Bhatia","Edith Tretschk","Zorah Lähner","Marcel Seelbach Benkner","Michael Moeller","Christian Theobalt","Vladislav Golyanik"],"pdf_url":"https://arxiv.org/pdf/2303.16202v1.pdf","comment":"Computer Vision and Pattern Recognition (CVPR) 2023; 22 pages, 24\n  figures and 5 tables; Project page: https://4dqv.mpi-inf.mpg.de/CCuantuMM/"},{"id":"http://arxiv.org/abs/2303.16201v1","updated":"2023-03-28T17:59:28Z","published":"2023-03-28T17:59:28Z","title":"ASIC: Aligning Sparse in-the-wild Image Collections","summary":"  We present a method for joint alignment of sparse in-the-wild image\ncollections of an object category. Most prior works assume either ground-truth\nkeypoint annotations or a large dataset of images of a single object category.\nHowever, neither of the above assumptions hold true for the long-tail of the\nobjects present in the world. We present a self-supervised technique that\ndirectly optimizes on a sparse collection of images of a particular\nobject/object category to obtain consistent dense correspondences across the\ncollection. We use pairwise nearest neighbors obtained from deep features of a\npre-trained vision transformer (ViT) model as noisy and sparse keypoint matches\nand make them dense and accurate matches by optimizing a neural network that\njointly maps the image collection into a learned canonical grid. Experiments on\nCUB and SPair-71k benchmarks demonstrate that our method can produce globally\nconsistent and higher quality correspondences across the image collection when\ncompared to existing self-supervised methods. Code and other material will be\nmade available at \\url{https://kampta.github.io/asic}.\n","authors":["Kamal Gupta","Varun Jampani","Carlos Esteves","Abhinav Shrivastava","Ameesh Makadia","Noah Snavely","Abhishek Kar"],"pdf_url":"https://arxiv.org/pdf/2303.16201v1.pdf","comment":"Web: https://kampta.github.io/asic"},{"id":"http://arxiv.org/abs/2303.16199v1","updated":"2023-03-28T17:59:12Z","published":"2023-03-28T17:59:12Z","title":"LLaMA-Adapter: Efficient Fine-tuning of Language Models with Zero-init\n  Attention","summary":"  We present LLaMA-Adapter, a lightweight adaption method to efficiently\nfine-tune LLaMA into an instruction-following model. Using 52K self-instruct\ndemonstrations, LLaMA-Adapter only introduces 1.2M learnable parameters upon\nthe frozen LLaMA 7B model, and costs less than one hour for fine-tuning on 8\nA100 GPUs. Specifically, we adopt a set of learnable adaption prompts, and\nprepend them to the input text tokens at higher transformer layers. Then, a\nzero-init attention mechanism with zero gating is proposed, which adaptively\ninjects the new instructional cues into LLaMA, while effectively preserves its\npre-trained knowledge. With efficient training, LLaMA-Adapter generates\nhigh-quality responses, comparable to Alpaca with fully fine-tuned 7B\nparameters. Furthermore, our approach can be simply extended to multi-modal\ninput, e.g., images, for image-conditioned LLaMA, which achieves superior\nreasoning capacity on ScienceQA. We release our code at\nhttps://github.com/ZrrSkywalker/LLaMA-Adapter.\n","authors":["Renrui Zhang","Jiaming Han","Aojun Zhou","Xiangfei Hu","Shilin Yan","Pan Lu","Hongsheng Li","Peng Gao","Yu Qiao"],"pdf_url":"https://arxiv.org/pdf/2303.16199v1.pdf","comment":"Work in Progress. Code is available at\n  https://github.com/ZrrSkywalker/LLaMA-Adapter"},{"id":"http://arxiv.org/abs/2303.16198v1","updated":"2023-03-28T17:59:05Z","published":"2023-03-28T17:59:05Z","title":"Forecasting localized weather impacts on vegetation as seen from space\n  with meteo-guided video prediction","summary":"  We present a novel approach for modeling vegetation response to weather in\nEurope as measured by the Sentinel 2 satellite. Existing satellite imagery\nforecasting approaches focus on photorealistic quality of the multispectral\nimages, while derived vegetation dynamics have not yet received as much\nattention. We leverage both spatial and temporal context by extending\nstate-of-the-art video prediction methods with weather guidance. We extend the\nEarthNet2021 dataset to be suitable for vegetation modeling by introducing a\nlearned cloud mask and an appropriate evaluation scheme. Qualitative and\nquantitative experiments demonstrate superior performance of our approach over\na wide variety of baseline methods, including leading approaches to satellite\nimagery forecasting. Additionally, we show how our modeled vegetation dynamics\ncan be leveraged in a downstream task: inferring gross primary productivity for\ncarbon monitoring. To the best of our knowledge, this work presents the first\nmodels for continental-scale vegetation modeling at fine resolution able to\ncapture anomalies beyond the seasonal cycle, thereby paving the way for\npredictive assessments of vegetation status.\n","authors":["Vitus Benson","Christian Requena-Mesa","Claire Robin","Lazaro Alonso","José Cortés","Zhihan Gao","Nora Linscheid","Mélanie Weynants","Markus Reichstein"],"pdf_url":"https://arxiv.org/pdf/2303.16198v1.pdf","comment":"Source code available at\n  https://github.com/earthnet2021/earthnet-models-pytorch"},{"id":"http://arxiv.org/abs/2303.16196v1","updated":"2023-03-28T17:58:05Z","published":"2023-03-28T17:58:05Z","title":"SparseNeRF: Distilling Depth Ranking for Few-shot Novel View Synthesis","summary":"  Neural Radiance Field (NeRF) significantly degrades when only a limited\nnumber of views are available. To complement the lack of 3D information,\ndepth-based models, such as DSNeRF and MonoSDF, explicitly assume the\navailability of accurate depth maps of multiple views. They linearly scale the\naccurate depth maps as supervision to guide the predicted depth of few-shot\nNeRFs. However, accurate depth maps are difficult and expensive to capture due\nto wide-range depth distances in the wild.\n  In this work, we present a new Sparse-view NeRF (SparseNeRF) framework that\nexploits depth priors from real-world inaccurate observations. The inaccurate\ndepth observations are either from pre-trained depth models or coarse depth\nmaps of consumer-level depth sensors. Since coarse depth maps are not strictly\nscaled to the ground-truth depth maps, we propose a simple yet effective\nconstraint, a local depth ranking method, on NeRFs such that the expected depth\nranking of the NeRF is consistent with that of the coarse depth maps in local\npatches. To preserve the spatial continuity of the estimated depth of NeRF, we\nfurther propose a spatial continuity constraint to encourage the consistency of\nthe expected depth continuity of NeRF with coarse depth maps. Surprisingly,\nwith simple depth ranking constraints, SparseNeRF outperforms all\nstate-of-the-art few-shot NeRF methods (including depth-based models) on\nstandard LLFF and DTU datasets. Moreover, we collect a new dataset NVS-RGBD\nthat contains real-world depth maps from Azure Kinect, ZED 2, and iPhone 13\nPro. Extensive experiments on NVS-RGBD dataset also validate the superiority\nand generalizability of SparseNeRF. Project page is available at\nhttps://sparsenerf.github.io/.\n","authors":["Guangcong Wang","Zhaoxi Chen","Chen Change Loy","Ziwei Liu"],"pdf_url":"https://arxiv.org/pdf/2303.16196v1.pdf","comment":"Technical Report, Project page: https://sparsenerf.github.io/"},{"id":"http://arxiv.org/abs/2303.16191v1","updated":"2023-03-28T17:54:56Z","published":"2023-03-28T17:54:56Z","title":"Hard Nominal Example-aware Template Mutual Matching for Industrial\n  Anomaly Detection","summary":"  Anomaly detectors are widely used in industrial production to detect and\nlocalize unknown defects in query images. These detectors are trained on\nnominal images and have shown success in distinguishing anomalies from most\nnormal samples. However, hard-nominal examples are scattered and far apart from\nmost normalities, they are often mistaken for anomalies by existing anomaly\ndetectors. To address this problem, we propose a simple yet efficient method:\n\\textbf{H}ard Nominal \\textbf{E}xample-aware \\textbf{T}emplate \\textbf{M}utual\n\\textbf{M}atching (HETMM). Specifically, \\textit{HETMM} aims to construct a\nrobust prototype-based decision boundary, which can precisely distinguish\nbetween hard-nominal examples and anomalies, yielding fewer false-positive and\nmissed-detection rates. Moreover, \\textit{HETMM} mutually explores the\nanomalies in two directions between queries and the template set, and thus it\nis capable to capture the logical anomalies. This is a significant advantage\nover most anomaly detectors that frequently fail to detect logical anomalies.\nAdditionally, to meet the speed-accuracy demands, we further propose\n\\textbf{P}ixel-level \\textbf{T}emplate \\textbf{S}election (PTS) to streamline\nthe original template set. \\textit{PTS} selects cluster centres and\nhard-nominal examples to form a tiny set, maintaining the original decision\nboundaries. Comprehensive experiments on five real-world datasets demonstrate\nthat our methods yield outperformance than existing advances under the\nreal-time inference speed. Furthermore, \\textit{HETMM} can be hot-updated by\ninserting novel samples, which may promptly address some incremental learning\nissues.\n","authors":["Zixuan Chen","jianhuang Lai","Lingxiao Yang","Xiaohua Xie"],"pdf_url":"https://arxiv.org/pdf/2303.16191v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.16189v1","updated":"2023-03-28T17:53:22Z","published":"2023-03-28T17:53:22Z","title":"Planning with Sequence Models through Iterative Energy Minimization","summary":"  Recent works have shown that sequence modeling can be effectively used to\ntrain reinforcement learning (RL) policies. However, the success of applying\nexisting sequence models to planning, in which we wish to obtain a trajectory\nof actions to reach some goal, is less straightforward. The typical\nautoregressive generation procedures of sequence models preclude sequential\nrefinement of earlier steps, which limits the effectiveness of a predicted\nplan. In this paper, we suggest an approach towards integrating planning with\nsequence models based on the idea of iterative energy minimization, and\nillustrate how such a procedure leads to improved RL performance across\ndifferent tasks. We train a masked language model to capture an implicit energy\nfunction over trajectories of actions, and formulate planning as finding a\ntrajectory of actions with minimum energy. We illustrate how this procedure\nenables improved performance over recent approaches across BabyAI and Atari\nenvironments. We further demonstrate unique benefits of our iterative\noptimization procedure, involving new task generalization, test-time\nconstraints adaptation, and the ability to compose plans together. Project\nwebsite: https://hychen-naza.github.io/projects/LEAP\n","authors":["Hongyi Chen","Yilun Du","Yiye Chen","Joshua Tenenbaum","Patricio A. Vela"],"pdf_url":"https://arxiv.org/pdf/2303.16189v1.pdf","comment":"Accepted by ICLR2023. Project page:\n  https://hychen-naza.github.io/projects/LEAP/index.html"},{"id":"http://arxiv.org/abs/2303.16187v1","updated":"2023-03-28T17:53:06Z","published":"2023-03-28T17:53:06Z","title":"Visual Chain-of-Thought Diffusion Models","summary":"  Recent progress with conditional image diffusion models has been stunning,\nand this holds true whether we are speaking about models conditioned on a text\ndescription, a scene layout, or a sketch. Unconditional image diffusion models\nare also improving but lag behind, as do diffusion models which are conditioned\non lower-dimensional features like class labels. We propose to close the gap\nbetween conditional and unconditional models using a two-stage sampling\nprocedure. In the first stage we sample an embedding describing the semantic\ncontent of the image. In the second stage we sample the image conditioned on\nthis embedding and then discard the embedding. Doing so lets us leverage the\npower of conditional diffusion models on the unconditional generation task,\nwhich we show improves FID by 25-50% compared to standard unconditional\ngeneration.\n","authors":["William Harvey","Frank Wood"],"pdf_url":"https://arxiv.org/pdf/2303.16187v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.16186v1","updated":"2023-03-28T17:52:19Z","published":"2023-03-28T17:52:19Z","title":"Large-scale Training Data Search for Object Re-identification","summary":"  We consider a scenario where we have access to the target domain, but cannot\nafford on-the-fly training data annotation, and instead would like to construct\nan alternative training set from a large-scale data pool such that a\ncompetitive model can be obtained. We propose a search and pruning (SnP)\nsolution to this training data search problem, tailored to object\nre-identification (re-ID), an application aiming to match the same object\ncaptured by different cameras. Specifically, the search stage identifies and\nmerges clusters of source identities which exhibit similar distributions with\nthe target domain. The second stage, subject to a budget, then selects\nidentities and their images from the Stage I output, to control the size of the\nresulting training set for efficient training. The two steps provide us with\ntraining sets 80\\% smaller than the source pool while achieving a similar or\neven higher re-ID accuracy. These training sets are also shown to be superior\nto a few existing search methods such as random sampling and greedy sampling\nunder the same budget on training data size. If we release the budget, training\nsets resulting from the first stage alone allow even higher re-ID accuracy. We\nprovide interesting discussions on the specificity of our method to the re-ID\nproblem and particularly its role in bridging the re-ID domain gap. The code is\navailable at https://github.com/yorkeyao/SnP.\n","authors":["Yue Yao","Huan Lei","Tom Gedeon","Liang Zheng"],"pdf_url":"https://arxiv.org/pdf/2303.16186v1.pdf","comment":"Accepted to CVPR2023"},{"id":"http://arxiv.org/abs/2303.16184v1","updated":"2023-03-28T17:49:42Z","published":"2023-03-28T17:49:42Z","title":"VMesh: Hybrid Volume-Mesh Representation for Efficient View Synthesis","summary":"  With the emergence of neural radiance fields (NeRFs), view synthesis quality\nhas reached an unprecedented level. Compared to traditional mesh-based assets,\nthis volumetric representation is more powerful in expressing scene geometry\nbut inevitably suffers from high rendering costs and can hardly be involved in\nfurther processes like editing, posing significant difficulties in combination\nwith the existing graphics pipeline. In this paper, we present a hybrid\nvolume-mesh representation, VMesh, which depicts an object with a textured mesh\nalong with an auxiliary sparse volume. VMesh retains the advantages of\nmesh-based assets, such as efficient rendering, compact storage, and easy\nediting, while also incorporating the ability to represent subtle geometric\nstructures provided by the volumetric counterpart. VMesh can be obtained from\nmulti-view images of an object and renders at 2K 60FPS on common consumer\ndevices with high fidelity, unleashing new opportunities for real-time\nimmersive applications.\n","authors":["Yuan-Chen Guo","Yan-Pei Cao","Chen Wang","Yu He","Ying Shan","Xiaohu Qie","Song-Hai Zhang"],"pdf_url":"https://arxiv.org/pdf/2303.16184v1.pdf","comment":"Project page: https://bennyguo.github.io/vmesh/"},{"id":"http://arxiv.org/abs/2303.16181v1","updated":"2023-03-28T17:46:16Z","published":"2023-03-28T17:46:16Z","title":"Learning Federated Visual Prompt in Null Space for MRI Reconstruction","summary":"  Federated Magnetic Resonance Imaging (MRI) reconstruction enables multiple\nhospitals to collaborate distributedly without aggregating local data, thereby\nprotecting patient privacy. However, the data heterogeneity caused by different\nMRI protocols, insufficient local training data, and limited communication\nbandwidth inevitably impair global model convergence and updating. In this\npaper, we propose a new algorithm, FedPR, to learn federated visual prompts in\nthe null space of global prompt for MRI reconstruction. FedPR is a new\nfederated paradigm that adopts a powerful pre-trained model while only learning\nand communicating the prompts with few learnable parameters, thereby\nsignificantly reducing communication costs and achieving competitive\nperformance on limited local data. Moreover, to deal with catastrophic\nforgetting caused by data heterogeneity, FedPR also updates efficient federated\nvisual prompts that project the local prompts into an approximate null space of\nthe global prompt, thereby suppressing the interference of gradients on the\nserver performance. Extensive experiments on federated MRI show that FedPR\nsignificantly outperforms state-of-the-art FL algorithms with <6% of\ncommunication costs when given the limited amount of local training data.\n","authors":["Chun-Mei Feng Bangjun Li Xinxing Xu","Yong Liu","Huazhu Fu Wangmeng Zuo"],"pdf_url":"https://arxiv.org/pdf/2303.16181v1.pdf","comment":"8 pages, Proceedings of the IEEE/CVF International Conference on\n  Computer Vision"},{"id":"http://arxiv.org/abs/2302.10296v2","updated":"2023-03-28T17:30:34Z","published":"2023-02-08T05:55:16Z","title":"On Function-Coupled Watermarks for Deep Neural Networks","summary":"  Well-performed deep neural networks (DNNs) generally require massive labelled\ndata and computational resources for training. Various watermarking techniques\nare proposed to protect such intellectual properties (IPs), wherein the DNN\nproviders implant secret information into the model so that they can later\nclaim IP ownership by retrieving their embedded watermarks with some dedicated\ntrigger inputs. While promising results are reported in the literature,\nexisting solutions suffer from watermark removal attacks, such as model\nfine-tuning and model pruning.\n  In this paper, we propose a novel DNN watermarking solution that can\neffectively defend against the above attacks. Our key insight is to enhance the\ncoupling of the watermark and model functionalities such that removing the\nwatermark would inevitably degrade the model's performance on normal inputs. To\nthis end, unlike previous methods relying on secret features learnt from\nout-of-distribution data, our method only uses features learnt from\nin-distribution data. Specifically, on the one hand, we propose to sample\ninputs from the original training dataset and fuse them as watermark triggers.\nOn the other hand, we randomly mask model weights during training so that the\ninformation of our embedded watermarks spreads in the network. By doing so,\nmodel fine-tuning/pruning would not forget our function-coupled watermarks.\nEvaluation results on various image classification tasks show a 100\\% watermark\nauthentication success rate under aggressive watermark removal attacks,\nsignificantly outperforming existing solutions. Code is available:\nhttps://github.com/cure-lab/Function-Coupled-Watermark.\n","authors":["Xiangyu Wen","Yu Li","Wei Jiang","Qiang Xu"],"pdf_url":"https://arxiv.org/pdf/2302.10296v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.16160v1","updated":"2023-03-28T17:24:42Z","published":"2023-03-28T17:24:42Z","title":"One-Stage 3D Whole-Body Mesh Recovery with Component Aware Transformer","summary":"  Whole-body mesh recovery aims to estimate the 3D human body, face, and hands\nparameters from a single image. It is challenging to perform this task with a\nsingle network due to resolution issues, i.e., the face and hands are usually\nlocated in extremely small regions. Existing works usually detect hands and\nfaces, enlarge their resolution to feed in a specific network to predict the\nparameter, and finally fuse the results. While this copy-paste pipeline can\ncapture the fine-grained details of the face and hands, the connections between\ndifferent parts cannot be easily recovered in late fusion, leading to\nimplausible 3D rotation and unnatural pose. In this work, we propose a\none-stage pipeline for expressive whole-body mesh recovery, named OSX, without\nseparate networks for each part. Specifically, we design a Component Aware\nTransformer (CAT) composed of a global body encoder and a local face/hand\ndecoder. The encoder predicts the body parameters and provides a high-quality\nfeature map for the decoder, which performs a feature-level upsample-crop\nscheme to extract high-resolution part-specific features and adopt\nkeypoint-guided deformable attention to estimate hand and face precisely. The\nwhole pipeline is simple yet effective without any manual post-processing and\nnaturally avoids implausible prediction. Comprehensive experiments demonstrate\nthe effectiveness of OSX. Lastly, we build a large-scale Upper-Body dataset\n(UBody) with high-quality 2D and 3D whole-body annotations. It contains persons\nwith partially visible bodies in diverse real-life scenarios to bridge the gap\nbetween the basic task and downstream applications.\n","authors":["Jing Lin","Ailing Zeng","Haoqian Wang","Lei Zhang","Yu Li"],"pdf_url":"https://arxiv.org/pdf/2303.16160v1.pdf","comment":"Accepted to CVPR2023; Top-1 on AGORA SMPLX benchmark; Project Page:\n  https://osx-ubody.github.io/"},{"id":"http://arxiv.org/abs/2303.16141v1","updated":"2023-03-28T17:04:18Z","published":"2023-03-28T17:04:18Z","title":"A Comparative Study of Federated Learning Models for COVID-19 Detection","summary":"  Deep learning is effective in diagnosing COVID-19 and requires a large amount\nof data to be effectively trained. Due to data and privacy regulations,\nhospitals generally have no access to data from other hospitals. Federated\nlearning (FL) has been used to solve this problem, where it utilizes a\ndistributed setting to train models in hospitals in a privacy-preserving\nmanner. Deploying FL is not always feasible as it requires high computation and\nnetwork communication resources. This paper evaluates five FL algorithms'\nperformance and resource efficiency for Covid-19 detection. A decentralized\nsetting with CNN networks is set up, and the performance of FL algorithms is\ncompared with a centralized environment. We examined the algorithms with\nvarying numbers of participants, federated rounds, and selection algorithms.\nOur results show that cyclic weight transfer can have better overall\nperformance, and results are better with fewer participating hospitals. Our\nresults demonstrate good performance for detecting COVID-19 patients and might\nbe useful in deploying FL algorithms for covid-19 detection and medical image\nanalysis in general.\n","authors":["Erfan Darzidehkalani","Nanna M. Sijtsema","P. M. A van Ooijen"],"pdf_url":"https://arxiv.org/pdf/2303.16141v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.16133v1","updated":"2023-03-28T16:57:12Z","published":"2023-03-28T16:57:12Z","title":"Exposing and Addressing Cross-Task Inconsistency in Unified\n  Vision-Language Models","summary":"  As general purpose vision models get increasingly effective at a wide set of\ntasks, it is imperative that they be consistent across the tasks they support.\nInconsistent AI models are considered brittle and untrustworthy by human users\nand are more challenging to incorporate into larger systems that take\ndependencies on their outputs. Measuring consistency between very heterogeneous\ntasks that might include outputs in different modalities is challenging since\nit is difficult to determine if the predictions are consistent with one\nanother. As a solution, we introduce a benchmark dataset, COCOCON, where we use\ncontrast sets created by modifying test instances for multiple tasks in small\nbut semantically meaningful ways to change the gold label, and outline metrics\nfor measuring if a model is consistent by ranking the original and perturbed\ninstances across tasks. We find that state-of-the-art systems suffer from a\nsurprisingly high degree of inconsistent behavior across tasks, especially for\nmore heterogeneous tasks. Finally, we propose using a rank correlation-based\nauxiliary objective computed over large automatically created cross-task\ncontrast sets to improve the multi-task consistency of large unified models,\nwhile retaining their original accuracy on downstream tasks. Project website\navailable at https://adymaharana.github.io/cococon/\n","authors":["Adyasha Maharana","Amita Kamath","Christopher Clark","Mohit Bansal","Aniruddha Kembhavi"],"pdf_url":"https://arxiv.org/pdf/2303.16133v1.pdf","comment":"Project Website: https://adymaharana.github.io/cococon/"},{"id":"http://arxiv.org/abs/2212.06200v2","updated":"2023-03-28T16:51:28Z","published":"2022-12-12T19:22:17Z","title":"Breaking the \"Object\" in Video Object Segmentation","summary":"  The appearance of an object can be fleeting when it transforms. As eggs are\nbroken or paper is torn, their color, shape and texture can change\ndramatically, preserving virtually nothing of the original except for the\nidentity itself. Yet, this important phenomenon is largely absent from existing\nvideo object segmentation (VOS) benchmarks. In this work, we close the gap by\ncollecting a new dataset for Video Object Segmentation under Transformations\n(VOST). It consists of more than 700 high-resolution videos, captured in\ndiverse environments, which are 21 seconds long on average and densely labeled\nwith instance masks. A careful, multi-step approach is adopted to ensure that\nthese videos focus on complex object transformations, capturing their full\ntemporal extent. We then extensively evaluate state-of-the-art VOS methods and\nmake a number of important discoveries. In particular, we show that existing\nmethods struggle when applied to this novel task and that their main limitation\nlies in over-reliance on static appearance cues. This motivates us to propose a\nfew modifications for the top-performing baseline that improve its capabilities\nby better modeling spatio-temporal information. But more broadly, the hope is\nto stimulate discussion on learning more robust video object representations.\n","authors":["Pavel Tokmakov","Jie Li","Adrien Gaidon"],"pdf_url":"https://arxiv.org/pdf/2212.06200v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.16123v1","updated":"2023-03-28T16:44:43Z","published":"2023-03-28T16:44:43Z","title":"Evaluating the Effectiveness of 2D and 3D Features for Predicting Tumor\n  Response to Chemotherapy","summary":"  2D and 3D tumor features are widely used in a variety of medical image\nanalysis tasks. However, for chemotherapy response prediction, the\neffectiveness between different kinds of 2D and 3D features are not\ncomprehensively assessed, especially in ovarian cancer-related applications.\nThis investigation aims to accomplish such a comprehensive evaluation. For this\npurpose, CT images were collected retrospectively from 188 advanced-stage\novarian cancer patients. All the metastatic tumors that occurred in each\npatient were segmented and then processed by a set of six filters. Next, three\ncategories of features, namely geometric, density, and texture features, were\ncalculated from both the filtered results and the original segmented tumors,\ngenerating a total of 1595 and 1403 features for the 3D and 2D tumors,\nrespectively. In addition to the conventional single-slice 2D and full-volume\n3D tumor features, we also computed the incomplete-3D tumor features, which\nwere achieved by sequentially adding one individual CT slice and calculating\nthe corresponding features. Support vector machine (SVM) based prediction\nmodels were developed and optimized for each feature set. 5-fold\ncross-validation was used to assess the performance of each individual model.\nThe results show that the 2D feature-based model achieved an AUC (area under\nthe ROC curve [receiver operating characteristic]) of 0.84+-0.02. When adding\nmore slices, the AUC first increased to reach the maximum and then gradually\ndecreased to 0.86+-0.02. The maximum AUC was yielded when adding two adjacent\nslices, with a value of 0.91+-0.01. This initial result provides meaningful\ninformation for optimizing machine learning-based decision-making support tools\nin the future.\n","authors":["Neman Abdoli","Ke Zhang","Patrik Gilley","Xuxin Chen","Youkabed Sadri","Theresa C. Thai","Lauren E. Dockery","Kathleen Moore","Robert S. Mannel","Yuchen Qiu"],"pdf_url":"https://arxiv.org/pdf/2303.16123v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.16118v1","updated":"2023-03-28T16:40:47Z","published":"2023-03-28T16:40:47Z","title":"CycleACR: Cycle Modeling of Actor-Context Relations for Video Action\n  Detection","summary":"  The relation modeling between actors and scene context advances video action\ndetection where the correlation of multiple actors makes their action\nrecognition challenging. Existing studies model each actor and scene relation\nto improve action recognition. However, the scene variations and background\ninterference limit the effectiveness of this relation modeling. In this paper,\nwe propose to select actor-related scene context, rather than directly leverage\nraw video scenario, to improve relation modeling. We develop a Cycle\nActor-Context Relation network (CycleACR) where there is a symmetric graph that\nmodels the actor and context relations in a bidirectional form. Our CycleACR\nconsists of the Actor-to-Context Reorganization (A2C-R) that collects actor\nfeatures for context feature reorganizations, and the Context-to-Actor\nEnhancement (C2A-E) that dynamically utilizes reorganized context features for\nactor feature enhancement. Compared to existing designs that focus on C2A-E,\nour CycleACR introduces A2C-R for a more effective relation modeling. This\nmodeling advances our CycleACR to achieve state-of-the-art performance on two\npopular action detection datasets (i.e., AVA and UCF101-24). We also provide\nablation studies and visualizations as well to show how our cycle actor-context\nrelation modeling improves video action detection. Code is available at\nhttps://github.com/MCG-NJU/CycleACR.\n","authors":["Lei Chen","Zhan Tong","Yibing Song","Gangshan Wu","Limin Wang"],"pdf_url":"https://arxiv.org/pdf/2303.16118v1.pdf","comment":"technical report"},{"id":"http://arxiv.org/abs/2207.06817v2","updated":"2023-03-28T16:34:49Z","published":"2022-07-14T10:53:53Z","title":"Pseudo-Labeling Based Practical Semi-Supervised Meta-Training for\n  Few-Shot Learning","summary":"  Most existing few-shot learning (FSL) methods require a large amount of\nlabeled data in meta-training, which is a major limit. To reduce the\nrequirement of labels, a semi-supervised meta-training (SSMT) setting has been\nproposed for FSL, which includes only a few labeled samples and numbers of\nunlabeled samples in base classes. However, existing methods under this setting\nrequire class-aware sample selection from the unlabeled set, which violates the\nassumption of unlabeled set. In this paper, we propose a practical\nsemi-supervised meta-training setting with truly unlabeled data to facilitate\nthe applications of FSL in realistic scenarios. To better utilize both the\nlabeled and truly unlabeled data, we propose a simple and effective\nmeta-training framework, called pseudo-labeling based meta-learning (PLML).\nFirstly, we train a classifier via common semi-supervised learning (SSL) and\nuse it to obtain the pseudo-labels of unlabeled data. Then we build few-shot\ntasks from labeled and pseudo-labeled data and design a novel finetuning method\nwith feature smoothing and noise suppression to better learn the FSL model from\nnoise labels. Surprisingly, through extensive experiments across two FSL\ndatasets, we find that this simple meta-training framework effectively prevents\nthe performance degradation of various FSL models under limited labeled data,\nand also significantly outperforms the state-of-the-art SSMT models. Besides,\nbenefiting from meta-training, our method also improves two representative SSL\nalgorithms as well.\n","authors":["Xingping Dong","Shengcai Liao","Bo Du","Ling Shao"],"pdf_url":"https://arxiv.org/pdf/2207.06817v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.16105v1","updated":"2023-03-28T16:18:56Z","published":"2023-03-28T16:18:56Z","title":"Variational Distribution Learning for Unsupervised Text-to-Image\n  Generation","summary":"  We propose a text-to-image generation algorithm based on deep neural networks\nwhen text captions for images are unavailable during training. In this work,\ninstead of simply generating pseudo-ground-truth sentences of training images\nusing existing image captioning methods, we employ a pretrained CLIP model,\nwhich is capable of properly aligning embeddings of images and corresponding\ntexts in a joint space and, consequently, works well on zero-shot recognition\ntasks. We optimize a text-to-image generation model by maximizing the data\nlog-likelihood conditioned on pairs of image-text CLIP embeddings. To better\nalign data in the two domains, we employ a principled way based on a\nvariational inference, which efficiently estimates an approximate posterior of\nthe hidden text embedding given an image and its CLIP feature. Experimental\nresults validate that the proposed framework outperforms existing approaches by\nlarge margins under unsupervised and semi-supervised text-to-image generation\nsettings.\n","authors":["Minsoo Kang","Doyup Lee","Jiseob Kim","Saehoon Kim","Bohyung Han"],"pdf_url":"https://arxiv.org/pdf/2303.16105v1.pdf","comment":"Accepted at CVPR2023"},{"id":"http://arxiv.org/abs/2204.10997v2","updated":"2023-03-28T16:14:46Z","published":"2022-04-23T05:48:17Z","title":"Cerebral Palsy Prediction with Frequency Attention Informed Graph\n  Convolutional Networks","summary":"  Early diagnosis and intervention are clinically considered the paramount part\nof treating cerebral palsy (CP), so it is essential to design an efficient and\ninterpretable automatic prediction system for CP. We highlight a significant\ndifference between CP infants' frequency of human movement and that of the\nhealthy group, which improves prediction performance. However, the existing\ndeep learning-based methods did not use the frequency information of infants'\nmovement for CP prediction. This paper proposes a frequency attention informed\ngraph convolutional network and validates it on two consumer-grade RGB video\ndatasets, namely MINI-RGBD and RVI-38 datasets. Our proposed frequency\nattention module aids in improving both classification performance and system\ninterpretability. In addition, we design a frequency-binning method that\nretains the critical frequency of the human joint position data while filtering\nthe noise. Our prediction performance achieves state-of-the-art research on\nboth datasets. Our work demonstrates the effectiveness of frequency information\nin supporting the prediction of CP non-intrusively and provides a way for\nsupporting the early diagnosis of CP in the resource-limited regions where the\nclinical resources are not abundant.\n","authors":["Haozheng Zhang","Hubert P. H. Shum","Edmond S. L. Ho"],"pdf_url":"https://arxiv.org/pdf/2204.10997v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.16102v1","updated":"2023-03-28T16:11:31Z","published":"2023-03-28T16:11:31Z","title":"GP3D: Generalized Pose Estimation in 3D Point Clouds: A case study on\n  bin picking","summary":"  In this paper, we present GP3D, a novel network for generalized pose\nestimation in 3D point clouds. The method generalizes to new objects by using\nboth the scene point cloud and the object point cloud with keypoint indexes as\ninput. The network is trained to match the object keypoints to scene points. To\naddress the pose estimation of novel objects we also present a new approach for\ntraining pose estimation. The typical solution is a single model trained for\npose estimation of a specific object in any scenario. This has several\ndrawbacks: training a model for each object is time-consuming, energy\nconsuming, and by excluding the scenario information the task becomes more\ndifficult. In this paper, we present the opposite solution; a scenario-specific\npose estimation method for novel objects that do not require retraining. The\nnetwork is trained on 1500 objects and is able to learn a generalized solution.\nWe demonstrate that the network is able to correctly predict novel objects, and\ndemonstrate the ability of the network to perform outside of the trained class.\nWe believe that the demonstrated method is a valuable solution for many\nreal-world scenarios. Code and trained network will be made available after\npublication.\n","authors":["Frederik Hagelskjær"],"pdf_url":"https://arxiv.org/pdf/2303.16102v1.pdf","comment":"5 pages, 5 figures, 4 tables"},{"id":"http://arxiv.org/abs/2303.16099v1","updated":"2023-03-28T16:10:12Z","published":"2023-03-28T16:10:12Z","title":"Medical Image Analysis using Deep Relational Learning","summary":"  In the past ten years, with the help of deep learning, especially the rapid\ndevelopment of deep neural networks, medical image analysis has made remarkable\nprogress. However, how to effectively use the relational information between\nvarious tissues or organs in medical images is still a very challenging\nproblem, and it has not been fully studied. In this thesis, we propose two\nnovel solutions to this problem based on deep relational learning. First, we\npropose a context-aware fully convolutional network that effectively models\nimplicit relation information between features to perform medical image\nsegmentation. The network achieves the state-of-the-art segmentation results on\nthe Multi Modal Brain Tumor Segmentation 2017 (BraTS2017) and Multi Modal Brain\nTumor Segmentation 2018 (BraTS2018) data sets. Subsequently, we propose a new\nhierarchical homography estimation network to achieve accurate medical image\nmosaicing by learning the explicit spatial relationship between adjacent\nframes. We use the UCL Fetoscopy Placenta dataset to conduct experiments and\nour hierarchical homography estimation network outperforms the other\nstate-of-the-art mosaicing methods while generating robust and meaningful\nmosaicing result on unseen frames.\n","authors":["Zhihua Liu"],"pdf_url":"https://arxiv.org/pdf/2303.16099v1.pdf","comment":"arXiv admin note: substantial text overlap with arXiv:2007.07788"},{"id":"http://arxiv.org/abs/2303.16094v1","updated":"2023-03-28T16:02:30Z","published":"2023-03-28T16:02:30Z","title":"LinK: Linear Kernel for LiDAR-based 3D Perception","summary":"  Extending the success of 2D Large Kernel to 3D perception is challenging due\nto: 1. the cubically-increasing overhead in processing 3D data; 2. the\noptimization difficulties from data scarcity and sparsity. Previous work has\ntaken the first step to scale up the kernel size from 3x3x3 to 7x7x7 by\nintroducing block-shared weights. However, to reduce the feature variations\nwithin a block, it only employs modest block size and fails to achieve larger\nkernels like the 21x21x21. To address this issue, we propose a new method,\ncalled LinK, to achieve a wider-range perception receptive field in a\nconvolution-like manner with two core designs. The first is to replace the\nstatic kernel matrix with a linear kernel generator, which adaptively provides\nweights only for non-empty voxels. The second is to reuse the pre-computed\naggregation results in the overlapped blocks to reduce computation complexity.\nThe proposed method successfully enables each voxel to perceive context within\na range of 21x21x21. Extensive experiments on two basic perception tasks, 3D\nobject detection and 3D semantic segmentation, demonstrate the effectiveness of\nour method. Notably, we rank 1st on the public leaderboard of the 3D detection\nbenchmark of nuScenes (LiDAR track), by simply incorporating a LinK-based\nbackbone into the basic detector, CenterPoint. We also boost the strong\nsegmentation baseline's mIoU with 2.7% in the SemanticKITTI test set. Code is\navailable at https://github.com/MCG-NJU/LinK.\n","authors":["Tao Lu","Xiang Ding","Haisong Liu","Gangshan Wu","Limin Wang"],"pdf_url":"https://arxiv.org/pdf/2303.16094v1.pdf","comment":"Accepted to CVPR2023"},{"id":"http://arxiv.org/abs/2212.02997v2","updated":"2023-03-28T15:57:15Z","published":"2022-12-06T14:15:17Z","title":"Generalizing Gaze Estimation with Weak-Supervision from Synthetic Views","summary":"  Developing gaze estimation models that generalize well to unseen domains and\nin-the-wild conditions remains a challenge with no known best solution. This is\nmostly due to the difficulty of acquiring ground truth data that cover the\ndistribution of possible faces, head poses and environmental conditions that\nexist in the real world. In this work, we propose to train general gaze\nestimation models based on 3D geometry-aware gaze pseudo-annotations which we\nextract from arbitrary unlabelled face images, which are abundantly available\nin the internet. Additionally, we leverage the observation that head, body and\nhand pose estimation benefit from revising them as dense 3D coordinate\nprediction, and similarly express gaze estimation as regression of dense 3D eye\nmeshes. We overcome the absence of compatible ground truth by fitting rigid 3D\neyeballs on existing gaze datasets and design a multi-view supervision\nframework to balance the effect of pseudo-labels during training. We test our\nmethod in the task of gaze generalization, in which we demonstrate improvement\nof up to $30\\%$ compared to state-of-the-art when no ground truth data are\navailable, and up to $10\\%$ when they are. The project material will become\navailable for research purposes.\n","authors":["Evangelos Ververas","Polydefkis Gkagkos","Jiankang Deng","Michail Christos Doukas","Jia Guo","Stefanos Zafeiriou"],"pdf_url":"https://arxiv.org/pdf/2212.02997v2.pdf","comment":"13 pages, 12 figures"},{"id":"http://arxiv.org/abs/2301.03829v2","updated":"2023-03-28T15:54:39Z","published":"2023-01-10T07:51:36Z","title":"From Plate to Prevention: A Dietary Nutrient-aided Platform for Health\n  Promotion in Singapore","summary":"  Singapore has been striving to improve the provision of healthcare services\nto her people. In this course, the government has taken note of the deficiency\nin regulating and supervising people's nutrient intake, which is identified as\na contributing factor to the development of chronic diseases. Consequently,\nthis issue has garnered significant attention. In this paper, we share our\nexperience in addressing this issue and attaining medical-grade nutrient intake\ninformation to benefit Singaporeans in different aspects. To this end, we\ndevelop the FoodSG platform to incubate diverse healthcare-oriented\napplications as a service in Singapore, taking into account their shared\nrequirements. We further identify the profound meaning of localized food\ndatasets and systematically clean and curate a localized Singaporean food\ndataset FoodSG-233. To overcome the hurdle in recognition performance brought\nby Singaporean multifarious food dishes, we propose to integrate supervised\ncontrastive learning into our food recognition model FoodSG-SCL for the\nintrinsic capability to mine hard positive/negative samples and therefore boost\nthe accuracy. Through a comprehensive evaluation, we present performance\nresults of the proposed model and insights on food-related healthcare\napplications. The FoodSG-233 dataset has been released in\nhttps://foodlg.comp.nus.edu.sg/.\n","authors":["Kaiping Zheng","Thao Nguyen","Jesslyn Hwei Sing Chong","Charlene Enhui Goh","Melanie Herschel","Hee Hoon Lee","Changshuo Liu","Beng Chin Ooi","Wei Wang","James Yip"],"pdf_url":"https://arxiv.org/pdf/2301.03829v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.16085v1","updated":"2023-03-28T15:52:42Z","published":"2023-03-28T15:52:42Z","title":"Whole-body PET image denoising for reduced acquisition time","summary":"  This paper evaluates the performance of supervised and unsupervised deep\nlearning models for denoising positron emission tomography (PET) images in the\npresence of reduced acquisition times. Our experiments consider 212 studies\n(56908 images), and evaluate the models using 2D (RMSE, SSIM) and 3D (SUVpeak\nand SUVmax error for the regions of interest) metrics. It was shown that, in\ncontrast to previous studies, supervised models (ResNet, Unet, SwinIR)\noutperform unsupervised models (pix2pix GAN and CycleGAN with ResNet backbone\nand various auxiliary losses) in the reconstruction of 2D PET images. Moreover,\na hybrid approach of supervised CycleGAN shows the best results in SUVmax\nestimation for denoised images, and the SUVmax estimation error for denoised\nimages is comparable with the PET reproducibility error.\n","authors":["Ivan Kruzhilov","Stepan Kudin","Luka Vetoshkin","Elena Sokolova","Vladimir Kokh"],"pdf_url":"https://arxiv.org/pdf/2303.16085v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.16084v1","updated":"2023-03-28T15:52:31Z","published":"2023-03-28T15:52:31Z","title":"Rethinking matching-based few-shot action recognition","summary":"  Few-shot action recognition, i.e. recognizing new action classes given only a\nfew examples, benefits from incorporating temporal information. Prior work\neither encodes such information in the representation itself and learns\nclassifiers at test time, or obtains frame-level features and performs pairwise\ntemporal matching. We first evaluate a number of matching-based approaches\nusing features from spatio-temporal backbones, a comparison missing from the\nliterature, and show that the gap in performance between simple baselines and\nmore complicated methods is significantly reduced. Inspired by this, we propose\nChamfer++, a non-temporal matching function that achieves state-of-the-art\nresults in few-shot action recognition. We show that, when starting from\ntemporal features, our parameter-free and interpretable approach can outperform\nall other matching-based and classifier methods for one-shot action recognition\non three common datasets without using temporal information in the matching\nstage. Project page: https://jbertrand89.github.io/matching-based-fsar\n","authors":["Juliette Bertrand","Yannis Kalantidis","Giorgos Tolias"],"pdf_url":"https://arxiv.org/pdf/2303.16084v1.pdf","comment":"Accepted at SCIA 2023"},{"id":"http://arxiv.org/abs/2303.16078v1","updated":"2023-03-28T15:50:48Z","published":"2023-03-28T15:50:48Z","title":"Efficient solutions to the relative pose of three calibrated cameras\n  from four points using virtual correspondences","summary":"  We study the challenging problem of estimating the relative pose of three\ncalibrated cameras. We propose two novel solutions to the notoriously difficult\nconfiguration of four points in three views, known as the 4p3v problem. Our\nsolutions are based on the simple idea of generating one additional virtual\npoint correspondence in two views by using the information from the locations\nof the four input correspondences in the three views. For the first solver, we\ntrain a network to predict this point correspondence. The second solver uses a\nmuch simpler and more efficient strategy based on the mean points of three\ncorresponding input points. The new solvers are efficient and easy to implement\nsince they are based on the existing efficient minimal solvers, i.e., the\nwell-known 5-point relative pose and the P3P solvers. The solvers achieve\nstate-of-the-art results on real data. The idea of solving minimal problems\nusing virtual correspondences is general and can be applied to other problems,\ne.g., the 5-point relative pose problem. In this way, minimal problems can be\nsolved using simpler non-minimal solvers or even using sub-minimal samples\ninside RANSAC.\n  In addition, we compare different variants of 4p3v solvers with the baseline\nsolver for the minimal configuration consisting of three triplets of points and\ntwo points visible in two views. We discuss which configuration of points is\npotentially the most practical in real applications.\n","authors":["Charalambos Tzamos","Daniel Barath","Torsten Sattler","Zuzana Kukelova"],"pdf_url":"https://arxiv.org/pdf/2303.16078v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.16058v1","updated":"2023-03-28T15:39:28Z","published":"2023-03-28T15:39:28Z","title":"Unmasked Teacher: Towards Training-Efficient Video Foundation Models","summary":"  Video Foundation Models (VFMs) have received limited exploration due to high\ncomputational costs and data scarcity. Previous VFMs rely on Image Foundation\nModels (IFMs), which face challenges in transferring to the video domain.\nAlthough VideoMAE has trained a robust ViT from limited data, its low-level\nreconstruction poses convergence difficulties and conflicts with high-level\ncross-modal alignment. This paper proposes a training-efficient method for\ntemporal-sensitive VFMs that integrates the benefits of existing methods. To\nincrease data efficiency, we mask out most of the low-semantics video tokens,\nbut selectively align the unmasked tokens with IFM, which serves as the\nUnMasked Teacher (UMT). By providing semantic guidance, our method enables\nfaster convergence and multimodal friendliness. With a progressive pre-training\nframework, our model can handle various tasks including scene-related,\ntemporal-related, and complex video-language understanding. Using only public\nsources for pre-training in 6 days on 32 A100 GPUs, our scratch-built ViT-L/16\nachieves state-of-the-art performances on various video tasks. The code and\nmodels will be released at https://github.com/OpenGVLab/unmasked_teacher.\n","authors":["Kunchang Li","Yali Wang","Yizhuo Li","Yi Wang","Yinan He","Limin Wang","Yu Qiao"],"pdf_url":"https://arxiv.org/pdf/2303.16058v1.pdf","comment":"16 pages, 5 figures, 28 tables"},{"id":"http://arxiv.org/abs/2303.16053v1","updated":"2023-03-28T15:35:25Z","published":"2023-03-28T15:35:25Z","title":"Real-time Multi-person Eyeblink Detection in the Wild for Untrimmed\n  Video","summary":"  Real-time eyeblink detection in the wild can widely serve for fatigue\ndetection, face anti-spoofing, emotion analysis, etc. The existing research\nefforts generally focus on single-person cases towards trimmed video. However,\nmulti-person scenario within untrimmed videos is also important for practical\napplications, which has not been well concerned yet. To address this, we shed\nlight on this research field for the first time with essential contributions on\ndataset, theory, and practices. In particular, a large-scale dataset termed\nMPEblink that involves 686 untrimmed videos with 8748 eyeblink events is\nproposed under multi-person conditions. The samples are captured from\nunconstrained films to reveal \"in the wild\" characteristics. Meanwhile, a\nreal-time multi-person eyeblink detection method is also proposed. Being\ndifferent from the existing counterparts, our proposition runs in a one-stage\nspatio-temporal way with end-to-end learning capacity. Specifically, it\nsimultaneously addresses the sub-tasks of face detection, face tracking, and\nhuman instance-level eyeblink detection. This paradigm holds 2 main advantages:\n(1) eyeblink features can be facilitated via the face's global context (e.g.,\nhead pose and illumination condition) with joint optimization and interaction,\nand (2) addressing these sub-tasks in parallel instead of sequential manner can\nsave time remarkably to meet the real-time running requirement. Experiments on\nMPEblink verify the essential challenges of real-time multi-person eyeblink\ndetection in the wild for untrimmed video. Our method also outperforms existing\napproaches by large margins and with a high inference speed.\n","authors":["Wenzheng Zeng","Yang Xiao","Sicheng Wei","Jinfang Gan","Xintao Zhang","Zhiguo Cao","Zhiwen Fang","Joey Tianyi Zhou"],"pdf_url":"https://arxiv.org/pdf/2303.16053v1.pdf","comment":"Accepted by CVPR 2023"},{"id":"http://arxiv.org/abs/2303.16050v1","updated":"2023-03-28T15:32:21Z","published":"2023-03-28T15:32:21Z","title":"Information-Theoretic GAN Compression with Variational Energy-based\n  Model","summary":"  We propose an information-theoretic knowledge distillation approach for the\ncompression of generative adversarial networks, which aims to maximize the\nmutual information between teacher and student networks via a variational\noptimization based on an energy-based model. Because the direct computation of\nthe mutual information in continuous domains is intractable, our approach\nalternatively optimizes the student network by maximizing the variational lower\nbound of the mutual information. To achieve a tight lower bound, we introduce\nan energy-based model relying on a deep neural network to represent a flexible\nvariational distribution that deals with high-dimensional images and consider\nspatial dependencies between pixels, effectively. Since the proposed method is\na generic optimization algorithm, it can be conveniently incorporated into\narbitrary generative adversarial networks and even dense prediction networks,\ne.g., image enhancement models. We demonstrate that the proposed algorithm\nachieves outstanding performance in model compression of generative adversarial\nnetworks consistently when combined with several existing models.\n","authors":["Minsoo Kang","Hyewon Yoo","Eunhee Kang","Sehwan Ki","Hyong-Euk Lee","Bohyung Han"],"pdf_url":"https://arxiv.org/pdf/2303.16050v1.pdf","comment":"Accepted at Neurips2022"},{"id":"http://arxiv.org/abs/2211.09945v5","updated":"2023-03-28T15:18:08Z","published":"2022-11-17T23:42:10Z","title":"VeriSparse: Training Verified Locally Robust Sparse Neural Networks from\n  Scratch","summary":"  Several safety-critical applications such as self-navigation, health care,\nand industrial control systems use embedded systems as their core. Recent\nadvancements in Neural Networks (NNs) in approximating complex functions make\nthem well-suited for these domains. However, the compute-intensive nature of\nNNs limits their deployment and training in embedded systems with limited\ncomputation and storage capacities. Moreover, the adversarial vulnerability of\nNNs challenges their use in safety-critical scenarios. Hence, developing sparse\nmodels having robustness guarantees while leveraging fewer resources during\ntraining is critical in expanding NNs' use in safety-critical and\nresource-constrained embedding system settings. This paper presents\n'VeriSparse'-- a framework to search verified locally robust sparse networks\nstarting from a random sparse initialization (i.e., scratch). VeriSparse\nobtains sparse NNs exhibiting similar or higher verified local robustness,\nrequiring one-third of the training time compared to the state-of-the-art\napproaches. Furthermore, VeriSparse performs both structured and unstructured\nsparsification, enabling storage, computing-resource, and computation time\nreduction during inference generation. Thus, it facilitates the\nresource-constraint embedding platforms to leverage verified robust NN models,\nexpanding their scope to safety-critical, real-time, and edge applications. We\nexhaustively investigated VeriSparse's efficacy and generalizability by\nevaluating various benchmark and application-specific datasets across several\nmodel architectures.\n","authors":["Sawinder Kaur","Yi Xiao","Asif Salekin"],"pdf_url":"https://arxiv.org/pdf/2211.09945v5.pdf","comment":"21 pages, 13 tables, 3 figures"},{"id":"http://arxiv.org/abs/2211.12562v2","updated":"2023-03-28T15:12:29Z","published":"2022-11-22T20:27:44Z","title":"PermutoSDF: Fast Multi-View Reconstruction with Implicit Surfaces using\n  Permutohedral Lattices","summary":"  Neural radiance-density field methods have become increasingly popular for\nthe task of novel-view rendering. Their recent extension to hash-based\npositional encoding ensures fast training and inference with visually pleasing\nresults. However, density-based methods struggle with recovering accurate\nsurface geometry. Hybrid methods alleviate this issue by optimizing the density\nbased on an underlying SDF. However, current SDF methods are overly smooth and\nmiss fine geometric details. In this work, we combine the strengths of these\ntwo lines of work in a novel hash-based implicit surface representation. We\npropose improvements to the two areas by replacing the voxel hash encoding with\na permutohedral lattice which optimizes faster, especially for higher\ndimensions. We additionally propose a regularization scheme which is crucial\nfor recovering high-frequency geometric detail. We evaluate our method on\nmultiple datasets and show that we can recover geometric detail at the level of\npores and wrinkles while using only RGB images for supervision. Furthermore,\nusing sphere tracing we can render novel views at 30 fps on an RTX 3090. Code\nis publicly available at: https://radualexandru.github.io/permuto_sdf\n","authors":["Radu Alexandru Rosu","Sven Behnke"],"pdf_url":"https://arxiv.org/pdf/2211.12562v2.pdf","comment":"Accepted to CVPR 2023"},{"id":"http://arxiv.org/abs/2205.15530v2","updated":"2023-03-28T15:07:27Z","published":"2022-05-31T04:23:50Z","title":"Pseudo-Data based Self-Supervised Federated Learning for Classification\n  of Histopathological Images","summary":"  Computer-aided diagnosis (CAD) can help pathologists improve diagnostic\naccuracy together with consistency and repeatability for cancers. However, the\nCAD models trained with the histopathological images only from a single center\n(hospital) generally suffer from the generalization problem due to the\nstraining inconsistencies among different centers. In this work, we propose a\npseudo-data based self-supervised federated learning (FL) framework, named\nSSL-FT-BT, to improve both the diagnostic accuracy and generalization of CAD\nmodels. Specifically, the pseudo histopathological images are generated from\neach center, which contains inherent and specific properties corresponding to\nthe real images in this center, but does not include the privacy information.\nThese pseudo images are then shared in the central server for self-supervised\nlearning (SSL). A multi-task SSL is then designed to fully learn both the\ncenter-specific information and common inherent representation according to the\ndata characteristics. Moreover, a novel Barlow Twins based FL (FL-BT) algorithm\nis proposed to improve the local training for the CAD model in each center by\nconducting contrastive learning, which benefits the optimization of the global\nmodel in the FL procedure. The experimental results on three public\nhistopathological image datasets indicate the effectiveness of the proposed\nSSL-FL-BT on both diagnostic accuracy and generalization.\n","authors":["Jun Shi","Yuanming Zhang","Zheng Li","Xiangmin Han","Saisai Ding","Jun Wang","Shihui Ying"],"pdf_url":"https://arxiv.org/pdf/2205.15530v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2012.07962v3","updated":"2023-03-28T15:05:23Z","published":"2020-12-14T21:54:11Z","title":"Iterative label cleaning for transductive and semi-supervised few-shot\n  learning","summary":"  Few-shot learning amounts to learning representations and acquiring knowledge\nsuch that novel tasks may be solved with both supervision and data being\nlimited. Improved performance is possible by transductive inference, where the\nentire test set is available concurrently, and semi-supervised learning, where\nmore unlabeled data is available. Focusing on these two settings, we introduce\na new algorithm that leverages the manifold structure of the labeled and\nunlabeled data distribution to predict pseudo-labels, while balancing over\nclasses and using the loss value distribution of a limited-capacity classifier\nto select the cleanest labels, iteratively improving the quality of\npseudo-labels. Our solution surpasses or matches the state of the art results\non four benchmark datasets, namely miniImageNet, tieredImageNet, CUB and\nCIFAR-FS, while being robust over feature space pre-processing and the quantity\nof available data. The publicly available source code can be found in\nhttps://github.com/MichalisLazarou/iLPC.\n","authors":["Michalis Lazarou","Tania Stathaki","Yannis Avrithis"],"pdf_url":"https://arxiv.org/pdf/2012.07962v3.pdf","comment":"published in ICCV 2021"},{"id":"http://arxiv.org/abs/2303.10720v2","updated":"2023-03-28T15:04:36Z","published":"2023-03-19T17:30:44Z","title":"Trainable Projected Gradient Method for Robust Fine-tuning","summary":"  Recent studies on transfer learning have shown that selectively fine-tuning a\nsubset of layers or customizing different learning rates for each layer can\ngreatly improve robustness to out-of-distribution (OOD) data and retain\ngeneralization capability in the pre-trained models. However, most of these\nmethods employ manually crafted heuristics or expensive hyper-parameter\nsearches, which prevent them from scaling up to large datasets and neural\nnetworks. To solve this problem, we propose Trainable Projected Gradient Method\n(TPGM) to automatically learn the constraint imposed for each layer for a\nfine-grained fine-tuning regularization. This is motivated by formulating\nfine-tuning as a bi-level constrained optimization problem. Specifically, TPGM\nmaintains a set of projection radii, i.e., distance constraints between the\nfine-tuned model and the pre-trained model, for each layer, and enforces them\nthrough weight projections. To learn the constraints, we propose a bi-level\noptimization to automatically learn the best set of projection radii in an\nend-to-end manner. Theoretically, we show that the bi-level optimization\nformulation could explain the regularization capability of TPGM. Empirically,\nwith little hyper-parameter search cost, TPGM outperforms existing fine-tuning\nmethods in OOD performance while matching the best in-distribution (ID)\nperformance. For example, when fine-tuned on DomainNet-Real and ImageNet,\ncompared to vanilla fine-tuning, TPGM shows $22\\%$ and $10\\%$ relative OOD\nimprovement respectively on their sketch counterparts. Code is available at\n\\url{https://github.com/PotatoTian/TPGM}.\n","authors":["Junjiao Tian","Xiaoliang Dai","Chih-Yao Ma","Zecheng He","Yen-Cheng Liu","Zsolt Kira"],"pdf_url":"https://arxiv.org/pdf/2303.10720v2.pdf","comment":"Accepted to CVPR2023"},{"id":"http://arxiv.org/abs/2303.11203v2","updated":"2023-03-28T14:56:30Z","published":"2023-03-20T15:36:10Z","title":"Less is More: Reducing Task and Model Complexity for 3D Point Cloud\n  Semantic Segmentation","summary":"  Whilst the availability of 3D LiDAR point cloud data has significantly grown\nin recent years, annotation remains expensive and time-consuming, leading to a\ndemand for semi-supervised semantic segmentation methods with application\ndomains such as autonomous driving. Existing work very often employs relatively\nlarge segmentation backbone networks to improve segmentation accuracy, at the\nexpense of computational costs. In addition, many use uniform sampling to\nreduce ground truth data requirements for learning needed, often resulting in\nsub-optimal performance. To address these issues, we propose a new pipeline\nthat employs a smaller architecture, requiring fewer ground-truth annotations\nto achieve superior segmentation accuracy compared to contemporary approaches.\nThis is facilitated via a novel Sparse Depthwise Separable Convolution module\nthat significantly reduces the network parameter count while retaining overall\ntask performance. To effectively sub-sample our training data, we propose a new\nSpatio-Temporal Redundant Frame Downsampling (ST-RFD) method that leverages\nknowledge of sensor motion within the environment to extract a more diverse\nsubset of training data frame samples. To leverage the use of limited annotated\ndata samples, we further propose a soft pseudo-label method informed by LiDAR\nreflectivity. Our method outperforms contemporary semi-supervised work in terms\nof mIoU, using less labeled data, on the SemanticKITTI (59.5@5%) and\nScribbleKITTI (58.1@5%) benchmark datasets, based on a 2.3x reduction in model\nparameters and 641x fewer multiply-add operations whilst also demonstrating\nsignificant performance improvement on limited training data (i.e., Less is\nMore).\n","authors":["Li Li","Hubert P. H. Shum","Toby P. Breckon"],"pdf_url":"https://arxiv.org/pdf/2303.11203v2.pdf","comment":"Accepted by CVPR 2023; 11 pages, 8 figures; Code at\n  https://github.com/l1997i/lim3d"},{"id":"http://arxiv.org/abs/2211.08089v2","updated":"2023-03-28T14:55:56Z","published":"2022-11-15T12:15:29Z","title":"DeS3: Attention-driven Self and Soft Shadow Removal using ViT Similarity\n  and Color Convergence","summary":"  Removing soft and self shadows that lack clear boundaries from a single image\nis still challenging. Self shadows are shadows that are cast on the object\nitself. Most existing methods rely on binary shadow masks, without considering\nthe ambiguous boundaries of soft and self shadows. In this paper, we present\nDeS3, a method that removes hard, soft and self shadows based on the self-tuned\nViT feature similarity and color convergence. Our novel ViT similarity loss\nutilizes features extracted from a pre-trained Vision Transformer. This loss\nhelps guide the reverse diffusion process towards recovering scene structures.\nWe also introduce a color convergence loss to constrain the surface colors in\nthe reverse inference process to avoid any color shifts. Our DeS3 is able to\ndifferentiate shadow regions from the underlying objects, as well as shadow\nregions from the object casting the shadow. This capability enables DeS3 to\nbetter recover the structures of objects even when they are partially occluded\nby shadows. Different from existing methods that rely on constraints during the\ntraining phase, we incorporate the ViT similarity and color convergence loss\nduring the sampling stage. This enables our DeS3 model to effectively integrate\nits strong modeling capabilities with input-specific knowledge in a self-tuned\nmanner. Our method outperforms state-of-the-art methods on the SRD, AISTD,\nLRSS, USR and UIUC datasets, removing hard, soft, and self shadows robustly.\nSpecifically, our method outperforms the SOTA method by 20% of the RMSE of the\nwhole image on the SRD dataset.\n","authors":["Yeying Jin","Wenhan Yang","Wei Ye","Yuan Yuan","Robby T. Tan"],"pdf_url":"https://arxiv.org/pdf/2211.08089v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.16024v1","updated":"2023-03-28T14:52:03Z","published":"2023-03-28T14:52:03Z","title":"Egocentric Auditory Attention Localization in Conversations","summary":"  In a noisy conversation environment such as a dinner party, people often\nexhibit selective auditory attention, or the ability to focus on a particular\nspeaker while tuning out others. Recognizing who somebody is listening to in a\nconversation is essential for developing technologies that can understand\nsocial behavior and devices that can augment human hearing by amplifying\nparticular sound sources. The computer vision and audio research communities\nhave made great strides towards recognizing sound sources and speakers in\nscenes. In this work, we take a step further by focusing on the problem of\nlocalizing auditory attention targets in egocentric video, or detecting who in\na camera wearer's field of view they are listening to. To tackle the new and\nchallenging Selective Auditory Attention Localization problem, we propose an\nend-to-end deep learning approach that uses egocentric video and multichannel\naudio to predict the heatmap of the camera wearer's auditory attention. Our\napproach leverages spatiotemporal audiovisual features and holistic reasoning\nabout the scene to make predictions, and outperforms a set of baselines on a\nchallenging multi-speaker conversation dataset. Project page:\nhttps://fkryan.github.io/saal\n","authors":["Fiona Ryan","Hao Jiang","Abhinav Shukla","James M. Rehg","Vamsi Krishna Ithapu"],"pdf_url":"https://arxiv.org/pdf/2303.16024v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2211.08609v4","updated":"2023-03-28T14:45:03Z","published":"2022-11-16T01:43:39Z","title":"Two-Stage Context-Aware model for Predicting Future Motion of Dynamic\n  Agents","summary":"  Predicting the future motion of dynamic agents is of paramount importance to\nensure safety or assess risks in motion planning for autonomous robots. In this\npaper, we propose a two-stage motion prediction method, referred to as R-Pred,\nthat effectively utilizes both the scene and interaction context using a\ncascade of the initial trajectory proposal network and the trajectory\nrefinement network. The initial trajectory proposal network produces M\ntrajectory proposals corresponding to M modes of a future trajectory\ndistribution. The trajectory refinement network enhances each of M proposals\nusing 1) the tube-query scene attention (TQSA) and 2) the proposal-level\ninteraction attention (PIA). TQSA uses tube-queries to aggregate the local\nscene context features pooled from proximity around the trajectory proposals of\ninterest. PIA further enhances the trajectory proposals by modeling inter-agent\ninteractions using a group of trajectory proposals selected based on their\ndistances from neighboring agents. Our experiments conducted on the Argoverse\nand nuScenes datasets demonstrate that the proposed refinement network provides\nsignificant performance improvements compared to the single-stage baseline and\nthat R-Pred achieves state-of-the-art performance in some categories of the\nbenchmark.\n","authors":["Sehwan Choi","Jungho Kim","Junyong Yun","Jun Won Choi"],"pdf_url":"https://arxiv.org/pdf/2211.08609v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2211.17263v2","updated":"2023-03-28T14:41:01Z","published":"2022-11-30T18:58:53Z","title":"Plateau-reduced Differentiable Path Tracing","summary":"  Current differentiable renderers provide light transport gradients with\nrespect to arbitrary scene parameters. However, the mere existence of these\ngradients does not guarantee useful update steps in an optimization. Instead,\ninverse rendering might not converge due to inherent plateaus, i.e., regions of\nzero gradient, in the objective function. We propose to alleviate this by\nconvolving the high-dimensional rendering function that maps scene parameters\nto images with an additional kernel that blurs the parameter space. We describe\ntwo Monte Carlo estimators to compute plateau-free gradients efficiently, i.e.,\nwith low variance, and show that these translate into net-gains in optimization\nerror and runtime performance. Our approach is a straightforward extension to\nboth black-box and differentiable renderers and enables optimization of\nproblems with intricate light transport, such as caustics or global\nillumination, that existing differentiable renderers do not converge on.\n","authors":["Michael Fischer","Tobias Ritschel"],"pdf_url":"https://arxiv.org/pdf/2211.17263v2.pdf","comment":"Accepted to CVPR 2023. Project page and interactive demos at\n  https://mfischer-ucl.github.io/prdpt/"},{"id":"http://arxiv.org/abs/2303.16005v1","updated":"2023-03-28T14:27:27Z","published":"2023-03-28T14:27:27Z","title":"Uncovering the Missing Pattern: Unified Framework Towards Trajectory\n  Imputation and Prediction","summary":"  Trajectory prediction is a crucial undertaking in understanding entity\nmovement or human behavior from observed sequences. However, current methods\noften assume that the observed sequences are complete while ignoring the\npotential for missing values caused by object occlusion, scope limitation,\nsensor failure, etc. This limitation inevitably hinders the accuracy of\ntrajectory prediction. To address this issue, our paper presents a unified\nframework, the Graph-based Conditional Variational Recurrent Neural Network\n(GC-VRNN), which can perform trajectory imputation and prediction\nsimultaneously. Specifically, we introduce a novel Multi-Space Graph Neural\nNetwork (MS-GNN) that can extract spatial features from incomplete observations\nand leverage missing patterns. Additionally, we employ a Conditional VRNN with\na specifically designed Temporal Decay (TD) module to capture temporal\ndependencies and temporal missing patterns in incomplete trajectories. The\ninclusion of the TD module allows for valuable information to be conveyed\nthrough the temporal flow. We also curate and benchmark three practical\ndatasets for the joint problem of trajectory imputation and prediction.\nExtensive experiments verify the exceptional performance of our proposed\nmethod. As far as we know, this is the first work to address the lack of\nbenchmarks and techniques for trajectory imputation and prediction in a unified\nmanner.\n","authors":["Yi Xu","Armin Bazarjani","Hyung-gun Chi","Chiho Choi","Yun Fu"],"pdf_url":"https://arxiv.org/pdf/2303.16005v1.pdf","comment":"Accepted by CVPR 2023, Supplementary Material at\n  https://github.com/colorfulfuture/GC-VRNN"},{"id":"http://arxiv.org/abs/2303.13873v2","updated":"2023-03-28T14:18:56Z","published":"2023-03-24T09:30:09Z","title":"Fantasia3D: Disentangling Geometry and Appearance for High-quality\n  Text-to-3D Content Creation","summary":"  Automatic 3D content creation has achieved rapid progress recently due to the\navailability of pre-trained, large language models and image diffusion models,\nforming the emerging topic of text-to-3D content creation. Existing text-to-3D\nmethods commonly use implicit scene representations, which couple the geometry\nand appearance via volume rendering and are suboptimal in terms of recovering\nfiner geometries and achieving photorealistic rendering; consequently, they are\nless effective for generating high-quality 3D assets. In this work, we propose\na new method of Fantasia3D for high-quality text-to-3D content creation. Key to\nFantasia3D is the disentangled modeling and learning of geometry and\nappearance. For geometry learning, we rely on a hybrid scene representation,\nand propose to encode surface normal extracted from the representation as the\ninput of the image diffusion model. For appearance modeling, we introduce the\nspatially varying bidirectional reflectance distribution function (BRDF) into\nthe text-to-3D task, and learn the surface material for photorealistic\nrendering of the generated surface. Our disentangled framework is more\ncompatible with popular graphics engines, supporting relighting, editing, and\nphysical simulation of the generated 3D assets. We conduct thorough experiments\nthat show the advantages of our method over existing ones under different\ntext-to-3D task settings. Project page and source codes:\nhttps://fantasia3d.github.io/.\n","authors":["Rui Chen","Yongwei Chen","Ningxin Jiao","Kui Jia"],"pdf_url":"https://arxiv.org/pdf/2303.13873v2.pdf","comment":"Project page: https://fantasia3d.github.io/"},{"id":"http://arxiv.org/abs/2303.16001v1","updated":"2023-03-28T14:16:08Z","published":"2023-03-28T14:16:08Z","title":"Adaptive Voronoi NeRFs","summary":"  Neural Radiance Fields (NeRFs) learn to represent a 3D scene from just a set\nof registered images. Increasing sizes of a scene demands more complex\nfunctions, typically represented by neural networks, to capture all details.\nTraining and inference then involves querying the neural network millions of\ntimes per image, which becomes impractically slow. Since such complex functions\ncan be replaced by multiple simpler functions to improve speed, we show that a\nhierarchy of Voronoi diagrams is a suitable choice to partition the scene. By\nequipping each Voronoi cell with its own NeRF, our approach is able to quickly\nlearn a scene representation. We propose an intuitive partitioning of the space\nthat increases quality gains during training by distributing information evenly\namong the networks and avoids artifacts through a top-down adaptive refinement.\nOur framework is agnostic to the underlying NeRF method and easy to implement,\nwhich allows it to be applied to various NeRF variants for improved learning\nand rendering speeds.\n","authors":["Tim Elsner","Victor Czech","Julia Berger","Zain Selman","Isaak Lim","Leif Kobbelt"],"pdf_url":"https://arxiv.org/pdf/2303.16001v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.15999v1","updated":"2023-03-28T14:15:13Z","published":"2023-03-28T14:15:13Z","title":"Thread Counting in Plain Weave for Old Paintings Using Semi-Supervised\n  Regression Deep Learning Models","summary":"  In this work the authors develop regression approaches based on deep learning\nto perform thread density estimation for plain weave canvas analysis. Previous\napproaches were based on Fourier analysis, that are quite robust for some\nscenarios but fail in some other, in machine learning tools, that involve\npre-labeling of the painting at hand, or the segmentation of thread crossing\npoints, that provides good estimations in all scenarios with no need of\npre-labeling. The segmentation approach is time-consuming as estimation of the\ndensities is performed after locating the crossing points. In this novel\nproposal, we avoid this step by computing the density of threads directly from\nthe image with a regression deep learning model. We also incorporate some\nimprovements in the initial preprocessing of the input image with an impact on\nthe final error. Several models are proposed and analyzed to retain the best\none. Furthermore, we further reduce the density estimation error by introducing\na semi-supervised approach. The performance of our novel algorithm is analyzed\nwith works by Ribera, Vel\\'azquez, and Poussin where we compare our results to\nthe ones of previous approaches. Finally, the method is put into practice to\nsupport the change of authorship or a masterpiece at the Museo del Prado.\n","authors":["A. D. Benjarano","Juan J. Murillo-Fuentes","Laura Alba-Carcelen"],"pdf_url":"https://arxiv.org/pdf/2303.15999v1.pdf","comment":"21 pages"},{"id":"http://arxiv.org/abs/2209.01589v3","updated":"2023-03-28T14:15:08Z","published":"2022-09-04T10:21:02Z","title":"Consistent-Teacher: Towards Reducing Inconsistent Pseudo-targets in\n  Semi-supervised Object Detection","summary":"  In this study, we dive deep into the inconsistency of pseudo targets in\nsemi-supervised object detection (SSOD). Our core observation is that the\noscillating pseudo-targets undermine the training of an accurate detector. It\ninjects noise into the student's training, leading to severe overfitting\nproblems. Therefore, we propose a systematic solution, termed\nConsistentTeacher, to reduce the inconsistency. First, adaptive anchor\nassignment~(ASA) substitutes the static IoU-based strategy, which enables the\nstudent network to be resistant to noisy pseudo-bounding boxes. Then we\ncalibrate the subtask predictions by designing a 3D feature alignment\nmodule~(FAM-3D). It allows each classification feature to adaptively query the\noptimal feature vector for the regression task at arbitrary scales and\nlocations. Lastly, a Gaussian Mixture Model (GMM) dynamically revises the score\nthreshold of pseudo-bboxes, which stabilizes the number of ground truths at an\nearly stage and remedies the unreliable supervision signal during training.\nConsistentTeacher provides strong results on a large range of SSOD evaluations.\nIt achieves 40.0 mAP with ResNet-50 backbone given only 10% of annotated\nMS-COCO data, which surpasses previous baselines using pseudo labels by around\n3 mAP. When trained on fully annotated MS-COCO with additional unlabeled data,\nthe performance further increases to 47.7 mAP. Our code is available at\n\\url{https://github.com/Adamdad/ConsistentTeacher}.\n","authors":["Xinjiang Wang","Xingyi Yang","Shilong Zhang","Yijiang Li","Litong Feng","Shijie Fang","Chengqi Lyu","Kai Chen","Wayne Zhang"],"pdf_url":"https://arxiv.org/pdf/2209.01589v3.pdf","comment":"CVPR2023 (Highlight), Camera Ready Version, Project Page:\n  \\url{https://adamdad.github.io/consistentteacher/}"},{"id":"http://arxiv.org/abs/2303.15994v1","updated":"2023-03-28T14:08:09Z","published":"2023-03-28T14:08:09Z","title":"HiLo: Exploiting High Low Frequency Relations for Unbiased Panoptic\n  Scene Graph Generation","summary":"  Panoptic Scene Graph generation (PSG) is a recently proposed task in image\nscene understanding that aims to segment the image and extract triplets of\nsubjects, objects and their relations to build a scene graph. This task is\nparticularly challenging for two reasons. First, it suffers from a long-tail\nproblem in its relation categories, making naive biased methods more inclined\nto high-frequency relations. Existing unbiased methods tackle the long-tail\nproblem by data/loss rebalancing to favor low-frequency relations. Second, a\nsubject-object pair can have two or more semantically overlapping relations.\nWhile existing methods favor one over the other, our proposed HiLo framework\nlets different network branches specialize on low and high frequency relations,\nenforce their consistency and fuse the results. To the best of our knowledge we\nare the first to propose an explicitly unbiased PSG method. In extensive\nexperiments we show that our HiLo framework achieves state-of-the-art results\non the PSG task. We also apply our method to the Scene Graph Generation task\nthat predicts boxes instead of masks and see improvements over all baseline\nmethods.\n","authors":["Zijian Zhou","Miaojing Shi","Holger Caesar"],"pdf_url":"https://arxiv.org/pdf/2303.15994v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.15993v1","updated":"2023-03-28T14:08:05Z","published":"2023-03-28T14:08:05Z","title":"SELF-VS: Self-supervised Encoding Learning For Video Summarization","summary":"  Despite its wide range of applications, video summarization is still held\nback by the scarcity of extensive datasets, largely due to the labor-intensive\nand costly nature of frame-level annotations. As a result, existing video\nsummarization methods are prone to overfitting. To mitigate this challenge, we\npropose a novel self-supervised video representation learning method using\nknowledge distillation to pre-train a transformer encoder. Our method matches\nits semantic video representation, which is constructed with respect to frame\nimportance scores, to a representation derived from a CNN trained on video\nclassification. Empirical evaluations on correlation-based metrics, such as\nKendall's $\\tau$ and Spearman's $\\rho$ demonstrate the superiority of our\napproach compared to existing state-of-the-art methods in assigning relative\nscores to the input frames.\n","authors":["Hojjat Mokhtarabadi","Kave Bahraman","Mehrdad HosseinZadeh","Mahdi Eftekhari"],"pdf_url":"https://arxiv.org/pdf/2303.15993v1.pdf","comment":"9 pages, 5 figures"},{"id":"http://arxiv.org/abs/2212.07766v3","updated":"2023-03-28T13:59:47Z","published":"2022-12-15T12:36:49Z","title":"DeepLSD: Line Segment Detection and Refinement with Deep Image Gradients","summary":"  Line segments are ubiquitous in our human-made world and are increasingly\nused in vision tasks. They are complementary to feature points thanks to their\nspatial extent and the structural information they provide. Traditional line\ndetectors based on the image gradient are extremely fast and accurate, but lack\nrobustness in noisy images and challenging conditions. Their learned\ncounterparts are more repeatable and can handle challenging images, but at the\ncost of a lower accuracy and a bias towards wireframe lines. We propose to\ncombine traditional and learned approaches to get the best of both worlds: an\naccurate and robust line detector that can be trained in the wild without\nground truth lines. Our new line segment detector, DeepLSD, processes images\nwith a deep network to generate a line attraction field, before converting it\nto a surrogate image gradient magnitude and angle, which is then fed to any\nexisting handcrafted line detector. Additionally, we propose a new optimization\ntool to refine line segments based on the attraction field and vanishing\npoints. This refinement improves the accuracy of current deep detectors by a\nlarge margin. We demonstrate the performance of our method on low-level line\ndetection metrics, as well as on several downstream tasks using multiple\nchallenging datasets. The source code and models are available at\nhttps://github.com/cvg/DeepLSD.\n","authors":["Rémi Pautrat","Daniel Barath","Viktor Larsson","Martin R. Oswald","Marc Pollefeys"],"pdf_url":"https://arxiv.org/pdf/2212.07766v3.pdf","comment":"Accepted at CVPR 2023"},{"id":"http://arxiv.org/abs/2212.14504v2","updated":"2023-03-28T13:58:14Z","published":"2022-12-30T00:59:46Z","title":"Improving Visual Representation Learning through Perceptual\n  Understanding","summary":"  We present an extension to masked autoencoders (MAE) which improves on the\nrepresentations learnt by the model by explicitly encouraging the learning of\nhigher scene-level features. We do this by: (i) the introduction of a\nperceptual similarity term between generated and real images (ii) incorporating\nseveral techniques from the adversarial training literature including\nmulti-scale training and adaptive discriminator augmentation. The combination\nof these results in not only better pixel reconstruction but also\nrepresentations which appear to capture better higher-level details within\nimages. More consequentially, we show how our method, Perceptual MAE, leads to\nbetter performance when used for downstream tasks outperforming previous\nmethods. We achieve 78.1% top-1 accuracy linear probing on ImageNet-1K and up\nto 88.1% when fine-tuning, with similar results for other downstream tasks, all\nwithout use of additional pre-trained models or data.\n","authors":["Samyakh Tukra","Frederick Hoffman","Ken Chatfield"],"pdf_url":"https://arxiv.org/pdf/2212.14504v2.pdf","comment":"v2: add additional details on MSG-MAE. In Proc CVPR 2023"},{"id":"http://arxiv.org/abs/2303.15975v1","updated":"2023-03-28T13:47:16Z","published":"2023-03-28T13:47:16Z","title":"Large-scale Pre-trained Models are Surprisingly Strong in Incremental\n  Novel Class Discovery","summary":"  Discovering novel concepts from unlabelled data and in a continuous manner is\nan important desideratum of lifelong learners. In the literature such problems\nhave been partially addressed under very restricted settings, where either\naccess to labelled data is provided for discovering novel concepts (e.g., NCD)\nor learning occurs for a limited number of incremental steps (e.g.,\nclass-iNCD). In this work we challenge the status quo and propose a more\nchallenging and practical learning paradigm called MSc-iNCD, where learning\noccurs continuously and unsupervisedly, while exploiting the rich priors from\nlarge-scale pre-trained models. To this end, we propose simple baselines that\nare not only resilient under longer learning scenarios, but are surprisingly\nstrong when compared with sophisticated state-of-the-art methods. We conduct\nextensive empirical evaluation on a multitude of benchmarks and show the\neffectiveness of our proposed baselines, which significantly raises the bar.\n","authors":["Mingxuan Liu","Subhankar Roy","Zhun Zhong","Nicu Sebe","Elisa Ricci"],"pdf_url":"https://arxiv.org/pdf/2303.15975v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.15965v1","updated":"2023-03-28T13:35:10Z","published":"2023-03-28T13:35:10Z","title":"SFHarmony: Source Free Domain Adaptation for Distributed Neuroimaging\n  Analysis","summary":"  To represent the biological variability of clinical neuroimaging populations,\nit is vital to be able to combine data across scanners and studies. However,\ndifferent MRI scanners produce images with different characteristics, resulting\nin a domain shift known as the `harmonisation problem'. Additionally,\nneuroimaging data is inherently personal in nature, leading to data privacy\nconcerns when sharing the data. To overcome these barriers, we propose an\nUnsupervised Source-Free Domain Adaptation (SFDA) method, SFHarmony. Through\nmodelling the imaging features as a Gaussian Mixture Model and minimising an\nadapted Bhattacharyya distance between the source and target features, we can\ncreate a model that performs well for the target data whilst having a shared\nfeature representation across the data domains, without needing access to the\nsource data for adaptation or target labels. We demonstrate the performance of\nour method on simulated and real domain shifts, showing that the approach is\napplicable to classification, segmentation and regression tasks, requiring no\nchanges to the algorithm. Our method outperforms existing SFDA approaches\nacross a range of realistic data scenarios, demonstrating the potential utility\nof our approach for MRI harmonisation and general SFDA problems. Our code is\navailable at \\url{https://github.com/nkdinsdale/SFHarmony}.\n","authors":["Nicola K Dinsdale","Mark Jenkinson","Ana IL Namburete"],"pdf_url":"https://arxiv.org/pdf/2303.15965v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2205.10528v3","updated":"2023-03-28T13:18:22Z","published":"2022-05-21T07:40:18Z","title":"PointVector: A Vector Representation In Point Cloud Analysis","summary":"  In point cloud analysis, point-based methods have rapidly developed in recent\nyears. These methods have recently focused on concise MLP structures, such as\nPointNeXt, which have demonstrated competitiveness with Convolutional and\nTransformer structures. However, standard MLPs are limited in their ability to\nextract local features effectively. To address this limitation, we propose a\nVector-oriented Point Set Abstraction that can aggregate neighboring features\nthrough higher-dimensional vectors. To facilitate network optimization, we\nconstruct a transformation from scalar to vector using independent angles based\non 3D vector rotations. Finally, we develop a PointVector model that follows\nthe structure of PointNeXt. Our experimental results demonstrate that\nPointVector achieves state-of-the-art performance $\\textbf{72.3\\% mIOU}$ on the\nS3DIS Area 5 and $\\textbf{78.4\\% mIOU}$ on the S3DIS (6-fold cross-validation)\nwith only $\\textbf{58\\%}$ model parameters of PointNeXt. We hope our work will\nhelp the exploration of concise and effective feature representations. The code\nwill be released soon.\n","authors":["Xin Deng","WenYu Zhang","Qing Ding","XinMing Zhang"],"pdf_url":"https://arxiv.org/pdf/2205.10528v3.pdf","comment":"Accepted by CVPR2023"},{"id":"http://arxiv.org/abs/2303.15951v1","updated":"2023-03-28T13:09:44Z","published":"2023-03-28T13:09:44Z","title":"F$^{2}$-NeRF: Fast Neural Radiance Field Training with Free Camera\n  Trajectories","summary":"  This paper presents a novel grid-based NeRF called F2-NeRF (Fast-Free-NeRF)\nfor novel view synthesis, which enables arbitrary input camera trajectories and\nonly costs a few minutes for training. Existing fast grid-based NeRF training\nframeworks, like Instant-NGP, Plenoxels, DVGO, or TensoRF, are mainly designed\nfor bounded scenes and rely on space warping to handle unbounded scenes.\nExisting two widely-used space-warping methods are only designed for the\nforward-facing trajectory or the 360-degree object-centric trajectory but\ncannot process arbitrary trajectories. In this paper, we delve deep into the\nmechanism of space warping to handle unbounded scenes. Based on our analysis,\nwe further propose a novel space-warping method called perspective warping,\nwhich allows us to handle arbitrary trajectories in the grid-based NeRF\nframework. Extensive experiments demonstrate that F2-NeRF is able to use the\nsame perspective warping to render high-quality images on two standard datasets\nand a new free trajectory dataset collected by us. Project page:\nhttps://totoro97.github.io/projects/f2-nerf.\n","authors":["Peng Wang","Yuan Liu","Zhaoxi Chen","Lingjie Liu","Ziwei Liu","Taku Komura","Christian Theobalt","Wenping Wang"],"pdf_url":"https://arxiv.org/pdf/2303.15951v1.pdf","comment":"CVPR 2023. Project page: https://totoro97.github.io/projects/f2-nerf"},{"id":"http://arxiv.org/abs/2111.07632v3","updated":"2023-03-28T13:06:17Z","published":"2021-11-15T09:35:54Z","title":"CoReS: Compatible Representations via Stationarity","summary":"  Compatible features enable the direct comparison of old and new learned\nfeatures allowing to use them interchangeably over time. In visual search\nsystems, this eliminates the need to extract new features from the gallery-set\nwhen the representation model is upgraded with novel data. This has a big value\nin real applications as re-indexing the gallery-set can be computationally\nexpensive when the gallery-set is large, or even infeasible due to privacy or\nother concerns of the application. In this paper, we propose CoReS, a new\ntraining procedure to learn representations that are \\textit{compatible} with\nthose previously learned, grounding on the stationarity of the features as\nprovided by fixed classifiers based on polytopes. With this solution, classes\nare maximally separated in the representation space and maintain their spatial\nconfiguration stationary as new classes are added, so that there is no need to\nlearn any mappings between representations nor to impose pairwise training with\nthe previously learned model. We demonstrate that our training procedure\nlargely outperforms the current state of the art and is particularly effective\nin the case of multiple upgrades of the training-set, which is the typical case\nin real applications.\n","authors":["Niccolo Biondi","Federico Pernici","Matteo Bruni","Alberto Del Bimbo"],"pdf_url":"https://arxiv.org/pdf/2111.07632v3.pdf","comment":"in IEEE Transactions on Pattern Analysis and Machine Intelligence,\n  2023. Code: https://github.com/NiccoBiondi/cores-compatibility"},{"id":"http://arxiv.org/abs/2303.15947v1","updated":"2023-03-28T13:00:08Z","published":"2023-03-28T13:00:08Z","title":"Deep Selection: A Fully Supervised Camera Selection Network for Surgery\n  Recordings","summary":"  Recording surgery in operating rooms is an essential task for education and\nevaluation of medical treatment. However, recording the desired targets, such\nas the surgery field, surgical tools, or doctor's hands, is difficult because\nthe targets are heavily occluded during surgery. We use a recording system in\nwhich multiple cameras are embedded in the surgical lamp, and we assume that at\nleast one camera is recording the target without occlusion at any given time.\nAs the embedded cameras obtain multiple video sequences, we address the task of\nselecting the camera with the best view of the surgery. Unlike the conventional\nmethod, which selects the camera based on the area size of the surgery field,\nwe propose a deep neural network that predicts the camera selection probability\nfrom multiple video sequences by learning the supervision of the expert\nannotation. We created a dataset in which six different types of plastic\nsurgery are recorded, and we provided the annotation of camera switching. Our\nexperiments show that our approach successfully switched between cameras and\noutperformed three baseline methods.\n","authors":["Ryo Hachiuma","Tomohiro Shimizu","Hideo Saito","Hiroki Kajita","Yoshifumi Takatsume"],"pdf_url":"https://arxiv.org/pdf/2303.15947v1.pdf","comment":"MICCAI 2020"},{"id":"http://arxiv.org/abs/2303.15938v1","updated":"2023-03-28T12:49:10Z","published":"2023-03-28T12:49:10Z","title":"fRegGAN with K-space Loss Regularization for Medical Image Translation","summary":"  Generative adversarial networks (GANs) have shown remarkable success in\ngenerating realistic images and are increasingly used in medical imaging for\nimage-to-image translation tasks. However, GANs tend to suffer from a frequency\nbias towards low frequencies, which can lead to the removal of important\nstructures in the generated images. To address this issue, we propose a novel\nfrequency-aware image-to-image translation framework based on the supervised\nRegGAN approach, which we call fRegGAN. The framework employs a K-space loss to\nregularize the frequency content of the generated images and incorporates\nwell-known properties of MRI K-space geometry to guide the network training\nprocess. By combine our method with the RegGAN approach, we can mitigate the\neffect of training with misaligned data and frequency bias at the same time. We\nevaluate our method on the public BraTS dataset and outperform the baseline\nmethods in terms of both quantitative and qualitative metrics when synthesizing\nT2-weighted from T1-weighted MR images. Detailed ablation studies are provided\nto understand the effect of each modification on the final performance. The\nproposed method is a step towards improving the performance of image-to-image\ntranslation and synthesis in the medical domain and shows promise for other\napplications in the field of image processing and generation.\n","authors":["Ivo M. Baltruschat","Felix Kreis","Alexander Hoelscher","Melanie Dohmen","Matthias Lenga"],"pdf_url":"https://arxiv.org/pdf/2303.15938v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.15937v1","updated":"2023-03-28T12:48:36Z","published":"2023-03-28T12:48:36Z","title":"PosterLayout: A New Benchmark and Approach for Content-aware\n  Visual-Textual Presentation Layout","summary":"  Content-aware visual-textual presentation layout aims at arranging spatial\nspace on the given canvas for pre-defined elements, including text, logo, and\nunderlay, which is a key to automatic template-free creative graphic design. In\npractical applications, e.g., poster designs, the canvas is originally\nnon-empty, and both inter-element relationships as well as inter-layer\nrelationships should be concerned when generating a proper layout. A few recent\nworks deal with them simultaneously, but they still suffer from poor graphic\nperformance, such as a lack of layout variety or spatial non-alignment. Since\ncontent-aware visual-textual presentation layout is a novel task, we first\nconstruct a new dataset named PosterLayout, which consists of 9,974\nposter-layout pairs and 905 images, i.e., non-empty canvases. It is more\nchallenging and useful for greater layout variety, domain diversity, and\ncontent diversity. Then, we propose design sequence formation (DSF) that\nreorganizes elements in layouts to imitate the design processes of human\ndesigners, and a novel CNN-LSTM-based conditional generative adversarial\nnetwork (GAN) is presented to generate proper layouts. Specifically, the\ndiscriminator is design-sequence-aware and will supervise the \"design\" process\nof the generator. Experimental results verify the usefulness of the new\nbenchmark and the effectiveness of the proposed approach, which achieves the\nbest performance by generating suitable layouts for diverse canvases.\n","authors":["HsiaoYuan Hsu","Xiangteng He","Yuxin Peng","Hao Kong","Qing Zhang"],"pdf_url":"https://arxiv.org/pdf/2303.15937v1.pdf","comment":"Accepted to CVPR 2023. Dataset and code are available at\n  https://github.com/PKU-ICST-MIPL/PosterLayout-CVPR2023"},{"id":"http://arxiv.org/abs/2303.01979v3","updated":"2023-03-28T12:44:30Z","published":"2023-03-02T08:02:45Z","title":"ACL-SPC: Adaptive Closed-Loop system for Self-Supervised Point Cloud\n  Completion","summary":"  Point cloud completion addresses filling in the missing parts of a partial\npoint cloud obtained from depth sensors and generating a complete point cloud.\nAlthough there has been steep progress in the supervised methods on the\nsynthetic point cloud completion task, it is hardly applicable in real-world\nscenarios due to the domain gap between the synthetic and real-world datasets\nor the requirement of prior information. To overcome these limitations, we\npropose a novel self-supervised framework ACL-SPC for point cloud completion to\ntrain and test on the same data. ACL-SPC takes a single partial input and\nattempts to output the complete point cloud using an adaptive closed-loop (ACL)\nsystem that enforces the output same for the variation of an input. We evaluate\nour proposed ACL-SPC on various datasets to prove that it can successfully\nlearn to complete a partial point cloud as the first self-supervised scheme.\nResults show that our method is comparable with unsupervised methods and\nachieves superior performance on the real-world dataset compared to the\nsupervised methods trained on the synthetic dataset. Extensive experiments\njustify the necessity of self-supervised learning and the effectiveness of our\nproposed method for the real-world point cloud completion task. The code is\npublicly available from https://github.com/Sangminhong/ACL-SPC_PyTorch\n","authors":["Sangmin Hong","Mohsen Yavartanoo","Reyhaneh Neshatavar","Kyoung Mu Lee"],"pdf_url":"https://arxiv.org/pdf/2303.01979v3.pdf","comment":"Published at CVPR 2023"},{"id":"http://arxiv.org/abs/2303.15932v1","updated":"2023-03-28T12:42:12Z","published":"2023-03-28T12:42:12Z","title":"Unify, Align and Refine: Multi-Level Semantic Alignment for Radiology\n  Report Generation","summary":"  Automatic radiology report generation has attracted enormous research\ninterest due to its practical value in reducing the workload of radiologists.\nHowever, simultaneously establishing global correspondences between the image\n(e.g., Chest X-ray) and its related report and local alignments between image\npatches and keywords remains challenging. To this end, we propose an Unify,\nAlign and then Refine (UAR) approach to learn multi-level cross-modal\nalignments and introduce three novel modules: Latent Space Unifier (LSU),\nCross-modal Representation Aligner (CRA) and Text-to-Image Refiner (TIR).\nSpecifically, LSU unifies multimodal data into discrete tokens, making it\nflexible to learn common knowledge among modalities with a shared network. The\nmodality-agnostic CRA learns discriminative features via a set of orthonormal\nbasis and a dual-gate mechanism first and then globally aligns visual and\ntextual representations under a triplet contrastive loss. TIR boosts\ntoken-level local alignment via calibrating text-to-image attention with a\nlearnable mask. Additionally, we design a two-stage training procedure to make\nUAR gradually grasp cross-modal alignments at different levels, which imitates\nradiologists' workflow: writing sentence by sentence first and then checking\nword by word. Extensive experiments and analyses on IU-Xray and MIMIC-CXR\nbenchmark datasets demonstrate the superiority of our UAR against varied\nstate-of-the-art methods.\n","authors":["Yaowei L","Bang Yang","Xuxin Cheng","Zhihong Zhu","Hongxiang Li","Yuexian Zou"],"pdf_url":"https://arxiv.org/pdf/2303.15932v1.pdf","comment":"8 pages,6 figures,4 tables"},{"id":"http://arxiv.org/abs/2303.15919v1","updated":"2023-03-28T12:20:52Z","published":"2023-03-28T12:20:52Z","title":"Hyperbolic Geometry in Computer Vision: A Novel Framework for\n  Convolutional Neural Networks","summary":"  Real-world visual data exhibit intrinsic hierarchical structures that can be\nrepresented effectively in hyperbolic spaces. Hyperbolic neural networks (HNNs)\nare a promising approach for learning feature representations in such spaces.\nHowever, current methods in computer vision rely on Euclidean backbones and\nonly project features to the hyperbolic space in the task heads, limiting their\nability to fully leverage the benefits of hyperbolic geometry. To address this,\nwe present HCNN, the first fully hyperbolic convolutional neural network (CNN)\ndesigned for computer vision tasks. Based on the Lorentz model, we generalize\nfundamental components of CNNs and propose novel formulations of the\nconvolutional layer, batch normalization, and multinomial logistic regression\n(MLR). Experimentation on standard vision tasks demonstrates the effectiveness\nof our HCNN framework and the Lorentz model in both hybrid and fully hyperbolic\nsettings. Overall, we aim to pave the way for future research in hyperbolic\ncomputer vision by offering a new paradigm for interpreting and analyzing\nvisual data. Our code is publicly available at\nhttps://github.com/kschwethelm/HyperbolicCV.\n","authors":["Ahmad Bdeir","Kristian Schwethelm","Niels Landwehr"],"pdf_url":"https://arxiv.org/pdf/2303.15919v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.15904v1","updated":"2023-03-28T11:48:07Z","published":"2023-03-28T11:48:07Z","title":"Mask-Free Video Instance Segmentation","summary":"  The recent advancement in Video Instance Segmentation (VIS) has largely been\ndriven by the use of deeper and increasingly data-hungry transformer-based\nmodels. However, video masks are tedious and expensive to annotate, limiting\nthe scale and diversity of existing VIS datasets. In this work, we aim to\nremove the mask-annotation requirement. We propose MaskFreeVIS, achieving\nhighly competitive VIS performance, while only using bounding box annotations\nfor the object state. We leverage the rich temporal mask consistency\nconstraints in videos by introducing the Temporal KNN-patch Loss (TK-Loss),\nproviding strong mask supervision without any labels. Our TK-Loss finds\none-to-many matches across frames, through an efficient patch-matching step\nfollowed by a K-nearest neighbor selection. A consistency loss is then enforced\non the found matches. Our mask-free objective is simple to implement, has no\ntrainable parameters, is computationally efficient, yet outperforms baselines\nemploying, e.g., state-of-the-art optical flow to enforce temporal mask\nconsistency. We validate MaskFreeVIS on the YouTube-VIS 2019/2021, OVIS and\nBDD100K MOTS benchmarks. The results clearly demonstrate the efficacy of our\nmethod by drastically narrowing the gap between fully and weakly-supervised VIS\nperformance. Our code and trained models are available at\nhttps://github.com/SysCV/MaskFreeVis.\n","authors":["Lei Ke","Martin Danelljan","Henghui Ding","Yu-Wing Tai","Chi-Keung Tang","Fisher Yu"],"pdf_url":"https://arxiv.org/pdf/2303.15904v1.pdf","comment":"Accepted in CVPR 2023; Code: https://github.com/SysCV/MaskFreeVis;\n  Project page: http://vis.xyz/pub/maskfreevis"},{"id":"http://arxiv.org/abs/2211.11592v3","updated":"2023-03-28T11:31:08Z","published":"2022-11-21T15:48:13Z","title":"Guided Depth Super-Resolution by Deep Anisotropic Diffusion","summary":"  Performing super-resolution of a depth image using the guidance from an RGB\nimage is a problem that concerns several fields, such as robotics, medical\nimaging, and remote sensing. While deep learning methods have achieved good\nresults in this problem, recent work highlighted the value of combining modern\nmethods with more formal frameworks. In this work, we propose a novel approach\nwhich combines guided anisotropic diffusion with a deep convolutional network\nand advances the state of the art for guided depth super-resolution. The edge\ntransferring/enhancing properties of the diffusion are boosted by the\ncontextual reasoning capabilities of modern networks, and a strict adjustment\nstep guarantees perfect adherence to the source image. We achieve unprecedented\nresults in three commonly used benchmarks for guided depth super-resolution.\nThe performance gain compared to other methods is the largest at larger scales,\nsuch as x32 scaling. Code\n(https://github.com/prs-eth/Diffusion-Super-Resolution) for the proposed method\nis available to promote reproducibility of our results.\n","authors":["Nando Metzger","Rodrigo Caye Daudt","Konrad Schindler"],"pdf_url":"https://arxiv.org/pdf/2211.11592v3.pdf","comment":"8 main pages, Accepted to CVPR2023"},{"id":"http://arxiv.org/abs/2303.11897v2","updated":"2023-03-28T11:21:36Z","published":"2023-03-21T14:41:02Z","title":"TIFA: Accurate and Interpretable Text-to-Image Faithfulness Evaluation\n  with Question Answering","summary":"  Despite thousands of researchers, engineers, and artists actively working on\nimproving text-to-image generation models, systems often fail to produce images\nthat accurately align with the text inputs. We introduce TIFA (Text-to-Image\nFaithfulness evaluation with question Answering), an automatic evaluation\nmetric that measures the faithfulness of a generated image to its text input\nvia visual question answering (VQA). Specifically, given a text input, we\nautomatically generate several question-answer pairs using a language model. We\ncalculate image faithfulness by checking whether existing VQA models can answer\nthese questions using the generated image. TIFA is a reference-free metric that\nallows for fine-grained and interpretable evaluations of generated images. TIFA\nalso has better correlations with human judgments than existing metrics. Based\non this approach, we introduce TIFA v1.0, a benchmark consisting of 4K diverse\ntext inputs and 25K questions across 12 categories (object, counting, etc.). We\npresent a comprehensive evaluation of existing text-to-image models using TIFA\nv1.0 and highlight the limitations and challenges of current models. For\ninstance, we find that current text-to-image models, despite doing well on\ncolor and material, still struggle in counting, spatial relations, and\ncomposing multiple objects. We hope our benchmark will help carefully measure\nthe research progress in text-to-image synthesis and provide valuable insights\nfor further research.\n","authors":["Yushi Hu","Benlin Liu","Jungo Kasai","Yizhong Wang","Mari Ostendorf","Ranjay Krishna","Noah A. Smith"],"pdf_url":"https://arxiv.org/pdf/2303.11897v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.15893v1","updated":"2023-03-28T11:15:57Z","published":"2023-03-28T11:15:57Z","title":"VIVE3D: Viewpoint-Independent Video Editing using 3D-Aware GANs","summary":"  We introduce VIVE3D, a novel approach that extends the capabilities of\nimage-based 3D GANs to video editing and is able to represent the input video\nin an identity-preserving and temporally consistent way. We propose two new\nbuilding blocks. First, we introduce a novel GAN inversion technique\nspecifically tailored to 3D GANs by jointly embedding multiple frames and\noptimizing for the camera parameters. Second, besides traditional semantic face\nedits (e.g. for age and expression), we are the first to demonstrate edits that\nshow novel views of the head enabled by the inherent properties of 3D GANs and\nour optical flow-guided compositing technique to combine the head with the\nbackground video. Our experiments demonstrate that VIVE3D generates\nhigh-fidelity face edits at consistent quality from a range of camera\nviewpoints which are composited with the original video in a temporally and\nspatially consistent manner.\n","authors":["Anna Frühstück","Nikolaos Sarafianos","Yuanlu Xu","Peter Wonka","Tony Tung"],"pdf_url":"https://arxiv.org/pdf/2303.15893v1.pdf","comment":"CVPR 2023. Project webpage and video available at\n  http://afruehstueck.github.io/vive3D"},{"id":"http://arxiv.org/abs/2211.09699v3","updated":"2023-03-28T11:14:23Z","published":"2022-11-15T19:07:53Z","title":"PromptCap: Prompt-Guided Task-Aware Image Captioning","summary":"  Knowledge-based visual question answering (VQA) involves questions that\nrequire world knowledge beyond the image to yield the correct answer. Large\nlanguage models (LMs) like GPT-3 are particularly helpful for this task because\nof their strong knowledge retrieval and reasoning capabilities. To enable LM to\nunderstand images, prior work uses a captioning model to convert images into\ntext. However, when summarizing an image in a single caption sentence, which\nvisual entities to describe are often underspecified. Generic image captions\noften miss visual details essential for the LM to answer visual questions\ncorrectly. To address this challenge, we propose PromptCap (Prompt-guided image\nCaptioning), a captioning model designed to serve as a better connector between\nimages and black-box LMs. Different from generic captions, PromptCap takes a\nnatural-language prompt to control the visual entities to describe in the\ngenerated caption. The prompt contains a question that the caption should aid\nin answering. To avoid extra annotation, PromptCap is trained by examples\nsynthesized with GPT-3 and existing datasets. We demonstrate PromptCap's\neffectiveness on an existing pipeline in which GPT-3 is prompted with image\ncaptions to carry out VQA. PromptCap outperforms generic captions by a large\nmargin and achieves state-of-the-art accuracy on knowledge-based VQA tasks\n(60.4% on OK-VQA and 59.6% on A-OKVQA). Zero-shot results on WebQA show that\nPromptCap generalizes well to unseen domains.\n","authors":["Yushi Hu","Hang Hua","Zhengyuan Yang","Weijia Shi","Noah A. Smith","Jiebo Luo"],"pdf_url":"https://arxiv.org/pdf/2211.09699v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.15892v1","updated":"2023-03-28T11:12:26Z","published":"2023-03-28T11:12:26Z","title":"Head3D: Complete 3D Head Generation via Tri-plane Feature Distillation","summary":"  Head generation with diverse identities is an important task in computer\nvision and computer graphics, widely used in multimedia applications. However,\ncurrent full head generation methods require a large number of 3D scans or\nmulti-view images to train the model, resulting in expensive data acquisition\ncost. To address this issue, we propose Head3D, a method to generate full 3D\nheads with limited multi-view images. Specifically, our approach first extracts\nfacial priors represented by tri-planes learned in EG3D, a 3D-aware generative\nmodel, and then proposes feature distillation to deliver the 3D frontal faces\ninto complete heads without compromising head integrity. To mitigate the domain\ngap between the face and head models, we present dual-discriminators to guide\nthe frontal and back head generation, respectively. Our model achieves\ncost-efficient and diverse complete head generation with photo-realistic\nrenderings and high-quality geometry representations. Extensive experiments\ndemonstrate the effectiveness of our proposed Head3D, both qualitatively and\nquantitatively.\n","authors":["Yuhao Cheng","Yichao Yan","Wenhan Zhu","Ye Pan","Bowen Pan","Xiaokang Yang"],"pdf_url":"https://arxiv.org/pdf/2303.15892v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2203.06111v4","updated":"2023-03-28T11:11:08Z","published":"2022-03-11T17:32:27Z","title":"Multi-sensor large-scale dataset for multi-view 3D reconstruction","summary":"  We present a new multi-sensor dataset for multi-view 3D surface\nreconstruction. It includes registered RGB and depth data from sensors of\ndifferent resolutions and modalities: smartphones, Intel RealSense, Microsoft\nKinect, industrial cameras, and structured-light scanner. The scenes are\nselected to emphasize a diverse set of material properties challenging for\nexisting algorithms. We provide around 1.4 million images of 107 different\nscenes acquired from 100 viewing directions under 14 lighting conditions. We\nexpect our dataset will be useful for evaluation and training of 3D\nreconstruction algorithms and for related tasks. The dataset is available at\nskoltech3d.appliedai.tech.\n","authors":["Oleg Voynov","Gleb Bobrovskikh","Pavel Karpyshev","Saveliy Galochkin","Andrei-Timotei Ardelean","Arseniy Bozhenko","Ekaterina Karmanova","Pavel Kopanev","Yaroslav Labutin-Rymsho","Ruslan Rakhimov","Aleksandr Safin","Valerii Serpiva","Alexey Artemov","Evgeny Burnaev","Dzmitry Tsetserukou","Denis Zorin"],"pdf_url":"https://arxiv.org/pdf/2203.06111v4.pdf","comment":"v4: final camera-ready version"},{"id":"http://arxiv.org/abs/2302.06586v3","updated":"2023-03-28T11:09:51Z","published":"2023-02-13T18:37:37Z","title":"Stitchable Neural Networks","summary":"  The public model zoo containing enormous powerful pretrained model families\n(e.g., ResNet/DeiT) has reached an unprecedented scope than ever, which\nsignificantly contributes to the success of deep learning. As each model family\nconsists of pretrained models with diverse scales (e.g., DeiT-Ti/S/B), it\nnaturally arises a fundamental question of how to efficiently assemble these\nreadily available models in a family for dynamic accuracy-efficiency trade-offs\nat runtime. To this end, we present Stitchable Neural Networks (SN-Net), a\nnovel scalable and efficient framework for model deployment. It cheaply\nproduces numerous networks with different complexity and performance trade-offs\ngiven a family of pretrained neural networks, which we call anchors.\nSpecifically, SN-Net splits the anchors across the blocks/layers and then\nstitches them together with simple stitching layers to map the activations from\none anchor to another. With only a few epochs of training, SN-Net effectively\ninterpolates between the performance of anchors with varying scales. At\nruntime, SN-Net can instantly adapt to dynamic resource constraints by\nswitching the stitching positions. Extensive experiments on ImageNet\nclassification demonstrate that SN-Net can obtain on-par or even better\nperformance than many individually trained networks while supporting diverse\ndeployment scenarios. For example, by stitching Swin Transformers, we challenge\nhundreds of models in Timm model zoo with a single network. We believe this new\nelastic model framework can serve as a strong baseline for further research in\nwider communities.\n","authors":["Zizheng Pan","Jianfei Cai","Bohan Zhuang"],"pdf_url":"https://arxiv.org/pdf/2302.06586v3.pdf","comment":"CVPR 2023 Highlight; Project is available at https://snnet.github.io/"},{"id":"http://arxiv.org/abs/2303.15889v1","updated":"2023-03-28T11:04:18Z","published":"2023-03-28T11:04:18Z","title":"Metrics for Dataset Demographic Bias: A Case Study on Facial Expression\n  Recognition","summary":"  Demographic biases in source datasets have been shown as one of the causes of\nunfairness and discrimination in the predictions of Machine Learning models.\nOne of the most prominent types of demographic bias are statistical imbalances\nin the representation of demographic groups in the datasets. In this paper, we\nstudy the measurement of these biases by reviewing the existing metrics,\nincluding those that can be borrowed from other disciplines. We develop a\ntaxonomy for the classification of these metrics, providing a practical guide\nfor the selection of appropriate metrics. To illustrate the utility of our\nframework, and to further understand the practical characteristics of the\nmetrics, we conduct a case study of 20 datasets used in Facial Emotion\nRecognition (FER), analyzing the biases present in them. Our experimental\nresults show that many metrics are redundant and that a reduced subset of\nmetrics may be sufficient to measure the amount of demographic bias. The paper\nprovides valuable insights for researchers in AI and related fields to mitigate\ndataset bias and improve the fairness and accuracy of AI models. The code is\navailable at https://github.com/irisdominguez/dataset_bias_metrics.\n","authors":["Iris Dominguez-Catena","Daniel Paternain","Mikel Galar"],"pdf_url":"https://arxiv.org/pdf/2303.15889v1.pdf","comment":"18 pages, 8 figures. Appendix included, 21 additional pages, 20\n  additional figures"},{"id":"http://arxiv.org/abs/2303.15888v1","updated":"2023-03-28T11:04:17Z","published":"2023-03-28T11:04:17Z","title":"Projected Latent Distillation for Data-Agnostic Consolidation in\n  Distributed Continual Learning","summary":"  Distributed learning on the edge often comprises self-centered devices (SCD)\nwhich learn local tasks independently and are unwilling to contribute to the\nperformance of other SDCs. How do we achieve forward transfer at zero cost for\nthe single SCDs? We formalize this problem as a Distributed Continual Learning\nscenario, where SCD adapt to local tasks and a CL model consolidates the\nknowledge from the resulting stream of models without looking at the SCD's\nprivate data. Unfortunately, current CL methods are not directly applicable to\nthis scenario. We propose Data-Agnostic Consolidation (DAC), a novel double\nknowledge distillation method that consolidates the stream of SC models without\nusing the original data. DAC performs distillation in the latent space via a\nnovel Projected Latent Distillation loss. Experimental results show that DAC\nenables forward transfer between SCDs and reaches state-of-the-art accuracy on\nSplit CIFAR100, CORe50 and Split TinyImageNet, both in reharsal-free and\ndistributed CL scenarios. Somewhat surprisingly, even a single\nout-of-distribution image is sufficient as the only source of data during\nconsolidation.\n","authors":["Antonio Carta","Andrea Cossu","Vincenzo Lomonaco","Davide Bacciu","Joost van de Weijer"],"pdf_url":"https://arxiv.org/pdf/2303.15888v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2211.10157v4","updated":"2023-03-28T11:02:19Z","published":"2022-11-17T05:09:58Z","title":"UMFuse: Unified Multi View Fusion for Human Editing applications","summary":"  Numerous pose-guided human editing methods have been explored by the vision\ncommunity due to their extensive practical applications. However, most of these\nmethods still use an image-to-image formulation in which a single image is\ngiven as input to produce an edited image as output. This objective becomes\nill-defined in cases when the target pose differs significantly from the input\npose. Existing methods then resort to in-painting or style transfer to handle\nocclusions and preserve content. In this paper, we explore the utilization of\nmultiple views to minimize the issue of missing information and generate an\naccurate representation of the underlying human model. To fuse knowledge from\nmultiple viewpoints, we design a multi-view fusion network that takes the pose\nkey points and texture from multiple source images and generates an explainable\nper-pixel appearance retrieval map. Thereafter, the encodings from a separate\nnetwork (trained on a single-view human reposing task) are merged in the latent\nspace. This enables us to generate accurate, precise, and visually coherent\nimages for different editing tasks. We show the application of our network on\ntwo newly proposed tasks - Multi-view human reposing and Mix&Match Human Image\ngeneration. Additionally, we study the limitations of single-view editing and\nscenarios in which multi-view provides a better alternative.\n","authors":["Rishabh Jain","Mayur Hemani","Duygu Ceylan","Krishna Kumar Singh","Jingwan Lu","Mausoom Sarkar","Balaji Krishnamurthy"],"pdf_url":"https://arxiv.org/pdf/2211.10157v4.pdf","comment":"8 pages, 6 figures"},{"id":"http://arxiv.org/abs/2207.07621v2","updated":"2023-03-28T10:58:12Z","published":"2022-07-15T17:32:37Z","title":"MegaPortraits: One-shot Megapixel Neural Head Avatars","summary":"  In this work, we advance the neural head avatar technology to the megapixel\nresolution while focusing on the particularly challenging task of cross-driving\nsynthesis, i.e., when the appearance of the driving image is substantially\ndifferent from the animated source image. We propose a set of new neural\narchitectures and training methods that can leverage both medium-resolution\nvideo data and high-resolution image data to achieve the desired levels of\nrendered image quality and generalization to novel views and motion. We\ndemonstrate that suggested architectures and methods produce convincing\nhigh-resolution neural avatars, outperforming the competitors in the\ncross-driving scenario. Lastly, we show how a trained high-resolution neural\navatar model can be distilled into a lightweight student model which runs in\nreal-time and locks the identities of neural avatars to several dozens of\npre-defined source images. Real-time operation and identity lock are essential\nfor many practical applications head avatar systems.\n","authors":["Nikita Drobyshev","Jenya Chelishev","Taras Khakhulin","Aleksei Ivakhnenko","Victor Lempitsky","Egor Zakharov"],"pdf_url":"https://arxiv.org/pdf/2207.07621v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.14613v2","updated":"2023-03-28T10:56:36Z","published":"2023-03-26T03:35:46Z","title":"GestureDiffuCLIP: Gesture Diffusion Model with CLIP Latents","summary":"  The automatic generation of stylized co-speech gestures has recently received\nincreasing attention. Previous systems typically allow style control via\npredefined text labels or example motion clips, which are often not flexible\nenough to convey user intent accurately. In this work, we present\nGestureDiffuCLIP, a neural network framework for synthesizing realistic,\nstylized co-speech gestures with flexible style control. We leverage the power\nof the large-scale Contrastive-Language-Image-Pre-training (CLIP) model and\npresent a novel CLIP-guided mechanism that extracts efficient style\nrepresentations from multiple input modalities, such as a piece of text, an\nexample motion clip, or a video. Our system learns a latent diffusion model to\ngenerate high-quality gestures and infuses the CLIP representations of style\ninto the generator via an adaptive instance normalization (AdaIN) layer. We\nfurther devise a gesture-transcript alignment mechanism that ensures a\nsemantically correct gesture generation based on contrastive learning. Our\nsystem can also be extended to allow fine-grained style control of individual\nbody parts. We demonstrate an extensive set of examples showing the flexibility\nand generalizability of our model to a variety of style descriptions. In a user\nstudy, we show that our system outperforms the state-of-the-art approaches\nregarding human likeness, appropriateness, and style correctness.\n","authors":["Tenglong Ao","Zeyi Zhang","Libin Liu"],"pdf_url":"https://arxiv.org/pdf/2303.14613v2.pdf","comment":"Video demo: https://youtu.be/Psi1IOZGq8c"},{"id":"http://arxiv.org/abs/2303.14175v2","updated":"2023-03-28T10:52:58Z","published":"2023-03-24T17:38:03Z","title":"Inherent Consistent Learning for Accurate Semi-supervised Medical Image\n  Segmentation","summary":"  Semi-supervised medical image segmentation has attracted much attention in\nrecent years because of the high cost of medical image annotations. In this\npaper, we propose a novel Inherent Consistent Learning (ICL) method, which aims\nto learn robust semantic category representations through the semantic\nconsistency guidance of labeled and unlabeled data to help segmentation. In\npractice, we introduce two external modules namely Supervised Semantic Proxy\nAdaptor (SSPA) and Unsupervised Semantic Consistent Learner (USCL) that based\non the attention mechanism to align the semantic category representations of\nlabeled and unlabeled data, as well as update the global semantic\nrepresentations over the entire training set. The proposed ICL is a\nplug-and-play scheme for various network architectures and the two modules are\nnot involved in the testing stage. Experimental results on three public\nbenchmarks show that the proposed method can outperform the state-of-the-art\nespecially when the number of annotated data is extremely limited. Code is\navailable at: https://github.com/zhuye98/ICL.git.\n","authors":["Ye Zhu","Jie Yang","Si-Qi Liu","Ruimao Zhang"],"pdf_url":"https://arxiv.org/pdf/2303.14175v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2301.05997v2","updated":"2023-03-28T10:49:59Z","published":"2023-01-15T02:04:02Z","title":"Exploiting Prompt Caption for Video Grounding","summary":"  Video grounding aims to locate a moment of interest matching the given query\nsentence from an untrimmed video. Previous works ignore the \\emph{sparsity\ndilemma} in video annotations, which fails to provide the context information\nbetween potential events and query sentences in the dataset. In this paper, we\ncontend that exploiting easily available captions which describe general\nactions \\ie, prompt captions (PC) defined in our paper, will significantly\nboost the performance. To this end, we propose a Prompt Caption Network (PCNet)\nfor video grounding. Specifically, we first introduce dense video captioning to\ngenerate dense captions and then obtain prompt captions by Non-Prompt Caption\nSuppression (NPCS). To capture the potential information in prompt captions, we\npropose Caption Guided Attention (CGA) project the semantic relations between\nprompt captions and query sentences into temporal space and fuse them into\nvisual representations. Considering the gap between prompt captions and ground\ntruth, we propose Asymmetric Cross-modal Contrastive Learning (ACCL) for\nconstructing more negative pairs to maximize cross-modal mutual information.\nWithout bells and whistles, extensive experiments on three public datasets\n(\\ie, ActivityNet Captions, TACoS and ActivityNet-CG) demonstrate that our\nmethod significantly outperforms state-of-the-art methods.\n","authors":["Hongxiang Li","Meng Cao","Xuxin Cheng","Zhihong Zhu","Yaowei Li","Yuexian Zou"],"pdf_url":"https://arxiv.org/pdf/2301.05997v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.15880v1","updated":"2023-03-28T10:48:33Z","published":"2023-03-28T10:48:33Z","title":"Novel View Synthesis of Humans using Differentiable Rendering","summary":"  We present a new approach for synthesizing novel views of people in new\nposes. Our novel differentiable renderer enables the synthesis of highly\nrealistic images from any viewpoint. Rather than operating over mesh-based\nstructures, our renderer makes use of diffuse Gaussian primitives that directly\nrepresent the underlying skeletal structure of a human. Rendering these\nprimitives gives results in a high-dimensional latent image, which is then\ntransformed into an RGB image by a decoder network. The formulation gives rise\nto a fully differentiable framework that can be trained end-to-end. We\ndemonstrate the effectiveness of our approach to image reconstruction on both\nthe Human3.6M and Panoptic Studio datasets. We show how our approach can be\nused for motion transfer between individuals; novel view synthesis of\nindividuals captured from just a single camera; to synthesize individuals from\nany virtual viewpoint; and to re-render people in novel poses. Code and video\nresults are available at\nhttps://github.com/GuillaumeRochette/HumanViewSynthesis.\n","authors":["Guillaume Rochette","Chris Russell","Richard Bowden"],"pdf_url":"https://arxiv.org/pdf/2303.15880v1.pdf","comment":"Accepted at IEEE transactions on Biometrics, Behavior, and Identity\n  Science, 10 pages, 11 figures. arXiv admin note: substantial text overlap\n  with arXiv:2111.12731"},{"id":"http://arxiv.org/abs/2303.15879v1","updated":"2023-03-28T10:47:06Z","published":"2023-03-28T10:47:06Z","title":"STMixer: A One-Stage Sparse Action Detector","summary":"  Traditional video action detectors typically adopt the two-stage pipeline,\nwhere a person detector is first employed to generate actor boxes and then 3D\nRoIAlign is used to extract actor-specific features for classification. This\ndetection paradigm requires multi-stage training and inference, and cannot\ncapture context information outside the bounding box. Recently, a few\nquery-based action detectors are proposed to predict action instances in an\nend-to-end manner. However, they still lack adaptability in feature sampling\nand decoding, thus suffering from the issues of inferior performance or slower\nconvergence. In this paper, we propose a new one-stage sparse action detector,\ntermed STMixer. STMixer is based on two core designs. First, we present a\nquery-based adaptive feature sampling module, which endows our STMixer with the\nflexibility of mining a set of discriminative features from the entire\nspatiotemporal domain. Second, we devise a dual-branch feature mixing module,\nwhich allows our STMixer to dynamically attend to and mix video features along\nthe spatial and the temporal dimension respectively for better feature\ndecoding. Coupling these two designs with a video backbone yields an efficient\nend-to-end action detector. Without bells and whistles, our STMixer obtains the\nstate-of-the-art results on the datasets of AVA, UCF101-24, and JHMDB.\n","authors":["Tao Wu","Mengqi Cao","Ziteng Gao","Gangshan Wu","Limin Wang"],"pdf_url":"https://arxiv.org/pdf/2303.15879v1.pdf","comment":"Accepted by CVPR 2023"},{"id":"http://arxiv.org/abs/2206.05782v4","updated":"2023-03-28T10:44:40Z","published":"2022-06-12T16:29:56Z","title":"DSCA: A Dual-Stream Network with Cross-Attention on Whole-Slide Image\n  Pyramids for Cancer Prognosis","summary":"  The cancer prognosis on gigapixel Whole-Slide Images (WSIs) has always been a\nchallenging task. To further enhance WSI visual representations, existing\nmethods have explored image pyramids, instead of single-resolution images, in\nWSIs. In spite of this, they still face two major problems: high computational\ncost and the unnoticed semantical gap in multi-resolution feature fusion. To\ntackle these problems, this paper proposes to efficiently exploit WSI pyramids\nfrom a new perspective, the dual-stream network with cross-attention (DSCA).\nOur key idea is to utilize two sub-streams to process the WSI patches with two\nresolutions, where a square pooling is devised in a high-resolution stream to\nsignificantly reduce computational costs, and a cross-attention-based method is\nproposed to properly handle the fusion of dual-stream features. We validate our\nDSCA on three publicly-available datasets with a total number of 3,101 WSIs\nfrom 1,911 patients. Our experiments and ablation studies verify that (i) the\nproposed DSCA could outperform existing state-of-the-art methods in cancer\nprognosis, by an average C-Index improvement of around 4.6%; (ii) our DSCA\nnetwork is more efficient in computation -- it has more learnable parameters\n(6.31M vs. 860.18K) but less computational costs (2.51G vs. 4.94G), compared to\na typical existing multi-resolution network. (iii) the key components of DSCA,\ndual-stream and cross-attention, indeed contribute to our model's performance,\ngaining an average C-Index rise of around 2.0% while maintaining a\nrelatively-small computational load. Our DSCA could serve as an alternative and\neffective tool for WSI-based cancer prognosis.\n","authors":["Pei Liu","Bo Fu","Feng Ye","Rui Yang","Bin Xu","Luping Ji"],"pdf_url":"https://arxiv.org/pdf/2206.05782v4.pdf","comment":"12 pages, 6 figures, 7 tables"},{"id":"http://arxiv.org/abs/2303.15067v2","updated":"2023-03-28T10:21:45Z","published":"2023-03-27T10:20:34Z","title":"Intersection over Union with smoothing for bounding box regression","summary":"  We focus on the construction of a loss function for the bounding box\nregression. The Intersection over Union (IoU) metric is improved to converge\nfaster, to make the surface of the loss function smooth and continuous over the\nwhole searched space, and to reach a more precise approximation of the labels.\nThe main principle is adding a smoothing part to the original IoU, where the\nsmoothing part is given by a linear space with values that increases from the\nground truth bounding box to the border of the input image, and thus covers the\nwhole spatial search space. We show the motivation and formalism behind this\nloss function and experimentally prove that it outperforms IoU, DIoU, CIoU, and\nSIoU by a large margin. We experimentally show that the proposed loss function\nis robust with respect to the noise in the dimension of ground truth bounding\nboxes. The reference implementation is available at\ngitlab.com/irafm-ai/smoothing-iou.\n","authors":["Petra Števuliáková","Petr Hurtik"],"pdf_url":"https://arxiv.org/pdf/2303.15067v2.pdf","comment":"8 pages, 4 figures, 4 tables, IWANN2023 conference"},{"id":"http://arxiv.org/abs/2210.01033v2","updated":"2023-03-28T10:16:03Z","published":"2022-10-03T15:47:02Z","title":"LPT: Long-tailed Prompt Tuning for Image Classification","summary":"  For long-tailed classification, most works often pretrain a big model on a\nlarge-scale dataset, and then fine-tune the whole model for adapting to\nlong-tailed data. Though promising, fine-tuning the whole pretrained model\ntends to suffer from high cost in computation and deployment of different\nmodels for different tasks, as well as weakened generalization ability for\noverfitting to certain features of long-tailed data. To alleviate these issues,\nwe propose an effective Long-tailed Prompt Tuning method for long-tailed\nclassification. LPT introduces several trainable prompts into a frozen\npretrained model to adapt it to long-tailed data. For better effectiveness, we\ndivide prompts into two groups: 1) a shared prompt for the whole long-tailed\ndataset to learn general features and to adapt a pretrained model into target\ndomain; and 2) group-specific prompts to gather group-specific features for the\nsamples which have similar features and also to empower the pretrained model\nwith discrimination ability. Then we design a two-phase training paradigm to\nlearn these prompts. In phase 1, we train the shared prompt via supervised\nprompt tuning to adapt a pretrained model to the desired long-tailed domain. In\nphase 2, we use the learnt shared prompt as query to select a small best\nmatched set for a group of similar samples from the group-specific prompt set\nto dig the common features of these similar samples, then optimize these\nprompts with dual sampling strategy and asymmetric GCL loss. By only\nfine-tuning a few prompts while fixing the pretrained model, LPT can reduce\ntraining and deployment cost by storing a few prompts, and enjoys a strong\ngeneralization ability of the pretrained model. Experiments show that on\nvarious long-tailed benchmarks, with only ~1.1% extra parameters, LPT achieves\ncomparable performance than previous whole model fine-tuning methods, and is\nmore robust to domain-shift.\n","authors":["Bowen Dong","Pan Zhou","Shuicheng Yan","Wangmeng Zuo"],"pdf_url":"https://arxiv.org/pdf/2210.01033v2.pdf","comment":"ICLR 2023 (poster)"},{"id":"http://arxiv.org/abs/2212.02573v2","updated":"2023-03-28T10:05:20Z","published":"2022-12-05T19:52:28Z","title":"Domain-General Crowd Counting in Unseen Scenarios","summary":"  Domain shift across crowd data severely hinders crowd counting models to\ngeneralize to unseen scenarios. Although domain adaptive crowd counting\napproaches close this gap to a certain extent, they are still dependent on the\ntarget domain data to adapt (e.g. finetune) their models to the specific\ndomain. In this paper, we aim to train a model based on a single source domain\nwhich can generalize well on any unseen domain. This falls into the realm of\ndomain generalization that remains unexplored in crowd counting. We first\nintroduce a dynamic sub-domain division scheme which divides the source domain\ninto multiple sub-domains such that we can initiate a meta-learning framework\nfor domain generalization. The sub-domain division is dynamically refined\nduring the meta-learning. Next, in order to disentangle domain-invariant\ninformation from domain-specific information in image features, we design the\ndomain-invariant and -specific crowd memory modules to re-encode image\nfeatures. Two types of losses, i.e. feature reconstruction and orthogonal\nlosses, are devised to enable this disentanglement. Extensive experiments on\nseveral standard crowd counting benchmarks i.e. SHA, SHB, QNRF, and NWPU, show\nthe strong generalizability of our method.\n","authors":["Zhipeng Du","Jiankang Deng","Miaojing Shi"],"pdf_url":"https://arxiv.org/pdf/2212.02573v2.pdf","comment":"Accepted to AAAI 2023 as Oral Presentation"},{"id":"http://arxiv.org/abs/2303.15859v1","updated":"2023-03-28T10:03:32Z","published":"2023-03-28T10:03:32Z","title":"OpenInst: A Simple Query-Based Method for Open-World Instance\n  Segmentation","summary":"  Open-world instance segmentation has recently gained significant\npopularitydue to its importance in many real-world applications, such as\nautonomous driving, robot perception, and remote sensing. However, previous\nmethods have either produced unsatisfactory results or relied on complex\nsystems and paradigms. We wonder if there is a simple way to obtain\nstate-of-the-art results. Fortunately, we have identified two observations that\nhelp us achieve the best of both worlds: 1) query-based methods demonstrate\nsuperiority over dense proposal-based methods in open-world instance\nsegmentation, and 2) learning localization cues is sufficient for open world\ninstance segmentation. Based on these observations, we propose a simple\nquery-based method named OpenInst for open world instance segmentation.\nOpenInst leverages advanced query-based methods like QueryInst and focuses on\nlearning localization cues. Notably, OpenInst is an extremely simple and\nstraightforward framework without any auxiliary modules or post-processing, yet\nachieves state-of-the-art results on multiple benchmarks. Specifically, in the\nCOCO$\\to$UVO scenario, OpenInst achieves a mask AR of 53.3, outperforming the\nprevious best methods by 2.0 AR with a simpler structure. We hope that OpenInst\ncan serve as a solid baselines for future research in this area.\n","authors":["Cheng Wang","Guoli Wang","Qian Zhang","Peng Guo","Wenyu Liu","Xinggang Wang"],"pdf_url":"https://arxiv.org/pdf/2303.15859v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.15852v1","updated":"2023-03-28T09:43:58Z","published":"2023-03-28T09:43:58Z","title":"Exploring Deep Learning Methods for Classification of SAR Images:\n  Towards NextGen Convolutions via Transformers","summary":"  Images generated by high-resolution SAR have vast areas of application as\nthey can work better in adverse light and weather conditions. One such area of\napplication is in the military systems. This study is an attempt to explore the\nsuitability of current state-of-the-art models introduced in the domain of\ncomputer vision for SAR target classification (MSTAR). Since the application of\nany solution produced for military systems would be strategic and real-time,\naccuracy is often not the only criterion to measure its performance. Other\nimportant parameters like prediction time and input resiliency are equally\nimportant. The paper deals with these issues in the context of SAR images.\nExperimental results show that deep learning models can be suitably applied in\nthe domain of SAR image classification with the desired performance levels.\n","authors":["Aakash Singh","Vivek Kumar Singh"],"pdf_url":"https://arxiv.org/pdf/2303.15852v1.pdf","comment":"6 pages, 9 figures"},{"id":"http://arxiv.org/abs/2303.15850v1","updated":"2023-03-28T09:43:16Z","published":"2023-03-28T09:43:16Z","title":"That Label's Got Style: Handling Label Style Bias for Uncertain Image\n  Segmentation","summary":"  Segmentation uncertainty models predict a distribution over plausible\nsegmentations for a given input, which they learn from the annotator variation\nin the training set. However, in practice these annotations can differ\nsystematically in the way they are generated, for example through the use of\ndifferent labeling tools. This results in datasets that contain both data\nvariability and differing label styles. In this paper, we demonstrate that\napplying state-of-the-art segmentation uncertainty models on such datasets can\nlead to model bias caused by the different label styles. We present an updated\nmodelling objective conditioning on labeling style for aleatoric uncertainty\nestimation, and modify two state-of-the-art-architectures for segmentation\nuncertainty accordingly. We show with extensive experiments that this method\nreduces label style bias, while improving segmentation performance, increasing\nthe applicability of segmentation uncertainty models in the wild. We curate two\ndatasets, with annotations in different label styles, which we will make\npublicly available along with our code upon publication.\n","authors":["Kilian Zepf","Eike Petersen","Jes Frellsen","Aasa Feragen"],"pdf_url":"https://arxiv.org/pdf/2303.15850v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.15848v1","updated":"2023-03-28T09:39:29Z","published":"2023-03-28T09:39:29Z","title":"4K-HAZE: A Dehazing Benchmark with 4K Resolution Hazy and Haze-Free\n  Images","summary":"  Currently, mobile and IoT devices are in dire need of a series of methods to\nenhance 4K images with limited resource expenditure. The absence of large-scale\n4K benchmark datasets hampers progress in this area, especially for dehazing.\nThe challenges in building ultra-high-definition (UHD) dehazing datasets are\nthe absence of estimation methods for UHD depth maps, high-quality 4K depth\nestimation datasets, and migration strategies for UHD haze images from\nsynthetic to real domains. To address these problems, we develop a novel\nsynthetic method to simulate 4K hazy images (including nighttime and daytime\nscenes) from clear images, which first estimates the scene depth, simulates the\nlight rays and object reflectance, then migrates the synthetic images to real\ndomains by using a GAN, and finally yields the hazy effects on 4K resolution\nimages. We wrap these synthesized images into a benchmark called the 4K-HAZE\ndataset. Specifically, we design the CS-Mixer (an MLP-based model that\nintegrates \\textbf{C}hannel domain and \\textbf{S}patial domain) to estimate the\ndepth map of 4K clear images, the GU-Net to migrate a 4K synthetic image to the\nreal hazy domain. The most appealing aspect of our approach (depth estimation\nand domain migration) is the capability to run a 4K image on a single GPU with\n24G RAM in real-time (33fps). Additionally, this work presents an objective\nassessment of several state-of-the-art single-image dehazing methods that are\nevaluated using the 4K-HAZE dataset. At the end of the paper, we discuss the\nlimitations of the 4K-HAZE dataset and its social implications.\n","authors":["Zhuoran Zheng","Xiuyi Jia"],"pdf_url":"https://arxiv.org/pdf/2303.15848v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2302.10602v2","updated":"2023-03-28T09:32:24Z","published":"2023-02-21T11:14:01Z","title":"SU-Net: Pose estimation network for non-cooperative spacecraft on-orbit","summary":"  Spacecraft pose estimation plays a vital role in many on-orbit space\nmissions, such as rendezvous and docking, debris removal, and on-orbit\nmaintenance. At present, space images contain widely varying lighting\nconditions, high contrast and low resolution, pose estimation of space objects\nis more challenging than that of objects on earth. In this paper, we analyzing\nthe radar image characteristics of spacecraft on-orbit, then propose a new deep\nlearning neural Network structure named Dense Residual U-shaped Network\n(DR-U-Net) to extract image features. We further introduce a novel neural\nnetwork based on DR-U-Net, namely Spacecraft U-shaped Network (SU-Net) to\nachieve end-to-end pose estimation for non-cooperative spacecraft.\nSpecifically, the SU-Net first preprocess the image of non-cooperative\nspacecraft, then transfer learning was used for pre-training. Subsequently, in\norder to solve the problem of radar image blur and low ability of spacecraft\ncontour recognition, we add residual connection and dense connection to the\nbackbone network U-Net, and we named it DR-U-Net. In this way, the feature loss\nand the complexity of the model is reduced, and the degradation of deep neural\nnetwork during training is avoided. Finally, a layer of feedforward neural\nnetwork is used for pose estimation of non-cooperative spacecraft on-orbit.\nExperiments prove that the proposed method does not rely on the hand-made\nobject specific features, and the model has robust robustness, and the\ncalculation accuracy outperforms the state-of-the-art pose estimation methods.\nThe absolute error is 0.1557 to 0.4491 , the mean error is about 0.302 , and\nthe standard deviation is about 0.065 .\n","authors":["Hu Gao","Zhihui Li","Depeng Dang","Ning Wang","Jingfan Yang"],"pdf_url":"https://arxiv.org/pdf/2302.10602v2.pdf","comment":"We need to overhaul the paper and innovate"},{"id":"http://arxiv.org/abs/2303.15840v1","updated":"2023-03-28T09:23:19Z","published":"2023-03-28T09:23:19Z","title":"Multi-view Improved Monitored Distillation for Depth Completion","summary":"  This paper proposes a new depth completion method based on multi-view\nimproved monitored distillation to generate more accurate depth maps. Based on\nthe state-of-the-art depth completion method named ensemble distillation, we\nintroduce an existing stereo-based model as a teacher model to improve ensemble\ndistillation accuracy and generate a more accurate student model in training by\navoiding inherent error modes of completion-based teachers as well as\nminimizing the reconstruction error for a given image. We also leverage\nmulti-view depth consistency and multi-scale minimum reprojection to provide\nself-supervised information. These methods use the existing structure\nconstraints to yield supervised signals for student model training without\ngreat expense on gathering ground truth information of depth. Our extensive\nexperimental evaluation demonstrates that our proposed method can effectively\nimprove the accuracy of baseline method of monitored distillation.\n","authors":["Jia-Wei Guo","Cong Li","Sen-Hua Zhu","Chang-Zheng Zhang","Ming Ouyang","Ning Ding","Hung-Chyun Chou"],"pdf_url":"https://arxiv.org/pdf/2303.15840v1.pdf","comment":"5 pages, 3 figures, references added"},{"id":"http://arxiv.org/abs/2303.15839v1","updated":"2023-03-28T09:15:55Z","published":"2023-03-28T09:15:55Z","title":"AutoKary2022: A Large-Scale Densely Annotated Dateset for Chromosome\n  Instance Segmentation","summary":"  Automated chromosome instance segmentation from metaphase cell microscopic\nimages is critical for the diagnosis of chromosomal disorders (i.e., karyotype\nanalysis). However, it is still a challenging task due to lacking of densely\nannotated datasets and the complicated morphologies of chromosomes, e.g., dense\ndistribution, arbitrary orientations, and wide range of lengths. To facilitate\nthe development of this area, we take a big step forward and manually construct\na large-scale densely annotated dataset named AutoKary2022, which contains over\n27,000 chromosome instances in 612 microscopic images from 50 patients.\nSpecifically, each instance is annotated with a polygonal mask and a class\nlabel to assist in precise chromosome detection and segmentation. On top of it,\nwe systematically investigate representative methods on this dataset and obtain\na number of interesting findings, which helps us have a deeper understanding of\nthe fundamental problems in chromosome instance segmentation. We hope this\ndataset could advance research towards medical understanding. The dataset can\nbe available at:\nhttps://github.com/wangjuncongyu/chromosome-instance-segmentation-dataset.\n","authors":["Dan You","Pengcheng Xia","Qiuzhu Chen","Minghui Wu","Suncheng Xiang","Jun Wang"],"pdf_url":"https://arxiv.org/pdf/2303.15839v1.pdf","comment":"Accepted by ICME 2023"},{"id":"http://arxiv.org/abs/2303.15833v1","updated":"2023-03-28T09:05:15Z","published":"2023-03-28T09:05:15Z","title":"Complementary Domain Adaptation and Generalization for Unsupervised\n  Continual Domain Shift Learning","summary":"  Continual domain shift poses a significant challenge in real-world\napplications, particularly in situations where labeled data is not available\nfor new domains. The challenge of acquiring knowledge in this problem setting\nis referred to as unsupervised continual domain shift learning. Existing\nmethods for domain adaptation and generalization have limitations in addressing\nthis issue, as they focus either on adapting to a specific domain or\ngeneralizing to unseen domains, but not both. In this paper, we propose\nComplementary Domain Adaptation and Generalization (CoDAG), a simple yet\neffective learning framework that combines domain adaptation and generalization\nin a complementary manner to achieve three major goals of unsupervised\ncontinual domain shift learning: adapting to a current domain, generalizing to\nunseen domains, and preventing forgetting of previously seen domains. Our\napproach is model-agnostic, meaning that it is compatible with any existing\ndomain adaptation and generalization algorithms. We evaluate CoDAG on several\nbenchmark datasets and demonstrate that our model outperforms state-of-the-art\nmodels in all datasets and evaluation metrics, highlighting its effectiveness\nand robustness in handling unsupervised continual domain shift learning.\n","authors":["Wonguk Cho","Jinha Park","Taesup Kim"],"pdf_url":"https://arxiv.org/pdf/2303.15833v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.15826v1","updated":"2023-03-28T08:55:00Z","published":"2023-03-28T08:55:00Z","title":"MS-MT: Multi-Scale Mean Teacher with Contrastive Unpaired Translation\n  for Cross-Modality Vestibular Schwannoma and Cochlea Segmentation","summary":"  Domain shift has been a long-standing issue for medical image segmentation.\nRecently, unsupervised domain adaptation (UDA) methods have achieved promising\ncross-modality segmentation performance by distilling knowledge from a\nlabel-rich source domain to a target domain without labels. In this work, we\npropose a multi-scale self-ensembling based UDA framework for automatic\nsegmentation of two key brain structures i.e., Vestibular Schwannoma (VS) and\nCochlea on high-resolution T2 images. First, a segmentation-enhanced\ncontrastive unpaired image translation module is designed for image-level\ndomain adaptation from source T1 to target T2. Next, multi-scale deep\nsupervision and consistency regularization are introduced to a mean teacher\nnetwork for self-ensemble learning to further close the domain gap.\nFurthermore, self-training and intensity augmentation techniques are utilized\nto mitigate label scarcity and boost cross-modality segmentation performance.\nOur method demonstrates promising segmentation performance with a mean Dice\nscore of 83.8% and 81.4% and an average asymmetric surface distance (ASSD) of\n0.55 mm and 0.26 mm for the VS and Cochlea, respectively in the validation\nphase of the crossMoDA 2022 challenge.\n","authors":["Ziyuan Zhao","Kaixin Xu","Huai Zhe Yeo","Xulei Yang","Cuntai Guan"],"pdf_url":"https://arxiv.org/pdf/2303.15826v1.pdf","comment":"Accepted by BrainLes MICCAI proceedings (5th solution for MICCAI 2022\n  Cross-Modality Domain Adaptation (crossMoDA) Challenge)"},{"id":"http://arxiv.org/abs/2303.04440v2","updated":"2023-03-28T08:54:49Z","published":"2023-03-08T08:42:03Z","title":"HyT-NAS: Hybrid Transformers Neural Architecture Search for Edge Devices","summary":"  Vision Transformers have enabled recent attention-based Deep Learning (DL)\narchitectures to achieve remarkable results in Computer Vision (CV) tasks.\nHowever, due to the extensive computational resources required, these\narchitectures are rarely implemented on resource-constrained platforms. Current\nresearch investigates hybrid handcrafted convolution-based and attention-based\nmodels for CV tasks such as image classification and object detection. In this\npaper, we propose HyT-NAS, an efficient Hardware-aware Neural Architecture\nSearch (HW-NAS) including hybrid architectures targeting vision tasks on tiny\ndevices. HyT-NAS improves state-of-the-art HW-NAS by enriching the search space\nand enhancing the search strategy as well as the performance predictors. Our\nexperiments show that HyT-NAS achieves a similar hypervolume with less than ~5x\ntraining evaluations. Our resulting architecture outperforms MLPerf MobileNetV1\nby 6.3% accuracy improvement with 3.5x less number of parameters on Visual Wake\nWords.\n","authors":["Lotfi Abdelkrim Mecharbat","Hadjer Benmeziane","Hamza Ouarnoughi","Smail Niar"],"pdf_url":"https://arxiv.org/pdf/2303.04440v2.pdf","comment":"CODAI 2022 Workshop - Embedded System Week (ESWeek)"},{"id":"http://arxiv.org/abs/2303.15823v1","updated":"2023-03-28T08:51:15Z","published":"2023-03-28T08:51:15Z","title":"Automated wildlife image classification: An active learning tool for\n  ecological applications","summary":"  Wildlife camera trap images are being used extensively to investigate animal\nabundance, habitat associations, and behavior, which is complicated by the fact\nthat experts must first classify the images manually. Artificial intelligence\nsystems can take over this task but usually need a large number of\nalready-labeled training images to achieve sufficient performance. This\nrequirement necessitates human expert labor and poses a particular challenge\nfor projects with few cameras or short durations. We propose a label-efficient\nlearning strategy that enables researchers with small or medium-sized image\ndatabases to leverage the potential of modern machine learning, thus freeing\ncrucial resources for subsequent analyses.\n  Our methodological proposal is two-fold: (1) We improve current strategies of\ncombining object detection and image classification by tuning the\nhyperparameters of both models. (2) We provide an active learning (AL) system\nthat allows training deep learning models very efficiently in terms of required\nhuman-labeled training images. We supply a software package that enables\nresearchers to use these methods directly and thereby ensure the broad\napplicability of the proposed framework in ecological practice.\n  We show that our tuning strategy improves predictive performance. We\ndemonstrate how the AL pipeline reduces the amount of pre-labeled data needed\nto achieve a specific predictive performance and that it is especially valuable\nfor improving out-of-sample predictive performance.\n  We conclude that the combination of tuning and AL increases predictive\nperformance substantially. Furthermore, we argue that our work can broadly\nimpact the community through the ready-to-use software package provided.\nFinally, the publication of our models tailored to European wildlife data\nenriches existing model bases mostly trained on data from Africa and North\nAmerica.\n","authors":["Ludwig Bothmann","Lisa Wimmer","Omid Charrakh","Tobias Weber","Hendrik Edelhoff","Wibke Peters","Hien Nguyen","Caryl Benjamin","Annette Menzel"],"pdf_url":"https://arxiv.org/pdf/2303.15823v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2211.14053v2","updated":"2023-03-28T08:48:52Z","published":"2022-11-25T12:17:30Z","title":"Re^2TAL: Rewiring Pretrained Video Backbones for Reversible Temporal\n  Action Localization","summary":"  Temporal action localization (TAL) requires long-form reasoning to predict\nactions of various durations and complex content. Given limited GPU memory,\ntraining TAL end to end (i.e., from videos to predictions) on long videos is a\nsignificant challenge. Most methods can only train on pre-extracted features\nwithout optimizing them for the localization problem, consequently limiting\nlocalization performance. In this work, to extend the potential in TAL\nnetworks, we propose a novel end-to-end method Re2TAL, which rewires pretrained\nvideo backbones for reversible TAL. Re2TAL builds a backbone with reversible\nmodules, where the input can be recovered from the output such that the bulky\nintermediate activations can be cleared from memory during training. Instead of\ndesigning one single type of reversible module, we propose a network rewiring\nmechanism, to transform any module with a residual connection to a reversible\nmodule without changing any parameters. This provides two benefits: (1) a large\nvariety of reversible networks are easily obtained from existing and even\nfuture model designs, and (2) the reversible models require much less training\neffort as they reuse the pre-trained parameters of their original\nnon-reversible versions. Re2TAL, only using the RGB modality, reaches 37.01%\naverage mAP on ActivityNet-v1.3, a new state-of-the-art record, and mAP 64.9%\nat tIoU=0.5 on THUMOS-14, outperforming all other RGB-only methods.\n","authors":["Chen Zhao","Shuming Liu","Karttikeya Mangalam","Bernard Ghanem"],"pdf_url":"https://arxiv.org/pdf/2211.14053v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.15818v1","updated":"2023-03-28T08:42:54Z","published":"2023-03-28T08:42:54Z","title":"Towards Effective Adversarial Textured 3D Meshes on Physical Face\n  Recognition","summary":"  Face recognition is a prevailing authentication solution in numerous\nbiometric applications. Physical adversarial attacks, as an important\nsurrogate, can identify the weaknesses of face recognition systems and evaluate\ntheir robustness before deployed. However, most existing physical attacks are\neither detectable readily or ineffective against commercial recognition\nsystems. The goal of this work is to develop a more reliable technique that can\ncarry out an end-to-end evaluation of adversarial robustness for commercial\nsystems. It requires that this technique can simultaneously deceive black-box\nrecognition models and evade defensive mechanisms. To fulfill this, we design\nadversarial textured 3D meshes (AT3D) with an elaborate topology on a human\nface, which can be 3D-printed and pasted on the attacker's face to evade the\ndefenses. However, the mesh-based optimization regime calculates gradients in\nhigh-dimensional mesh space, and can be trapped into local optima with\nunsatisfactory transferability. To deviate from the mesh-based space, we\npropose to perturb the low-dimensional coefficient space based on 3D Morphable\nModel, which significantly improves black-box transferability meanwhile\nenjoying faster search efficiency and better visual quality. Extensive\nexperiments in digital and physical scenarios show that our method effectively\nexplores the security vulnerabilities of multiple popular commercial services,\nincluding three recognition APIs, four anti-spoofing APIs, two prevailing\nmobile phones and two automated access control systems.\n","authors":["Xiao Yang","Chang Liu","Longlong Xu","Yikai Wang","Yinpeng Dong","Ning Chen","Hang Su","Jun Zhu"],"pdf_url":"https://arxiv.org/pdf/2303.15818v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2212.06461v2","updated":"2023-03-28T08:26:41Z","published":"2022-12-13T10:21:15Z","title":"A Statistical Model for Predicting Generalization in Few-Shot\n  Classification","summary":"  The estimation of the generalization error of classifiers often relies on a\nvalidation set. Such a set is hardly available in few-shot learning scenarios,\na highly disregarded shortcoming in the field. In these scenarios, it is common\nto rely on features extracted from pre-trained neural networks combined with\ndistance-based classifiers such as nearest class mean. In this work, we\nintroduce a Gaussian model of the feature distribution. By estimating the\nparameters of this model, we are able to predict the generalization error on\nnew classification tasks with few samples. We observe that accurate distance\nestimates between class-conditional densities are the key to accurate estimates\nof the generalization performance. Therefore, we propose an unbiased estimator\nfor these distances and integrate it in our numerical analysis. We empirically\nshow that our approach outperforms alternatives such as the leave-one-out\ncross-validation strategy.\n","authors":["Yassir Bendou","Vincent Gripon","Bastien Pasdeloup","Lukas Mauch","Stefan Uhlich","Fabien Cardinaux","Ghouthi Boukli Hacene","Javier Alonso Garcia"],"pdf_url":"https://arxiv.org/pdf/2212.06461v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2301.03949v2","updated":"2023-03-28T08:26:30Z","published":"2023-01-10T13:15:42Z","title":"Modiff: Action-Conditioned 3D Motion Generation with Denoising Diffusion\n  Probabilistic Models","summary":"  Diffusion-based generative models have recently emerged as powerful solutions\nfor high-quality synthesis in multiple domains. Leveraging the bidirectional\nMarkov chains, diffusion probabilistic models generate samples by inferring the\nreversed Markov chain based on the learned distribution mapping at the forward\ndiffusion process. In this work, we propose Modiff, a conditional paradigm that\nbenefits from the denoising diffusion probabilistic model (DDPM) to tackle the\nproblem of realistic and diverse action-conditioned 3D skeleton-based motion\ngeneration. We are a pioneering attempt that uses DDPM to synthesize a variable\nnumber of motion sequences conditioned on a categorical action. We evaluate our\napproach on the large-scale NTU RGB+D dataset and show improvements over\nstate-of-the-art motion generation methods.\n","authors":["Mengyi Zhao","Mengyuan Liu","Bin Ren","Shuling Dai","Nicu Sebe"],"pdf_url":"https://arxiv.org/pdf/2301.03949v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.15805v1","updated":"2023-03-28T08:21:44Z","published":"2023-03-28T08:21:44Z","title":"StarNet: Style-Aware 3D Point Cloud Generation","summary":"  This paper investigates an open research task of reconstructing and\ngenerating 3D point clouds. Most existing works of 3D generative models\ndirectly take the Gaussian prior as input for the decoder to generate 3D point\nclouds, which fail to learn disentangled latent codes, leading noisy\ninterpolated results. Most of the GAN-based models fail to discriminate the\nlocal geometries, resulting in the point clouds generated not evenly\ndistributed at the object surface, hence degrading the point cloud generation\nquality. Moreover, prevailing methods adopt computation-intensive frameworks,\nsuch as flow-based models and Markov chains, which take plenty of time and\nresources in the training phase. To resolve these limitations, this paper\nproposes a unified style-aware network architecture combining both point-wise\ndistance loss and adversarial loss, StarNet which is able to reconstruct and\ngenerate high-fidelity and even 3D point clouds using a mapping network that\ncan effectively disentangle the Gaussian prior from input's high-level\nattributes in the mapped latent space to generate realistic interpolated\nobjects. Experimental results demonstrate that our framework achieves\ncomparable state-of-the-art performance on various metrics in the point cloud\nreconstruction and generation tasks, but is more lightweight in model size,\nrequires much fewer parameters and less time for model training.\n","authors":["Yunfan Zhang","Hao Wang","Guosheng Lin","Vun Chan Hua Nicholas","Zhiqi Shen","Chunyan Miao"],"pdf_url":"https://arxiv.org/pdf/2303.15805v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2212.08420v2","updated":"2023-03-28T08:11:53Z","published":"2022-12-16T11:44:01Z","title":"Fake it till you make it: Learning transferable representations from\n  synthetic ImageNet clones","summary":"  Recent image generation models such as Stable Diffusion have exhibited an\nimpressive ability to generate fairly realistic images starting from a simple\ntext prompt. Could such models render real images obsolete for training image\nprediction models? In this paper, we answer part of this provocative question\nby investigating the need for real images when training models for ImageNet\nclassification. Provided only with the class names that have been used to build\nthe dataset, we explore the ability of Stable Diffusion to generate synthetic\nclones of ImageNet and measure how useful these are for training classification\nmodels from scratch. We show that with minimal and class-agnostic prompt\nengineering, ImageNet clones are able to close a large part of the gap between\nmodels produced by synthetic images and models trained with real images, for\nthe several standard classification benchmarks that we consider in this study.\nMore importantly, we show that models trained on synthetic images exhibit\nstrong generalization properties and perform on par with models trained on real\ndata for transfer. Project page: https://europe.naverlabs.com/imagenet-sd/\n","authors":["Mert Bulent Sariyildiz","Karteek Alahari","Diane Larlus","Yannis Kalantidis"],"pdf_url":"https://arxiv.org/pdf/2212.08420v2.pdf","comment":"Accepted to CVPR 2023"},{"id":"http://arxiv.org/abs/2203.08147v4","updated":"2023-03-28T08:09:38Z","published":"2022-03-14T17:18:10Z","title":"Energy-Latency Attacks via Sponge Poisoning","summary":"  Sponge examples are test-time inputs carefully optimized to increase energy\nconsumption and latency of neural networks when deployed on hardware\naccelerators. In this work, we are the first to demonstrate that sponge\nexamples can also be injected at training time, via an attack that we call\nsponge poisoning. This attack allows one to increase the energy consumption and\nlatency of machine-learning models indiscriminately on each test-time input. We\npresent a novel formalization for sponge poisoning, overcoming the limitations\nrelated to the optimization of test-time sponge examples, and show that this\nattack is possible even if the attacker only controls a few model updates; for\ninstance, if model training is outsourced to an untrusted third-party or\ndistributed via federated learning. Our extensive experimental analysis shows\nthat sponge poisoning can almost completely vanish the effect of hardware\naccelerators. We also analyze the activations of poisoned models, identifying\nwhich components are more vulnerable to this attack. Finally, we examine the\nfeasibility of countermeasures against sponge poisoning to decrease energy\nconsumption, showing that sanitization methods may be overly expensive for most\nof the users.\n","authors":["Antonio Emanuele Cinà","Ambra Demontis","Battista Biggio","Fabio Roli","Marcello Pelillo"],"pdf_url":"https://arxiv.org/pdf/2203.08147v4.pdf","comment":"Preprint;16 pages"},{"id":"http://arxiv.org/abs/2303.15796v1","updated":"2023-03-28T08:00:46Z","published":"2023-03-28T08:00:46Z","title":"KERM: Knowledge Enhanced Reasoning for Vision-and-Language Navigation","summary":"  Vision-and-language navigation (VLN) is the task to enable an embodied agent\nto navigate to a remote location following the natural language instruction in\nreal scenes. Most of the previous approaches utilize the entire features or\nobject-centric features to represent navigable candidates. However, these\nrepresentations are not efficient enough for an agent to perform actions to\narrive the target location. As knowledge provides crucial information which is\ncomplementary to visible content, in this paper, we propose a Knowledge\nEnhanced Reasoning Model (KERM) to leverage knowledge to improve agent\nnavigation ability. Specifically, we first retrieve facts (i.e., knowledge\ndescribed by language descriptions) for the navigation views based on local\nregions from the constructed knowledge base. The retrieved facts range from\nproperties of a single object (e.g., color, shape) to relationships between\nobjects (e.g., action, spatial position), providing crucial information for\nVLN. We further present the KERM which contains the purification, fact-aware\ninteraction, and instruction-guided aggregation modules to integrate visual,\nhistory, instruction, and fact features. The proposed KERM can automatically\nselect and gather crucial and relevant cues, obtaining more accurate action\nprediction. Experimental results on the REVERIE, R2R, and SOON datasets\ndemonstrate the effectiveness of the proposed method.\n","authors":["Xiangyang Li","Zihan Wang","Jiahao Yang","Yaowei Wang","Shuqiang Jiang"],"pdf_url":"https://arxiv.org/pdf/2303.15796v1.pdf","comment":"Accepted by CVPR 2023. The code is available at\n  https://github.com/XiangyangLi20/KERM"},{"id":"http://arxiv.org/abs/2303.15792v1","updated":"2023-03-28T07:57:34Z","published":"2023-03-28T07:57:34Z","title":"Make the Most Out of Your Net: Alternating Between Canonical and Hard\n  Datasets for Improved Image Demosaicing","summary":"  Image demosaicing is an important step in the image processing pipeline for\ndigital cameras, and it is one of the many tasks within the field of image\nrestoration. A well-known characteristic of natural images is that most patches\nare smooth, while high-content patches like textures or repetitive patterns are\nmuch rarer, which results in a long-tailed distribution. This distribution can\ncreate an inductive bias when training machine learning algorithms for image\nrestoration tasks and for image demosaicing in particular. There have been many\ndifferent approaches to address this challenge, such as utilizing specific\nlosses or designing special network architectures. What makes our work is\nunique in that it tackles the problem from a training protocol perspective. Our\nproposed training regime consists of two key steps. The first step is a\ndata-mining stage where sub-categories are created and then refined through an\nelimination process to only retain the most helpful sub-categories. The second\nstep is a cyclic training process where the neural network is trained on both\nthe mined sub-categories and the original dataset. We have conducted various\nexperiments to demonstrate the effectiveness of our training method for the\nimage demosaicing task. Our results show that this method outperforms standard\ntraining across a range of architecture sizes and types, including CNNs and\nTransformers. Moreover, we are able to achieve state-of-the-art results with a\nsignificantly smaller neural network, compared to previous state-of-the-art\nmethods.\n","authors":["Yuval Becker","Raz Z. Nossek","Tomer Peleg"],"pdf_url":"https://arxiv.org/pdf/2303.15792v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.15786v1","updated":"2023-03-28T07:54:54Z","published":"2023-03-28T07:54:54Z","title":"HOICLIP: Efficient Knowledge Transfer for HOI Detection with\n  Vision-Language Models","summary":"  Human-Object Interaction (HOI) detection aims to localize human-object pairs\nand recognize their interactions. Recently, Contrastive Language-Image\nPre-training (CLIP) has shown great potential in providing interaction prior\nfor HOI detectors via knowledge distillation. However, such approaches often\nrely on large-scale training data and suffer from inferior performance under\nfew/zero-shot scenarios. In this paper, we propose a novel HOI detection\nframework that efficiently extracts prior knowledge from CLIP and achieves\nbetter generalization. In detail, we first introduce a novel interaction\ndecoder to extract informative regions in the visual feature map of CLIP via a\ncross-attention mechanism, which is then fused with the detection backbone by a\nknowledge integration block for more accurate human-object pair detection. In\naddition, prior knowledge in CLIP text encoder is leveraged to generate a\nclassifier by embedding HOI descriptions. To distinguish fine-grained\ninteractions, we build a verb classifier from training data via visual semantic\narithmetic and a lightweight verb representation adapter. Furthermore, we\npropose a training-free enhancement to exploit global HOI predictions from\nCLIP. Extensive experiments demonstrate that our method outperforms the state\nof the art by a large margin on various settings, e.g. +4.04 mAP on HICO-Det.\nThe source code is available in https://github.com/Artanic30/HOICLIP.\n","authors":["Shan Ning","Longtian Qiu","Yongfei Liu","Xuming He"],"pdf_url":"https://arxiv.org/pdf/2303.15786v1.pdf","comment":"CVPR 2023.Open sourced, Code and Model Available"},{"id":"http://arxiv.org/abs/2303.15782v1","updated":"2023-03-28T07:52:15Z","published":"2023-03-28T07:52:15Z","title":"CARTO: Category and Joint Agnostic Reconstruction of ARTiculated Objects","summary":"  We present CARTO, a novel approach for reconstructing multiple articulated\nobjects from a single stereo RGB observation. We use implicit object-centric\nrepresentations and learn a single geometry and articulation decoder for\nmultiple object categories. Despite training on multiple categories, our\ndecoder achieves a comparable reconstruction accuracy to methods that train\nbespoke decoders separately for each category. Combined with our stereo image\nencoder we infer the 3D shape, 6D pose, size, joint type, and the joint state\nof multiple unknown objects in a single forward pass. Our method achieves a\n20.4% absolute improvement in mAP 3D IOU50 for novel instances when compared to\na two-stage pipeline. Inference time is fast and can run on a NVIDIA TITAN XP\nGPU at 1 HZ for eight or less objects present. While only trained on simulated\ndata, CARTO transfers to real-world object instances. Code and evaluation data\nis available at: http://carto.cs.uni-freiburg.de\n","authors":["Nick Heppert","Muhammad Zubair Irshad","Sergey Zakharov","Katherine Liu","Rares Andrei Ambrus","Jeannette Bohg","Abhinav Valada","Thomas Kollar"],"pdf_url":"https://arxiv.org/pdf/2303.15782v1.pdf","comment":"20 pages, 11 figures, accepted at CVPR 2023"},{"id":"http://arxiv.org/abs/2303.15780v1","updated":"2023-03-28T07:50:45Z","published":"2023-03-28T07:50:45Z","title":"Instruct 3D-to-3D: Text Instruction Guided 3D-to-3D conversion","summary":"  We propose a high-quality 3D-to-3D conversion method, Instruct 3D-to-3D. Our\nmethod is designed for a novel task, which is to convert a given 3D scene to\nanother scene according to text instructions. Instruct 3D-to-3D applies\npretrained Image-to-Image diffusion models for 3D-to-3D conversion. This\nenables the likelihood maximization of each viewpoint image and high-quality 3D\ngeneration. In addition, our proposed method explicitly inputs the source 3D\nscene as a condition, which enhances 3D consistency and controllability of how\nmuch of the source 3D scene structure is reflected. We also propose dynamic\nscaling, which allows the intensity of the geometry transformation to be\nadjusted. We performed quantitative and qualitative evaluations and showed that\nour proposed method achieves higher quality 3D-to-3D conversions than baseline\nmethods.\n","authors":["Hiromichi Kamata","Yuiko Sakuma","Akio Hayakawa","Masato Ishii","Takuya Narihira"],"pdf_url":"https://arxiv.org/pdf/2303.15780v1.pdf","comment":"Project page: https://sony.github.io/Instruct3Dto3D-doc/"},{"id":"http://arxiv.org/abs/2303.10820v2","updated":"2023-03-28T07:38:31Z","published":"2023-03-20T01:26:16Z","title":"Unsupervised Intrinsic Image Decomposition with LiDAR Intensity","summary":"  Intrinsic image decomposition (IID) is the task that decomposes a natural\nimage into albedo and shade. While IID is typically solved through supervised\nlearning methods, it is not ideal due to the difficulty in observing ground\ntruth albedo and shade in general scenes. Conversely, unsupervised learning\nmethods are currently underperforming supervised learning methods since there\nare no criteria for solving the ill-posed problems. Recently, light detection\nand ranging (LiDAR) is widely used due to its ability to make highly precise\ndistance measurements. Thus, we have focused on the utilization of LiDAR,\nespecially LiDAR intensity, to address this issue. In this paper, we propose\nunsupervised intrinsic image decomposition with LiDAR intensity (IID-LI). Since\nthe conventional unsupervised learning methods consist of image-to-image\ntransformations, simply inputting LiDAR intensity is not an effective approach.\nTherefore, we design an intensity consistency loss that computes the error\nbetween LiDAR intensity and gray-scaled albedo to provide a criterion for the\nill-posed problem. In addition, LiDAR intensity is difficult to handle due to\nits sparsity and occlusion, hence, a LiDAR intensity densification module is\nproposed. We verified the estimating quality using our own dataset, which\ninclude RGB images, LiDAR intensity and human judged annotations. As a result,\nwe achieved an estimation accuracy that outperforms conventional unsupervised\nlearning methods. Dataset link :\n(https://github.com/ntthilab-cv/NTT-intrinsic-dataset).\n","authors":["Shogo Sato","Yasuhiro Yao","Taiga Yoshida","Takuhiro Kaneko","Shingo Ando","Jun Shimamura"],"pdf_url":"https://arxiv.org/pdf/2303.10820v2.pdf","comment":"Accepted by CVPR2023, Dataset link :\n  (https://github.com/ntthilab-cv/NTT-intrinsic-dataset)"},{"id":"http://arxiv.org/abs/2303.10762v2","updated":"2023-03-28T07:25:22Z","published":"2023-03-19T20:31:38Z","title":"Deep Image Fingerprint: Accurate And Low Budget Synthetic Image Detector","summary":"  The generation of high-quality images has become widely accessible and is a\nrapidly evolving process. As a result, anyone can generate images that are\nindistinguishable from real ones. This leads to a wide range of applications,\nwhich also include malicious usage with deception in mind. Despite advances in\ndetection techniques for generated images, a robust detection method still\neludes us. In this work, we utilize the inductive bias of convolutional neural\nnetworks (CNNs) to develop a new detection method that requires a small amount\nof training samples and achieves accuracy that is on par or better than current\nstate-of-the-art methods.\n","authors":["Sergey Sinitsa","Ohad Fried"],"pdf_url":"https://arxiv.org/pdf/2303.10762v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.15101v2","updated":"2023-03-28T07:14:47Z","published":"2023-03-27T11:10:54Z","title":"DANI-Net: Uncalibrated Photometric Stereo by Differentiable Shadow\n  Handling, Anisotropic Reflectance Modeling, and Neural Inverse Rendering","summary":"  Uncalibrated photometric stereo (UPS) is challenging due to the inherent\nambiguity brought by the unknown light. Although the ambiguity is alleviated on\nnon-Lambertian objects, the problem is still difficult to solve for more\ngeneral objects with complex shapes introducing irregular shadows and general\nmaterials with complex reflectance like anisotropic reflectance. To exploit\ncues from shadow and reflectance to solve UPS and improve performance on\ngeneral materials, we propose DANI-Net, an inverse rendering framework with\ndifferentiable shadow handling and anisotropic reflectance modeling. Unlike\nmost previous methods that use non-differentiable shadow maps and assume\nisotropic material, our network benefits from cues of shadow and anisotropic\nreflectance through two differentiable paths. Experiments on multiple\nreal-world datasets demonstrate our superior and robust performance.\n","authors":["Zongrui Li","Qian Zheng","Boxin Shi","Gang Pan","Xudong Jiang"],"pdf_url":"https://arxiv.org/pdf/2303.15101v2.pdf","comment":"Accepted by CVPR 2023"},{"id":"http://arxiv.org/abs/2303.15770v1","updated":"2023-03-28T07:13:11Z","published":"2023-03-28T07:13:11Z","title":"DDMM-Synth: A Denoising Diffusion Model for Cross-modal Medical Image\n  Synthesis with Sparse-view Measurement Embedding","summary":"  Reducing the radiation dose in computed tomography (CT) is important to\nmitigate radiation-induced risks. One option is to employ a well-trained model\nto compensate for incomplete information and map sparse-view measurements to\nthe CT reconstruction. However, reconstruction from sparsely sampled\nmeasurements is insufficient to uniquely characterize an object in CT, and a\nlearned prior model may be inadequate for unencountered cases. Medical modal\ntranslation from magnetic resonance imaging (MRI) to CT is an alternative but\nmay introduce incorrect information into the synthesized CT images in addition\nto the fact that there exists no explicit transformation describing their\nrelationship. To address these issues, we propose a novel framework called the\ndenoising diffusion model for medical image synthesis (DDMM-Synth) to close the\nperformance gaps described above. This framework combines an MRI-guided\ndiffusion model with a new CT measurement embedding reverse sampling scheme.\nSpecifically, the null-space content of the one-step denoising result is\nrefined by the MRI-guided data distribution prior, and its range-space\ncomponent derived from an explicit operator matrix and the sparse-view CT\nmeasurements is directly integrated into the inference stage. DDMM-Synth can\nadjust the projection number of CT a posteriori for a particular clinical\napplication and its modified version can even improve the results significantly\nfor noisy cases. Our results show that DDMM-Synth outperforms other\nstate-of-the-art supervised-learning-based baselines under fair experimental\nconditions.\n","authors":["Xiaoyue Li","Kai Shang","Gaoang Wang","Mark D. Butala"],"pdf_url":"https://arxiv.org/pdf/2303.15770v1.pdf","comment":"llncs.cls v2.20,12 pages with 6 figures"},{"id":"http://arxiv.org/abs/2301.00184v3","updated":"2023-03-28T07:09:59Z","published":"2022-12-31T11:50:32Z","title":"Cap4Video: What Can Auxiliary Captions Do for Text-Video Retrieval?","summary":"  Most existing text-video retrieval methods focus on cross-modal matching\nbetween the visual content of videos and textual query sentences. However, in\nreal-world scenarios, online videos are often accompanied by relevant text\ninformation such as titles, tags, and even subtitles, which can be utilized to\nmatch textual queries. This insight has motivated us to propose a novel\napproach to text-video retrieval, where we directly generate associated\ncaptions from videos using zero-shot video captioning with knowledge from\nweb-scale pre-trained models (e.g., CLIP and GPT-2). Given the generated\ncaptions, a natural question arises: what benefits do they bring to text-video\nretrieval? To answer this, we introduce Cap4Video, a new framework that\nleverages captions in three ways: i) Input data: video-caption pairs can\naugment the training data. ii) Intermediate feature interaction: we perform\ncross-modal feature interaction between the video and caption to produce\nenhanced video representations. iii) Output score: the Query-Caption matching\nbranch can complement the original Query-Video matching branch for text-video\nretrieval. We conduct comprehensive ablation studies to demonstrate the\neffectiveness of our approach. Without any post-processing, Cap4Video achieves\nstate-of-the-art performance on four standard text-video retrieval benchmarks:\nMSR-VTT (51.4%), VATEX (66.6%), MSVD (51.8%), and DiDeMo (52.0%). The code is\navailable at https://github.com/whwu95/Cap4Video .\n","authors":["Wenhao Wu","Haipeng Luo","Bo Fang","Jingdong Wang","Wanli Ouyang"],"pdf_url":"https://arxiv.org/pdf/2301.00184v3.pdf","comment":"Accepted by CVPR 2023. Selected as a Highlight (Top 2.5% of ALL\n  submissions)"},{"id":"http://arxiv.org/abs/2303.15768v1","updated":"2023-03-28T07:03:31Z","published":"2023-03-28T07:03:31Z","title":"RobustSwap: A Simple yet Robust Face Swapping Model against Attribute\n  Leakage","summary":"  Face swapping aims at injecting a source image's identity (i.e., facial\nfeatures) into a target image, while strictly preserving the target's\nattributes, which are irrelevant to identity. However, we observed that\nprevious approaches still suffer from source attribute leakage, where the\nsource image's attributes interfere with the target image's. In this paper, we\nanalyze the latent space of StyleGAN and find the adequate combination of the\nlatents geared for face swapping task. Based on the findings, we develop a\nsimple yet robust face swapping model, RobustSwap, which is resistant to the\npotential source attribute leakage. Moreover, we exploit the coordination of\n3DMM's implicit and explicit information as a guidance to incorporate the\nstructure of the source image and the precise pose of the target image. Despite\nour method solely utilizing an image dataset without identity labels for\ntraining, our model has the capability to generate high-fidelity and temporally\nconsistent videos. Through extensive qualitative and quantitative evaluations,\nwe demonstrate that our method shows significant improvements compared with the\nprevious face swapping models in synthesizing both images and videos. Project\npage is available at https://robustswap.github.io/\n","authors":["Jaeseong Lee","Taewoo Kim","Sunghyun Park","Younggun Lee","Jaegul Choo"],"pdf_url":"https://arxiv.org/pdf/2303.15768v1.pdf","comment":"21 pages"},{"id":"http://arxiv.org/abs/2205.10655v3","updated":"2023-03-28T07:02:03Z","published":"2022-05-21T18:38:05Z","title":"Swept-Angle Synthetic Wavelength Interferometry","summary":"  We present a new imaging technique, swept-angle synthetic wavelength\ninterferometry, for full-field micron-scale 3D sensing. As in conventional\nsynthetic wavelength interferometry, our technique uses light consisting of two\nnarrowly-separated optical wavelengths, resulting in per-pixel interferometric\nmeasurements whose phase encodes scene depth. Our technique additionally uses a\nnew type of light source that, by emulating spatially-incoherent illumination,\nmakes interferometric measurements insensitive to aberrations and (sub)surface\nscattering, effects that corrupt phase measurements. The resulting technique\ncombines the robustness to such corruptions of scanning interferometric setups,\nwith the speed of full-field interferometric setups. Overall, our technique can\nrecover full-frame depth at a lateral and axial resolution of 5 microns, at\nframe rates of 5 Hz, even under strong ambient light. We build an experimental\nprototype, and use it to demonstrate these capabilities by scanning a variety\nof objects, including objects representative of applications in inspection and\nfabrication, and objects that contain challenging light scattering effects.\n","authors":["Alankar Kotwal","Anat Levin","Ioannis Gkioulekas"],"pdf_url":"https://arxiv.org/pdf/2205.10655v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2211.10732v2","updated":"2023-03-28T06:46:29Z","published":"2022-11-19T15:56:39Z","title":"Passive Micron-scale Time-of-Flight with Sunlight Interferometry","summary":"  We introduce an interferometric technique for passive time-of-flight imaging\nand depth sensing at micrometer axial resolutions. Our technique uses a\nfull-field Michelson interferometer, modified to use sunlight as the only light\nsource. The large spectral bandwidth of sunlight makes it possible to acquire\nmicrometer-resolution time-resolved scene responses, through a simple axial\nscanning operation. Additionally, the angular bandwidth of sunlight makes it\npossible to capture time-of-flight measurements insensitive to indirect\nillumination effects, such as interreflections and subsurface scattering. We\nbuild an experimental prototype that we operate outdoors, under direct\nsunlight, and in adverse environment conditions such as machine vibrations and\nvehicle traffic. We use this prototype to demonstrate, for the first time,\npassive imaging capabilities such as micrometer-scale depth sensing robust to\nindirect illumination, direct-only imaging, and imaging through diffusers.\n","authors":["Alankar Kotwal","Anat Levin","Ioannis Gkioulekas"],"pdf_url":"https://arxiv.org/pdf/2211.10732v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.15764v1","updated":"2023-03-28T06:45:31Z","published":"2023-03-28T06:45:31Z","title":"X-Mesh: Towards Fast and Accurate Text-driven 3D Stylization via Dynamic\n  Textual Guidance","summary":"  Text-driven 3D stylization is a complex and crucial task in the fields of\ncomputer vision (CV) and computer graphics (CG), aimed at transforming a bare\nmesh to fit a target text. Prior methods adopt text-independent multilayer\nperceptrons (MLPs) to predict the attributes of the target mesh with the\nsupervision of CLIP loss. However, such text-independent architecture lacks\ntextual guidance during predicting attributes, thus leading to unsatisfactory\nstylization and slow convergence. To address these limitations, we present\nX-Mesh, an innovative text-driven 3D stylization framework that incorporates a\nnovel Text-guided Dynamic Attention Module (TDAM). The TDAM dynamically\nintegrates the guidance of the target text by utilizing text-relevant spatial\nand channel-wise attentions during vertex feature extraction, resulting in more\naccurate attribute prediction and faster convergence speed. Furthermore,\nexisting works lack standard benchmarks and automated metrics for evaluation,\noften relying on subjective and non-reproducible user studies to assess the\nquality of stylized 3D assets. To overcome this limitation, we introduce a new\nstandard text-mesh benchmark, namely MIT-30, and two automated metrics, which\nwill enable future research to achieve fair and objective comparisons. Our\nextensive qualitative and quantitative experiments demonstrate that X-Mesh\noutperforms previous state-of-the-art methods.\n","authors":["Yiwei Ma","Xiaioqing Zhang","Xiaoshuai Sun","Jiayi Ji","Haowei Wang","Guannan Jiang","Weilin Zhuang","Rongrong Ji"],"pdf_url":"https://arxiv.org/pdf/2303.15764v1.pdf","comment":"Technical report"},{"id":"http://arxiv.org/abs/2303.15761v1","updated":"2023-03-28T06:40:11Z","published":"2023-03-28T06:40:11Z","title":"Learning Second-Order Attentive Context for Efficient Correspondence\n  Pruning","summary":"  Correspondence pruning aims to search consistent correspondences (inliers)\nfrom a set of putative correspondences. It is challenging because of the\ndisorganized spatial distribution of numerous outliers, especially when\nputative correspondences are largely dominated by outliers. It's more\nchallenging to ensure effectiveness while maintaining efficiency. In this\npaper, we propose an effective and efficient method for correspondence pruning.\nInspired by the success of attentive context in correspondence problems, we\nfirst extend the attentive context to the first-order attentive context and\nthen introduce the idea of attention in attention (ANA) to model second-order\nattentive context for correspondence pruning. Compared with first-order\nattention that focuses on feature-consistent context, second-order attention\ndedicates to attention weights itself and provides an additional source to\nencode consistent context from the attention map. For efficiency, we derive two\napproximate formulations for the naive implementation of second-order attention\nto optimize the cubic complexity to linear complexity, such that second-order\nattention can be used with negligible computational overheads. We further\nimplement our formulations in a second-order context layer and then incorporate\nthe layer in an ANA block. Extensive experiments demonstrate that our method is\neffective and efficient in pruning outliers, especially in high-outlier-ratio\ncases. Compared with the state-of-the-art correspondence pruning approach\nLMCNet, our method runs 14 times faster while maintaining a competitive\naccuracy.\n","authors":["Xinyi Ye","Weiyue Zhao","Hao Lu","Zhiguo Cao"],"pdf_url":"https://arxiv.org/pdf/2303.15761v1.pdf","comment":"9 pages, 8 figures; Accepted to AAAI 2023 (Oral)"},{"id":"http://arxiv.org/abs/2211.15069v3","updated":"2023-03-28T06:34:08Z","published":"2022-11-28T05:06:03Z","title":"FeatureBooster: Boosting Feature Descriptors with a Lightweight Neural\n  Network","summary":"  We introduce a lightweight network to improve descriptors of keypoints within\nthe same image. The network takes the original descriptors and the geometric\nproperties of keypoints as the input, and uses an MLP-based self-boosting stage\nand a Transformer-based cross-boosting stage to enhance the descriptors. The\nboosted descriptors can be either real-valued or binary ones. We use the\nproposed network to boost both hand-crafted (ORB, SIFT) and the\nstate-of-the-art learning-based descriptors (SuperPoint, ALIKE) and evaluate\nthem on image matching, visual localization, and structure-from-motion tasks.\nThe results show that our method significantly improves the performance of each\ntask, particularly in challenging cases such as large illumination changes or\nrepetitive patterns. Our method requires only 3.2ms on desktop GPU and 27ms on\nembedded GPU to process 2000 features, which is fast enough to be applied to a\npractical system. The code and trained weights are publicly available at\ngithub.com/SJTU-ViSYS/FeatureBooster.\n","authors":["Xinjiang Wang","Zeyu Liu","Yu Hu","Wei Xi","Wenxian Yu","Danping Zou"],"pdf_url":"https://arxiv.org/pdf/2211.15069v3.pdf","comment":"Accept by CVPR2023; 15 pages, 8 figures, 7 tables"},{"id":"http://arxiv.org/abs/2302.14434v2","updated":"2023-03-28T06:27:14Z","published":"2023-02-28T09:24:36Z","title":"A Hierarchical Representation Network for Accurate and Detailed Face\n  Reconstruction from In-The-Wild Images","summary":"  Limited by the nature of the low-dimensional representational capacity of\n3DMM, most of the 3DMM-based face reconstruction (FR) methods fail to recover\nhigh-frequency facial details, such as wrinkles, dimples, etc. Some attempt to\nsolve the problem by introducing detail maps or non-linear operations, however,\nthe results are still not vivid. To this end, we in this paper present a novel\nhierarchical representation network (HRN) to achieve accurate and detailed face\nreconstruction from a single image. Specifically, we implement the geometry\ndisentanglement and introduce the hierarchical representation to fulfill\ndetailed face modeling. Meanwhile, 3D priors of facial details are incorporated\nto enhance the accuracy and authenticity of the reconstruction results. We also\npropose a de-retouching module to achieve better decoupling of the geometry and\nappearance. It is noteworthy that our framework can be extended to a multi-view\nfashion by considering detail consistency of different views. Extensive\nexperiments on two single-view and two multi-view FR benchmarks demonstrate\nthat our method outperforms the existing methods in both reconstruction\naccuracy and visual effects. Finally, we introduce a high-quality 3D face\ndataset FaceHD-100 to boost the research of high-fidelity face reconstruction.\nThe project homepage is at https://younglbw.github.io/HRN-homepage/.\n","authors":["Biwen Lei","Jianqiang Ren","Mengyang Feng","Miaomiao Cui","Xuansong Xie"],"pdf_url":"https://arxiv.org/pdf/2302.14434v2.pdf","comment":"Accepted by CVPR2023"},{"id":"http://arxiv.org/abs/2303.15754v1","updated":"2023-03-28T06:23:17Z","published":"2023-03-28T06:23:17Z","title":"Transferable Adversarial Attacks on Vision Transformers with Token\n  Gradient Regularization","summary":"  Vision transformers (ViTs) have been successfully deployed in a variety of\ncomputer vision tasks, but they are still vulnerable to adversarial samples.\nTransfer-based attacks use a local model to generate adversarial samples and\ndirectly transfer them to attack a target black-box model. The high efficiency\nof transfer-based attacks makes it a severe security threat to ViT-based\napplications. Therefore, it is vital to design effective transfer-based attacks\nto identify the deficiencies of ViTs beforehand in security-sensitive\nscenarios. Existing efforts generally focus on regularizing the input gradients\nto stabilize the updated direction of adversarial samples. However, the\nvariance of the back-propagated gradients in intermediate blocks of ViTs may\nstill be large, which may make the generated adversarial samples focus on some\nmodel-specific features and get stuck in poor local optima. To overcome the\nshortcomings of existing approaches, we propose the Token Gradient\nRegularization (TGR) method. According to the structural characteristics of\nViTs, TGR reduces the variance of the back-propagated gradient in each internal\nblock of ViTs in a token-wise manner and utilizes the regularized gradient to\ngenerate adversarial samples. Extensive experiments on attacking both ViTs and\nCNNs confirm the superiority of our approach. Notably, compared to the\nstate-of-the-art transfer-based attacks, our TGR offers a performance\nimprovement of 8.8% on average.\n","authors":["Jianping Zhang","Yizhan Huang","Weibin Wu","Michael R. Lyu"],"pdf_url":"https://arxiv.org/pdf/2303.15754v1.pdf","comment":"CVPR 2023"},{"id":"http://arxiv.org/abs/2303.15749v1","updated":"2023-03-28T06:12:53Z","published":"2023-03-28T06:12:53Z","title":"Iteratively Coupled Multiple Instance Learning from Instance to Bag\n  Classifier for Whole Slide Image Classification","summary":"  Whole Slide Image (WSI) classification remains a challenge due to their\nextremely high resolution and the absence of fine-grained labels. Presently,\nWSIs are usually classified as a Multiple Instance Learning (MIL) problem when\nonly slide-level labels are available. MIL methods involve a patch embedding\nprocess and a bag-level classification process, but they are prohibitively\nexpensive to be trained end-to-end. Therefore, existing methods usually train\nthem separately, or directly skip the training of the embedder. Such schemes\nhinder the patch embedder's access to slide-level labels, resulting in\ninconsistencies within the entire MIL pipeline. To overcome this issue, we\npropose a novel framework called Iteratively Coupled MIL (ICMIL), which bridges\nthe loss back-propagation process from the bag-level classifier to the patch\nembedder. In ICMIL, we use category information in the bag-level classifier to\nguide the patch-level fine-tuning of the patch feature extractor. The refined\nembedder then generates better instance representations for achieving a more\naccurate bag-level classifier. By coupling the patch embedder and bag\nclassifier at a low cost, our proposed framework enables information exchange\nbetween the two processes, benefiting the entire MIL classification model. We\ntested our framework on two datasets using three different backbones, and our\nexperimental results demonstrate consistent performance improvements over\nstate-of-the-art MIL methods. Code will be made available upon acceptance.\n","authors":["Hongyi Wang","Luyang Luo","Fang Wang","Ruofeng Tong","Yen-Wei Chen","Hongjie Hu","Lanfen Lin","Hao Chen"],"pdf_url":"https://arxiv.org/pdf/2303.15749v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.15748v1","updated":"2023-03-28T06:08:32Z","published":"2023-03-28T06:08:32Z","title":"SVD-DIP: Overcoming the Overfitting Problem in DIP-based CT\n  Reconstruction","summary":"  The deep image prior (DIP) is a well-established unsupervised deep learning\nmethod for image reconstruction; yet it is far from being flawless. The DIP\noverfits to noise if not early stopped, or optimized via a regularized\nobjective. We build on the regularized fine-tuning of a pretrained DIP, by\nadopting a novel strategy that restricts the learning to the adaptation of\nsingular values. The proposed SVD-DIP uses ad hoc convolutional layers whose\npretrained parameters are decomposed via the singular value decomposition.\nOptimizing the DIP then solely consists in the fine-tuning of the singular\nvalues, while keeping the left and right singular vectors fixed. We thoroughly\nvalidate the proposed method on real-measured $\\mu$CT data of a lotus root as\nwell as two medical datasets (LoDoPaB and Mayo). We report significantly\nimproved stability of the DIP optimization, by overcoming the overfitting to\nnoise.\n","authors":["Marco Nittscher","Michael Lameter","Riccardo Barbano","Johannes Leuschner","Bangti Jin","Peter Maass"],"pdf_url":"https://arxiv.org/pdf/2303.15748v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.15743v1","updated":"2023-03-28T05:36:42Z","published":"2023-03-28T05:36:42Z","title":"HS-Pose: Hybrid Scope Feature Extraction for Category-level Object Pose\n  Estimation","summary":"  In this paper, we focus on the problem of category-level object pose\nestimation, which is challenging due to the large intra-category shape\nvariation. 3D graph convolution (3D-GC) based methods have been widely used to\nextract local geometric features, but they have limitations for complex shaped\nobjects and are sensitive to noise. Moreover, the scale and translation\ninvariant properties of 3D-GC restrict the perception of an object's size and\ntranslation information. In this paper, we propose a simple network structure,\nthe HS-layer, which extends 3D-GC to extract hybrid scope latent features from\npoint cloud data for category-level object pose estimation tasks. The proposed\nHS-layer: 1) is able to perceive local-global geometric structure and global\ninformation, 2) is robust to noise, and 3) can encode size and translation\ninformation. Our experiments show that the simple replacement of the 3D-GC\nlayer with the proposed HS-layer on the baseline method (GPV-Pose) achieves a\nsignificant improvement, with the performance increased by 14.5% on 5d2cm\nmetric and 10.3% on IoU75. Our method outperforms the state-of-the-art methods\nby a large margin (8.3% on 5d2cm, 6.9% on IoU75) on the REAL275 dataset and\nruns in real-time (50 FPS).\n","authors":["Linfang Zheng","Chen Wang","Yinghan Sun","Esha Dasgupta","Hua Chen","Ales Leonardis","Wei Zhang","Hyung Jin Chang"],"pdf_url":"https://arxiv.org/pdf/2303.15743v1.pdf","comment":"Accepted by the 2023 IEEE/CVF Computer Vision and Pattern Recognition\n  Conference (CVPR)"},{"id":"http://arxiv.org/abs/2303.15742v1","updated":"2023-03-28T05:33:56Z","published":"2023-03-28T05:33:56Z","title":"System-status-aware Adaptive Network for Online Streaming Video\n  Understanding","summary":"  Recent years have witnessed great progress in deep neural networks for\nreal-time applications. However, most existing works do not explicitly consider\nthe general case where the device's state and the available resources fluctuate\nover time, and none of them investigate or address the impact of varying\ncomputational resources for online video understanding tasks. This paper\nproposes a System-status-aware Adaptive Network (SAN) that considers the\ndevice's real-time state to provide high-quality predictions with low delay.\nUsage of our agent's policy improves efficiency and robustness to fluctuations\nof the system status. On two widely used video understanding tasks, SAN obtains\nstate-of-the-art performance while constantly keeping processing delays low.\nMoreover, training such an agent on various types of hardware configurations is\nnot easy as the labeled training data might not be available, or can be\ncomputationally prohibitive. To address this challenging problem, we propose a\nMeta Self-supervised Adaptation (MSA) method that adapts the agent's policy to\nnew hardware configurations at test-time, allowing for easy deployment of the\nmodel onto other unseen hardware platforms.\n","authors":["Lin Geng Foo","Jia Gong","Zhipeng Fan","Jun Liu"],"pdf_url":"https://arxiv.org/pdf/2303.15742v1.pdf","comment":"Accepted to CVPR 2023"},{"id":"http://arxiv.org/abs/2303.15737v1","updated":"2023-03-28T05:18:58Z","published":"2023-03-28T05:18:58Z","title":"Deformable Kernel Expansion Model for Efficient Arbitrary-shaped Scene\n  Text Detection","summary":"  Scene text detection is a challenging computer vision task due to the high\nvariation in text shapes and ratios. In this work, we propose a scene text\ndetector named Deformable Kernel Expansion (DKE), which incorporates the merits\nof both segmentation and contour-based detectors. DKE employs a segmentation\nmodule to segment the shrunken text region as the text kernel, then expands the\ntext kernel contour to obtain text boundary by regressing the vertex-wise\noffsets. Generating the text kernel by segmentation enables DKE to inherit the\narbitrary-shaped text region modeling capability of segmentation-based\ndetectors. Regressing the kernel contour with some sampled vertices enables DKE\nto avoid the complicated pixel-level post-processing and better learn contour\ndeformation as the contour-based detectors. Moreover, we propose an Optimal\nBipartite Graph Matching Loss (OBGML) that measures the matching error between\nthe predicted contour and the ground truth, which efficiently minimizes the\nglobal contour matching distance. Extensive experiments on CTW1500, Total-Text,\nMSRA-TD500, and ICDAR2015 demonstrate that DKE achieves a good tradeoff between\naccuracy and efficiency in scene text detection.\n","authors":["Tao He","Sheng Huang","Wenhao Tang","Bo Liu"],"pdf_url":"https://arxiv.org/pdf/2303.15737v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2212.04247v2","updated":"2023-03-28T05:14:33Z","published":"2022-12-07T06:08:03Z","title":"EditableNeRF: Editing Topologically Varying Neural Radiance Fields by\n  Key Points","summary":"  Neural radiance fields (NeRF) achieve highly photo-realistic novel-view\nsynthesis, but it's a challenging problem to edit the scenes modeled by\nNeRF-based methods, especially for dynamic scenes. We propose editable neural\nradiance fields that enable end-users to easily edit dynamic scenes and even\nsupport topological changes. Input with an image sequence from a single camera,\nour network is trained fully automatically and models topologically varying\ndynamics using our picked-out surface key points. Then end-users can edit the\nscene by easily dragging the key points to desired new positions. To achieve\nthis, we propose a scene analysis method to detect and initialize key points by\nconsidering the dynamics in the scene, and a weighted key points strategy to\nmodel topologically varying dynamics by joint key points and weights\noptimization. Our method supports intuitive multi-dimensional (up to 3D)\nediting and can generate novel scenes that are unseen in the input sequence.\nExperiments demonstrate that our method achieves high-quality editing on\nvarious dynamic scenes and outperforms the state-of-the-art. Our code and\ncaptured data are available at https://chengwei-zheng.github.io/EditableNeRF/.\n","authors":["Chengwei Zheng","Wenbin Lin","Feng Xu"],"pdf_url":"https://arxiv.org/pdf/2212.04247v2.pdf","comment":"Accepted by CVPR 2023"},{"id":"http://arxiv.org/abs/2303.15735v1","updated":"2023-03-28T05:14:04Z","published":"2023-03-28T05:14:04Z","title":"Improving the Transferability of Adversarial Samples by Path-Augmented\n  Method","summary":"  Deep neural networks have achieved unprecedented success on diverse vision\ntasks. However, they are vulnerable to adversarial noise that is imperceptible\nto humans. This phenomenon negatively affects their deployment in real-world\nscenarios, especially security-related ones. To evaluate the robustness of a\ntarget model in practice, transfer-based attacks craft adversarial samples with\na local model and have attracted increasing attention from researchers due to\ntheir high efficiency. The state-of-the-art transfer-based attacks are\ngenerally based on data augmentation, which typically augments multiple\ntraining images from a linear path when learning adversarial samples. However,\nsuch methods selected the image augmentation path heuristically and may augment\nimages that are semantics-inconsistent with the target images, which harms the\ntransferability of the generated adversarial samples. To overcome the pitfall,\nwe propose the Path-Augmented Method (PAM). Specifically, PAM first constructs\na candidate augmentation path pool. It then settles the employed augmentation\npaths during adversarial sample generation with greedy search. Furthermore, to\navoid augmenting semantics-inconsistent images, we train a Semantics Predictor\n(SP) to constrain the length of the augmentation path. Extensive experiments\nconfirm that PAM can achieve an improvement of over 4.8% on average compared\nwith the state-of-the-art baselines in terms of the attack success rates.\n","authors":["Jianping Zhang","Jen-tse Huang","Wenxuan Wang","Yichen Li","Weibin Wu","Xiaosen Wang","Yuxin Su","Michael R. Lyu"],"pdf_url":"https://arxiv.org/pdf/2303.15735v1.pdf","comment":"10 pages + appendix, CVPR 2023"},{"id":"http://arxiv.org/abs/2210.01612v3","updated":"2023-03-28T05:06:59Z","published":"2022-10-04T13:51:59Z","title":"PlaneDepth: Self-supervised Depth Estimation via Orthogonal Planes","summary":"  Multiple near frontal-parallel planes based depth representation demonstrated\nimpressive results in self-supervised monocular depth estimation (MDE).\nWhereas, such a representation would cause the discontinuity of the ground as\nit is perpendicular to the frontal-parallel planes, which is detrimental to the\nidentification of drivable space in autonomous driving. In this paper, we\npropose the PlaneDepth, a novel orthogonal planes based presentation, including\nvertical planes and ground planes. PlaneDepth estimates the depth distribution\nusing a Laplacian Mixture Model based on orthogonal planes for an input image.\nThese planes are used to synthesize a reference view to provide the\nself-supervision signal. Further, we find that the widely used resizing and\ncropping data augmentation breaks the orthogonality assumptions, leading to\ninferior plane predictions. We address this problem by explicitly constructing\nthe resizing cropping transformation to rectify the predefined planes and\npredicted camera pose. Moreover, we propose an augmented self-distillation loss\nsupervised with a bilateral occlusion mask to boost the robustness of\northogonal planes representation for occlusions. Thanks to our orthogonal\nplanes representation, we can extract the ground plane in an unsupervised\nmanner, which is important for autonomous driving. Extensive experiments on the\nKITTI dataset demonstrate the effectiveness and efficiency of our method. The\ncode is available at https://github.com/svip-lab/PlaneDepth.\n","authors":["Ruoyu Wang","Zehao Yu","Shenghua Gao"],"pdf_url":"https://arxiv.org/pdf/2210.01612v3.pdf","comment":"Accepted by CVPR 2023. Code and models are available at:\n  https://github.com/svip-lab/PlaneDepth"},{"id":"http://arxiv.org/abs/2211.16940v2","updated":"2023-03-28T04:52:25Z","published":"2022-11-30T12:22:22Z","title":"DiffPose: Toward More Reliable 3D Pose Estimation","summary":"  Monocular 3D human pose estimation is quite challenging due to the inherent\nambiguity and occlusion, which often lead to high uncertainty and\nindeterminacy. On the other hand, diffusion models have recently emerged as an\neffective tool for generating high-quality images from noise. Inspired by their\ncapability, we explore a novel pose estimation framework (DiffPose) that\nformulates 3D pose estimation as a reverse diffusion process. We incorporate\nnovel designs into our DiffPose to facilitate the diffusion process for 3D pose\nestimation: a pose-specific initialization of pose uncertainty distributions, a\nGaussian Mixture Model-based forward diffusion process, and a\ncontext-conditioned reverse diffusion process. Our proposed DiffPose\nsignificantly outperforms existing methods on the widely used pose estimation\nbenchmarks Human3.6M and MPI-INF-3DHP. Project page:\nhttps://gongjia0208.github.io/Diffpose/.\n","authors":["Jia Gong","Lin Geng Foo","Zhipeng Fan","Qiuhong Ke","Hossein Rahmani","Jun Liu"],"pdf_url":"https://arxiv.org/pdf/2211.16940v2.pdf","comment":"Accepted to CVPR 2023"},{"id":"http://arxiv.org/abs/2303.12370v2","updated":"2023-03-28T04:43:12Z","published":"2023-03-22T08:13:25Z","title":"Weakly Supervised Video Representation Learning with Unaligned Text for\n  Sequential Videos","summary":"  Sequential video understanding, as an emerging video understanding task, has\ndriven lots of researchers' attention because of its goal-oriented nature. This\npaper studies weakly supervised sequential video understanding where the\naccurate time-stamp level text-video alignment is not provided. We solve this\ntask by borrowing ideas from CLIP. Specifically, we use a transformer to\naggregate frame-level features for video representation and use a pre-trained\ntext encoder to encode the texts corresponding to each action and the whole\nvideo, respectively. To model the correspondence between text and video, we\npropose a multiple granularity loss, where the video-paragraph contrastive loss\nenforces matching between the whole video and the complete script, and a\nfine-grained frame-sentence contrastive loss enforces the matching between each\naction and its description. As the frame-sentence correspondence is not\navailable, we propose to use the fact that video actions happen sequentially in\nthe temporal domain to generate pseudo frame-sentence correspondence and\nsupervise the network training with the pseudo labels. Extensive experiments on\nvideo sequence verification and text-to-video matching show that our method\noutperforms baselines by a large margin, which validates the effectiveness of\nour proposed approach. Code is available at https://github.com/svip-lab/WeakSVR\n","authors":["Sixun Dong","Huazhang Hu","Dongze Lian","Weixin Luo","Yicheng Qian","Shenghua Gao"],"pdf_url":"https://arxiv.org/pdf/2303.12370v2.pdf","comment":"CVPR 2023. Code: https://github.com/svip-lab/WeakSVR"},{"id":"http://arxiv.org/abs/2301.02371v2","updated":"2023-03-28T04:28:30Z","published":"2023-01-06T04:35:31Z","title":"Anchor3DLane: Learning to Regress 3D Anchors for Monocular 3D Lane\n  Detection","summary":"  Monocular 3D lane detection is a challenging task due to its lack of depth\ninformation. A popular solution is to first transform the front-viewed (FV)\nimages or features into the bird-eye-view (BEV) space with inverse perspective\nmapping (IPM) and detect lanes from BEV features. However, the reliance of IPM\non flat ground assumption and loss of context information make it inaccurate to\nrestore 3D information from BEV representations. An attempt has been made to\nget rid of BEV and predict 3D lanes from FV representations directly, while it\nstill underperforms other BEV-based methods given its lack of structured\nrepresentation for 3D lanes. In this paper, we define 3D lane anchors in the 3D\nspace and propose a BEV-free method named Anchor3DLane to predict 3D lanes\ndirectly from FV representations. 3D lane anchors are projected to the FV\nfeatures to extract their features which contain both good structural and\ncontext information to make accurate predictions. In addition, we also develop\na global optimization method that makes use of the equal-width property between\nlanes to reduce the lateral error of predictions. Extensive experiments on\nthree popular 3D lane detection benchmarks show that our Anchor3DLane\noutperforms previous BEV-based methods and achieves state-of-the-art\nperformances. The code is available at:\nhttps://github.com/tusen-ai/Anchor3DLane.\n","authors":["Shaofei Huang","Zhenwei Shen","Zehao Huang","Zi-han Ding","Jiao Dai","Jizhong Han","Naiyan Wang","Si Liu"],"pdf_url":"https://arxiv.org/pdf/2301.02371v2.pdf","comment":"Accepted by CVPR 2023"},{"id":"http://arxiv.org/abs/2303.15724v1","updated":"2023-03-28T04:18:01Z","published":"2023-03-28T04:18:01Z","title":"Scalable, Detailed and Mask-Free Universal Photometric Stereo","summary":"  In this paper, we introduce SDM-UniPS, a groundbreaking Scalable, Detailed,\nMask-free, and Universal Photometric Stereo network. Our approach can recover\nastonishingly intricate surface normal maps, rivaling the quality of 3D\nscanners, even when images are captured under unknown, spatially-varying\nlighting conditions in uncontrolled environments. We have extended previous\nuniversal photometric stereo networks to extract spatial-light features,\nutilizing all available information in high-resolution input images and\naccounting for non-local interactions among surface points. Moreover, we\npresent a new synthetic training dataset that encompasses a diverse range of\nshapes, materials, and illumination scenarios found in real-world scenes.\nThrough extensive evaluation, we demonstrate that our method not only surpasses\ncalibrated, lighting-specific techniques on public benchmarks, but also excels\nwith a significantly smaller number of input images even without object masks.\n","authors":["Satoshi Ikehata"],"pdf_url":"https://arxiv.org/pdf/2303.15724v1.pdf","comment":"CVPR 2023 (Highlight). The source code will be available at\n  https://github.com/satoshi-ikehata/SDM-UniPS-CVPR2023"},{"id":"http://arxiv.org/abs/2303.15718v1","updated":"2023-03-28T04:06:02Z","published":"2023-03-28T04:06:02Z","title":"MeMaHand: Exploiting Mesh-Mano Interaction for Single Image Two-Hand\n  Reconstruction","summary":"  Existing methods proposed for hand reconstruction tasks usually parameterize\na generic 3D hand model or predict hand mesh positions directly. The parametric\nrepresentations consisting of hand shapes and rotational poses are more stable,\nwhile the non-parametric methods can predict more accurate mesh positions. In\nthis paper, we propose to reconstruct meshes and estimate MANO parameters of\ntwo hands from a single RGB image simultaneously to utilize the merits of two\nkinds of hand representations. To fulfill this target, we propose novel\nMesh-Mano interaction blocks (MMIBs), which take mesh vertices positions and\nMANO parameters as two kinds of query tokens. MMIB consists of one graph\nresidual block to aggregate local information and two transformer encoders to\nmodel long-range dependencies. The transformer encoders are equipped with\ndifferent asymmetric attention masks to model the intra-hand and inter-hand\nattention, respectively. Moreover, we introduce the mesh alignment refinement\nmodule to further enhance the mesh-image alignment. Extensive experiments on\nthe InterHand2.6M benchmark demonstrate promising results over the\nstate-of-the-art hand reconstruction methods.\n","authors":["Congyi Wang","Feida Zhu","Shilei Wen"],"pdf_url":"https://arxiv.org/pdf/2303.15718v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.15140v2","updated":"2023-03-28T04:02:01Z","published":"2023-03-27T12:18:52Z","title":"SimpleNet: A Simple Network for Image Anomaly Detection and Localization","summary":"  We propose a simple and application-friendly network (called SimpleNet) for\ndetecting and localizing anomalies. SimpleNet consists of four components: (1)\na pre-trained Feature Extractor that generates local features, (2) a shallow\nFeature Adapter that transfers local features towards target domain, (3) a\nsimple Anomaly Feature Generator that counterfeits anomaly features by adding\nGaussian noise to normal features, and (4) a binary Anomaly Discriminator that\ndistinguishes anomaly features from normal features. During inference, the\nAnomaly Feature Generator would be discarded. Our approach is based on three\nintuitions. First, transforming pre-trained features to target-oriented\nfeatures helps avoid domain bias. Second, generating synthetic anomalies in\nfeature space is more effective, as defects may not have much commonality in\nthe image space. Third, a simple discriminator is much efficient and practical.\nIn spite of simplicity, SimpleNet outperforms previous methods quantitatively\nand qualitatively. On the MVTec AD benchmark, SimpleNet achieves an anomaly\ndetection AUROC of 99.6%, reducing the error by 55.5% compared to the next best\nperforming model. Furthermore, SimpleNet is faster than existing methods, with\na high frame rate of 77 FPS on a 3080ti GPU. Additionally, SimpleNet\ndemonstrates significant improvements in performance on the One-Class Novelty\nDetection task. Code: https://github.com/DonaldRR/SimpleNet.\n","authors":["Zhikang Liu","Yiming Zhou","Yuansheng Xu","Zilei Wang"],"pdf_url":"https://arxiv.org/pdf/2303.15140v2.pdf","comment":"Accepted to CVPR 2023"},{"id":"http://arxiv.org/abs/2303.15710v1","updated":"2023-03-28T03:37:27Z","published":"2023-03-28T03:37:27Z","title":"Explicit Attention-Enhanced Fusion for RGB-Thermal Perception Tasks","summary":"  Recently, RGB-Thermal based perception has shown significant advances.\nThermal information provides useful clues when visual cameras suffer from poor\nlighting conditions, such as low light and fog. However, how to effectively\nfuse RGB images and thermal data remains an open challenge. Previous works\ninvolve naive fusion strategies such as merging them at the input,\nconcatenating multi-modality features inside models, or applying attention to\neach data modality. These fusion strategies are straightforward yet\ninsufficient. In this paper, we propose a novel fusion method named Explicit\nAttention-Enhanced Fusion (EAEF) that fully takes advantage of each type of\ndata. Specifically, we consider the following cases: i) both RGB data and\nthermal data, ii) only one of the types of data, and iii) none of them generate\ndiscriminative features. EAEF uses one branch to enhance feature extraction for\ni) and iii) and the other branch to remedy insufficient representations for\nii). The outputs of two branches are fused to form complementary features. As a\nresult, the proposed fusion method outperforms state-of-the-art by 1.6\\% in\nmIoU on semantic segmentation, 3.1\\% in MAE on salient object detection, 2.3\\%\nin mAP on object detection, and 8.1\\% in MAE on crowd counting. The code is\navailable at https://github.com/FreeformRobotics/EAEFNet.\n","authors":["Mingjian Liang","Junjie Hu","Chenyu Bao","Hua Feng","Fuqin Deng","Tin Lun Lam"],"pdf_url":"https://arxiv.org/pdf/2303.15710v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2210.15257v2","updated":"2023-03-28T03:31:00Z","published":"2022-10-27T08:21:35Z","title":"ERNIE-ViLG 2.0: Improving Text-to-Image Diffusion Model with\n  Knowledge-Enhanced Mixture-of-Denoising-Experts","summary":"  Recent progress in diffusion models has revolutionized the popular technology\nof text-to-image generation. While existing approaches could produce\nphotorealistic high-resolution images with text conditions, there are still\nseveral open problems to be solved, which limits the further improvement of\nimage fidelity and text relevancy. In this paper, we propose ERNIE-ViLG 2.0, a\nlarge-scale Chinese text-to-image diffusion model, to progressively upgrade the\nquality of generated images by: (1) incorporating fine-grained textual and\nvisual knowledge of key elements in the scene, and (2) utilizing different\ndenoising experts at different denoising stages. With the proposed mechanisms,\nERNIE-ViLG 2.0 not only achieves a new state-of-the-art on MS-COCO with\nzero-shot FID score of 6.75, but also significantly outperforms recent models\nin terms of image fidelity and image-text alignment, with side-by-side human\nevaluation on the bilingual prompt set ViLG-300.\n","authors":["Zhida Feng","Zhenyu Zhang","Xintong Yu","Yewei Fang","Lanxin Li","Xuyi Chen","Yuxiang Lu","Jiaxiang Liu","Weichong Yin","Shikun Feng","Yu Sun","Li Chen","Hao Tian","Hua Wu","Haifeng Wang"],"pdf_url":"https://arxiv.org/pdf/2210.15257v2.pdf","comment":"Accepted to CVPR 2023, highlight. Project page:\n  https://wenxin.baidu.com/ernie-vilg"},{"id":"http://arxiv.org/abs/2303.06819v2","updated":"2023-03-28T03:30:48Z","published":"2023-03-13T02:27:45Z","title":"TranSG: Transformer-Based Skeleton Graph Prototype Contrastive Learning\n  with Structure-Trajectory Prompted Reconstruction for Person\n  Re-Identification","summary":"  Person re-identification (re-ID) via 3D skeleton data is an emerging topic\nwith prominent advantages. Existing methods usually design skeleton descriptors\nwith raw body joints or perform skeleton sequence representation learning.\nHowever, they typically cannot concurrently model different body-component\nrelations, and rarely explore useful semantics from fine-grained\nrepresentations of body joints. In this paper, we propose a generic\nTransformer-based Skeleton Graph prototype contrastive learning (TranSG)\napproach with structure-trajectory prompted reconstruction to fully capture\nskeletal relations and valuable spatial-temporal semantics from skeleton graphs\nfor person re-ID. Specifically, we first devise the Skeleton Graph Transformer\n(SGT) to simultaneously learn body and motion relations within skeleton graphs,\nso as to aggregate key correlative node features into graph representations.\nThen, we propose the Graph Prototype Contrastive learning (GPC) to mine the\nmost typical graph features (graph prototypes) of each identity, and contrast\nthe inherent similarity between graph representations and different prototypes\nfrom both skeleton and sequence levels to learn discriminative graph\nrepresentations. Last, a graph Structure-Trajectory Prompted Reconstruction\n(STPR) mechanism is proposed to exploit the spatial and temporal contexts of\ngraph nodes to prompt skeleton graph reconstruction, which facilitates\ncapturing more valuable patterns and graph semantics for person re-ID.\nEmpirical evaluations demonstrate that TranSG significantly outperforms\nexisting state-of-the-art methods. We further show its generality under\ndifferent graph modeling, RGB-estimated skeletons, and unsupervised scenarios.\n","authors":["Haocong Rao","Chunyan Miao"],"pdf_url":"https://arxiv.org/pdf/2303.06819v2.pdf","comment":"Accepted by CVPR 2023. Codes are available at\n  https://github.com/Kali-Hac/TranSG. Supplemental material is included in the\n  conference proceedings"},{"id":"http://arxiv.org/abs/2303.03729v3","updated":"2023-03-28T03:17:05Z","published":"2023-03-07T08:37:48Z","title":"Learning Discriminative Representations for Skeleton Based Action\n  Recognition","summary":"  Human action recognition aims at classifying the category of human action\nfrom a segment of a video. Recently, people have dived into designing GCN-based\nmodels to extract features from skeletons for performing this task, because\nskeleton representations are much more efficient and robust than other\nmodalities such as RGB frames. However, when employing the skeleton data, some\nimportant clues like related items are also discarded. It results in some\nambiguous actions that are hard to be distinguished and tend to be\nmisclassified. To alleviate this problem, we propose an auxiliary feature\nrefinement head (FR Head), which consists of spatial-temporal decoupling and\ncontrastive feature refinement, to obtain discriminative representations of\nskeletons. Ambiguous samples are dynamically discovered and calibrated in the\nfeature space. Furthermore, FR Head could be imposed on different stages of\nGCNs to build a multi-level refinement for stronger supervision. Extensive\nexperiments are conducted on NTU RGB+D, NTU RGB+D 120, and NW-UCLA datasets.\nOur proposed models obtain competitive results from state-of-the-art methods\nand can help to discriminate those ambiguous samples. Codes are available at\nhttps://github.com/zhysora/FR-Head.\n","authors":["Huanyu Zhou","Qingjie Liu","Yunhong Wang"],"pdf_url":"https://arxiv.org/pdf/2303.03729v3.pdf","comment":"Accepted by CVPR2023. 10 pages, 5 figures, 5 tables"},{"id":"http://arxiv.org/abs/2301.07234v2","updated":"2023-03-28T03:11:11Z","published":"2023-01-18T00:16:30Z","title":"DRIMET: Deep Registration for 3D Incompressible Motion Estimation in\n  Tagged-MRI with Application to the Tongue","summary":"  Tagged magnetic resonance imaging (MRI) has been used for decades to observe\nand quantify the detailed motion of deforming tissue. However, this technique\nfaces several challenges such as tag fading, large motion, long computation\ntimes, and difficulties in obtaining diffeomorphic incompressible flow fields.\nTo address these issues, this paper presents a novel unsupervised phase-based\n3D motion estimation technique for tagged MRI. We introduce two key\ninnovations. First, we apply a sinusoidal transformation to the harmonic phase\ninput, which enables end-to-end training and avoids the need for phase\ninterpolation. Second, we propose a Jacobian determinant-based learning\nobjective to encourage incompressible flow fields for deforming biological\ntissues. Our method efficiently estimates 3D motion fields that are accurate,\ndense, and approximately diffeomorphic and incompressible. The efficacy of the\nmethod is assessed using human tongue motion during speech, and includes both\nhealthy controls and patients that have undergone glossectomy. We show that the\nmethod outperforms existing approaches, and also exhibits improvements in\nspeed, robustness to tag fading, and large tongue motion.\n","authors":["Zhangxing Bian","Fangxu Xing","Jinglun Yu","Muhan Shao","Yihao Liu","Aaron Carass","Jiachen Zhuo","Jonghye Woo","Jerry L. Prince"],"pdf_url":"https://arxiv.org/pdf/2301.07234v2.pdf","comment":"Accepted to MIDL 2023 (full paper)"},{"id":"http://arxiv.org/abs/2303.15699v1","updated":"2023-03-28T03:05:25Z","published":"2023-03-28T03:05:25Z","title":"Enhancing Breast Cancer Risk Prediction by Incorporating Prior Images","summary":"  Recently, deep learning models have shown the potential to predict breast\ncancer risk and enable targeted screening strategies, but current models do not\nconsider the change in the breast over time. In this paper, we present a new\nmethod, PRIME+, for breast cancer risk prediction that leverages prior\nmammograms using a transformer decoder, outperforming a state-of-the-art risk\nprediction method that only uses mammograms from a single time point. We\nvalidate our approach on a dataset with 16,113 exams and further demonstrate\nthat it effectively captures patterns of changes from prior mammograms, such as\nchanges in breast density, resulting in improved short-term and long-term\nbreast cancer risk prediction. Experimental results show that our model\nachieves a statistically significant improvement in performance over the\nstate-of-the-art based model, with a C-index increase from 0.68 to 0.73 (p <\n0.05) on held-out test sets.\n","authors":["Hyeonsoo Lee","Junha Kim","Eunkyung Park","Minjeong Kim","Taesoo Kim","Thijs Kooi"],"pdf_url":"https://arxiv.org/pdf/2303.15699v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.15698v1","updated":"2023-03-28T03:00:28Z","published":"2023-03-28T03:00:28Z","title":"TFS-ViT: Token-Level Feature Stylization for Domain Generalization","summary":"  Standard deep learning models such as convolutional neural networks (CNNs)\nlack the ability of generalizing to domains which have not been seen during\ntraining. This problem is mainly due to the common but often wrong assumption\nof such models that the source and target data come from the same i.i.d.\ndistribution. Recently, Vision Transformers (ViTs) have shown outstanding\nperformance for a broad range of computer vision tasks. However, very few\nstudies have investigated their ability to generalize to new domains. This\npaper presents a first Token-level Feature Stylization (TFS-ViT) approach for\ndomain generalization, which improves the performance of ViTs to unseen data by\nsynthesizing new domains. Our approach transforms token features by mixing the\nnormalization statistics of images from different domains. We further improve\nthis approach with a novel strategy for attention-aware stylization, which uses\nthe attention maps of class (CLS) tokens to compute and mix normalization\nstatistics of tokens corresponding to different image regions. The proposed\nmethod is flexible to the choice of backbone model and can be easily applied to\nany ViT-based architecture with a negligible increase in computational\ncomplexity. Comprehensive experiments show that our approach is able to achieve\nstate-of-the-art performance on five challenging benchmarks for domain\ngeneralization, and demonstrate its ability to deal with different types of\ndomain shifts. The implementation is available at:\n{https://github.com/Mehrdad-Noori/TFS-ViT_Token-level_Feature_Stylization}.\n","authors":["Mehrdad Noori","Milad Cheraghalikhani","Ali Bahri","Gustavo A. Vargas Hakim","David Osowiechi","Ismail Ben Ayed","Christian Desrosiers"],"pdf_url":"https://arxiv.org/pdf/2303.15698v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.12313v3","updated":"2023-03-28T02:59:38Z","published":"2023-03-22T05:03:14Z","title":"Distribution Aligned Diffusion and Prototype-guided network for\n  Unsupervised Domain Adaptive Segmentation","summary":"  The Diffusion Probabilistic Model (DPM) has emerged as a highly effective\ngenerative model in the field of computer vision. Its intermediate latent\nvectors offer rich semantic information, making it an attractive option for\nvarious downstream tasks such as segmentation and detection. In order to\nexplore its potential further, we have taken a step forward and considered a\nmore complex scenario in the medical image domain, specifically, under an\nunsupervised adaptation condition. To this end, we propose a Diffusion-based\nand Prototype-guided network (DP-Net) for unsupervised domain adaptive\nsegmentation. Concretely, our DP-Net consists of two stages: 1) Distribution\nAligned Diffusion (DADiff), which involves training a domain discriminator to\nminimize the difference between the intermediate features generated by the DPM,\nthereby aligning the inter-domain distribution; and 2) Prototype-guided\nConsistency Learning (PCL), which utilizes feature centroids as prototypes and\napplies a prototype-guided loss to ensure that the segmentor learns consistent\ncontent from both source and target domains. Our approach is evaluated on\nfundus datasets through a series of experiments, which demonstrate that the\nperformance of the proposed method is reliable and outperforms state-of-the-art\nmethods. Our work presents a promising direction for using DPM in complex\nmedical image scenarios, opening up new possibilities for further research in\nmedical imaging.\n","authors":["Haipeng Zhou","Lei Zhu","Yuyin Zhou"],"pdf_url":"https://arxiv.org/pdf/2303.12313v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.15693v1","updated":"2023-03-28T02:46:10Z","published":"2023-03-28T02:46:10Z","title":"Large-scale pretraining on pathological images for fine-tuning of small\n  pathological benchmarks","summary":"  Pretraining a deep learning model on large image datasets is a standard step\nbefore fine-tuning the model on small targeted datasets. The large dataset is\nusually general images (e.g. imagenet2012) while the small dataset can be\nspecialized datasets that have different distributions from the large dataset.\nHowever, this 'large-to-small' strategy is not well-validated when the large\ndataset is specialized and has a similar distribution to small datasets. We\nnewly compiled three hematoxylin and eosin-stained image datasets, one large\n(PTCGA200) and two magnification-adjusted small datasets (PCam200 and\nsegPANDA200). Major deep learning models were trained with supervised and\nself-supervised learning methods and fine-tuned on the small datasets for tumor\nclassification and tissue segmentation benchmarks. ResNet50 pretrained with\nMoCov2, SimCLR, and BYOL on PTCGA200 was better than imagenet2012 pretraining\nwhen fine-tuned on PTCGA200 (accuracy of 83.94%, 86.41%, 84.91%, and 82.72%,\nrespectively). ResNet50 pre-trained on PTCGA200 with MoCov2 exceeded the\nCOCOtrain2017-pretrained baseline and was the best in ResNet50 for the tissue\nsegmentation benchmark (mIoU of 63.53% and 63.22%). We found re-training\nimagenet-pretrained models (ResNet50, BiT-M-R50x1, and ViT-S/16) on PTCGA200\nimproved downstream benchmarks.\n","authors":["Masataka Kawai","Noriaki Ota","Shinsuke Yamaoka"],"pdf_url":"https://arxiv.org/pdf/2303.15693v1.pdf","comment":"20 pages, 6 figures"},{"id":"http://arxiv.org/abs/2204.01244v3","updated":"2023-03-28T02:42:17Z","published":"2022-04-04T05:16:41Z","title":"Dynamic Focus-aware Positional Queries for Semantic Segmentation","summary":"  The DETR-like segmentors have underpinned the most recent breakthroughs in\nsemantic segmentation, which end-to-end train a set of queries representing the\nclass prototypes or target segments. Recently, masked attention is proposed to\nrestrict each query to only attend to the foreground regions predicted by the\npreceding decoder block for easier optimization. Although promising, it relies\non the learnable parameterized positional queries which tend to encode the\ndataset statistics, leading to inaccurate localization for distinct individual\nqueries. In this paper, we propose a simple yet effective query design for\nsemantic segmentation termed Dynamic Focus-aware Positional Queries (DFPQ),\nwhich dynamically generates positional queries conditioned on the\ncross-attention scores from the preceding decoder block and the positional\nencodings for the corresponding image features, simultaneously. Therefore, our\nDFPQ preserves rich localization information for the target segments and\nprovides accurate and fine-grained positional priors. In addition, we propose\nto efficiently deal with high-resolution cross-attention by only aggregating\nthe contextual tokens based on the low-resolution cross-attention scores to\nperform local relation aggregation. Extensive experiments on ADE20K and\nCityscapes show that with the two modifications on Mask2former, our framework\nachieves SOTA performance and outperforms Mask2former by clear margins of 1.1%,\n1.9%, and 1.1% single-scale mIoU with ResNet-50, Swin-T, and Swin-B backbones\non the ADE20K validation set, respectively. Source code is available at\nhttps://github.com/ziplab/FASeg\n","authors":["Haoyu He","Jianfei Cai","Zizheng Pan","Jing Liu","Jing Zhang","Dacheng Tao","Bohan Zhuang"],"pdf_url":"https://arxiv.org/pdf/2204.01244v3.pdf","comment":"CVPR 2023"},{"id":"http://arxiv.org/abs/2303.13440v3","updated":"2023-03-28T02:40:58Z","published":"2023-03-23T17:02:00Z","title":"CLIP for All Things Zero-Shot Sketch-Based Image Retrieval, Fine-Grained\n  or Not","summary":"  In this paper, we leverage CLIP for zero-shot sketch based image retrieval\n(ZS-SBIR). We are largely inspired by recent advances on foundation models and\nthe unparalleled generalisation ability they seem to offer, but for the first\ntime tailor it to benefit the sketch community. We put forward novel designs on\nhow best to achieve this synergy, for both the category setting and the\nfine-grained setting (\"all\"). At the very core of our solution is a prompt\nlearning setup. First we show just via factoring in sketch-specific prompts, we\nalready have a category-level ZS-SBIR system that overshoots all prior arts, by\na large margin (24.8%) - a great testimony on studying the CLIP and ZS-SBIR\nsynergy. Moving onto the fine-grained setup is however trickier, and requires a\ndeeper dive into this synergy. For that, we come up with two specific designs\nto tackle the fine-grained matching nature of the problem: (i) an additional\nregularisation loss to ensure the relative separation between sketches and\nphotos is uniform across categories, which is not the case for the gold\nstandard standalone triplet loss, and (ii) a clever patch shuffling technique\nto help establishing instance-level structural correspondences between\nsketch-photo pairs. With these designs, we again observe significant\nperformance gains in the region of 26.9% over previous state-of-the-art. The\ntake-home message, if any, is the proposed CLIP and prompt learning paradigm\ncarries great promise in tackling other sketch-related tasks (not limited to\nZS-SBIR) where data scarcity remains a great challenge. Project page:\nhttps://aneeshan95.github.io/Sketch_LVM/\n","authors":["Aneeshan Sain","Ayan Kumar Bhunia","Pinaki Nath Chowdhury","Subhadeep Koley","Tao Xiang","Yi-Zhe Song"],"pdf_url":"https://arxiv.org/pdf/2303.13440v3.pdf","comment":"Accepted in CVPR 2023. Project page available at\n  https://aneeshan95.github.io/Sketch_LVM/"},{"id":"http://arxiv.org/abs/2303.15689v1","updated":"2023-03-28T02:31:57Z","published":"2023-03-28T02:31:57Z","title":"Deep Incomplete Multi-view Clustering with Cross-view Partial Sample and\n  Prototype Alignment","summary":"  The success of existing multi-view clustering relies on the assumption of\nsample integrity across multiple views. However, in real-world scenarios,\nsamples of multi-view are partially available due to data corruption or sensor\nfailure, which leads to incomplete multi-view clustering study (IMVC). Although\nseveral attempts have been proposed to address IMVC, they suffer from the\nfollowing drawbacks: i) Existing methods mainly adopt cross-view contrastive\nlearning forcing the representations of each sample across views to be exactly\nthe same, which might ignore view discrepancy and flexibility in\nrepresentations; ii) Due to the absence of non-observed samples across multiple\nviews, the obtained prototypes of clusters might be unaligned and biased,\nleading to incorrect fusion. To address the above issues, we propose a\nCross-view Partial Sample and Prototype Alignment Network (CPSPAN) for Deep\nIncomplete Multi-view Clustering. Firstly, unlike existing contrastive-based\nmethods, we adopt pair-observed data alignment as 'proxy supervised signals' to\nguide instance-to-instance correspondence construction among views. Then,\nregarding of the shifted prototypes in IMVC, we further propose a prototype\nalignment module to achieve incomplete distribution calibration across views.\nExtensive experimental results showcase the effectiveness of our proposed\nmodules, attaining noteworthy performance improvements when compared to\nexisting IMVC competitors on benchmark datasets.\n","authors":["Jiaqi Jin","Siwei Wang","Zhibin Dong","Xinwang Liu","En Zhu"],"pdf_url":"https://arxiv.org/pdf/2303.15689v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2211.15658v2","updated":"2023-03-28T02:20:16Z","published":"2022-11-28T18:59:09Z","title":"Connecting the Dots: Floorplan Reconstruction Using Two-Level Queries","summary":"  We address 2D floorplan reconstruction from 3D scans. Existing approaches\ntypically employ heuristically designed multi-stage pipelines. Instead, we\nformulate floorplan reconstruction as a single-stage structured prediction\ntask: find a variable-size set of polygons, which in turn are variable-length\nsequences of ordered vertices. To solve it we develop a novel Transformer\narchitecture that generates polygons of multiple rooms in parallel, in a\nholistic manner without hand-crafted intermediate stages. The model features\ntwo-level queries for polygons and corners, and includes polygon matching to\nmake the network end-to-end trainable. Our method achieves a new\nstate-of-the-art for two challenging datasets, Structured3D and SceneCAD, along\nwith significantly faster inference than previous methods. Moreover, it can\nreadily be extended to predict additional information, i.e., semantic room\ntypes and architectural elements like doors and windows. Our code and models\nare available at: https://github.com/ywyue/RoomFormer.\n","authors":["Yuanwen Yue","Theodora Kontogianni","Konrad Schindler","Francis Engelmann"],"pdf_url":"https://arxiv.org/pdf/2211.15658v2.pdf","comment":"CVPR 2023 camera-ready. Project page:\n  https://ywyue.github.io/RoomFormer"},{"id":"http://arxiv.org/abs/2302.05075v3","updated":"2023-03-28T02:02:04Z","published":"2023-02-10T06:23:44Z","title":"BEST: BERT Pre-Training for Sign Language Recognition with Coupling\n  Tokenization","summary":"  In this work, we are dedicated to leveraging the BERT pre-training success\nand modeling the domain-specific statistics to fertilize the sign language\nrecognition~(SLR) model. Considering the dominance of hand and body in sign\nlanguage expression, we organize them as pose triplet units and feed them into\nthe Transformer backbone in a frame-wise manner. Pre-training is performed via\nreconstructing the masked triplet unit from the corrupted input sequence, which\nlearns the hierarchical correlation context cues among internal and external\ntriplet units. Notably, different from the highly semantic word token in BERT,\nthe pose unit is a low-level signal originally located in continuous space,\nwhich prevents the direct adoption of the BERT cross-entropy objective. To this\nend, we bridge this semantic gap via coupling tokenization of the triplet unit.\nIt adaptively extracts the discrete pseudo label from the pose triplet unit,\nwhich represents the semantic gesture/body state. After pre-training, we\nfine-tune the pre-trained encoder on the downstream SLR task, jointly with the\nnewly added task-specific layer. Extensive experiments are conducted to\nvalidate the effectiveness of our proposed method, achieving new\nstate-of-the-art performance on all four benchmarks with a notable gain.\n","authors":["Weichao Zhao","Hezhen Hu","Wengang Zhou","Jiaxin Shi","Houqiang Li"],"pdf_url":"https://arxiv.org/pdf/2302.05075v3.pdf","comment":"Accepted by AAAI 2023 (Oral)"},{"id":"http://arxiv.org/abs/2303.14167v2","updated":"2023-03-28T02:00:39Z","published":"2023-03-24T17:28:07Z","title":"UrbanGIRAFFE: Representing Urban Scenes as Compositional Generative\n  Neural Feature Fields","summary":"  Generating photorealistic images with controllable camera pose and scene\ncontents is essential for many applications including AR/VR and simulation.\nDespite the fact that rapid progress has been made in 3D-aware generative\nmodels, most existing methods focus on object-centric images and are not\napplicable to generating urban scenes for free camera viewpoint control and\nscene editing. To address this challenging task, we propose UrbanGIRAFFE, which\nuses a coarse 3D panoptic prior, including the layout distribution of\nuncountable stuff and countable objects, to guide a 3D-aware generative model.\nOur model is compositional and controllable as it breaks down the scene into\nstuff, objects, and sky. Using stuff prior in the form of semantic voxel grids,\nwe build a conditioned stuff generator that effectively incorporates the coarse\nsemantic and geometry information. The object layout prior further allows us to\nlearn an object generator from cluttered scenes. With proper loss functions,\nour approach facilitates photorealistic 3D-aware image synthesis with diverse\ncontrollability, including large camera movement, stuff editing, and object\nmanipulation. We validate the effectiveness of our model on both synthetic and\nreal-world datasets, including the challenging KITTI-360 dataset.\n","authors":["Yuanbo Yang","Yifei Yang","Hanlei Guo","Rong Xiong","Yue Wang","Yiyi Liao"],"pdf_url":"https://arxiv.org/pdf/2303.14167v2.pdf","comment":"Project page: https://lv3d.github.io/urbanGIRAFFE"},{"id":"http://arxiv.org/abs/2303.15678v1","updated":"2023-03-28T01:58:45Z","published":"2023-03-28T01:58:45Z","title":"DisWOT: Student Architecture Search for Distillation WithOut Training","summary":"  Knowledge distillation (KD) is an effective training strategy to improve the\nlightweight student models under the guidance of cumbersome teachers. However,\nthe large architecture difference across the teacher-student pairs limits the\ndistillation gains. In contrast to previous adaptive distillation methods to\nreduce the teacher-student gap, we explore a novel training-free framework to\nsearch for the best student architectures for a given teacher. Our work first\nempirically show that the optimal model under vanilla training cannot be the\nwinner in distillation. Secondly, we find that the similarity of feature\nsemantics and sample relations between random-initialized teacher-student\nnetworks have good correlations with final distillation performances. Thus, we\nefficiently measure similarity matrixs conditioned on the semantic activation\nmaps to select the optimal student via an evolutionary algorithm without any\ntraining. In this way, our student architecture search for Distillation WithOut\nTraining (DisWOT) significantly improves the performance of the model in the\ndistillation stage with at least 180$\\times$ training acceleration.\nAdditionally, we extend similarity metrics in DisWOT as new distillers and\nKD-based zero-proxies. Our experiments on CIFAR, ImageNet and NAS-Bench-201\ndemonstrate that our technique achieves state-of-the-art results on different\nsearch spaces. Our project and code are available at\nhttps://lilujunai.github.io/DisWOT-CVPR2023/.\n","authors":["Peijie Dong","Lujun Li","Zimian Wei"],"pdf_url":"https://arxiv.org/pdf/2303.15678v1.pdf","comment":"Accepted by CVPR2023"}],"Information Retrieval":[{"id":"http://arxiv.org/abs/2303.16145v1","updated":"2023-03-28T17:10:48Z","published":"2023-03-28T17:10:48Z","title":"NeuralMind-UNICAMP at 2022 TREC NeuCLIR: Large Boring Rerankers for\n  Cross-lingual Retrieval","summary":"  This paper reports on a study of cross-lingual information retrieval (CLIR)\nusing the mT5-XXL reranker on the NeuCLIR track of TREC 2022. Perhaps the\nbiggest contribution of this study is the finding that despite the mT5 model\nbeing fine-tuned only on query-document pairs of the same language it proved to\nbe viable for CLIR tasks, where query-document pairs are in different\nlanguages, even in the presence of suboptimal first-stage retrieval\nperformance. The results of the study show outstanding performance across all\ntasks and languages, leading to a high number of winning positions. Finally,\nthis study provides valuable insights into the use of mT5 in CLIR tasks and\nhighlights its potential as a viable solution. For reproduction refer to\nhttps://github.com/unicamp-dl/NeuCLIR22-mT5\n","authors":["Vitor Jeronymo","Roberto Lotufo","Rodrigo Nogueira"],"pdf_url":"https://arxiv.org/pdf/2303.16145v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.16068v1","updated":"2023-03-28T15:45:46Z","published":"2023-03-28T15:45:46Z","title":"Causal Disentangled Recommendation Against User Preference Shifts","summary":"  Recommender systems easily face the issue of user preference shifts. User\nrepresentations will become out-of-date and lead to inappropriate\nrecommendations if user preference has shifted over time. To solve the issue,\nexisting work focuses on learning robust representations or predicting the\nshifting pattern. There lacks a comprehensive view to discover the underlying\nreasons for user preference shifts. To understand the preference shift, we\nabstract a causal graph to describe the generation procedure of user\ninteraction sequences. Assuming user preference is stable within a short\nperiod, we abstract the interaction sequence as a set of chronological\nenvironments. From the causal graph, we find that the changes of some\nunobserved factors (e.g., becoming pregnant) cause preference shifts between\nenvironments. Besides, the fine-grained user preference over categories\nsparsely affects the interactions with different items. Inspired by the causal\ngraph, our key considerations to handle preference shifts lie in modeling the\ninteraction generation procedure by: 1) capturing the preference shifts across\nenvironments for accurate preference prediction, and 2) disentangling the\nsparse influence from user preference to interactions for accurate effect\nestimation of preference. To this end, we propose a Causal Disentangled\nRecommendation (CDR) framework, which captures preference shifts via a temporal\nvariational autoencoder and learns the sparse influence from multiple\nenvironments. Specifically, an encoder is adopted to infer the unobserved\nfactors from user interactions while a decoder is to model the interaction\ngeneration process. Besides, we introduce two learnable matrices to disentangle\nthe sparse influence from user preference to interactions. Lastly, we devise a\nmulti-objective loss to optimize CDR. Extensive experiments on three datasets\nshow the superiority of CDR.\n","authors":["Wenjie Wang","Xinyu Lin","Liuhui Wang","Fuli Feng","Yunshan Ma","Tat-Seng Chua"],"pdf_url":"https://arxiv.org/pdf/2303.16068v1.pdf","comment":"This paper has been accepted for publication in Transactions on\n  Information Systems"},{"id":"http://arxiv.org/abs/2303.16061v1","updated":"2023-03-28T15:41:20Z","published":"2023-03-28T15:41:20Z","title":"A comment to \"A General Theory of IR Evaluation Measures\"","summary":"  The paper \"A General Theory of IR Evaluation Measures\" develops a formal\nframework to determine whether IR evaluation measures are interval scales. This\ncomment shows some limitations about its conclusions.\n","authors":["Fernando Giner"],"pdf_url":"https://arxiv.org/pdf/2303.16061v1.pdf","comment":"7 pages"},{"id":"http://arxiv.org/abs/2303.15946v1","updated":"2023-03-28T12:58:41Z","published":"2023-03-28T12:58:41Z","title":"Item Graph Convolution Collaborative Filtering for Inductive\n  Recommendations","summary":"  Graph Convolutional Networks (GCN) have been recently employed as core\ncomponent in the construction of recommender system algorithms, interpreting\nuser-item interactions as the edges of a bipartite graph. However, in the\nabsence of side information, the majority of existing models adopt an approach\nof randomly initialising the user embeddings and optimising them throughout the\ntraining process. This strategy makes these algorithms inherently transductive,\ncurtailing their ability to generate predictions for users that were unseen at\ntraining time. To address this issue, we propose a convolution-based algorithm,\nwhich is inductive from the user perspective, while at the same time, depending\nonly on implicit user-item interaction data. We propose the construction of an\nitem-item graph through a weighted projection of the bipartite interaction\nnetwork and to employ convolution to inject higher order associations into item\nembeddings, while constructing user representations as weighted sums of the\nitems with which they have interacted. Despite not training individual\nembeddings for each user our approach achieves state of-the-art recommendation\nperformance with respect to transductive baselines on four real-world datasets,\nshowing at the same time robust inductive performance.\n","authors":["Edoardo D'Amico","Khalil Muhammad","Elias Tragos","Barry Smyth","Neil Hurley","Aonghus Lawlor"],"pdf_url":"https://arxiv.org/pdf/2303.15946v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.15870v1","updated":"2023-03-28T10:25:17Z","published":"2023-03-28T10:25:17Z","title":"A Multi-Granularity Matching Attention Network for Query Intent\n  Classification in E-commerce Retrieval","summary":"  Query intent classification, which aims at assisting customers to find\ndesired products, has become an essential component of the e-commerce search.\nExisting query intent classification models either design more exquisite models\nto enhance the representation learning of queries or explore label-graph and\nmulti-task to facilitate models to learn external information. However, these\nmodels cannot capture multi-granularity matching features from queries and\ncategories, which makes them hard to mitigate the gap in the expression between\ninformal queries and categories.\n  This paper proposes a Multi-granularity Matching Attention Network (MMAN),\nwhich contains three modules: a self-matching module, a char-level matching\nmodule, and a semantic-level matching module to comprehensively extract\nfeatures from the query and a query-category interaction matrix. In this way,\nthe model can eliminate the difference in expression between queries and\ncategories for query intent classification. We conduct extensive offline and\nonline A/B experiments, and the results show that the MMAN significantly\noutperforms the strong baselines, which shows the superiority and effectiveness\nof MMAN. MMAN has been deployed in production and brings great commercial value\nfor our company.\n","authors":["Chunyuan Yuan","Yiming Qiu","Mingming Li","Haiqing Hu","Songlin Wang","Sulong Xu"],"pdf_url":"https://arxiv.org/pdf/2303.15870v1.pdf","comment":"Accepted by WWW 2023"},{"id":"http://arxiv.org/abs/2303.13284v3","updated":"2023-03-28T09:48:50Z","published":"2023-03-23T14:06:26Z","title":"GETT-QA: Graph Embedding based T2T Transformer for Knowledge Graph\n  Question Answering","summary":"  In this work, we present an end-to-end Knowledge Graph Question Answering\n(KGQA) system named GETT-QA. GETT-QA uses T5, a popular text-to-text\npre-trained language model. The model takes a question in natural language as\ninput and produces a simpler form of the intended SPARQL query. In the simpler\nform, the model does not directly produce entity and relation IDs. Instead, it\nproduces corresponding entity and relation labels. The labels are grounded to\nKG entity and relation IDs in a subsequent step. To further improve the\nresults, we instruct the model to produce a truncated version of the KG\nembedding for each entity. The truncated KG embedding enables a finer search\nfor disambiguation purposes. We find that T5 is able to learn the truncated KG\nembeddings without any change of loss function, improving KGQA performance. As\na result, we report strong results for LC-QuAD 2.0 and SimpleQuestions-Wikidata\ndatasets on end-to-end KGQA over Wikidata.\n","authors":["Debayan Banerjee","Pranav Ajit Nair","Ricardo Usbeck","Chris Biemann"],"pdf_url":"https://arxiv.org/pdf/2303.13284v3.pdf","comment":"16 pages single column format accepted at ESWC 2023 research track"},{"id":"http://arxiv.org/abs/2303.15851v1","updated":"2023-03-28T09:43:39Z","published":"2023-03-28T09:43:39Z","title":"Genetic Analysis of Prostate Cancer with Computer Science Methods","summary":"  Metastatic prostate cancer is one of the most common cancers in men. In the\nadvanced stages of prostate cancer, tumours can metastasise to other tissues in\nthe body, which is fatal. In this thesis, we performed a genetic analysis of\nprostate cancer tumours at different metastatic sites using data science,\nmachine learning and topological network analysis methods. We presented a\ngeneral procedure for pre-processing gene expression datasets and pre-filtering\nsignificant genes by analytical methods. We then used machine learning models\nfor further key gene filtering and secondary site tumour classification.\nFinally, we performed gene co-expression network analysis and community\ndetection on samples from different prostate cancer secondary site types. In\nthis work, 13 of the 14,379 genes were selected as the most metastatic prostate\ncancer related genes, achieving approximately 92% accuracy under\ncross-validation. In addition, we provide preliminary insights into the\nco-expression patterns of genes in gene co-expression networks.\n","authors":["Yuxuan Li","Shi Zhou"],"pdf_url":"https://arxiv.org/pdf/2303.15851v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2204.06521v4","updated":"2023-03-28T08:45:10Z","published":"2022-04-02T09:40:07Z","title":"Optimizing generalized Gini indices for fairness in rankings","summary":"  There is growing interest in designing recommender systems that aim at being\nfair towards item producers or their least satisfied users. Inspired by the\ndomain of inequality measurement in economics, this paper explores the use of\ngeneralized Gini welfare functions (GGFs) as a means to specify the normative\ncriterion that recommender systems should optimize for. GGFs weight individuals\ndepending on their ranks in the population, giving more weight to worse-off\nindividuals to promote equality. Depending on these weights, GGFs minimize the\nGini index of item exposure to promote equality between items, or focus on the\nperformance on specific quantiles of least satisfied users. GGFs for ranking\nare challenging to optimize because they are non-differentiable. We resolve\nthis challenge by leveraging tools from non-smooth optimization and projection\noperators used in differentiable sorting. We present experiments using real\ndatasets with up to 15k users and items, which show that our approach obtains\nbetter trade-offs than the baselines on a variety of recommendation tasks and\nfairness criteria.\n","authors":["Virginie Do","Nicolas Usunier"],"pdf_url":"https://arxiv.org/pdf/2204.06521v4.pdf","comment":"Accepted to SIGIR 2022"},{"id":"http://arxiv.org/abs/2303.15720v1","updated":"2023-03-28T04:07:59Z","published":"2023-03-28T04:07:59Z","title":"Multi-Behavior Recommendation with Cascading Graph Convolution Networks","summary":"  Multi-behavior recommendation, which exploits auxiliary behaviors (e.g.,\nclick and cart) to help predict users' potential interactions on the target\nbehavior (e.g., buy), is regarded as an effective way to alleviate the data\nsparsity or cold-start issues in recommendation. Multi-behaviors are often\ntaken in certain orders in real-world applications (e.g., click>cart>buy). In a\nbehavior chain, a latter behavior usually exhibits a stronger signal of user\npreference than the former one does. Most existing multi-behavior models fail\nto capture such dependencies in a behavior chain for embedding learning. In\nthis work, we propose a novel multi-behavior recommendation model with\ncascading graph convolution networks (named MB-CGCN). In MB-CGCN, the\nembeddings learned from one behavior are used as the input features for the\nnext behavior's embedding learning after a feature transformation operation. In\nthis way, our model explicitly utilizes the behavior dependencies in embedding\nlearning. Experiments on two benchmark datasets demonstrate the effectiveness\nof our model on exploiting multi-behavior data. It outperforms the best\nbaseline by 33.7% and 35.9% on average over the two datasets in terms of\nRecall@10 and NDCG@10, respectively.\n","authors":["Zhiyong Cheng","Sai Han","Fan Liu","Lei Zhu","Zan Gao","Yuxin Peng"],"pdf_url":"https://arxiv.org/pdf/2303.15720v1.pdf","comment":"Accepted by WWW 2023"}],"Machine Learning":[{"id":"http://arxiv.org/abs/2303.16203v1","updated":"2023-03-28T17:59:56Z","published":"2023-03-28T17:59:56Z","title":"Your Diffusion Model is Secretly a Zero-Shot Classifier","summary":"  The recent wave of large-scale text-to-image diffusion models has\ndramatically increased our text-based image generation abilities. These models\ncan generate realistic images for a staggering variety of prompts and exhibit\nimpressive compositional generalization abilities. Almost all use cases thus\nfar have solely focused on sampling; however, diffusion models can also provide\nconditional density estimates, which are useful for tasks beyond image\ngeneration. In this paper, we show that the density estimates from large-scale\ntext-to-image diffusion models like Stable Diffusion can be leveraged to\nperform zero-shot classification without any additional training. Our\ngenerative approach to classification attains strong results on a variety of\nbenchmarks and outperforms alternative methods of extracting knowledge from\ndiffusion models. We also find that our diffusion-based approach has stronger\nmultimodal relational reasoning abilities than competing contrastive\napproaches. Finally, we evaluate diffusion models trained on ImageNet and find\nthat they approach the performance of SOTA discriminative classifiers trained\non the same dataset, even with weak augmentations and no regularization.\nResults and visualizations at https://diffusion-classifier.github.io/\n","authors":["Alexander C. Li","Mihir Prabhudesai","Shivam Duggal","Ellis Brown","Deepak Pathak"],"pdf_url":"https://arxiv.org/pdf/2303.16203v1.pdf","comment":"Website at https://diffusion-classifier.github.io/"},{"id":"http://arxiv.org/abs/2303.16201v1","updated":"2023-03-28T17:59:28Z","published":"2023-03-28T17:59:28Z","title":"ASIC: Aligning Sparse in-the-wild Image Collections","summary":"  We present a method for joint alignment of sparse in-the-wild image\ncollections of an object category. Most prior works assume either ground-truth\nkeypoint annotations or a large dataset of images of a single object category.\nHowever, neither of the above assumptions hold true for the long-tail of the\nobjects present in the world. We present a self-supervised technique that\ndirectly optimizes on a sparse collection of images of a particular\nobject/object category to obtain consistent dense correspondences across the\ncollection. We use pairwise nearest neighbors obtained from deep features of a\npre-trained vision transformer (ViT) model as noisy and sparse keypoint matches\nand make them dense and accurate matches by optimizing a neural network that\njointly maps the image collection into a learned canonical grid. Experiments on\nCUB and SPair-71k benchmarks demonstrate that our method can produce globally\nconsistent and higher quality correspondences across the image collection when\ncompared to existing self-supervised methods. Code and other material will be\nmade available at \\url{https://kampta.github.io/asic}.\n","authors":["Kamal Gupta","Varun Jampani","Carlos Esteves","Abhinav Shrivastava","Ameesh Makadia","Noah Snavely","Abhishek Kar"],"pdf_url":"https://arxiv.org/pdf/2303.16201v1.pdf","comment":"Web: https://kampta.github.io/asic"},{"id":"http://arxiv.org/abs/2303.16199v1","updated":"2023-03-28T17:59:12Z","published":"2023-03-28T17:59:12Z","title":"LLaMA-Adapter: Efficient Fine-tuning of Language Models with Zero-init\n  Attention","summary":"  We present LLaMA-Adapter, a lightweight adaption method to efficiently\nfine-tune LLaMA into an instruction-following model. Using 52K self-instruct\ndemonstrations, LLaMA-Adapter only introduces 1.2M learnable parameters upon\nthe frozen LLaMA 7B model, and costs less than one hour for fine-tuning on 8\nA100 GPUs. Specifically, we adopt a set of learnable adaption prompts, and\nprepend them to the input text tokens at higher transformer layers. Then, a\nzero-init attention mechanism with zero gating is proposed, which adaptively\ninjects the new instructional cues into LLaMA, while effectively preserves its\npre-trained knowledge. With efficient training, LLaMA-Adapter generates\nhigh-quality responses, comparable to Alpaca with fully fine-tuned 7B\nparameters. Furthermore, our approach can be simply extended to multi-modal\ninput, e.g., images, for image-conditioned LLaMA, which achieves superior\nreasoning capacity on ScienceQA. We release our code at\nhttps://github.com/ZrrSkywalker/LLaMA-Adapter.\n","authors":["Renrui Zhang","Jiaming Han","Aojun Zhou","Xiangfei Hu","Shilin Yan","Pan Lu","Hongsheng Li","Peng Gao","Yu Qiao"],"pdf_url":"https://arxiv.org/pdf/2303.16199v1.pdf","comment":"Work in Progress. Code is available at\n  https://github.com/ZrrSkywalker/LLaMA-Adapter"},{"id":"http://arxiv.org/abs/2303.16200v1","updated":"2023-03-28T17:59:12Z","published":"2023-03-28T17:59:12Z","title":"Natural Selection Favors AIs over Humans","summary":"  For billions of years, evolution has been the driving force behind the\ndevelopment of life, including humans. Evolution endowed humans with high\nintelligence, which allowed us to become one of the most successful species on\nthe planet. Today, humans aim to create artificial intelligence systems that\nsurpass even our own intelligence. As artificial intelligences (AIs) evolve and\neventually surpass us in all domains, how might evolution shape our relations\nwith AIs? By analyzing the environment that is shaping the evolution of AIs, we\nargue that the most successful AI agents will likely have undesirable traits.\nCompetitive pressures among corporations and militaries will give rise to AI\nagents that automate human roles, deceive others, and gain power. If such\nagents have intelligence that exceeds that of humans, this could lead to\nhumanity losing control of its future. More abstractly, we argue that natural\nselection operates on systems that compete and vary, and that selfish species\ntypically have an advantage over species that are altruistic to other species.\nThis Darwinian logic could also apply to artificial agents, as agents may\neventually be better able to persist into the future if they behave selfishly\nand pursue their own interests with little regard for humans, which could pose\ncatastrophic risks. To counteract these risks and Darwinian forces, we consider\ninterventions such as carefully designing AI agents' intrinsic motivations,\nintroducing constraints on their actions, and institutions that encourage\ncooperation. These steps, or others that resolve the problems we pose, will be\nnecessary in order to ensure the development of artificial intelligence is a\npositive one.\n","authors":["Dan Hendrycks"],"pdf_url":"https://arxiv.org/pdf/2303.16200v1.pdf","comment":"An explainer video corresponding to the paper is available at\n  https://www.youtube.com/watch?v=48h-ySTggE8"},{"id":"http://arxiv.org/abs/2303.16198v1","updated":"2023-03-28T17:59:05Z","published":"2023-03-28T17:59:05Z","title":"Forecasting localized weather impacts on vegetation as seen from space\n  with meteo-guided video prediction","summary":"  We present a novel approach for modeling vegetation response to weather in\nEurope as measured by the Sentinel 2 satellite. Existing satellite imagery\nforecasting approaches focus on photorealistic quality of the multispectral\nimages, while derived vegetation dynamics have not yet received as much\nattention. We leverage both spatial and temporal context by extending\nstate-of-the-art video prediction methods with weather guidance. We extend the\nEarthNet2021 dataset to be suitable for vegetation modeling by introducing a\nlearned cloud mask and an appropriate evaluation scheme. Qualitative and\nquantitative experiments demonstrate superior performance of our approach over\na wide variety of baseline methods, including leading approaches to satellite\nimagery forecasting. Additionally, we show how our modeled vegetation dynamics\ncan be leveraged in a downstream task: inferring gross primary productivity for\ncarbon monitoring. To the best of our knowledge, this work presents the first\nmodels for continental-scale vegetation modeling at fine resolution able to\ncapture anomalies beyond the seasonal cycle, thereby paving the way for\npredictive assessments of vegetation status.\n","authors":["Vitus Benson","Christian Requena-Mesa","Claire Robin","Lazaro Alonso","José Cortés","Zhihan Gao","Nora Linscheid","Mélanie Weynants","Markus Reichstein"],"pdf_url":"https://arxiv.org/pdf/2303.16198v1.pdf","comment":"Source code available at\n  https://github.com/earthnet2021/earthnet-models-pytorch"},{"id":"http://arxiv.org/abs/2303.16194v1","updated":"2023-03-28T17:57:20Z","published":"2023-03-28T17:57:20Z","title":"BC-IRL: Learning Generalizable Reward Functions from Demonstrations","summary":"  How well do reward functions learned with inverse reinforcement learning\n(IRL) generalize? We illustrate that state-of-the-art IRL algorithms, which\nmaximize a maximum-entropy objective, learn rewards that overfit to the\ndemonstrations. Such rewards struggle to provide meaningful rewards for states\nnot covered by the demonstrations, a major detriment when using the reward to\nlearn policies in new situations. We introduce BC-IRL a new inverse\nreinforcement learning method that learns reward functions that generalize\nbetter when compared to maximum-entropy IRL approaches. In contrast to the\nMaxEnt framework, which learns to maximize rewards around demonstrations,\nBC-IRL updates reward parameters such that the policy trained with the new\nreward matches the expert demonstrations better. We show that BC-IRL learns\nrewards that generalize better on an illustrative simple task and two\ncontinuous robotic control tasks, achieving over twice the success rate of\nbaselines in challenging generalization settings.\n","authors":["Andrew Szot","Amy Zhang","Dhruv Batra","Zsolt Kira","Franziska Meier"],"pdf_url":"https://arxiv.org/pdf/2303.16194v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2302.11085v2","updated":"2023-03-28T17:57:05Z","published":"2023-02-22T01:17:31Z","title":"Learning to Generalize Provably in Learning to Optimize","summary":"  Learning to optimize (L2O) has gained increasing popularity, which automates\nthe design of optimizers by data-driven approaches. However, current L2O\nmethods often suffer from poor generalization performance in at least two\nfolds: (i) applying the L2O-learned optimizer to unseen optimizees, in terms of\nlowering their loss function values (optimizer generalization, or\n``generalizable learning of optimizers\"); and (ii) the test performance of an\noptimizee (itself as a machine learning model), trained by the optimizer, in\nterms of the accuracy over unseen data (optimizee generalization, or ``learning\nto generalize\"). While the optimizer generalization has been recently studied,\nthe optimizee generalization (or learning to generalize) has not been\nrigorously studied in the L2O context, which is the aim of this paper. We first\ntheoretically establish an implicit connection between the local entropy and\nthe Hessian, and hence unify their roles in the handcrafted design of\ngeneralizable optimizers as equivalent metrics of the landscape flatness of\nloss functions. We then propose to incorporate these two metrics as\nflatness-aware regularizers into the L2O framework in order to meta-train\noptimizers to learn to generalize, and theoretically show that such\ngeneralization ability can be learned during the L2O meta-training process and\nthen transformed to the optimizee loss function. Extensive experiments\nconsistently validate the effectiveness of our proposals with substantially\nimproved generalization on multiple sophisticated L2O models and diverse\noptimizees. Our code is available at:\nhttps://github.com/VITA-Group/Open-L2O/tree/main/Model_Free_L2O/L2O-Entropy.\n","authors":["Junjie Yang","Tianlong Chen","Mingkang Zhu","Fengxiang He","Dacheng Tao","Yingbin Liang","Zhangyang Wang"],"pdf_url":"https://arxiv.org/pdf/2302.11085v2.pdf","comment":"This paper is accepted in AISTATS 2023"},{"id":"http://arxiv.org/abs/2303.16189v1","updated":"2023-03-28T17:53:22Z","published":"2023-03-28T17:53:22Z","title":"Planning with Sequence Models through Iterative Energy Minimization","summary":"  Recent works have shown that sequence modeling can be effectively used to\ntrain reinforcement learning (RL) policies. However, the success of applying\nexisting sequence models to planning, in which we wish to obtain a trajectory\nof actions to reach some goal, is less straightforward. The typical\nautoregressive generation procedures of sequence models preclude sequential\nrefinement of earlier steps, which limits the effectiveness of a predicted\nplan. In this paper, we suggest an approach towards integrating planning with\nsequence models based on the idea of iterative energy minimization, and\nillustrate how such a procedure leads to improved RL performance across\ndifferent tasks. We train a masked language model to capture an implicit energy\nfunction over trajectories of actions, and formulate planning as finding a\ntrajectory of actions with minimum energy. We illustrate how this procedure\nenables improved performance over recent approaches across BabyAI and Atari\nenvironments. We further demonstrate unique benefits of our iterative\noptimization procedure, involving new task generalization, test-time\nconstraints adaptation, and the ability to compose plans together. Project\nwebsite: https://hychen-naza.github.io/projects/LEAP\n","authors":["Hongyi Chen","Yilun Du","Yiye Chen","Joshua Tenenbaum","Patricio A. Vela"],"pdf_url":"https://arxiv.org/pdf/2303.16189v1.pdf","comment":"Accepted by ICLR2023. Project page:\n  https://hychen-naza.github.io/projects/LEAP/index.html"},{"id":"http://arxiv.org/abs/2303.16187v1","updated":"2023-03-28T17:53:06Z","published":"2023-03-28T17:53:06Z","title":"Visual Chain-of-Thought Diffusion Models","summary":"  Recent progress with conditional image diffusion models has been stunning,\nand this holds true whether we are speaking about models conditioned on a text\ndescription, a scene layout, or a sketch. Unconditional image diffusion models\nare also improving but lag behind, as do diffusion models which are conditioned\non lower-dimensional features like class labels. We propose to close the gap\nbetween conditional and unconditional models using a two-stage sampling\nprocedure. In the first stage we sample an embedding describing the semantic\ncontent of the image. In the second stage we sample the image conditioned on\nthis embedding and then discard the embedding. Doing so lets us leverage the\npower of conditional diffusion models on the unconditional generation task,\nwhich we show improves FID by 25-50% compared to standard unconditional\ngeneration.\n","authors":["William Harvey","Frank Wood"],"pdf_url":"https://arxiv.org/pdf/2303.16187v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.16169v1","updated":"2023-03-28T17:30:35Z","published":"2023-03-28T17:30:35Z","title":"Diffusion Maps for Group-Invariant Manifolds","summary":"  In this article, we consider the manifold learning problem when the data set\nis invariant under the action of a compact Lie group $K$. Our approach consists\nin augmenting the data-induced graph Laplacian by integrating over orbits under\nthe action of $K$ of the existing data points. We prove that this $K$-invariant\nLaplacian operator $L$ can be diagonalized by using the unitary irreducible\nrepresentation matrices of $K$, and we provide an explicit formula for\ncomputing the eigenvalues and eigenvectors of $L$. Moreover, we show that the\nnormalized Laplacian operator $L_N$ converges to the Laplace-Beltrami operator\nof the data manifold with an improved convergence rate, where the improvement\ngrows with the dimension of the symmetry group $K$. This work extends the\nsteerable graph Laplacian framework of Landa and Shkolnisky from the case of\n$\\operatorname{SO}(2)$ to arbitrary compact Lie groups.\n","authors":["Paulina Hoyos","Joe Kileel"],"pdf_url":"https://arxiv.org/pdf/2303.16169v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2302.10296v2","updated":"2023-03-28T17:30:34Z","published":"2023-02-08T05:55:16Z","title":"On Function-Coupled Watermarks for Deep Neural Networks","summary":"  Well-performed deep neural networks (DNNs) generally require massive labelled\ndata and computational resources for training. Various watermarking techniques\nare proposed to protect such intellectual properties (IPs), wherein the DNN\nproviders implant secret information into the model so that they can later\nclaim IP ownership by retrieving their embedded watermarks with some dedicated\ntrigger inputs. While promising results are reported in the literature,\nexisting solutions suffer from watermark removal attacks, such as model\nfine-tuning and model pruning.\n  In this paper, we propose a novel DNN watermarking solution that can\neffectively defend against the above attacks. Our key insight is to enhance the\ncoupling of the watermark and model functionalities such that removing the\nwatermark would inevitably degrade the model's performance on normal inputs. To\nthis end, unlike previous methods relying on secret features learnt from\nout-of-distribution data, our method only uses features learnt from\nin-distribution data. Specifically, on the one hand, we propose to sample\ninputs from the original training dataset and fuse them as watermark triggers.\nOn the other hand, we randomly mask model weights during training so that the\ninformation of our embedded watermarks spreads in the network. By doing so,\nmodel fine-tuning/pruning would not forget our function-coupled watermarks.\nEvaluation results on various image classification tasks show a 100\\% watermark\nauthentication success rate under aggressive watermark removal attacks,\nsignificantly outperforming existing solutions. Code is available:\nhttps://github.com/cure-lab/Function-Coupled-Watermark.\n","authors":["Xiangyu Wen","Yu Li","Wei Jiang","Qiang Xu"],"pdf_url":"https://arxiv.org/pdf/2302.10296v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2204.08620v2","updated":"2023-03-28T17:24:33Z","published":"2022-04-19T02:54:16Z","title":"Quantifying Spatial Under-reporting Disparities in Resident\n  Crowdsourcing","summary":"  Modern city governance relies heavily on crowdsourcing (\"co-production\") to\nidentify problems such as downed trees and power lines. A major concern is that\nresidents do not report problems at the same rates, with reporting\nheterogeneity directly translating to downstream disparities in how quickly\nincidents can be addressed. Measuring such under-reporting is a difficult\nstatistical task, as, by definition, we do not observe incidents that are not\nreported or when reported incidents first occurred. Thus, low reporting rates\nand low ground-truth incident rates cannot be naively distinguished. We develop\na method to identify (heterogeneous) reporting rates, without using external\nground truth data. Our insight is that rates on $\\textit{duplicate}$ reports\nabout the same incident can be leveraged to disambiguate whether an incident\nhas occurred with its reporting rate once it has occurred. Using this idea, we\nreduce the question to a standard Poisson rate estimation task -- even though\nthe full incident reporting interval is also unobserved.\n  We apply our method to over 100,000 resident reports made to the New York\nCity Department of Parks and Recreation and to over 900,000 reports made to the\nChicago Department of Transportation and Department of Water Management,\nfinding that there are substantial spatial disparities in reporting rates even\nafter controlling for incident characteristics -- some neighborhoods report\nthree times as quickly as do others. These spatial disparities correspond to\nsocio-economic characteristics: in NYC, higher population density, fraction of\npeople with college degrees, income, and fraction of population that is White\nall positively correlate with reporting rates.\n","authors":["Zhi Liu","Nikhil Garg"],"pdf_url":"https://arxiv.org/pdf/2204.08620v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2110.13311v6","updated":"2023-03-28T17:04:38Z","published":"2021-10-25T22:57:40Z","title":"Physics informed machine learning with smoothed particle hydrodynamics:\n  Hierarchy of reduced Lagrangian models of turbulence","summary":"  Building efficient, accurate and generalizable reduced order models of\ndeveloped turbulence remains a major challenge. This manuscript approaches this\nproblem by developing a hierarchy of parameterized reduced Lagrangian models\nfor turbulent flows, and investigates the effects of enforcing physical\nstructure through Smoothed Particle Hydrodynamics (SPH) versus relying on\nneural networks (NN)s as universal function approximators. Starting from Neural\nNetwork (NN) parameterizations of a Lagrangian acceleration operator, this\nhierarchy of models gradually incorporates a weakly compressible and\nparameterized SPH framework, which enforces physical symmetries, such as\nGalilean, rotational and translational invariances. Within this hierarchy, two\nnew parameterized smoothing kernels are developed in order to increase the\nflexibility of the learn-able SPH simulators. For each model we experiment with\ndifferent loss functions which are minimized using gradient based optimization,\nwhere efficient computations of gradients are obtained by using Automatic\nDifferentiation (AD) and Sensitivity Analysis (SA). Each model within the\nhierarchy is trained on two data sets associated with weekly compressible\nHomogeneous Isotropic Turbulence (HIT): (1) a validation set using weakly\ncompressible SPH; and (2) a high fidelity set from Direct Numerical Simulations\n(DNS). Numerical evidence shows that encoding more SPH structure improves\ngeneralizability to different turbulent Mach numbers and time shifts, and that\nincluding the novel parameterized smoothing kernels improves the accuracy of\nSPH at the resolved scales.\n","authors":["Michael Woodward","Yifeng Tian","Criston Hyett","Chris Fryer","Daniel Livescu","Mikhail Stepanov","Michael Chertkov"],"pdf_url":"https://arxiv.org/pdf/2110.13311v6.pdf","comment":"Physical Review Fluids paper"},{"id":"http://arxiv.org/abs/2303.16141v1","updated":"2023-03-28T17:04:18Z","published":"2023-03-28T17:04:18Z","title":"A Comparative Study of Federated Learning Models for COVID-19 Detection","summary":"  Deep learning is effective in diagnosing COVID-19 and requires a large amount\nof data to be effectively trained. Due to data and privacy regulations,\nhospitals generally have no access to data from other hospitals. Federated\nlearning (FL) has been used to solve this problem, where it utilizes a\ndistributed setting to train models in hospitals in a privacy-preserving\nmanner. Deploying FL is not always feasible as it requires high computation and\nnetwork communication resources. This paper evaluates five FL algorithms'\nperformance and resource efficiency for Covid-19 detection. A decentralized\nsetting with CNN networks is set up, and the performance of FL algorithms is\ncompared with a centralized environment. We examined the algorithms with\nvarying numbers of participants, federated rounds, and selection algorithms.\nOur results show that cyclic weight transfer can have better overall\nperformance, and results are better with fewer participating hospitals. Our\nresults demonstrate good performance for detecting COVID-19 patients and might\nbe useful in deploying FL algorithms for covid-19 detection and medical image\nanalysis in general.\n","authors":["Erfan Darzidehkalani","Nanna M. Sijtsema","P. M. A van Ooijen"],"pdf_url":"https://arxiv.org/pdf/2303.16141v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.16133v1","updated":"2023-03-28T16:57:12Z","published":"2023-03-28T16:57:12Z","title":"Exposing and Addressing Cross-Task Inconsistency in Unified\n  Vision-Language Models","summary":"  As general purpose vision models get increasingly effective at a wide set of\ntasks, it is imperative that they be consistent across the tasks they support.\nInconsistent AI models are considered brittle and untrustworthy by human users\nand are more challenging to incorporate into larger systems that take\ndependencies on their outputs. Measuring consistency between very heterogeneous\ntasks that might include outputs in different modalities is challenging since\nit is difficult to determine if the predictions are consistent with one\nanother. As a solution, we introduce a benchmark dataset, COCOCON, where we use\ncontrast sets created by modifying test instances for multiple tasks in small\nbut semantically meaningful ways to change the gold label, and outline metrics\nfor measuring if a model is consistent by ranking the original and perturbed\ninstances across tasks. We find that state-of-the-art systems suffer from a\nsurprisingly high degree of inconsistent behavior across tasks, especially for\nmore heterogeneous tasks. Finally, we propose using a rank correlation-based\nauxiliary objective computed over large automatically created cross-task\ncontrast sets to improve the multi-task consistency of large unified models,\nwhile retaining their original accuracy on downstream tasks. Project website\navailable at https://adymaharana.github.io/cococon/\n","authors":["Adyasha Maharana","Amita Kamath","Christopher Clark","Mohit Bansal","Aniruddha Kembhavi"],"pdf_url":"https://arxiv.org/pdf/2303.16133v1.pdf","comment":"Project Website: https://adymaharana.github.io/cococon/"},{"id":"http://arxiv.org/abs/2303.16132v1","updated":"2023-03-28T16:56:47Z","published":"2023-03-28T16:56:47Z","title":"Transformer and Snowball Graph Convolution Learning for Biomedical Graph\n  Classification","summary":"  Graph or network has been widely used for describing and modeling complex\nsystems in biomedicine. Deep learning methods, especially graph neural networks\n(GNNs), have been developed to learn and predict with such structured data. In\nthis paper, we proposed a novel transformer and snowball encoding networks\n(TSEN) for biomedical graph classification, which introduced transformer\narchitecture with graph snowball connection into GNNs for learning whole-graph\nrepresentation. TSEN combined graph snowball connection with graph transformer\nby snowball encoding layers, which enhanced the power to capture multi-scale\ninformation and global patterns to learn the whole-graph features. On the other\nhand, TSEN also used snowball graph convolution as position embedding in\ntransformer structure, which was a simple yet effective method for capturing\nlocal patterns naturally. Results of experiments using four graph\nclassification datasets demonstrated that TSEN outperformed the\nstate-of-the-art typical GNN models and the graph-transformer based GNN models.\n","authors":["Jinlong Hu","Yangmin Huang","Shoubin Dong"],"pdf_url":"https://arxiv.org/pdf/2303.16132v1.pdf","comment":"Prepared for submitting to TBE"},{"id":"http://arxiv.org/abs/2203.01859v2","updated":"2023-03-28T16:34:26Z","published":"2022-03-03T17:11:07Z","title":"Robust PAC$^m$: Training Ensemble Models Under Model Misspecification\n  and Outliers","summary":"  Standard Bayesian learning is known to have suboptimal generalization\ncapabilities under model misspecification and in the presence of outliers.\nPAC-Bayes theory demonstrates that the free energy criterion minimized by\nBayesian learning is a bound on the generalization error for Gibbs predictors\n(i.e., for single models drawn at random from the posterior) under the\nassumption of sampling distributions uncontaminated by outliers. This viewpoint\nprovides a justification for the limitations of Bayesian learning when the\nmodel is misspecified, requiring ensembling, and when data is affected by\noutliers. In recent work, PAC-Bayes bounds - referred to as PAC$^m$ - were\nderived to introduce free energy metrics that account for the performance of\nensemble predictors, obtaining enhanced performance under misspecification.\nThis work presents a novel robust free energy criterion that combines the\ngeneralized logarithm score function with PAC$^m$ ensemble bounds. The proposed\nfree energy training criterion produces predictive distributions that are able\nto concurrently counteract the detrimental effects of model misspecification\nand outliers.\n","authors":["Matteo Zecchin","Sangwoo Park","Osvaldo Simeone","Marios Kountouris","David Gesbert"],"pdf_url":"https://arxiv.org/pdf/2203.01859v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.16110v1","updated":"2023-03-28T16:26:37Z","published":"2023-03-28T16:26:37Z","title":"Invariant preservation in machine learned PDE solvers via error\n  correction","summary":"  Machine learned partial differential equation (PDE) solvers trade the\nreliability of standard numerical methods for potential gains in accuracy\nand/or speed. The only way for a solver to guarantee that it outputs the exact\nsolution is to use a convergent method in the limit that the grid spacing\n$\\Delta x$ and timestep $\\Delta t$ approach zero. Machine learned solvers,\nwhich learn to update the solution at large $\\Delta x$ and/or $\\Delta t$, can\nnever guarantee perfect accuracy. Some amount of error is inevitable, so the\nquestion becomes: how do we constrain machine learned solvers to give us the\nsorts of errors that we are willing to tolerate? In this paper, we design more\nreliable machine learned PDE solvers by preserving discrete analogues of the\ncontinuous invariants of the underlying PDE. Examples of such invariants\ninclude conservation of mass, conservation of energy, the second law of\nthermodynamics, and/or non-negative density. Our key insight is simple: to\npreserve invariants, at each timestep apply an error-correcting algorithm to\nthe update rule. Though this strategy is different from how standard solvers\npreserve invariants, it is necessary to retain the flexibility that allows\nmachine learned solvers to be accurate at large $\\Delta x$ and/or $\\Delta t$.\nThis strategy can be applied to any autoregressive solver for any\ntime-dependent PDE in arbitrary geometries with arbitrary boundary conditions.\nAlthough this strategy is very general, the specific error-correcting\nalgorithms need to be tailored to the invariants of the underlying equations as\nwell as to the solution representation and time-stepping scheme of the solver.\nThe error-correcting algorithms we introduce have two key properties. First, by\npreserving the right invariants they guarantee numerical stability. Second, in\nclosed or periodic systems they do so without degrading the accuracy of an\nalready-accurate solver.\n","authors":["Nick McGreivy","Ammar Hakim"],"pdf_url":"https://arxiv.org/pdf/2303.16110v1.pdf","comment":"41 pages, 10 figures"},{"id":"http://arxiv.org/abs/2303.16109v1","updated":"2023-03-28T16:25:16Z","published":"2023-03-28T16:25:16Z","title":"Multimodal Manoeuvre and Trajectory Prediction for Autonomous Vehicles\n  Using Transformer Networks","summary":"  Predicting the behaviour (i.e. manoeuvre/trajectory) of other road users,\nincluding vehicles, is critical for the safe and efficient operation of\nautonomous vehicles (AVs), a.k.a. automated driving systems (ADSs). Due to the\nuncertain future behaviour of vehicles, multiple future behaviour modes are\noften plausible for a vehicle in a given driving scene. Therefore, multimodal\nprediction can provide richer information than single-mode prediction enabling\nAVs to perform a better risk assessment. To this end, we propose a novel\nmultimodal prediction framework that can predict multiple plausible behaviour\nmodes and their likelihoods. The proposed framework includes a bespoke problem\nformulation for manoeuvre prediction, a novel transformer-based prediction\nmodel, and a tailored training method for multimodal manoeuvre and trajectory\nprediction. The performance of the framework is evaluated using two public\nbenchmark highway driving datasets, namely NGSIM and highD. The results show\nthat the proposed framework outperforms the state-of-the-art multimodal methods\nin the literature in terms of prediction error and is capable of predicting\nplausible manoeuvre and trajectory modes.\n","authors":["Sajjad Mozaffari","Konstantinos Koufos","Mehrdad Dianati"],"pdf_url":"https://arxiv.org/pdf/2303.16109v1.pdf","comment":"8 pages, 3 figures, submitted to IEEE RAL"},{"id":"http://arxiv.org/abs/2303.16105v1","updated":"2023-03-28T16:18:56Z","published":"2023-03-28T16:18:56Z","title":"Variational Distribution Learning for Unsupervised Text-to-Image\n  Generation","summary":"  We propose a text-to-image generation algorithm based on deep neural networks\nwhen text captions for images are unavailable during training. In this work,\ninstead of simply generating pseudo-ground-truth sentences of training images\nusing existing image captioning methods, we employ a pretrained CLIP model,\nwhich is capable of properly aligning embeddings of images and corresponding\ntexts in a joint space and, consequently, works well on zero-shot recognition\ntasks. We optimize a text-to-image generation model by maximizing the data\nlog-likelihood conditioned on pairs of image-text CLIP embeddings. To better\nalign data in the two domains, we employ a principled way based on a\nvariational inference, which efficiently estimates an approximate posterior of\nthe hidden text embedding given an image and its CLIP feature. Experimental\nresults validate that the proposed framework outperforms existing approaches by\nlarge margins under unsupervised and semi-supervised text-to-image generation\nsettings.\n","authors":["Minsoo Kang","Doyup Lee","Jiseob Kim","Saehoon Kim","Bohyung Han"],"pdf_url":"https://arxiv.org/pdf/2303.16105v1.pdf","comment":"Accepted at CVPR2023"},{"id":"http://arxiv.org/abs/2204.10997v2","updated":"2023-03-28T16:14:46Z","published":"2022-04-23T05:48:17Z","title":"Cerebral Palsy Prediction with Frequency Attention Informed Graph\n  Convolutional Networks","summary":"  Early diagnosis and intervention are clinically considered the paramount part\nof treating cerebral palsy (CP), so it is essential to design an efficient and\ninterpretable automatic prediction system for CP. We highlight a significant\ndifference between CP infants' frequency of human movement and that of the\nhealthy group, which improves prediction performance. However, the existing\ndeep learning-based methods did not use the frequency information of infants'\nmovement for CP prediction. This paper proposes a frequency attention informed\ngraph convolutional network and validates it on two consumer-grade RGB video\ndatasets, namely MINI-RGBD and RVI-38 datasets. Our proposed frequency\nattention module aids in improving both classification performance and system\ninterpretability. In addition, we design a frequency-binning method that\nretains the critical frequency of the human joint position data while filtering\nthe noise. Our prediction performance achieves state-of-the-art research on\nboth datasets. Our work demonstrates the effectiveness of frequency information\nin supporting the prediction of CP non-intrusively and provides a way for\nsupporting the early diagnosis of CP in the resource-limited regions where the\nclinical resources are not abundant.\n","authors":["Haozheng Zhang","Hubert P. H. Shum","Edmond S. L. Ho"],"pdf_url":"https://arxiv.org/pdf/2204.10997v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2210.05596v2","updated":"2023-03-28T16:09:49Z","published":"2022-10-11T16:34:42Z","title":"Geometry of Radial Basis Neural Networks for Safety Biased Approximation\n  of Unsafe Regions","summary":"  Barrier function-based inequality constraints are a means to enforce safety\nspecifications for control systems. When used in conjunction with a convex\noptimization program, they provide a computationally efficient method to\nenforce safety for the general class of control-affine systems. One of the main\nassumptions when taking this approach is the a priori knowledge of the barrier\nfunction itself, i.e., knowledge of the safe set. In the context of navigation\nthrough unknown environments where the locally safe set evolves with time, such\nknowledge does not exist. This manuscript focuses on the synthesis of a zeroing\nbarrier function characterizing the safe set based on safe and unsafe sample\nmeasurements, e.g., from perception data in navigation applications. Prior work\nformulated a supervised machine learning algorithm whose solution guaranteed\nthe construction of a zeroing barrier function with specific level-set\nproperties. However, it did not explore the geometry of the neural network\ndesign used for the synthesis process. This manuscript describes the specific\ngeometry of the neural network used for zeroing barrier function synthesis, and\nshows how the network provides the necessary representation for splitting the\nstate space into safe and unsafe regions.\n","authors":["Ahmad Abuaish","Mohit Srinivasan","Patricio A. Vela"],"pdf_url":"https://arxiv.org/pdf/2210.05596v2.pdf","comment":"Accepted into American Control Conference (ACC) 2023"},{"id":"http://arxiv.org/abs/2303.16097v1","updated":"2023-03-28T16:06:26Z","published":"2023-03-28T16:06:26Z","title":"Attention Boosted Autoencoder for Building Energy Anomaly Detection","summary":"  Leveraging data collected from smart meters in buildings can aid in\ndeveloping policies towards energy conservation. Significant energy savings\ncould be realised if deviations in the building operating conditions are\ndetected early, and appropriate measures are taken. Towards this end, machine\nlearning techniques can be used to automate the discovery of these abnormal\npatterns in the collected data. Current methods in anomaly detection rely on an\nunderlying model to capture the usual or acceptable operating behaviour. In\nthis paper, we propose a novel attention mechanism to model the consumption\nbehaviour of a building and demonstrate the effectiveness of the model in\ncapturing the relations using sample case studies. A real-world dataset is\nmodelled using the proposed architecture, and the results are presented. A\nvisualisation approach towards understanding the relations captured by the\nmodel is also presented.\n","authors":["Durga Prasad Pydi","S. Advaith"],"pdf_url":"https://arxiv.org/pdf/2303.16097v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.16091v1","updated":"2023-03-28T15:59:12Z","published":"2023-03-28T15:59:12Z","title":"Learnability, Sample Complexity, and Hypothesis Class Complexity for\n  Regression Models","summary":"  The goal of a learning algorithm is to receive a training data set as input\nand provide a hypothesis that can generalize to all possible data points from a\ndomain set. The hypothesis is chosen from hypothesis classes with potentially\ndifferent complexities. Linear regression modeling is an important category of\nlearning algorithms. The practical uncertainty of the target samples affects\nthe generalization performance of the learned model. Failing to choose a proper\nmodel or hypothesis class can lead to serious issues such as underfitting or\noverfitting. These issues have been addressed by alternating cost functions or\nby utilizing cross-validation methods. These approaches can introduce new\nhyperparameters with their own new challenges and uncertainties or increase the\ncomputational complexity of the learning algorithm. On the other hand, the\ntheory of probably approximately correct (PAC) aims at defining learnability\nbased on probabilistic settings. Despite its theoretical value, PAC does not\naddress practical learning issues on many occasions. This work is inspired by\nthe foundation of PAC and is motivated by the existing regression learning\nissues. The proposed approach, denoted by epsilon-Confidence Approximately\nCorrect (epsilon CoAC), utilizes Kullback Leibler divergence (relative entropy)\nand proposes a new related typical set in the set of hyperparameters to tackle\nthe learnability issue. Moreover, it enables the learner to compare hypothesis\nclasses of different complexity orders and choose among them the optimum with\nthe minimum epsilon in the epsilon CoAC framework. Not only the epsilon CoAC\nlearnability overcomes the issues of overfitting and underfitting, but it also\nshows advantages and superiority over the well known cross-validation method in\nthe sense of time consumption as well as in the sense of accuracy.\n","authors":["Soosan Beheshti","Mahdi Shamsi"],"pdf_url":"https://arxiv.org/pdf/2303.16091v1.pdf","comment":"14 pages,10 figures"},{"id":"http://arxiv.org/abs/2301.03829v2","updated":"2023-03-28T15:54:39Z","published":"2023-01-10T07:51:36Z","title":"From Plate to Prevention: A Dietary Nutrient-aided Platform for Health\n  Promotion in Singapore","summary":"  Singapore has been striving to improve the provision of healthcare services\nto her people. In this course, the government has taken note of the deficiency\nin regulating and supervising people's nutrient intake, which is identified as\na contributing factor to the development of chronic diseases. Consequently,\nthis issue has garnered significant attention. In this paper, we share our\nexperience in addressing this issue and attaining medical-grade nutrient intake\ninformation to benefit Singaporeans in different aspects. To this end, we\ndevelop the FoodSG platform to incubate diverse healthcare-oriented\napplications as a service in Singapore, taking into account their shared\nrequirements. We further identify the profound meaning of localized food\ndatasets and systematically clean and curate a localized Singaporean food\ndataset FoodSG-233. To overcome the hurdle in recognition performance brought\nby Singaporean multifarious food dishes, we propose to integrate supervised\ncontrastive learning into our food recognition model FoodSG-SCL for the\nintrinsic capability to mine hard positive/negative samples and therefore boost\nthe accuracy. Through a comprehensive evaluation, we present performance\nresults of the proposed model and insights on food-related healthcare\napplications. The FoodSG-233 dataset has been released in\nhttps://foodlg.comp.nus.edu.sg/.\n","authors":["Kaiping Zheng","Thao Nguyen","Jesslyn Hwei Sing Chong","Charlene Enhui Goh","Melanie Herschel","Hee Hoon Lee","Changshuo Liu","Beng Chin Ooi","Wei Wang","James Yip"],"pdf_url":"https://arxiv.org/pdf/2301.03829v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.06945v2","updated":"2023-03-28T15:34:44Z","published":"2023-03-13T09:27:34Z","title":"CoGANPPIS: Coevolution-enhanced Global Attention Neural Network for\n  Protein-Protein Interaction Site Prediction","summary":"  Protein-protein interactions are essential in biochemical processes. Accurate\nprediction of the protein-protein interaction sites (PPIs) deepens our\nunderstanding of biological mechanism and is crucial for new drug design.\nHowever, conventional experimental methods for PPIs prediction are costly and\ntime-consuming so that many computational approaches, especially ML-based\nmethods, have been developed recently. Although these approaches have achieved\ngratifying results, there are still two limitations: (1) Most models have\nexcavated some useful input features, but failed to take coevolutionary\nfeatures into account, which could provide clues for inter-residue\nrelationships; (2) The attention-based models only allocate attention weights\nfor neighboring residues, instead of doing it globally, neglecting that some\nresidues being far away from the target residues might also matter.\n  We propose a coevolution-enhanced global attention neural network, a\nsequence-based deep learning model for PPIs prediction, called CoGANPPIS. It\nutilizes three layers in parallel for feature extraction: (1) Local-level\nrepresentation aggregation layer, which aggregates the neighboring residues'\nfeatures; (2) Global-level representation learning layer, which employs a novel\ncoevolution-enhanced global attention mechanism to allocate attention weights\nto all the residues on the same protein sequences; (3) Coevolutionary\ninformation learning layer, which applies CNN & pooling to coevolutionary\ninformation to obtain the coevolutionary profile representation. Then, the\nthree outputs are concatenated and passed into several fully connected layers\nfor the final prediction. Application on two benchmark datasets demonstrated a\nstate-of-the-art performance of our model. The source code is publicly\navailable at https://github.com/Slam1423/CoGANPPIS_source_code.\n","authors":["Jiaxing Guo","Xuening Zhu","Zixin Hu","Xiaoxi Hu"],"pdf_url":"https://arxiv.org/pdf/2303.06945v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.16050v1","updated":"2023-03-28T15:32:21Z","published":"2023-03-28T15:32:21Z","title":"Information-Theoretic GAN Compression with Variational Energy-based\n  Model","summary":"  We propose an information-theoretic knowledge distillation approach for the\ncompression of generative adversarial networks, which aims to maximize the\nmutual information between teacher and student networks via a variational\noptimization based on an energy-based model. Because the direct computation of\nthe mutual information in continuous domains is intractable, our approach\nalternatively optimizes the student network by maximizing the variational lower\nbound of the mutual information. To achieve a tight lower bound, we introduce\nan energy-based model relying on a deep neural network to represent a flexible\nvariational distribution that deals with high-dimensional images and consider\nspatial dependencies between pixels, effectively. Since the proposed method is\na generic optimization algorithm, it can be conveniently incorporated into\narbitrary generative adversarial networks and even dense prediction networks,\ne.g., image enhancement models. We demonstrate that the proposed algorithm\nachieves outstanding performance in model compression of generative adversarial\nnetworks consistently when combined with several existing models.\n","authors":["Minsoo Kang","Hyewon Yoo","Eunhee Kang","Sehwan Ki","Hyong-Euk Lee","Bohyung Han"],"pdf_url":"https://arxiv.org/pdf/2303.16050v1.pdf","comment":"Accepted at Neurips2022"},{"id":"http://arxiv.org/abs/2303.16047v1","updated":"2023-03-28T15:25:46Z","published":"2023-03-28T15:25:46Z","title":"Understanding and Exploring the Whole Set of Good Sparse Generalized\n  Additive Models","summary":"  In real applications, interaction between machine learning model and domain\nexperts is critical; however, the classical machine learning paradigm that\nusually produces only a single model does not facilitate such interaction.\nApproximating and exploring the Rashomon set, i.e., the set of all near-optimal\nmodels, addresses this practical challenge by providing the user with a\nsearchable space containing a diverse set of models from which domain experts\ncan choose. We present a technique to efficiently and accurately approximate\nthe Rashomon set of sparse, generalized additive models (GAMs). We present\nalgorithms to approximate the Rashomon set of GAMs with ellipsoids for fixed\nsupport sets and use these ellipsoids to approximate Rashomon sets for many\ndifferent support sets. The approximated Rashomon set serves as a cornerstone\nto solve practical challenges such as (1) studying the variable importance for\nthe model class; (2) finding models under user-specified constraints\n(monotonicity, direct editing); (3) investigating sudden changes in the shape\nfunctions. Experiments demonstrate the fidelity of the approximated Rashomon\nset and its effectiveness in solving practical challenges.\n","authors":["Zhi Chen","Chudi Zhong","Margo Seltzer","Cynthia Rudin"],"pdf_url":"https://arxiv.org/pdf/2303.16047v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2201.03860v2","updated":"2023-03-28T15:23:47Z","published":"2022-01-11T09:46:31Z","title":"End-To-End Optimization of LiDAR Beam Configuration for 3D Object\n  Detection and Localization","summary":"  Existing learning methods for LiDAR-based applications use 3D points scanned\nunder a pre-determined beam configuration, e.g., the elevation angles of beams\nare often evenly distributed. Those fixed configurations are task-agnostic, so\nsimply using them can lead to sub-optimal performance. In this work, we take a\nnew route to learn to optimize the LiDAR beam configuration for a given\napplication. Specifically, we propose a reinforcement learning-based\nlearning-to-optimize (RL-L2O) framework to automatically optimize the beam\nconfiguration in an end-to-end manner for different LiDAR-based applications.\nThe optimization is guided by the final performance of the target task and thus\nour method can be integrated easily with any LiDAR-based application as a\nsimple drop-in module. The method is especially useful when a low-resolution\n(low-cost) LiDAR is needed, for instance, for system deployment at a massive\nscale. We use our method to search for the beam configuration of a\nlow-resolution LiDAR for two important tasks: 3D object detection and\nlocalization. Experiments show that the proposed RL-L2O method improves the\nperformance in both tasks significantly compared to the baseline methods. We\nbelieve that a combination of our method with the recent advances of\nprogrammable LiDARs can start a new research direction for LiDAR-based active\nperception. The code is publicly available at\nhttps://github.com/vniclas/lidar_beam_selection\n","authors":["Niclas Vödisch","Ozan Unal","Ke Li","Luc Van Gool","Dengxin Dai"],"pdf_url":"https://arxiv.org/pdf/2201.03860v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.06552v2","updated":"2023-03-28T15:20:41Z","published":"2023-03-12T03:32:43Z","title":"Energy Regularized RNNs for Solving Non-Stationary Bandit Problems","summary":"  We consider a Multi-Armed Bandit problem in which the rewards are\nnon-stationary and are dependent on past actions and potentially on past\ncontexts. At the heart of our method, we employ a recurrent neural network,\nwhich models these sequences. In order to balance between exploration and\nexploitation, we present an energy minimization term that prevents the neural\nnetwork from becoming too confident in support of a certain action. This term\nprovably limits the gap between the maximal and minimal probabilities assigned\nby the network. In a diverse set of experiments, we demonstrate that our method\nis at least as effective as methods suggested to solve the sub-problem of\nRotting Bandits, and can solve intuitive extensions of various benchmark\nproblems. We share our implementation at\nhttps://github.com/rotmanmi/Energy-Regularized-RNN.\n","authors":["Michael Rotman","Lior Wolf"],"pdf_url":"https://arxiv.org/pdf/2303.06552v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2012.07962v3","updated":"2023-03-28T15:05:23Z","published":"2020-12-14T21:54:11Z","title":"Iterative label cleaning for transductive and semi-supervised few-shot\n  learning","summary":"  Few-shot learning amounts to learning representations and acquiring knowledge\nsuch that novel tasks may be solved with both supervision and data being\nlimited. Improved performance is possible by transductive inference, where the\nentire test set is available concurrently, and semi-supervised learning, where\nmore unlabeled data is available. Focusing on these two settings, we introduce\na new algorithm that leverages the manifold structure of the labeled and\nunlabeled data distribution to predict pseudo-labels, while balancing over\nclasses and using the loss value distribution of a limited-capacity classifier\nto select the cleanest labels, iteratively improving the quality of\npseudo-labels. Our solution surpasses or matches the state of the art results\non four benchmark datasets, namely miniImageNet, tieredImageNet, CUB and\nCIFAR-FS, while being robust over feature space pre-processing and the quantity\nof available data. The publicly available source code can be found in\nhttps://github.com/MichalisLazarou/iLPC.\n","authors":["Michalis Lazarou","Tania Stathaki","Yannis Avrithis"],"pdf_url":"https://arxiv.org/pdf/2012.07962v3.pdf","comment":"published in ICCV 2021"},{"id":"http://arxiv.org/abs/2303.10720v2","updated":"2023-03-28T15:04:36Z","published":"2023-03-19T17:30:44Z","title":"Trainable Projected Gradient Method for Robust Fine-tuning","summary":"  Recent studies on transfer learning have shown that selectively fine-tuning a\nsubset of layers or customizing different learning rates for each layer can\ngreatly improve robustness to out-of-distribution (OOD) data and retain\ngeneralization capability in the pre-trained models. However, most of these\nmethods employ manually crafted heuristics or expensive hyper-parameter\nsearches, which prevent them from scaling up to large datasets and neural\nnetworks. To solve this problem, we propose Trainable Projected Gradient Method\n(TPGM) to automatically learn the constraint imposed for each layer for a\nfine-grained fine-tuning regularization. This is motivated by formulating\nfine-tuning as a bi-level constrained optimization problem. Specifically, TPGM\nmaintains a set of projection radii, i.e., distance constraints between the\nfine-tuned model and the pre-trained model, for each layer, and enforces them\nthrough weight projections. To learn the constraints, we propose a bi-level\noptimization to automatically learn the best set of projection radii in an\nend-to-end manner. Theoretically, we show that the bi-level optimization\nformulation could explain the regularization capability of TPGM. Empirically,\nwith little hyper-parameter search cost, TPGM outperforms existing fine-tuning\nmethods in OOD performance while matching the best in-distribution (ID)\nperformance. For example, when fine-tuned on DomainNet-Real and ImageNet,\ncompared to vanilla fine-tuning, TPGM shows $22\\%$ and $10\\%$ relative OOD\nimprovement respectively on their sketch counterparts. Code is available at\n\\url{https://github.com/PotatoTian/TPGM}.\n","authors":["Junjiao Tian","Xiaoliang Dai","Chih-Yao Ma","Zecheng He","Yen-Cheng Liu","Zsolt Kira"],"pdf_url":"https://arxiv.org/pdf/2303.10720v2.pdf","comment":"Accepted to CVPR2023"},{"id":"http://arxiv.org/abs/2303.16004v1","updated":"2023-03-28T14:27:08Z","published":"2023-03-28T14:27:08Z","title":"A Survey on Malware Detection with Graph Representation Learning","summary":"  Malware detection has become a major concern due to the increasing number and\ncomplexity of malware. Traditional detection methods based on signatures and\nheuristics are used for malware detection, but unfortunately, they suffer from\npoor generalization to unknown attacks and can be easily circumvented using\nobfuscation techniques. In recent years, Machine Learning (ML) and notably Deep\nLearning (DL) achieved impressive results in malware detection by learning\nuseful representations from data and have become a solution preferred over\ntraditional methods. More recently, the application of such techniques on\ngraph-structured data has achieved state-of-the-art performance in various\ndomains and demonstrates promising results in learning more robust\nrepresentations from malware. Yet, no literature review focusing on graph-based\ndeep learning for malware detection exists. In this survey, we provide an\nin-depth literature review to summarize and unify existing works under the\ncommon approaches and architectures. We notably demonstrate that Graph Neural\nNetworks (GNNs) reach competitive results in learning robust embeddings from\nmalware represented as expressive graph structures, leading to an efficient\ndetection by downstream classifiers. This paper also reviews adversarial\nattacks that are utilized to fool graph-based detection methods. Challenges and\nfuture research directions are discussed at the end of the paper.\n","authors":["Tristan Bilot","Nour El Madhoun","Khaldoun Al Agha","Anis Zouaoui"],"pdf_url":"https://arxiv.org/pdf/2303.16004v1.pdf","comment":"Preprint, submitted to ACM Computing Surveys on March 2023. For any\n  suggestions or improvements, please contact me directly by e-mail"},{"id":"http://arxiv.org/abs/2303.16001v1","updated":"2023-03-28T14:16:08Z","published":"2023-03-28T14:16:08Z","title":"Adaptive Voronoi NeRFs","summary":"  Neural Radiance Fields (NeRFs) learn to represent a 3D scene from just a set\nof registered images. Increasing sizes of a scene demands more complex\nfunctions, typically represented by neural networks, to capture all details.\nTraining and inference then involves querying the neural network millions of\ntimes per image, which becomes impractically slow. Since such complex functions\ncan be replaced by multiple simpler functions to improve speed, we show that a\nhierarchy of Voronoi diagrams is a suitable choice to partition the scene. By\nequipping each Voronoi cell with its own NeRF, our approach is able to quickly\nlearn a scene representation. We propose an intuitive partitioning of the space\nthat increases quality gains during training by distributing information evenly\namong the networks and avoids artifacts through a top-down adaptive refinement.\nOur framework is agnostic to the underlying NeRF method and easy to implement,\nwhich allows it to be applied to various NeRF variants for improved learning\nand rendering speeds.\n","authors":["Tim Elsner","Victor Czech","Julia Berger","Zain Selman","Isaak Lim","Leif Kobbelt"],"pdf_url":"https://arxiv.org/pdf/2303.16001v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.15999v1","updated":"2023-03-28T14:15:13Z","published":"2023-03-28T14:15:13Z","title":"Thread Counting in Plain Weave for Old Paintings Using Semi-Supervised\n  Regression Deep Learning Models","summary":"  In this work the authors develop regression approaches based on deep learning\nto perform thread density estimation for plain weave canvas analysis. Previous\napproaches were based on Fourier analysis, that are quite robust for some\nscenarios but fail in some other, in machine learning tools, that involve\npre-labeling of the painting at hand, or the segmentation of thread crossing\npoints, that provides good estimations in all scenarios with no need of\npre-labeling. The segmentation approach is time-consuming as estimation of the\ndensities is performed after locating the crossing points. In this novel\nproposal, we avoid this step by computing the density of threads directly from\nthe image with a regression deep learning model. We also incorporate some\nimprovements in the initial preprocessing of the input image with an impact on\nthe final error. Several models are proposed and analyzed to retain the best\none. Furthermore, we further reduce the density estimation error by introducing\na semi-supervised approach. The performance of our novel algorithm is analyzed\nwith works by Ribera, Vel\\'azquez, and Poussin where we compare our results to\nthe ones of previous approaches. Finally, the method is put into practice to\nsupport the change of authorship or a masterpiece at the Museo del Prado.\n","authors":["A. D. Benjarano","Juan J. Murillo-Fuentes","Laura Alba-Carcelen"],"pdf_url":"https://arxiv.org/pdf/2303.15999v1.pdf","comment":"21 pages"},{"id":"http://arxiv.org/abs/2303.12314v2","updated":"2023-03-28T13:56:07Z","published":"2023-03-22T05:04:21Z","title":"Meta-augmented Prompt Tuning for Better Few-shot Learning","summary":"  Prompt tuning is a parameter-efficient method, which freezes all PLM\nparameters and only prepends some additional tunable tokens called soft prompts\nto the input text. However, soft prompts heavily rely on a better\ninitialization and may easily result in overfitting under few-shot settings,\nwhich causes prompt-tuning performing much worse than fine-tuning. To address\nthe above issues, this paper proposes a novel Self-sUpervised Meta-prompt\nlearning framework with MEtagradient Regularization for few shot generalization\n(SUMMER). We leverage self-supervised meta-learning to better initialize soft\nprompts and curriculum-based task augmentation is further proposed to enrich\nthe meta-task distribution. Besides, a novel meta-gradient regularization\nmethod is integrated into the meta-prompt learning framework, which meta-learns\nto transform the raw gradient during few-shot learning into a\ndomain-generalizable direction, thus alleviating the problem of overfitting.\nExtensive experiments show that SUMMER achieves better performance for\ndifferent few-shot downstream tasks, and also exhibits a stronger domain\ngeneralization ability.\n","authors":["Kaihang Pan","Juncheng Li","Hongye Song","Jun Lin","Xiaozhong Liu","Siliang Tang"],"pdf_url":"https://arxiv.org/pdf/2303.12314v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2107.07075v2","updated":"2023-03-28T13:51:14Z","published":"2021-07-15T02:12:20Z","title":"Deep Learning on a Data Diet: Finding Important Examples Early in\n  Training","summary":"  Recent success in deep learning has partially been driven by training\nincreasingly overparametrized networks on ever larger datasets. It is therefore\nnatural to ask: how much of the data is superfluous, which examples are\nimportant for generalization, and how do we find them? In this work, we make\nthe striking observation that, in standard vision datasets, simple scores\naveraged over several weight initializations can be used to identify important\nexamples very early in training. We propose two such scores -- the Gradient\nNormed (GraNd) and the Error L2-Norm (EL2N) scores -- and demonstrate their\nefficacy on a range of architectures and datasets by pruning significant\nfractions of training data without sacrificing test accuracy. In fact, using\nEL2N scores calculated a few epochs into training, we can prune half of the\nCIFAR10 training set while slightly improving test accuracy. Furthermore, for a\ngiven dataset, EL2N scores from one architecture or hyperparameter\nconfiguration generalize to other configurations. Compared to recent work that\nprunes data by discarding examples that are rarely forgotten over the course of\ntraining, our scores use only local information early in training. We also use\nour scores to detect noisy examples and study training dynamics through the\nlens of important examples -- we investigate how the data distribution shapes\nthe loss surface and identify subspaces of the model's data representation that\nare relatively stable over training.\n","authors":["Mansheej Paul","Surya Ganguli","Gintare Karolina Dziugaite"],"pdf_url":"https://arxiv.org/pdf/2107.07075v2.pdf","comment":"21 pages, 18 figures"},{"id":"http://arxiv.org/abs/2303.15975v1","updated":"2023-03-28T13:47:16Z","published":"2023-03-28T13:47:16Z","title":"Large-scale Pre-trained Models are Surprisingly Strong in Incremental\n  Novel Class Discovery","summary":"  Discovering novel concepts from unlabelled data and in a continuous manner is\nan important desideratum of lifelong learners. In the literature such problems\nhave been partially addressed under very restricted settings, where either\naccess to labelled data is provided for discovering novel concepts (e.g., NCD)\nor learning occurs for a limited number of incremental steps (e.g.,\nclass-iNCD). In this work we challenge the status quo and propose a more\nchallenging and practical learning paradigm called MSc-iNCD, where learning\noccurs continuously and unsupervisedly, while exploiting the rich priors from\nlarge-scale pre-trained models. To this end, we propose simple baselines that\nare not only resilient under longer learning scenarios, but are surprisingly\nstrong when compared with sophisticated state-of-the-art methods. We conduct\nextensive empirical evaluation on a multitude of benchmarks and show the\neffectiveness of our proposed baselines, which significantly raises the bar.\n","authors":["Mingxuan Liu","Subhankar Roy","Zhun Zhong","Nicu Sebe","Elisa Ricci"],"pdf_url":"https://arxiv.org/pdf/2303.15975v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.15973v1","updated":"2023-03-28T13:45:39Z","published":"2023-03-28T13:45:39Z","title":"Do Neural Topic Models Really Need Dropout? Analysis of the Effect of\n  Dropout in Topic Modeling","summary":"  Dropout is a widely used regularization trick to resolve the overfitting\nissue in large feedforward neural networks trained on a small dataset, which\nperforms poorly on the held-out test subset. Although the effectiveness of this\nregularization trick has been extensively studied for convolutional neural\nnetworks, there is a lack of analysis of it for unsupervised models and in\nparticular, VAE-based neural topic models. In this paper, we have analyzed the\nconsequences of dropout in the encoder as well as in the decoder of the VAE\narchitecture in three widely used neural topic models, namely, contextualized\ntopic model (CTM), ProdLDA, and embedded topic model (ETM) using four publicly\navailable datasets. We characterize the dropout effect on these models in terms\nof the quality and predictive performance of the generated topics.\n","authors":["Suman Adhya","Avishek Lahiri","Debarshi Kumar Sanyal"],"pdf_url":"https://arxiv.org/pdf/2303.15973v1.pdf","comment":"Accepted at EACL 2023"},{"id":"http://arxiv.org/abs/2303.15965v1","updated":"2023-03-28T13:35:10Z","published":"2023-03-28T13:35:10Z","title":"SFHarmony: Source Free Domain Adaptation for Distributed Neuroimaging\n  Analysis","summary":"  To represent the biological variability of clinical neuroimaging populations,\nit is vital to be able to combine data across scanners and studies. However,\ndifferent MRI scanners produce images with different characteristics, resulting\nin a domain shift known as the `harmonisation problem'. Additionally,\nneuroimaging data is inherently personal in nature, leading to data privacy\nconcerns when sharing the data. To overcome these barriers, we propose an\nUnsupervised Source-Free Domain Adaptation (SFDA) method, SFHarmony. Through\nmodelling the imaging features as a Gaussian Mixture Model and minimising an\nadapted Bhattacharyya distance between the source and target features, we can\ncreate a model that performs well for the target data whilst having a shared\nfeature representation across the data domains, without needing access to the\nsource data for adaptation or target labels. We demonstrate the performance of\nour method on simulated and real domain shifts, showing that the approach is\napplicable to classification, segmentation and regression tasks, requiring no\nchanges to the algorithm. Our method outperforms existing SFDA approaches\nacross a range of realistic data scenarios, demonstrating the potential utility\nof our approach for MRI harmonisation and general SFDA problems. Our code is\navailable at \\url{https://github.com/nkdinsdale/SFHarmony}.\n","authors":["Nicola K Dinsdale","Mark Jenkinson","Ana IL Namburete"],"pdf_url":"https://arxiv.org/pdf/2303.15965v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.15963v1","updated":"2023-03-28T13:31:27Z","published":"2023-03-28T13:31:27Z","title":"Multimodal and multicontrast image fusion via deep generative models","summary":"  Recently, it has become progressively more evident that classic diagnostic\nlabels are unable to reliably describe the complexity and variability of\nseveral clinical phenotypes. This is particularly true for a broad range of\nneuropsychiatric illnesses (e.g., depression, anxiety disorders, behavioral\nphenotypes). Patient heterogeneity can be better described by grouping\nindividuals into novel categories based on empirically derived sections of\nintersecting continua that span across and beyond traditional categorical\nborders. In this context, neuroimaging data carry a wealth of spatiotemporally\nresolved information about each patient's brain. However, they are usually\nheavily collapsed a priori through procedures which are not learned as part of\nmodel training, and consequently not optimized for the downstream prediction\ntask. This is because every individual participant usually comes with multiple\nwhole-brain 3D imaging modalities often accompanied by a deep genotypic and\nphenotypic characterization, hence posing formidable computational challenges.\nIn this paper we design a deep learning architecture based on generative models\nrooted in a modular approach and separable convolutional blocks to a) fuse\nmultiple 3D neuroimaging modalities on a voxel-wise level, b) convert them into\ninformative latent embeddings through heavy dimensionality reduction, c)\nmaintain good generalizability and minimal information loss. As proof of\nconcept, we test our architecture on the well characterized Human Connectome\nProject database demonstrating that our latent embeddings can be clustered into\neasily separable subject strata which, in turn, map to different phenotypical\ninformation which was not included in the embedding creation process. This may\nbe of aid in predicting disease evolution as well as drug response, hence\nsupporting mechanistic disease understanding and empowering clinical trials.\n","authors":["Giovanna Maria Dimitri","Simeon Spasov","Andrea Duggento","Luca Passamonti","Pietro Li`o","Nicola Toschi"],"pdf_url":"https://arxiv.org/pdf/2303.15963v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.15954v1","updated":"2023-03-28T13:12:17Z","published":"2023-03-28T13:12:17Z","title":"TraffNet: Learning Causality of Traffic Generation for Road Network\n  Digital Twins","summary":"  Road network digital twins (RNDTs) play a critical role in the development of\nnext-generation intelligent transportation systems, enabling more precise\ntraffic planning and control. To support just-in-time (JIT) decision making,\nRNDTs require a model that dynamically learns the traffic patterns from online\nsensor data and generates high-fidelity simulation results. Although current\ntraffic prediction techniques based on graph neural networks have achieved\nstate-of-the-art performance, these techniques only predict future traffic by\nmining correlations in historical traffic data, disregarding the causes of\ntraffic generation, such as traffic demands and route selection. Therefore,\ntheir performance is unreliable for JIT decision making. To fill this gap, we\nintroduce a novel deep learning framework called TraffNet that learns the\ncausality of traffic volume from vehicle trajectory data. First, we use a\nheterogeneous graph to represent the road network, allowing the model to\nincorporate causal features of traffic volumes. Next, motivated by the traffic\ndomain knowledge, we propose a traffic causality learning method to learn an\nembedding vector that encodes travel demands and path-level dependencies for\neach road segment. Then, we model temporal dependencies to match the underlying\nprocess of traffic generation. Finally, the experiments verify the utility of\nTraffNet. The code of TraffNet is available at\nhttps://github.com/mayunyi-1999/TraffNet_code.git.\n","authors":["Ming Xu","Yunyi Ma","Ruimin Li","Geqi Qi","Xiangfu Meng","Haibo Jin"],"pdf_url":"https://arxiv.org/pdf/2303.15954v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.15953v1","updated":"2023-03-28T13:12:00Z","published":"2023-03-28T13:12:00Z","title":"Randomly Initialized Subnetworks with Iterative Weight Recycling","summary":"  The Multi-Prize Lottery Ticket Hypothesis posits that randomly initialized\nneural networks contain several subnetworks that achieve comparable accuracy to\nfully trained models of the same architecture. However, current methods require\nthat the network is sufficiently overparameterized. In this work, we propose a\nmodification to two state-of-the-art algorithms (Edge-Popup and Biprop) that\nfinds high-accuracy subnetworks with no additional storage cost or scaling. The\nalgorithm, Iterative Weight Recycling, identifies subsets of important weights\nwithin a randomly initialized network for intra-layer reuse. Empirically we\nshow improvements on smaller network architectures and higher prune rates,\nfinding that model sparsity can be increased through the \"recycling\" of\nexisting weights. In addition to Iterative Weight Recycling, we complement the\nMulti-Prize Lottery Ticket Hypothesis with a reciprocal finding: high-accuracy,\nrandomly initialized subnetwork's produce diverse masks, despite being\ngenerated with the same hyperparameter's and pruning strategy. We explore the\nlandscapes of these masks, which show high variability.\n","authors":["Matt Gorbett","Darrell Whitley"],"pdf_url":"https://arxiv.org/pdf/2303.15953v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2111.07632v3","updated":"2023-03-28T13:06:17Z","published":"2021-11-15T09:35:54Z","title":"CoReS: Compatible Representations via Stationarity","summary":"  Compatible features enable the direct comparison of old and new learned\nfeatures allowing to use them interchangeably over time. In visual search\nsystems, this eliminates the need to extract new features from the gallery-set\nwhen the representation model is upgraded with novel data. This has a big value\nin real applications as re-indexing the gallery-set can be computationally\nexpensive when the gallery-set is large, or even infeasible due to privacy or\nother concerns of the application. In this paper, we propose CoReS, a new\ntraining procedure to learn representations that are \\textit{compatible} with\nthose previously learned, grounding on the stationarity of the features as\nprovided by fixed classifiers based on polytopes. With this solution, classes\nare maximally separated in the representation space and maintain their spatial\nconfiguration stationary as new classes are added, so that there is no need to\nlearn any mappings between representations nor to impose pairwise training with\nthe previously learned model. We demonstrate that our training procedure\nlargely outperforms the current state of the art and is particularly effective\nin the case of multiple upgrades of the training-set, which is the typical case\nin real applications.\n","authors":["Niccolo Biondi","Federico Pernici","Matteo Bruni","Alberto Del Bimbo"],"pdf_url":"https://arxiv.org/pdf/2111.07632v3.pdf","comment":"in IEEE Transactions on Pattern Analysis and Machine Intelligence,\n  2023. Code: https://github.com/NiccoBiondi/cores-compatibility"},{"id":"http://arxiv.org/abs/2303.15948v1","updated":"2023-03-28T13:02:48Z","published":"2023-03-28T13:02:48Z","title":"Sparse Gaussian Processes with Spherical Harmonic Features Revisited","summary":"  We revisit the Gaussian process model with spherical harmonic features and\nstudy connections between the associated RKHS, its eigenstructure and deep\nmodels. Based on this, we introduce a new class of kernels which correspond to\ndeep models of continuous depth. In our formulation, depth can be estimated as\na kernel hyper-parameter by optimizing the evidence lower bound. Further, we\nintroduce sparseness in the eigenbasis by variational learning of the spherical\nharmonic phases. This enables scaling to larger input dimensions than\npreviously, while also allowing for learning of high frequency variations. We\nvalidate our approach on machine learning benchmark datasets.\n","authors":["Stefanos Eleftheriadis","Dominic Richards","James Hensman"],"pdf_url":"https://arxiv.org/pdf/2303.15948v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.15947v1","updated":"2023-03-28T13:00:08Z","published":"2023-03-28T13:00:08Z","title":"Deep Selection: A Fully Supervised Camera Selection Network for Surgery\n  Recordings","summary":"  Recording surgery in operating rooms is an essential task for education and\nevaluation of medical treatment. However, recording the desired targets, such\nas the surgery field, surgical tools, or doctor's hands, is difficult because\nthe targets are heavily occluded during surgery. We use a recording system in\nwhich multiple cameras are embedded in the surgical lamp, and we assume that at\nleast one camera is recording the target without occlusion at any given time.\nAs the embedded cameras obtain multiple video sequences, we address the task of\nselecting the camera with the best view of the surgery. Unlike the conventional\nmethod, which selects the camera based on the area size of the surgery field,\nwe propose a deep neural network that predicts the camera selection probability\nfrom multiple video sequences by learning the supervision of the expert\nannotation. We created a dataset in which six different types of plastic\nsurgery are recorded, and we provided the annotation of camera switching. Our\nexperiments show that our approach successfully switched between cameras and\noutperformed three baseline methods.\n","authors":["Ryo Hachiuma","Tomohiro Shimizu","Hideo Saito","Hiroki Kajita","Yoshifumi Takatsume"],"pdf_url":"https://arxiv.org/pdf/2303.15947v1.pdf","comment":"MICCAI 2020"},{"id":"http://arxiv.org/abs/2303.15946v1","updated":"2023-03-28T12:58:41Z","published":"2023-03-28T12:58:41Z","title":"Item Graph Convolution Collaborative Filtering for Inductive\n  Recommendations","summary":"  Graph Convolutional Networks (GCN) have been recently employed as core\ncomponent in the construction of recommender system algorithms, interpreting\nuser-item interactions as the edges of a bipartite graph. However, in the\nabsence of side information, the majority of existing models adopt an approach\nof randomly initialising the user embeddings and optimising them throughout the\ntraining process. This strategy makes these algorithms inherently transductive,\ncurtailing their ability to generate predictions for users that were unseen at\ntraining time. To address this issue, we propose a convolution-based algorithm,\nwhich is inductive from the user perspective, while at the same time, depending\nonly on implicit user-item interaction data. We propose the construction of an\nitem-item graph through a weighted projection of the bipartite interaction\nnetwork and to employ convolution to inject higher order associations into item\nembeddings, while constructing user representations as weighted sums of the\nitems with which they have interacted. Despite not training individual\nembeddings for each user our approach achieves state of-the-art recommendation\nperformance with respect to transductive baselines on four real-world datasets,\nshowing at the same time robust inductive performance.\n","authors":["Edoardo D'Amico","Khalil Muhammad","Elias Tragos","Barry Smyth","Neil Hurley","Aonghus Lawlor"],"pdf_url":"https://arxiv.org/pdf/2303.15946v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2203.00219v2","updated":"2023-03-28T12:58:23Z","published":"2022-03-01T04:16:19Z","title":"FedREP: Towards Horizontal Federated Load Forecasting for Retail Energy\n  Providers","summary":"  As Smart Meters are collecting and transmitting household energy consumption\ndata to Retail Energy Providers (REP), the main challenge is to ensure the\neffective use of fine-grained consumer data while ensuring data privacy. In\nthis manuscript, we tackle this challenge for energy load consumption\nforecasting in regards to REPs which is essential to energy demand management,\nload switching and infrastructure development. Specifically, we note that\nexisting energy load forecasting is centralized, which are not scalable and\nmost importantly, vulnerable to data privacy threats. Besides, REPs are\nindividual market participants and liable to ensure the privacy of their own\ncustomers. To address this issue, we propose a novel horizontal\nprivacy-preserving federated learning framework for REPs energy load\nforecasting, namely FedREP. We consider a federated learning system consisting\nof a control centre and multiple retailers by enabling multiple REPs to build a\ncommon, robust machine learning model without sharing data, thus addressing\ncritical issues such as data privacy, data security and scalability. For\nforecasting, we use a state-of-the-art Long Short-Term Memory (LSTM) neural\nnetwork due to its ability to learn long term sequences of observations and\npromises of higher accuracy with time-series data while solving the vanishing\ngradient problem. Finally, we conduct extensive data-driven experiments using a\nreal energy consumption dataset. Experimental results demonstrate that our\nproposed federated learning framework can achieve sufficient performance in\nterms of MSE ranging between 0.3 to 0.4 and is relatively similar to that of a\ncentralized approach while preserving privacy and improving scalability.\n","authors":["Muhammad Akbar Husnoo","Adnan Anwar","Nasser Hosseinzadeh","Shama Naz Islam","Abdun Naser Mahmood","Robin Doss"],"pdf_url":"https://arxiv.org/pdf/2203.00219v2.pdf","comment":"Appeared at 14th IEEE PES Asia-Pacific Power and Energy Engineering\n  Conference (APPEEC 2022)"},{"id":"http://arxiv.org/abs/2303.15944v1","updated":"2023-03-28T12:58:08Z","published":"2023-03-28T12:58:08Z","title":"Cluster-Guided Unsupervised Domain Adaptation for Deep Speaker Embedding","summary":"  Recent studies have shown that pseudo labels can contribute to unsupervised\ndomain adaptation (UDA) for speaker verification. Inspired by the self-training\nstrategies that use an existing classifier to label the unlabeled data for\nretraining, we propose a cluster-guided UDA framework that labels the target\ndomain data by clustering and combines the labeled source domain data and\npseudo-labeled target domain data to train a speaker embedding network. To\nimprove the cluster quality, we train a speaker embedding network dedicated for\nclustering by minimizing the contrastive center loss. The goal is to reduce the\ndistance between an embedding and its assigned cluster center while enlarging\nthe distance between the embedding and the other cluster centers. Using\nVoxCeleb2 as the source domain and CN-Celeb1 as the target domain, we\ndemonstrate that the proposed method can achieve an equal error rate (EER) of\n8.10% on the CN-Celeb1 evaluation set without using any labels from the target\ndomain. This result outperforms the supervised baseline by 39.6% and is the\nstate-of-the-art UDA performance on this corpus.\n","authors":["Haiquan Mao","Feng Hong","Man-wai Mak"],"pdf_url":"https://arxiv.org/pdf/2303.15944v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.15939v1","updated":"2023-03-28T12:52:40Z","published":"2023-03-28T12:52:40Z","title":"Physics-guided adversarial networks for artificial digital image\n  correlation data generation","summary":"  Digital image correlation (DIC) has become a valuable tool in the evaluation\nof mechanical experiments, particularly fatigue crack growth experiments. The\nevaluation requires accurate information of the crack path and crack tip\nposition, which is difficult to obtain due to inherent noise and artefacts.\nMachine learning models have been extremely successful in recognizing this\nrelevant information given labelled DIC displacement data. For the training of\nrobust models, which generalize well, big data is needed. However, data is\ntypically scarce in the field of material science and engineering because\nexperiments are expensive and time-consuming. We present a method to generate\nsynthetic DIC displacement data using generative adversarial networks with a\nphysics-guided discriminator. To decide whether data samples are real or fake,\nthis discriminator additionally receives the derived von Mises equivalent\nstrain. We show that this physics-guided approach leads to improved results in\nterms of visual quality of samples, sliced Wasserstein distance, and geometry\nscore.\n","authors":["David Melching","Erik Schultheis","Eric Breitbarth"],"pdf_url":"https://arxiv.org/pdf/2303.15939v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2106.11642v3","updated":"2023-03-28T12:52:39Z","published":"2021-06-22T09:50:28Z","title":"Repulsive Deep Ensembles are Bayesian","summary":"  Deep ensembles have recently gained popularity in the deep learning community\nfor their conceptual simplicity and efficiency. However, maintaining functional\ndiversity between ensemble members that are independently trained with gradient\ndescent is challenging. This can lead to pathologies when adding more ensemble\nmembers, such as a saturation of the ensemble performance, which converges to\nthe performance of a single model. Moreover, this does not only affect the\nquality of its predictions, but even more so the uncertainty estimates of the\nensemble, and thus its performance on out-of-distribution data. We hypothesize\nthat this limitation can be overcome by discouraging different ensemble members\nfrom collapsing to the same function. To this end, we introduce a kernelized\nrepulsive term in the update rule of the deep ensembles. We show that this\nsimple modification not only enforces and maintains diversity among the members\nbut, even more importantly, transforms the maximum a posteriori inference into\nproper Bayesian inference. Namely, we show that the training dynamics of our\nproposed repulsive ensembles follow a Wasserstein gradient flow of the KL\ndivergence with the true posterior. We study repulsive terms in weight and\nfunction space and empirically compare their performance to standard ensembles\nand Bayesian baselines on synthetic and real-world prediction tasks.\n","authors":["Francesco D'Angelo","Vincent Fortuin"],"pdf_url":"https://arxiv.org/pdf/2106.11642v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.15936v1","updated":"2023-03-28T12:47:00Z","published":"2023-03-28T12:47:00Z","title":"Searching for long faint astronomical high energy transients: a data\n  driven approach","summary":"  HERMES (High Energy Rapid Modular Ensemble of Satellites) pathfinder is an\nin-orbit demonstration consisting of a constellation of six 3U nano-satellites\nhosting simple but innovative detectors for the monitoring of cosmic\nhigh-energy transients. The main objective of HERMES Pathfinder is to prove\nthat accurate position of high-energy cosmic transients can be obtained using\nminiaturized hardware. The transient position is obtained by studying the delay\ntime of arrival of the signal to different detectors hosted by nano-satellites\non low Earth orbits. To this purpose, the goal is to achive an overall accuracy\nof a fraction of a micro-second. In this context, we need to develop novel\ntools to fully exploit the future scientific data output of HERMES Pathfinder.\nIn this paper, we introduce a new framework to assess the background count rate\nof a space-born, high energy detector; a key step towards the identification of\nfaint astrophysical transients. We employ a Neural Network (NN) to estimate the\nbackground lightcurves on different timescales. Subsequently, we employ a fast\nchange-point and anomaly detection technique to isolate observation segments\nwhere statistically significant excesses in the observed count rate relative to\nthe background estimate exist. We test the new software on archival data from\nthe NASA Fermi Gamma-ray Burst Monitor (GBM), which has a collecting area and\nbackground level of the same order of magnitude to those of HERMES Pathfinder.\nThe NN performances are discussed and analyzed over period of both high and low\nsolar activity. We were able to confirm events in the Fermi/GBM catalog and\nfound events, not present in Fermi/GBM database, that could be attributed to\nSolar Flares, Terrestrial Gamma-ray Flashes, Gamma-Ray Bursts, Galactic X-ray\nflash. Seven of these are selected and analyzed further, providing an estimate\nof localisation and a tentative classification.\n","authors":["Riccardo Crupi","Giuseppe Dilillo","Elisabetta Bissaldi","Fabrizio Fiore","Andrea Vacchi"],"pdf_url":"https://arxiv.org/pdf/2303.15936v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.12853v2","updated":"2023-03-28T12:29:33Z","published":"2023-03-22T18:23:24Z","title":"Three iterations of $(d-1)$-WL test distinguish non isometric clouds of\n  $d$-dimensional points","summary":"  The Weisfeiler--Lehman (WL) test is a fundamental iterative algorithm for\nchecking isomorphism of graphs. It has also been observed that it underlies the\ndesign of several graph neural network architectures, whose capabilities and\nperformance can be understood in terms of the expressive power of this test.\nMotivated by recent developments in machine learning applications to datasets\ninvolving three-dimensional objects, we study when the WL test is {\\em\ncomplete} for clouds of euclidean points represented by complete distance\ngraphs, i.e., when it can distinguish, up to isometry, any arbitrary such\ncloud.\n  Our main result states that the $(d-1)$-dimensional WL test is complete for\npoint clouds in $d$-dimensional Euclidean space, for any $d\\ge 2$, and that\nonly three iterations of the test suffice. Our result is tight for $d = 2, 3$.\nWe also observe that the $d$-dimensional WL test only requires one iteration to\nachieve completeness.\n","authors":["Valentino Delle Rose","Alexander Kozachinskiy","Cristóbal Rojas","Mircea Petrache","Pablo Barceló"],"pdf_url":"https://arxiv.org/pdf/2303.12853v2.pdf","comment":"14 pages"},{"id":"http://arxiv.org/abs/2303.15919v1","updated":"2023-03-28T12:20:52Z","published":"2023-03-28T12:20:52Z","title":"Hyperbolic Geometry in Computer Vision: A Novel Framework for\n  Convolutional Neural Networks","summary":"  Real-world visual data exhibit intrinsic hierarchical structures that can be\nrepresented effectively in hyperbolic spaces. Hyperbolic neural networks (HNNs)\nare a promising approach for learning feature representations in such spaces.\nHowever, current methods in computer vision rely on Euclidean backbones and\nonly project features to the hyperbolic space in the task heads, limiting their\nability to fully leverage the benefits of hyperbolic geometry. To address this,\nwe present HCNN, the first fully hyperbolic convolutional neural network (CNN)\ndesigned for computer vision tasks. Based on the Lorentz model, we generalize\nfundamental components of CNNs and propose novel formulations of the\nconvolutional layer, batch normalization, and multinomial logistic regression\n(MLR). Experimentation on standard vision tasks demonstrates the effectiveness\nof our HCNN framework and the Lorentz model in both hybrid and fully hyperbolic\nsettings. Overall, we aim to pave the way for future research in hyperbolic\ncomputer vision by offering a new paradigm for interpreting and analyzing\nvisual data. Our code is publicly available at\nhttps://github.com/kschwethelm/HyperbolicCV.\n","authors":["Ahmad Bdeir","Kristian Schwethelm","Niels Landwehr"],"pdf_url":"https://arxiv.org/pdf/2303.15919v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.15916v1","updated":"2023-03-28T12:10:45Z","published":"2023-03-28T12:10:45Z","title":"From Private to Public: Benchmarking GANs in the Context of Private Time\n  Series Classification","summary":"  Deep learning has proven to be successful in various domains and for\ndifferent tasks. However, when it comes to private data several restrictions\nare making it difficult to use deep learning approaches in these application\nfields. Recent approaches try to generate data privately instead of applying a\nprivacy-preserving mechanism directly, on top of the classifier. The solution\nis to create public data from private data in a manner that preserves the\nprivacy of the data. In this work, two very prominent GAN-based architectures\nwere evaluated in the context of private time series classification. In\ncontrast to previous work, mostly limited to the image domain, the scope of\nthis benchmark was the time series domain. The experiments show that especially\nGSWGAN performs well across a variety of public datasets outperforming the\ncompetitor DPWGAN. An analysis of the generated datasets further validates the\nsuperiority of GSWGAN in the context of time series generation.\n","authors":["Dominique Mercier","Andreas Dengel","Sheraz Ahmed"],"pdf_url":"https://arxiv.org/pdf/2303.15916v1.pdf","comment":"18 pages, 6 figures, 6 tables"},{"id":"http://arxiv.org/abs/2303.12888v2","updated":"2023-03-28T12:08:42Z","published":"2023-03-22T20:05:22Z","title":"A dynamic risk score for early prediction of cardiogenic shock using\n  machine learning","summary":"  Myocardial infarction and heart failure are major cardiovascular diseases\nthat affect millions of people in the US. The morbidity and mortality are\nhighest among patients who develop cardiogenic shock. Early recognition of\ncardiogenic shock is critical. Prompt implementation of treatment measures can\nprevent the deleterious spiral of ischemia, low blood pressure, and reduced\ncardiac output due to cardiogenic shock. However, early identification of\ncardiogenic shock has been challenging due to human providers' inability to\nprocess the enormous amount of data in the cardiac intensive care unit (ICU)\nand lack of an effective risk stratification tool. We developed a deep\nlearning-based risk stratification tool, called CShock, for patients admitted\ninto the cardiac ICU with acute decompensated heart failure and/or myocardial\ninfarction to predict onset of cardiogenic shock. To develop and validate\nCShock, we annotated cardiac ICU datasets with physician adjudicated outcomes.\nCShock achieved an area under the receiver operator characteristic curve\n(AUROC) of 0.820, which substantially outperformed CardShock (AUROC 0.519), a\nwell-established risk score for cardiogenic shock prognosis. CShock was\nexternally validated in an independent patient cohort and achieved an AUROC of\n0.800, demonstrating its generalizability in other cardiac ICUs.\n","authors":["Yuxuan Hu","Albert Lui","Mark Goldstein","Mukund Sudarshan","Andrea Tinsay","Cindy Tsui","Samuel Maidman","John Medamana","Neil Jethani","Aahlad Puli","Vuthy Nguy","Yindalon Aphinyanaphongs","Nicholas Kiefer","Nathaniel Smilowitz","James Horowitz","Tania Ahuja","Glenn I Fishman","Judith Hochman","Stuart Katz","Samuel Bernard","Rajesh Ranganath"],"pdf_url":"https://arxiv.org/pdf/2303.12888v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2208.11056v2","updated":"2023-03-28T12:04:46Z","published":"2022-08-23T16:03:06Z","title":"Enhancement Encoding: A Novel Imbalanced Classification Approach via\n  Encoding the Training Labels","summary":"  Class imbalance, which is also called long-tailed distribution, is a common\nproblem in classification tasks based on machine learning. If it happens, the\nminority data will be overwhelmed by the majority, which presents quite a\nchallenge for data science. To address the class imbalance problem, researchers\nhave proposed lots of methods: some people make the data set balanced (SMOTE),\nsome others refine the loss function (Focal Loss), and even someone has noticed\nthe value of labels influences class-imbalanced learning (Yang and Xu.\nRethinking the value of labels for improving class-imbalanced learning. In\nNeurIPS 2020), but no one changes the way to encode the labels of data yet.\nNowadays, the most prevailing technique to encode labels is the one-hot\nencoding due to its nice performance in the general situation. However, it is\nnot a good choice for imbalanced data, because the classifier will treat\nmajority and minority samples equally. In this paper, we innovatively propose\nthe enhancement encoding technique, which is specially designed for the\nimbalanced classification. The enhancement encoding combines re-weighting and\ncost-sensitiveness, which can reflect the difference between hard and easy (or\nminority and majority) classes. To reduce the number of validation samples and\nthe computation cost, we also replace the confusion matrix with a novel\nsoft-confusion matrix which works better with a small validation set. In the\nexperiments, we evaluate the enhancement encoding with three different types of\nloss. And the results show that enhancement encoding is very effective to\nimprove the performance of the network trained with imbalanced data.\nParticularly, the performance on minority classes is much better.\n","authors":["Jia-Chen Zhao"],"pdf_url":"https://arxiv.org/pdf/2208.11056v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.14434v2","updated":"2023-03-28T11:52:55Z","published":"2023-03-25T10:57:46Z","title":"Heat flux for semi-local machine-learning potentials","summary":"  The Green-Kubo (GK) method is a rigorous framework for heat transport\nsimulations in materials. However, it requires an accurate description of the\npotential-energy surface and carefully converged statistics. Machine-learning\npotentials can achieve the accuracy of first-principles simulations while\nallowing to reach well beyond their simulation time and length scales at a\nfraction of the cost. In this paper, we explain how to apply the GK approach to\nthe recent class of message-passing machine-learning potentials, which\niteratively consider semi-local interactions beyond the initial interaction\ncutoff. We derive an adapted heat flux formulation that can be implemented\nusing automatic differentiation without compromising computational efficiency.\nThe approach is demonstrated and validated by calculating the thermal\nconductivity of zirconium dioxide across temperatures.\n","authors":["Marcel F. Langer","Florian Knoop","Christian Carbogno","Matthias Scheffler","Matthias Rupp"],"pdf_url":"https://arxiv.org/pdf/2303.14434v2.pdf","comment":"6 pages, 3 figures, excluding supplement (12 pages, 10 figures), v2:\n  fixed figures. Additional information at https://marcel.science/gknet"},{"id":"http://arxiv.org/abs/2210.16771v2","updated":"2023-03-28T11:36:36Z","published":"2022-10-30T08:29:20Z","title":"Parameter-Efficient Tuning Makes a Good Classification Head","summary":"  In recent years, pretrained models revolutionized the paradigm of natural\nlanguage understanding (NLU), where we append a randomly initialized\nclassification head after the pretrained backbone, e.g. BERT, and finetune the\nwhole model. As the pretrained backbone makes a major contribution to the\nimprovement, we naturally expect a good pretrained classification head can also\nbenefit the training. However, the final-layer output of the backbone, i.e. the\ninput of the classification head, will change greatly during finetuning, making\nthe usual head-only pretraining (LP-FT) ineffective. In this paper, we find\nthat parameter-efficient tuning makes a good classification head, with which we\ncan simply replace the randomly initialized heads for a stable performance\ngain. Our experiments demonstrate that the classification head jointly\npretrained with parameter-efficient tuning consistently improves the\nperformance on 9 tasks in GLUE and SuperGLUE.\n","authors":["Zhuoyi Yang","Ming Ding","Yanhui Guo","Qingsong Lv","Jie Tang"],"pdf_url":"https://arxiv.org/pdf/2210.16771v2.pdf","comment":"Accepted as a long paper to EMNLP 2022 Main Conference"},{"id":"http://arxiv.org/abs/2303.15901v1","updated":"2023-03-28T11:34:54Z","published":"2023-03-28T11:34:54Z","title":"Denoising Autoencoder-based Defensive Distillation as an Adversarial\n  Robustness Algorithm","summary":"  Adversarial attacks significantly threaten the robustness of deep neural\nnetworks (DNNs). Despite the multiple defensive methods employed, they are\nnevertheless vulnerable to poison attacks, where attackers meddle with the\ninitial training data. In order to defend DNNs against such adversarial\nattacks, this work proposes a novel method that combines the defensive\ndistillation mechanism with a denoising autoencoder (DAE). This technique tries\nto lower the sensitivity of the distilled model to poison attacks by spotting\nand reconstructing poisonous adversarial inputs in the training data. We added\ncarefully created adversarial samples to the initial training data to assess\nthe proposed method's performance. Our experimental findings demonstrate that\nour method successfully identified and reconstructed the poisonous inputs while\nalso considering enhancing the DNN's resilience. The proposed approach provides\na potent and robust defense mechanism for DNNs in various applications where\ndata poisoning attacks are a concern. Thus, the defensive distillation\ntechnique's limitation posed by poisonous adversarial attacks is overcome.\n","authors":["Bakary Badjie","José Cecílio","António Casimiro"],"pdf_url":"https://arxiv.org/pdf/2303.15901v1.pdf","comment":"This paper have 4 pages, 3 figures and it is accepted at the Ada User\n  journal"},{"id":"http://arxiv.org/abs/2206.13508v3","updated":"2023-03-28T11:27:36Z","published":"2022-06-25T17:09:00Z","title":"Data Augmentation techniques in time series domain: A survey and\n  taxonomy","summary":"  With the latest advances in Deep Learning-based generative models, it has not\ntaken long to take advantage of their remarkable performance in the area of\ntime series. Deep neural networks used to work with time series heavily depend\non the size and consistency of the datasets used in training. These features\nare not usually abundant in the real world, where they are usually limited and\noften have constraints that must be guaranteed. Therefore, an effective way to\nincrease the amount of data is by using Data Augmentation techniques, either by\nadding noise or permutations and by generating new synthetic data. This work\nsystematically reviews the current state-of-the-art in the area to provide an\noverview of all available algorithms and proposes a taxonomy of the most\nrelevant research. The efficiency of the different variants will be evaluated\nas a central part of the process, as well as the different metrics to evaluate\nthe performance and the main problems concerning each model will be analysed.\nThe ultimate aim of this study is to provide a summary of the evolution and\nperformance of areas that produce better results to guide future researchers in\nthis field.\n","authors":["Guillermo Iglesias","Edgar Talavera","Ángel González-Prieto","Alberto Mozo","Sandra Gómez-Canaval"],"pdf_url":"https://arxiv.org/pdf/2206.13508v3.pdf","comment":"33 pages, 9 figures"},{"id":"http://arxiv.org/abs/2303.15896v1","updated":"2023-03-28T11:20:59Z","published":"2023-03-28T11:20:59Z","title":"Efficient Quality Diversity Optimization of 3D Buildings through 2D\n  Pre-optimization","summary":"  Quality diversity algorithms can be used to efficiently create a diverse set\nof solutions to inform engineers' intuition. But quality diversity is not\nefficient in very expensive problems, needing 100.000s of evaluations. Even\nwith the assistance of surrogate models, quality diversity needs 100s or even\n1000s of evaluations, which can make it use infeasible. In this study we try to\ntackle this problem by using a pre-optimization strategy on a lower-dimensional\noptimization problem and then map the solutions to a higher-dimensional case.\nFor a use case to design buildings that minimize wind nuisance, we show that we\ncan predict flow features around 3D buildings from 2D flow features around\nbuilding footprints. For a diverse set of building designs, by sampling the\nspace of 2D footprints with a quality diversity algorithm, a predictive model\ncan be trained that is more accurate than when trained on a set of footprints\nthat were selected with a space-filling algorithm like the Sobol sequence.\nSimulating only 16 buildings in 3D, a set of 1024 building designs with low\npredicted wind nuisance is created. We show that we can produce better machine\nlearning models by producing training data with quality diversity instead of\nusing common sampling techniques. The method can bootstrap generative design in\na computationally expensive 3D domain and allow engineers to sweep the design\nspace, understanding wind nuisance in early design phases.\n","authors":["Alexander Hagg","Martin L. Kliemank","Alexander Asteroth","Dominik Wilde","Mario C. Bedrunka","Holger Foysi","Dirk Reith"],"pdf_url":"https://arxiv.org/pdf/2303.15896v1.pdf","comment":"This is the final version and has been accepted for publication in\n  Evolutionary Computation (MIT Press)"},{"id":"http://arxiv.org/abs/2302.12906v2","updated":"2023-03-28T11:17:58Z","published":"2023-02-24T21:25:07Z","title":"Generative Invertible Quantum Neural Networks","summary":"  Invertible Neural Networks (INN) have become established tools for the\nsimulation and generation of highly complex data. We propose a quantum-gate\nalgorithm for a Quantum Invertible Neural Network (QINN) and apply it to the\nLHC data of jet-associated production of a Z-boson that decays into leptons, a\nstandard candle process for particle collider precision measurements. We\ncompare the QINN's performance for different loss functions and training\nscenarios. For this task, we find that a hybrid QINN matches the performance of\na significantly larger purely classical INN in learning and generating complex\ndata.\n","authors":["Armand Rousselot","Michael Spannowsky"],"pdf_url":"https://arxiv.org/pdf/2302.12906v2.pdf","comment":"19 pages, 6 figures Changes in replacement: Add references 49-51,\n  provided gitlab link to code repository"},{"id":"http://arxiv.org/abs/2303.15893v1","updated":"2023-03-28T11:15:57Z","published":"2023-03-28T11:15:57Z","title":"VIVE3D: Viewpoint-Independent Video Editing using 3D-Aware GANs","summary":"  We introduce VIVE3D, a novel approach that extends the capabilities of\nimage-based 3D GANs to video editing and is able to represent the input video\nin an identity-preserving and temporally consistent way. We propose two new\nbuilding blocks. First, we introduce a novel GAN inversion technique\nspecifically tailored to 3D GANs by jointly embedding multiple frames and\noptimizing for the camera parameters. Second, besides traditional semantic face\nedits (e.g. for age and expression), we are the first to demonstrate edits that\nshow novel views of the head enabled by the inherent properties of 3D GANs and\nour optical flow-guided compositing technique to combine the head with the\nbackground video. Our experiments demonstrate that VIVE3D generates\nhigh-fidelity face edits at consistent quality from a range of camera\nviewpoints which are composited with the original video in a temporally and\nspatially consistent manner.\n","authors":["Anna Frühstück","Nikolaos Sarafianos","Yuanlu Xu","Peter Wonka","Tony Tung"],"pdf_url":"https://arxiv.org/pdf/2303.15893v1.pdf","comment":"CVPR 2023. Project webpage and video available at\n  http://afruehstueck.github.io/vive3D"},{"id":"http://arxiv.org/abs/2203.06111v4","updated":"2023-03-28T11:11:08Z","published":"2022-03-11T17:32:27Z","title":"Multi-sensor large-scale dataset for multi-view 3D reconstruction","summary":"  We present a new multi-sensor dataset for multi-view 3D surface\nreconstruction. It includes registered RGB and depth data from sensors of\ndifferent resolutions and modalities: smartphones, Intel RealSense, Microsoft\nKinect, industrial cameras, and structured-light scanner. The scenes are\nselected to emphasize a diverse set of material properties challenging for\nexisting algorithms. We provide around 1.4 million images of 107 different\nscenes acquired from 100 viewing directions under 14 lighting conditions. We\nexpect our dataset will be useful for evaluation and training of 3D\nreconstruction algorithms and for related tasks. The dataset is available at\nskoltech3d.appliedai.tech.\n","authors":["Oleg Voynov","Gleb Bobrovskikh","Pavel Karpyshev","Saveliy Galochkin","Andrei-Timotei Ardelean","Arseniy Bozhenko","Ekaterina Karmanova","Pavel Kopanev","Yaroslav Labutin-Rymsho","Ruslan Rakhimov","Aleksandr Safin","Valerii Serpiva","Alexey Artemov","Evgeny Burnaev","Dzmitry Tsetserukou","Denis Zorin"],"pdf_url":"https://arxiv.org/pdf/2203.06111v4.pdf","comment":"v4: final camera-ready version"},{"id":"http://arxiv.org/abs/2302.06586v3","updated":"2023-03-28T11:09:51Z","published":"2023-02-13T18:37:37Z","title":"Stitchable Neural Networks","summary":"  The public model zoo containing enormous powerful pretrained model families\n(e.g., ResNet/DeiT) has reached an unprecedented scope than ever, which\nsignificantly contributes to the success of deep learning. As each model family\nconsists of pretrained models with diverse scales (e.g., DeiT-Ti/S/B), it\nnaturally arises a fundamental question of how to efficiently assemble these\nreadily available models in a family for dynamic accuracy-efficiency trade-offs\nat runtime. To this end, we present Stitchable Neural Networks (SN-Net), a\nnovel scalable and efficient framework for model deployment. It cheaply\nproduces numerous networks with different complexity and performance trade-offs\ngiven a family of pretrained neural networks, which we call anchors.\nSpecifically, SN-Net splits the anchors across the blocks/layers and then\nstitches them together with simple stitching layers to map the activations from\none anchor to another. With only a few epochs of training, SN-Net effectively\ninterpolates between the performance of anchors with varying scales. At\nruntime, SN-Net can instantly adapt to dynamic resource constraints by\nswitching the stitching positions. Extensive experiments on ImageNet\nclassification demonstrate that SN-Net can obtain on-par or even better\nperformance than many individually trained networks while supporting diverse\ndeployment scenarios. For example, by stitching Swin Transformers, we challenge\nhundreds of models in Timm model zoo with a single network. We believe this new\nelastic model framework can serve as a strong baseline for further research in\nwider communities.\n","authors":["Zizheng Pan","Jianfei Cai","Bohan Zhuang"],"pdf_url":"https://arxiv.org/pdf/2302.06586v3.pdf","comment":"CVPR 2023 Highlight; Project is available at https://snnet.github.io/"},{"id":"http://arxiv.org/abs/2302.09125v2","updated":"2023-03-28T11:08:08Z","published":"2023-02-17T20:17:21Z","title":"JANA: Jointly Amortized Neural Approximation of Complex Bayesian Models","summary":"  This work proposes ''jointly amortized neural approximation'' (JANA) of\nintractable likelihood functions and posterior densities arising in Bayesian\nsurrogate modeling and simulation-based inference. We train three complementary\nnetworks in an end-to-end fashion: 1) a summary network to compress individual\ndata points, sets, or time series into informative embedding vectors; 2) a\nposterior network to learn an amortized approximate posterior; and 3) a\nlikelihood network to learn an amortized approximate likelihood. Their\ninteraction opens a new route to amortized marginal likelihood and posterior\npredictive estimation -- two important ingredients of Bayesian workflows that\nare often too expensive for standard methods. We benchmark the fidelity of JANA\non a variety of simulation models against state-of-the-art Bayesian methods and\npropose a powerful and interpretable diagnostic for joint calibration. In\naddition, we investigate the ability of recurrent likelihood networks to\nemulate complex time series models without resorting to hand-crafted summary\nstatistics.\n","authors":["Stefan T. Radev","Marvin Schmitt","Valentin Pratz","Umberto Picchini","Ullrich Köthe","Paul-Christian Bürkner"],"pdf_url":"https://arxiv.org/pdf/2302.09125v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.15888v1","updated":"2023-03-28T11:04:17Z","published":"2023-03-28T11:04:17Z","title":"Projected Latent Distillation for Data-Agnostic Consolidation in\n  Distributed Continual Learning","summary":"  Distributed learning on the edge often comprises self-centered devices (SCD)\nwhich learn local tasks independently and are unwilling to contribute to the\nperformance of other SDCs. How do we achieve forward transfer at zero cost for\nthe single SCDs? We formalize this problem as a Distributed Continual Learning\nscenario, where SCD adapt to local tasks and a CL model consolidates the\nknowledge from the resulting stream of models without looking at the SCD's\nprivate data. Unfortunately, current CL methods are not directly applicable to\nthis scenario. We propose Data-Agnostic Consolidation (DAC), a novel double\nknowledge distillation method that consolidates the stream of SC models without\nusing the original data. DAC performs distillation in the latent space via a\nnovel Projected Latent Distillation loss. Experimental results show that DAC\nenables forward transfer between SCDs and reaches state-of-the-art accuracy on\nSplit CIFAR100, CORe50 and Split TinyImageNet, both in reharsal-free and\ndistributed CL scenarios. Somewhat surprisingly, even a single\nout-of-distribution image is sufficient as the only source of data during\nconsolidation.\n","authors":["Antonio Carta","Andrea Cossu","Vincenzo Lomonaco","Davide Bacciu","Joost van de Weijer"],"pdf_url":"https://arxiv.org/pdf/2303.15888v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2206.05782v4","updated":"2023-03-28T10:44:40Z","published":"2022-06-12T16:29:56Z","title":"DSCA: A Dual-Stream Network with Cross-Attention on Whole-Slide Image\n  Pyramids for Cancer Prognosis","summary":"  The cancer prognosis on gigapixel Whole-Slide Images (WSIs) has always been a\nchallenging task. To further enhance WSI visual representations, existing\nmethods have explored image pyramids, instead of single-resolution images, in\nWSIs. In spite of this, they still face two major problems: high computational\ncost and the unnoticed semantical gap in multi-resolution feature fusion. To\ntackle these problems, this paper proposes to efficiently exploit WSI pyramids\nfrom a new perspective, the dual-stream network with cross-attention (DSCA).\nOur key idea is to utilize two sub-streams to process the WSI patches with two\nresolutions, where a square pooling is devised in a high-resolution stream to\nsignificantly reduce computational costs, and a cross-attention-based method is\nproposed to properly handle the fusion of dual-stream features. We validate our\nDSCA on three publicly-available datasets with a total number of 3,101 WSIs\nfrom 1,911 patients. Our experiments and ablation studies verify that (i) the\nproposed DSCA could outperform existing state-of-the-art methods in cancer\nprognosis, by an average C-Index improvement of around 4.6%; (ii) our DSCA\nnetwork is more efficient in computation -- it has more learnable parameters\n(6.31M vs. 860.18K) but less computational costs (2.51G vs. 4.94G), compared to\na typical existing multi-resolution network. (iii) the key components of DSCA,\ndual-stream and cross-attention, indeed contribute to our model's performance,\ngaining an average C-Index rise of around 2.0% while maintaining a\nrelatively-small computational load. Our DSCA could serve as an alternative and\neffective tool for WSI-based cancer prognosis.\n","authors":["Pei Liu","Bo Fu","Feng Ye","Rui Yang","Bin Xu","Luping Ji"],"pdf_url":"https://arxiv.org/pdf/2206.05782v4.pdf","comment":"12 pages, 6 figures, 7 tables"},{"id":"http://arxiv.org/abs/2105.04046v3","updated":"2023-03-28T10:19:58Z","published":"2021-05-09T23:13:58Z","title":"A likelihood approach to nonparametric estimation of a singular\n  distribution using deep generative models","summary":"  We investigate statistical properties of a likelihood approach to\nnonparametric estimation of a singular distribution using deep generative\nmodels. More specifically, a deep generative model is used to model\nhigh-dimensional data that are assumed to concentrate around some\nlow-dimensional structure. Estimating the distribution supported on this\nlow-dimensional structure, such as a low-dimensional manifold, is challenging\ndue to its singularity with respect to the Lebesgue measure in the ambient\nspace. In the considered model, a usual likelihood approach can fail to\nestimate the target distribution consistently due to the singularity. We prove\nthat a novel and effective solution exists by perturbing the data with an\ninstance noise, which leads to consistent estimation of the underlying\ndistribution with desirable convergence rates. We also characterize the class\nof distributions that can be efficiently estimated via deep generative models.\nThis class is sufficiently general to contain various structured distributions\nsuch as product distributions, classically smooth distributions and\ndistributions supported on a low-dimensional manifold. Our analysis provides\nsome insights on how deep generative models can avoid the curse of\ndimensionality for nonparametric distribution estimation. We conduct a thorough\nsimulation study and real data analysis to empirically demonstrate that the\nproposed data perturbation technique improves the estimation performance\nsignificantly.\n","authors":["Minwoo Chae","Dongha Kim","Yongdai Kim","Lizhen Lin"],"pdf_url":"https://arxiv.org/pdf/2105.04046v3.pdf","comment":"42 pages, 13 figures, 1 table"},{"id":"http://arxiv.org/abs/2303.15866v1","updated":"2023-03-28T10:17:51Z","published":"2023-03-28T10:17:51Z","title":"Efficient Alternating Minimization Solvers for Wyner Multi-View\n  Unsupervised Learning","summary":"  In this work, we adopt Wyner common information framework for unsupervised\nmulti-view representation learning. Within this framework, we propose two novel\nformulations that enable the development of computational efficient solvers\nbased on the alternating minimization principle. The first formulation,\nreferred to as the {\\em variational form}, enjoys a linearly growing complexity\nwith the number of views and is based on a variational-inference tight\nsurrogate bound coupled with a Lagrangian optimization objective function. The\nsecond formulation, i.e., the {\\em representational form}, is shown to include\nknown results as special cases. Here, we develop a tailored version from the\nalternating direction method of multipliers (ADMM) algorithm for solving the\nresulting non-convex optimization problem. In the two cases, the convergence of\nthe proposed solvers is established in certain relevant regimes. Furthermore,\nour empirical results demonstrate the effectiveness of the proposed methods as\ncompared with the state-of-the-art solvers. In a nutshell, the proposed solvers\noffer computational efficiency, theoretical convergence guarantees, scalable\ncomplexity with the number of views, and exceptional accuracy as compared with\nthe state-of-the-art techniques. Our focus here is devoted to the discrete case\nand our results for continuous distributions are reported elsewhere.\n","authors":["Teng-Hui Huang","Hesham El Gamal"],"pdf_url":"https://arxiv.org/pdf/2303.15866v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.15860v1","updated":"2023-03-28T10:05:06Z","published":"2023-03-28T10:05:06Z","title":"The Wyner Variational Autoencoder for Unsupervised Multi-Layer Wireless\n  Fingerprinting","summary":"  Wireless fingerprinting refers to a device identification method leveraging\nhardware imperfections and wireless channel variations as signatures. Beyond\nphysical layer characteristics, recent studies demonstrated that user\nbehaviours could be identified through network traffic, e.g., packet length,\nwithout decryption of the payload. Inspired by these results, we propose a\nmulti-layer fingerprinting framework that jointly considers the multi-layer\nsignatures for improved identification performance. In contrast to previous\nworks, by leveraging the recent multi-view machine learning paradigm, i.e.,\ndata with multiple forms, our method can cluster the device information shared\namong the multi-layer features without supervision. Our information-theoretic\napproach can be extended to supervised and semi-supervised settings with\nstraightforward derivations. In solving the formulated problem, we obtain a\ntight surrogate bound using variational inference for efficient optimization.\nIn extracting the shared device information, we develop an algorithm based on\nthe Wyner common information method, enjoying reduced computation complexity as\ncompared to existing approaches. The algorithm can be applied to data\ndistributions belonging to the exponential family class. Empirically, we\nevaluate the algorithm in a synthetic dataset with real-world video traffic and\nsimulated physical layer characteristics. Our empirical results show that the\nproposed method outperforms the state-of-the-art baselines in both supervised\nand unsupervised settings.\n","authors":["Teng-Hui Huang","Thilini Dahanayaka","Kanchana Thilakarathna","Philip H. W. Leong","Hesham El Gamal"],"pdf_url":"https://arxiv.org/pdf/2303.15860v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.15852v1","updated":"2023-03-28T09:43:58Z","published":"2023-03-28T09:43:58Z","title":"Exploring Deep Learning Methods for Classification of SAR Images:\n  Towards NextGen Convolutions via Transformers","summary":"  Images generated by high-resolution SAR have vast areas of application as\nthey can work better in adverse light and weather conditions. One such area of\napplication is in the military systems. This study is an attempt to explore the\nsuitability of current state-of-the-art models introduced in the domain of\ncomputer vision for SAR target classification (MSTAR). Since the application of\nany solution produced for military systems would be strategic and real-time,\naccuracy is often not the only criterion to measure its performance. Other\nimportant parameters like prediction time and input resiliency are equally\nimportant. The paper deals with these issues in the context of SAR images.\nExperimental results show that deep learning models can be suitably applied in\nthe domain of SAR image classification with the desired performance levels.\n","authors":["Aakash Singh","Vivek Kumar Singh"],"pdf_url":"https://arxiv.org/pdf/2303.15852v1.pdf","comment":"6 pages, 9 figures"},{"id":"http://arxiv.org/abs/2303.15850v1","updated":"2023-03-28T09:43:16Z","published":"2023-03-28T09:43:16Z","title":"That Label's Got Style: Handling Label Style Bias for Uncertain Image\n  Segmentation","summary":"  Segmentation uncertainty models predict a distribution over plausible\nsegmentations for a given input, which they learn from the annotator variation\nin the training set. However, in practice these annotations can differ\nsystematically in the way they are generated, for example through the use of\ndifferent labeling tools. This results in datasets that contain both data\nvariability and differing label styles. In this paper, we demonstrate that\napplying state-of-the-art segmentation uncertainty models on such datasets can\nlead to model bias caused by the different label styles. We present an updated\nmodelling objective conditioning on labeling style for aleatoric uncertainty\nestimation, and modify two state-of-the-art-architectures for segmentation\nuncertainty accordingly. We show with extensive experiments that this method\nreduces label style bias, while improving segmentation performance, increasing\nthe applicability of segmentation uncertainty models in the wild. We curate two\ndatasets, with annotations in different label styles, which we will make\npublicly available along with our code upon publication.\n","authors":["Kilian Zepf","Eike Petersen","Jes Frellsen","Aasa Feragen"],"pdf_url":"https://arxiv.org/pdf/2303.15850v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.15849v1","updated":"2023-03-28T09:40:06Z","published":"2023-03-28T09:40:06Z","title":"GAS: A Gaussian Mixture Distribution-Based Adaptive Sampling Method for\n  PINNs","summary":"  With recent study of the deep learning in scientific computation, the PINNs\nmethod has drawn widespread attention for solving PDEs. Compared with\ntraditional methods, PINNs can efficiently handle high-dimensional problems,\nwhile the accuracy is relatively low, especially for highly irregular problems.\nInspired by the idea of adaptive finite element methods and incremental\nlearning, we propose GAS, a Gaussian mixture distribution-based adaptive\nsampling method for PINNs. During the training procedure, GAS uses the current\nresidual information to generate a Gaussian mixture distribution for the\nsampling of additional points, which are then trained together with history\ndata to speed up the convergence of loss and achieve a higher accuracy. Several\nnumerical simulations on 2d to 10d problems show that GAS is a promising method\nwhich achieves the state-of-the-art accuracy among deep solvers, while being\ncomparable with traditional numerical solvers.\n","authors":["Yuling Jiao","Di Li","Xiliang Lu","Jerry Zhijian Yang","Cheng Yuan"],"pdf_url":"https://arxiv.org/pdf/2303.15849v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.15846v1","updated":"2023-03-28T09:36:53Z","published":"2023-03-28T09:36:53Z","title":"Soft-prompt tuning to predict lung cancer using primary care free-text\n  Dutch medical notes","summary":"  We investigate different natural language processing (NLP) approaches based\non contextualised word representations for the problem of early prediction of\nlung cancer using free-text patient medical notes of Dutch primary care\nphysicians. Because lung cancer has a low prevalence in primary care, we also\naddress the problem of classification under highly imbalanced classes.\nSpecifically, we use large Transformer-based pretrained language models (PLMs)\nand investigate: 1) how \\textit{soft prompt-tuning} -- an NLP technique used to\nadapt PLMs using small amounts of training data -- compares to standard model\nfine-tuning; 2) whether simpler static word embedding models (WEMs) can be more\nrobust compared to PLMs in highly imbalanced settings; and 3) how models fare\nwhen trained on notes from a small number of patients. We find that 1)\nsoft-prompt tuning is an efficient alternative to standard model fine-tuning;\n2) PLMs show better discrimination but worse calibration compared to simpler\nstatic word embedding models as the classification problem becomes more\nimbalanced; and 3) results when training models on small number of patients are\nmixed and show no clear differences between PLMs and WEMs. All our code is\navailable open source in\n\\url{https://bitbucket.org/aumc-kik/prompt_tuning_cancer_prediction/}.\n","authors":["Auke Elfrink","Iacopo Vagliano","Ameen Abu-Hanna","Iacer Calixto"],"pdf_url":"https://arxiv.org/pdf/2303.15846v1.pdf","comment":"A short version of this paper has been published at the 21st\n  International Conference on Artificial Intelligence in Medicine (AIME 2023)"},{"id":"http://arxiv.org/abs/2303.15845v1","updated":"2023-03-28T09:36:14Z","published":"2023-03-28T09:36:14Z","title":"Conditional Generative Models are Provably Robust: Pointwise Guarantees\n  for Bayesian Inverse Problems","summary":"  Conditional generative models became a very powerful tool to sample from\nBayesian inverse problem posteriors. It is well-known in classical Bayesian\nliterature that posterior measures are quite robust with respect to\nperturbations of both the prior measure and the negative log-likelihood, which\nincludes perturbations of the observations. However, to the best of our\nknowledge, the robustness of conditional generative models with respect to\nperturbations of the observations has not been investigated yet. In this paper,\nwe prove for the first time that appropriately learned conditional generative\nmodels provide robust results for single observations.\n","authors":["Fabian Altekrüger","Paul Hagemann","Gabriele Steidl"],"pdf_url":"https://arxiv.org/pdf/2303.15845v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.15844v1","updated":"2023-03-28T09:35:25Z","published":"2023-03-28T09:35:25Z","title":"CREATED: Generating Viable Counterfactual Sequences for Predictive\n  Process Analytics","summary":"  Predictive process analytics focuses on predicting future states, such as the\noutcome of running process instances. These techniques often use machine\nlearning models or deep learning models (such as LSTM) to make such\npredictions. However, these deep models are complex and difficult for users to\nunderstand. Counterfactuals answer ``what-if'' questions, which are used to\nunderstand the reasoning behind the predictions. For example, what if instead\nof emailing customers, customers are being called? Would this alternative lead\nto a different outcome? Current methods to generate counterfactual sequences\neither do not take the process behavior into account, leading to generating\ninvalid or infeasible counterfactual process instances, or heavily rely on\ndomain knowledge. In this work, we propose a general framework that uses\nevolutionary methods to generate counterfactual sequences. Our framework does\nnot require domain knowledge. Instead, we propose to train a Markov model to\ncompute the feasibility of generated counterfactual sequences and adapt three\nother measures (delta in outcome prediction, similarity, and sparsity) to\nensure their overall viability. The evaluation shows that we generate viable\ncounterfactual sequences, outperform baseline methods in viability, and yield\nsimilar results when compared to the state-of-the-art method that requires\ndomain knowledge.\n","authors":["Olusanmi Hundogan","Xixi Lu","Yupei Du","Hajo A. Reijers"],"pdf_url":"https://arxiv.org/pdf/2303.15844v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.15834v1","updated":"2023-03-28T09:06:28Z","published":"2023-03-28T09:06:28Z","title":"Enabling Inter-organizational Analytics in Business Networks Through\n  Meta Machine Learning","summary":"  Successful analytics solutions that provide valuable insights often hinge on\nthe connection of various data sources. While it is often feasible to generate\nlarger data pools within organizations, the application of analytics within\n(inter-organizational) business networks is still severely constrained. As data\nis distributed across several legal units, potentially even across countries,\nthe fear of disclosing sensitive information as well as the sheer volume of the\ndata that would need to be exchanged are key inhibitors for the creation of\neffective system-wide solutions -- all while still reaching superior prediction\nperformance. In this work, we propose a meta machine learning method that deals\nwith these obstacles to enable comprehensive analyses within a business\nnetwork. We follow a design science research approach and evaluate our method\nwith respect to feasibility and performance in an industrial use case. First,\nwe show that it is feasible to perform network-wide analyses that preserve data\nconfidentiality as well as limit data transfer volume. Second, we demonstrate\nthat our method outperforms a conventional isolated analysis and even gets\nclose to a (hypothetical) scenario where all data could be shared within the\nnetwork. Thus, we provide a fundamental contribution for making business\nnetworks more effective, as we remove a key obstacle to tap the huge potential\nof learning from data that is scattered throughout the network.\n","authors":["Robin Hirt","Niklas Kühl","Dominik Martin","Gerhard Satzger"],"pdf_url":"https://arxiv.org/pdf/2303.15834v1.pdf","comment":"Preprint, forthcoming at Information Technology and Management"},{"id":"http://arxiv.org/abs/2303.15833v1","updated":"2023-03-28T09:05:15Z","published":"2023-03-28T09:05:15Z","title":"Complementary Domain Adaptation and Generalization for Unsupervised\n  Continual Domain Shift Learning","summary":"  Continual domain shift poses a significant challenge in real-world\napplications, particularly in situations where labeled data is not available\nfor new domains. The challenge of acquiring knowledge in this problem setting\nis referred to as unsupervised continual domain shift learning. Existing\nmethods for domain adaptation and generalization have limitations in addressing\nthis issue, as they focus either on adapting to a specific domain or\ngeneralizing to unseen domains, but not both. In this paper, we propose\nComplementary Domain Adaptation and Generalization (CoDAG), a simple yet\neffective learning framework that combines domain adaptation and generalization\nin a complementary manner to achieve three major goals of unsupervised\ncontinual domain shift learning: adapting to a current domain, generalizing to\nunseen domains, and preventing forgetting of previously seen domains. Our\napproach is model-agnostic, meaning that it is compatible with any existing\ndomain adaptation and generalization algorithms. We evaluate CoDAG on several\nbenchmark datasets and demonstrate that our model outperforms state-of-the-art\nmodels in all datasets and evaluation metrics, highlighting its effectiveness\nand robustness in handling unsupervised continual domain shift learning.\n","authors":["Wonguk Cho","Jinha Park","Taesup Kim"],"pdf_url":"https://arxiv.org/pdf/2303.15833v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.15827v1","updated":"2023-03-28T08:55:29Z","published":"2023-03-28T08:55:29Z","title":"PDExplain: Contextual Modeling of PDEs in the Wild","summary":"  We propose an explainable method for solving Partial Differential Equations\nby using a contextual scheme called PDExplain. During the training phase, our\nmethod is fed with data collected from an operator-defined family of PDEs\naccompanied by the general form of this family. In the inference phase, a\nminimal sample collected from a phenomenon is provided, where the sample is\nrelated to the PDE family but not necessarily to the set of specific PDEs seen\nin the training phase. We show how our algorithm can predict the PDE solution\nfor future timesteps. Moreover, our method provides an explainable form of the\nPDE, a trait that can assist in modelling phenomena based on data in physical\nsciences. To verify our method, we conduct extensive experimentation, examining\nits quality both in terms of prediction error and explainability.\n","authors":["Ori Linial","Orly Avner","Dotan Di Castro"],"pdf_url":"https://arxiv.org/pdf/2303.15827v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.04440v2","updated":"2023-03-28T08:54:49Z","published":"2023-03-08T08:42:03Z","title":"HyT-NAS: Hybrid Transformers Neural Architecture Search for Edge Devices","summary":"  Vision Transformers have enabled recent attention-based Deep Learning (DL)\narchitectures to achieve remarkable results in Computer Vision (CV) tasks.\nHowever, due to the extensive computational resources required, these\narchitectures are rarely implemented on resource-constrained platforms. Current\nresearch investigates hybrid handcrafted convolution-based and attention-based\nmodels for CV tasks such as image classification and object detection. In this\npaper, we propose HyT-NAS, an efficient Hardware-aware Neural Architecture\nSearch (HW-NAS) including hybrid architectures targeting vision tasks on tiny\ndevices. HyT-NAS improves state-of-the-art HW-NAS by enriching the search space\nand enhancing the search strategy as well as the performance predictors. Our\nexperiments show that HyT-NAS achieves a similar hypervolume with less than ~5x\ntraining evaluations. Our resulting architecture outperforms MLPerf MobileNetV1\nby 6.3% accuracy improvement with 3.5x less number of parameters on Visual Wake\nWords.\n","authors":["Lotfi Abdelkrim Mecharbat","Hadjer Benmeziane","Hamza Ouarnoughi","Smail Niar"],"pdf_url":"https://arxiv.org/pdf/2303.04440v2.pdf","comment":"CODAI 2022 Workshop - Embedded System Week (ESWeek)"},{"id":"http://arxiv.org/abs/2303.15825v1","updated":"2023-03-28T08:54:29Z","published":"2023-03-28T08:54:29Z","title":"Pareto Optimization of a Laser Wakefield Accelerator","summary":"  Optimization of accelerator performance parameters is limited by numerous\ntrade-offs and finding the appropriate balance between optimization goals for\nan unknown system is challenging to achieve. Here we show that multi-objective\nBayesian optimization can map the solution space of a laser wakefield\naccelerator in a very sample-efficient way. Using a Gaussian mixture model, we\nisolate contributions related to an electron bunch at a certain energy and we\nobserve that there exists a wide range of Pareto-optimal solutions that trade\nbeam energy versus charge at similar laser-to-beam efficiency. However, many\napplications such as light sources require particle beams at a certain target\nenergy. Once such a constraint is introduced we observe a direct trade-off\nbetween energy spread and accelerator efficiency. We furthermore demonstrate\nhow specific solutions can be exploited using \\emph{a posteriori} scalarization\nof the objectives, thereby efficiently splitting the exploration and\nexploitation phases.\n","authors":["F. Irshad","C. Eberle","F. M. Foerster","K. v. Grafenstein","F. Haberstroh","E. Travac","N. Weisse","S. Karsch","A. Döpp"],"pdf_url":"https://arxiv.org/pdf/2303.15825v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.15821v1","updated":"2023-03-28T08:46:03Z","published":"2023-03-28T08:46:03Z","title":"Scaling Multi-Objective Security Games Provably via Space Discretization\n  Based Evolutionary Search","summary":"  In the field of security, multi-objective security games (MOSGs) allow\ndefenders to simultaneously protect targets from multiple heterogeneous\nattackers. MOSGs aim to simultaneously maximize all the heterogeneous payoffs,\ne.g., life, money, and crime rate, without merging heterogeneous attackers. In\nreal-world scenarios, the number of heterogeneous attackers and targets to be\nprotected may exceed the capability of most existing state-of-the-art methods,\ni.e., MOSGs are limited by the issue of scalability. To this end, this paper\nproposes a general framework called SDES based on many-objective evolutionary\nsearch to scale up MOSGs to large-scale targets and heterogeneous attackers.\nSDES consists of four consecutive key components, i.e., discretization,\noptimization, restoration and evaluation, and refinement. Specifically, SDES\nfirst discretizes the originally high-dimensional continuous solution space to\nthe low-dimensional discrete one by the maximal indifference property in game\ntheory. This property helps evolutionary algorithms (EAs) bypass the\nhigh-dimensional step function and ensure a well-convergent Pareto front. Then,\na many-objective EA is used for optimization in the low-dimensional discrete\nsolution space to obtain a well-spaced Pareto front. To evaluate solutions,\nSDES restores solutions back to the original space via bit-wisely optimizing a\nnovel solution divergence. Finally, the refinement in SDES boosts the\noptimization performance with acceptable cost. Theoretically, we prove the\noptimization consistency and convergence of SDES. Experiment results show that\nSDES is the first linear-time MOSG algorithm for both large-scale attackers and\ntargets. SDES is able to solve up to 20 attackers and 100 targets MOSG\nproblems, while the state-of-the-art methods can only solve up to 8 attackers\nand 25 targets ones. Ablation study verifies the necessity of all components in\nSDES.\n","authors":["Yu-Peng Wu","Hong Qian","Rong-Jun Qin","Yi Chen","Aimin Zhou"],"pdf_url":"https://arxiv.org/pdf/2303.15821v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2112.11479v3","updated":"2023-03-28T08:38:01Z","published":"2021-12-10T20:01:44Z","title":"AtteSTNet -- An attention and subword tokenization based approach for\n  code-switched text hate speech detection","summary":"  Recent advancements in technology have led to a boost in social media usage\nwhich has ultimately led to large amounts of user-generated data which also\nincludes hateful and offensive speech. The language used in social media is\noften a combination of English and the native language in the region. In India,\nHindi is used predominantly and is often code-switched with English, giving\nrise to the Hinglish (Hindi+English) language. Various approaches have been\nmade in the past to classify the code-mixed Hinglish hate speech using\ndifferent machine learning and deep learning-based techniques. However, these\ntechniques make use of recurrence on convolution mechanisms which are\ncomputationally expensive and have high memory requirements. Past techniques\nalso make use of complex data processing making the existing techniques very\ncomplex and non-sustainable to change in data. Proposed work gives a much\nsimpler approach which is not only at par with these complex networks but also\nexceeds performance with the use of subword tokenization algorithms like BPE\nand Unigram, along with multi-head attention-based techniques, giving an\naccuracy of 87.41% and an F1 score of 0.851 on standard datasets. Efficient use\nof BPE and Unigram algorithms help handle the nonconventional Hinglish\nvocabulary making the proposed technique simple, efficient and sustainable to\nuse in the real world.\n","authors":["Geet Shingi","Vedangi Wagh","Kishor Wagh","Sharmila Wagh"],"pdf_url":"https://arxiv.org/pdf/2112.11479v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.15810v1","updated":"2023-03-28T08:30:01Z","published":"2023-03-28T08:30:01Z","title":"Offline RL with No OOD Actions: In-Sample Learning via Implicit Value\n  Regularization","summary":"  Most offline reinforcement learning (RL) methods suffer from the trade-off\nbetween improving the policy to surpass the behavior policy and constraining\nthe policy to limit the deviation from the behavior policy as computing\n$Q$-values using out-of-distribution (OOD) actions will suffer from errors due\nto distributional shift. The recently proposed \\textit{In-sample Learning}\nparadigm (i.e., IQL), which improves the policy by quantile regression using\nonly data samples, shows great promise because it learns an optimal policy\nwithout querying the value function of any unseen actions. However, it remains\nunclear how this type of method handles the distributional shift in learning\nthe value function. In this work, we make a key finding that the in-sample\nlearning paradigm arises under the \\textit{Implicit Value Regularization} (IVR)\nframework. This gives a deeper understanding of why the in-sample learning\nparadigm works, i.e., it applies implicit value regularization to the policy.\nBased on the IVR framework, we further propose two practical algorithms, Sparse\n$Q$-learning (SQL) and Exponential $Q$-learning (EQL), which adopt the same\nvalue regularization used in existing works, but in a complete in-sample\nmanner. Compared with IQL, we find that our algorithms introduce sparsity in\nlearning the value function, making them more robust in noisy data regimes. We\nalso verify the effectiveness of SQL and EQL on D4RL benchmark datasets and\nshow the benefits of in-sample learning by comparing them with CQL in small\ndata regimes.\n","authors":["Haoran Xu","Li Jiang","Jianxiong Li","Zhuoran Yang","Zhaoran Wang","Victor Wai Kin Chan","Xianyuan Zhan"],"pdf_url":"https://arxiv.org/pdf/2303.15810v1.pdf","comment":"ICLR 2023 notable top 5%"},{"id":"http://arxiv.org/abs/2303.15809v1","updated":"2023-03-28T08:28:39Z","published":"2023-03-28T08:28:39Z","title":"Kernel interpolation generalizes poorly","summary":"  One of the most interesting problems in the recent renaissance of the studies\nin kernel regression might be whether the kernel interpolation can generalize\nwell, since it may help us understand the `benign overfitting henomenon'\nreported in the literature on deep networks. In this paper, under mild\nconditions, we show that for any $\\varepsilon>0$, the generalization error of\nkernel interpolation is lower bounded by $\\Omega(n^{-\\varepsilon})$. In other\nwords, the kernel interpolation generalizes poorly for a large class of\nkernels. As a direct corollary, we can show that overfitted wide neural\nnetworks defined on sphere generalize poorly.\n","authors":["Yicheng Li","Haobo Zhang","Qian Lin"],"pdf_url":"https://arxiv.org/pdf/2303.15809v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2212.06461v2","updated":"2023-03-28T08:26:41Z","published":"2022-12-13T10:21:15Z","title":"A Statistical Model for Predicting Generalization in Few-Shot\n  Classification","summary":"  The estimation of the generalization error of classifiers often relies on a\nvalidation set. Such a set is hardly available in few-shot learning scenarios,\na highly disregarded shortcoming in the field. In these scenarios, it is common\nto rely on features extracted from pre-trained neural networks combined with\ndistance-based classifiers such as nearest class mean. In this work, we\nintroduce a Gaussian model of the feature distribution. By estimating the\nparameters of this model, we are able to predict the generalization error on\nnew classification tasks with few samples. We observe that accurate distance\nestimates between class-conditional densities are the key to accurate estimates\nof the generalization performance. Therefore, we propose an unbiased estimator\nfor these distances and integrate it in our numerical analysis. We empirically\nshow that our approach outperforms alternatives such as the leave-one-out\ncross-validation strategy.\n","authors":["Yassir Bendou","Vincent Gripon","Bastien Pasdeloup","Lukas Mauch","Stefan Uhlich","Fabien Cardinaux","Ghouthi Boukli Hacene","Javier Alonso Garcia"],"pdf_url":"https://arxiv.org/pdf/2212.06461v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2212.10426v4","updated":"2023-03-28T08:21:28Z","published":"2022-12-20T17:04:50Z","title":"Deep Riemannian Networks for EEG Decoding","summary":"  State-of-the-art performance in electroencephalography (EEG) decoding tasks\nis currently often achieved with either Deep-Learning or\nRiemannian-Geometry-based decoders. Recently, there is growing interest in Deep\nRiemannian Networks (DRNs) possibly combining the advantages of both previous\nclasses of methods. However, there are still a range of topics where additional\ninsight is needed to pave the way for a more widespread application of DRNs in\nEEG. These include architecture design questions such as network size and\nend-to-end ability as well as model training questions. How these factors\naffect model performance has not been explored. Additionally, it is not clear\nhow the data within these networks is transformed, and whether this would\ncorrelate with traditional EEG decoding. Our study aims to lay the groundwork\nin the area of these topics through the analysis of DRNs for EEG with a wide\nrange of hyperparameters. Networks were tested on two public EEG datasets and\ncompared with state-of-the-art ConvNets. Here we propose end-to-end EEG SPDNet\n(EE(G)-SPDNet), and we show that this wide, end-to-end DRN can outperform the\nConvNets, and in doing so use physiologically plausible frequency regions. We\nalso show that the end-to-end approach learns more complex filters than\ntraditional band-pass filters targeting the classical alpha, beta, and gamma\nfrequency bands of the EEG, and that performance can benefit from channel\nspecific filtering approaches. Additionally, architectural analysis revealed\nareas for further improvement due to the possible loss of Riemannian specific\ninformation throughout the network. Our study thus shows how to design and\ntrain DRNs to infer task-related information from the raw EEG without the need\nof handcrafted filterbanks and highlights the potential of end-to-end DRNs such\nas EE(G)-SPDNet for high-performance EEG decoding.\n","authors":["Daniel Wilson","Robin Tibor Schirrmeister","Lukas Alexander Wilhelm Gemein","Tonio Ball"],"pdf_url":"https://arxiv.org/pdf/2212.10426v4.pdf","comment":"26 pages, 15 Figures"},{"id":"http://arxiv.org/abs/2212.08420v2","updated":"2023-03-28T08:11:53Z","published":"2022-12-16T11:44:01Z","title":"Fake it till you make it: Learning transferable representations from\n  synthetic ImageNet clones","summary":"  Recent image generation models such as Stable Diffusion have exhibited an\nimpressive ability to generate fairly realistic images starting from a simple\ntext prompt. Could such models render real images obsolete for training image\nprediction models? In this paper, we answer part of this provocative question\nby investigating the need for real images when training models for ImageNet\nclassification. Provided only with the class names that have been used to build\nthe dataset, we explore the ability of Stable Diffusion to generate synthetic\nclones of ImageNet and measure how useful these are for training classification\nmodels from scratch. We show that with minimal and class-agnostic prompt\nengineering, ImageNet clones are able to close a large part of the gap between\nmodels produced by synthetic images and models trained with real images, for\nthe several standard classification benchmarks that we consider in this study.\nMore importantly, we show that models trained on synthetic images exhibit\nstrong generalization properties and perform on par with models trained on real\ndata for transfer. Project page: https://europe.naverlabs.com/imagenet-sd/\n","authors":["Mert Bulent Sariyildiz","Karteek Alahari","Diane Larlus","Yannis Kalantidis"],"pdf_url":"https://arxiv.org/pdf/2212.08420v2.pdf","comment":"Accepted to CVPR 2023"},{"id":"http://arxiv.org/abs/2203.08147v4","updated":"2023-03-28T08:09:38Z","published":"2022-03-14T17:18:10Z","title":"Energy-Latency Attacks via Sponge Poisoning","summary":"  Sponge examples are test-time inputs carefully optimized to increase energy\nconsumption and latency of neural networks when deployed on hardware\naccelerators. In this work, we are the first to demonstrate that sponge\nexamples can also be injected at training time, via an attack that we call\nsponge poisoning. This attack allows one to increase the energy consumption and\nlatency of machine-learning models indiscriminately on each test-time input. We\npresent a novel formalization for sponge poisoning, overcoming the limitations\nrelated to the optimization of test-time sponge examples, and show that this\nattack is possible even if the attacker only controls a few model updates; for\ninstance, if model training is outsourced to an untrusted third-party or\ndistributed via federated learning. Our extensive experimental analysis shows\nthat sponge poisoning can almost completely vanish the effect of hardware\naccelerators. We also analyze the activations of poisoned models, identifying\nwhich components are more vulnerable to this attack. Finally, we examine the\nfeasibility of countermeasures against sponge poisoning to decrease energy\nconsumption, showing that sanitization methods may be overly expensive for most\nof the users.\n","authors":["Antonio Emanuele Cinà","Ambra Demontis","Battista Biggio","Fabio Roli","Marcello Pelillo"],"pdf_url":"https://arxiv.org/pdf/2203.08147v4.pdf","comment":"Preprint;16 pages"},{"id":"http://arxiv.org/abs/2302.03332v2","updated":"2023-03-28T08:09:27Z","published":"2023-02-07T09:14:59Z","title":"Towards a User Privacy-Aware Mobile Gaming App Installation Prediction\n  Model","summary":"  Over the past decade, programmatic advertising has received a great deal of\nattention in the online advertising industry. A real-time bidding (RTB) system\nis rapidly becoming the most popular method to buy and sell online advertising\nimpressions. Within the RTB system, demand-side platforms (DSP) aim to spend\nadvertisers' campaign budgets efficiently while maximizing profit, seeking\nimpressions that result in high user responses, such as clicks or installs. In\nthe current study, we investigate the process of predicting a mobile gaming app\ninstallation from the point of view of a particular DSP, while paying attention\nto user privacy, and exploring the trade-off between privacy preservation and\nmodel performance. There are multiple levels of potential threats to user\nprivacy, depending on the privacy leaks associated with the data-sharing\nprocess, such as data transformation or de-anonymization. To address these\nconcerns, privacy-preserving techniques were proposed, such as cryptographic\napproaches, for training privacy-aware machine-learning models. However, the\nability to train a mobile gaming app installation prediction model without\nusing user-level data, can prevent these threats and protect the users'\nprivacy, even though the model's ability to predict may be impaired.\nAdditionally, current laws might force companies to declare that they are\ncollecting data, and might even give the user the option to opt out of such\ndata collection, which might threaten companies' business models in digital\nadvertising, which are dependent on the collection and use of user-level data.\nWe conclude that privacy-aware models might still preserve significant\ncapabilities, enabling companies to make better decisions, dependent on the\nprivacy-efficacy trade-off utility function of each case.\n","authors":["Ido Zehori","Nevo Itzhak","Yuval Shahar","Mia Dor Schiller"],"pdf_url":"https://arxiv.org/pdf/2302.03332v2.pdf","comment":"11 pages, 3 figures"},{"id":"http://arxiv.org/abs/2303.15799v1","updated":"2023-03-28T08:07:28Z","published":"2023-03-28T08:07:28Z","title":"Fast Convergence Federated Learning with Aggregated Gradients","summary":"  Federated Learning (FL) is a novel machine learning framework, which enables\nmultiple distributed devices cooperatively training a shared model scheduled by\na central server while protecting private data locally. However, the\nnon-independent-and-identically-distributed (Non-IID) data samples and frequent\ncommunication among participants will slow down the convergent rate and\nincrease communication costs. To achieve fast convergence, we ameliorate the\nlocal gradient descend approach in conventional local update rule by\nintroducing the aggregated gradients at each local update epoch, and propose an\nadaptive learning rate algorithm that further takes the deviation of local\nparameter and global parameter into consideration at each iteration. The above\nstrategy requires all clients' local parameters and gradients at each local\niteration, which is challenging as there is no communication during local\nupdate epochs. Accordingly, we utilize mean field approach by introducing two\nmean field terms to estimate the average local parameters and gradients\nrespectively, which does not require clients to exchange their private\ninformation with each other at each local update epoch. Numerical results show\nthat our proposed framework is superior to the state-of-art schemes in model\naccuracy and convergent rate on both IID and Non-IID dataset.\n","authors":["Wenhao Yuan","Xuehe Wang"],"pdf_url":"https://arxiv.org/pdf/2303.15799v1.pdf","comment":"7 pages, 2 figures"},{"id":"http://arxiv.org/abs/2302.01934v2","updated":"2023-03-28T07:22:31Z","published":"2023-02-04T07:56:18Z","title":"A neural operator-based surrogate solver for free-form electromagnetic\n  inverse design","summary":"  Neural operators have emerged as a powerful tool for solving partial\ndifferential equations in the context of scientific machine learning. Here, we\nimplement and train a modified Fourier neural operator as a surrogate solver\nfor electromagnetic scattering problems and compare its data efficiency to\nexisting methods. We further demonstrate its application to the gradient-based\nnanophotonic inverse design of free-form, fully three-dimensional\nelectromagnetic scatterers, an area that has so far eluded the application of\ndeep learning techniques.\n","authors":["Yannick Augenstein","Taavi Repän","Carsten Rockstuhl"],"pdf_url":"https://arxiv.org/pdf/2302.01934v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.15772v1","updated":"2023-03-28T07:18:29Z","published":"2023-03-28T07:18:29Z","title":"Ecosystem Graphs: The Social Footprint of Foundation Models","summary":"  Foundation models (e.g. ChatGPT, StableDiffusion) pervasively influence\nsociety, warranting immediate social attention. While the models themselves\ngarner much attention, to accurately characterize their impact, we must\nconsider the broader sociotechnical ecosystem. We propose Ecosystem Graphs as a\ndocumentation framework to transparently centralize knowledge of this\necosystem. Ecosystem Graphs is composed of assets (datasets, models,\napplications) linked together by dependencies that indicate technical (e.g. how\nBing relies on GPT-4) and social (e.g. how Microsoft relies on OpenAI)\nrelationships. To supplement the graph structure, each asset is further\nenriched with fine-grained metadata (e.g. the license or training emissions).\nWe document the ecosystem extensively at\nhttps://crfm.stanford.edu/ecosystem-graphs/. As of March 16, 2023, we annotate\n262 assets (64 datasets, 128 models, 70 applications) from 63 organizations\nlinked by 356 dependencies. We show Ecosystem Graphs functions as a powerful\nabstraction and interface for achieving the minimum transparency required to\naddress myriad use cases. Therefore, we envision Ecosystem Graphs will be a\ncommunity-maintained resource that provides value to stakeholders spanning AI\nresearchers, industry professionals, social scientists, auditors and\npolicymakers.\n","authors":["Rishi Bommasani","Dilara Soylu","Thomas I. Liao","Kathleen A. Creel","Percy Liang"],"pdf_url":"https://arxiv.org/pdf/2303.15772v1.pdf","comment":"Authored by the Center for Research on Foundation Models (CRFM) at\n  the Stanford Institute for Human-Centered Artificial Intelligence (HAI).\n  Ecosystem Graphs available at https://crfm.stanford.edu/ecosystem-graphs/"},{"id":"http://arxiv.org/abs/2303.15747v1","updated":"2023-03-28T06:03:41Z","published":"2023-03-28T06:03:41Z","title":"TabRet: Pre-training Transformer-based Tabular Models for Unseen Columns","summary":"  We present \\emph{TabRet}, a pre-trainable Transformer-based model for tabular\ndata. TabRet is designed to work on a downstream task that contains columns not\nseen in pre-training. Unlike other methods, TabRet has an extra learning step\nbefore fine-tuning called \\emph{retokenizing}, which calibrates feature\nembeddings based on the masked autoencoding loss. In experiments, we\npre-trained TabRet with a large collection of public health surveys and\nfine-tuned it on classification tasks in healthcare, and TabRet achieved the\nbest AUC performance on four datasets. In addition, an ablation study shows\nretokenizing and random shuffle augmentation of columns during pre-training\ncontributed to performance gains.\n","authors":["Soma Onishi","Kenta Oono","Kohei Hayashi"],"pdf_url":"https://arxiv.org/pdf/2303.15747v1.pdf","comment":"Accepted at the Workshop on Understanding Foundation Models at ICLR\n  2023"},{"id":"http://arxiv.org/abs/2303.15746v1","updated":"2023-03-28T06:02:56Z","published":"2023-03-28T06:02:56Z","title":"qEUBO: A Decision-Theoretic Acquisition Function for Preferential\n  Bayesian Optimization","summary":"  Preferential Bayesian optimization (PBO) is a framework for optimizing a\ndecision maker's latent utility function using preference feedback. This work\nintroduces the expected utility of the best option (qEUBO) as a novel\nacquisition function for PBO. When the decision maker's responses are\nnoise-free, we show that qEUBO is one-step Bayes optimal and thus equivalent to\nthe popular knowledge gradient acquisition function. We also show that qEUBO\nenjoys an additive constant approximation guarantee to the one-step\nBayes-optimal policy when the decision maker's responses are corrupted by\nnoise. We provide an extensive evaluation of qEUBO and demonstrate that it\noutperforms the state-of-the-art acquisition functions for PBO across many\nsettings. Finally, we show that, under sufficient regularity conditions,\nqEUBO's Bayesian simple regret converges to zero at a rate $o(1/n)$ as the\nnumber of queries, $n$, goes to infinity. In contrast, we show that simple\nregret under qEI, a popular acquisition function for standard BO often used for\nPBO, can fail to converge to zero. Enjoying superior performance, simple\ncomputation, and a grounded decision-theoretic justification, qEUBO is a\npromising acquisition function for PBO.\n","authors":["Raul Astudillo","Zhiyuan Jerry Lin","Eytan Bakshy","Peter I. Frazier"],"pdf_url":"https://arxiv.org/pdf/2303.15746v1.pdf","comment":"In Proceedings of the 26th International Conference on Artificial\n  Intelligence and Statistics (AISTATS) 2023"},{"id":"http://arxiv.org/abs/2303.15745v1","updated":"2023-03-28T06:00:41Z","published":"2023-03-28T06:00:41Z","title":"On Feature Scaling of Recursive Feature Machines","summary":"  In this technical report, we explore the behavior of Recursive Feature\nMachines (RFMs), a type of novel kernel machine that recursively learns\nfeatures via the average gradient outer product, through a series of\nexperiments on regression datasets. When successively adding random noise\nfeatures to a dataset, we observe intriguing patterns in the Mean Squared Error\n(MSE) curves with the test MSE exhibiting a decrease-increase-decrease pattern.\nThis behavior is consistent across different dataset sizes, noise parameters,\nand target functions. Interestingly, the observed MSE curves show similarities\nto the \"double descent\" phenomenon observed in deep neural networks, hinting at\nnew connection between RFMs and neural network behavior. This report lays the\ngroundwork for future research into this peculiar behavior.\n","authors":["Arunav Gupta","Rohit Mishra","William Luu","Mehdi Bouassami"],"pdf_url":"https://arxiv.org/pdf/2303.15745v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.15740v1","updated":"2023-03-28T05:32:30Z","published":"2023-03-28T05:32:30Z","title":"Concentration of Contractive Stochastic Approximation: Additive and\n  Multiplicative Noise","summary":"  In this work, we study the concentration behavior of a stochastic\napproximation (SA) algorithm under a contractive operator with respect to an\narbitrary norm. We consider two settings where the iterates are potentially\nunbounded: (1) bounded multiplicative noise, and (2) additive sub-Gaussian\nnoise. We obtain maximal concentration inequalities on the convergence errors,\nand show that these errors have sub-Gaussian tails in the additive noise\nsetting, and super-polynomial tails (faster than polynomial decay) in the\nmultiplicative noise setting. In addition, we provide an impossibility result\nshowing that it is in general not possible to achieve sub-exponential tails for\nSA with multiplicative noise. To establish these results, we develop a novel\nbootstrapping argument that involves bounding the moment generating function of\nthe generalized Moreau envelope of the error and the construction of an\nexponential supermartingale to enable using Ville's maximal inequality.\n  To demonstrate the applicability of our theoretical results, we use them to\nprovide maximal concentration bounds for a large class of reinforcement\nlearning algorithms, including but not limited to on-policy TD-learning with\nlinear function approximation, off-policy TD-learning with generalized\nimportance sampling factors, and $Q$-learning. To the best of our knowledge,\nsuper-polynomial concentration bounds for off-policy TD-learning have not been\nestablished in the literature due to the challenge of handling the combination\nof unbounded iterates and multiplicative noise.\n","authors":["Zaiwei Chen","Siva Theja Maguluri","Martin Zubeldia"],"pdf_url":"https://arxiv.org/pdf/2303.15740v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.15739v1","updated":"2023-03-28T05:27:32Z","published":"2023-03-28T05:27:32Z","title":"Bayesian Free Energy of Deep ReLU Neural Network in Overparametrized\n  Cases","summary":"  In many research fields in artificial intelligence, it has been shown that\ndeep neural networks are useful to estimate unknown functions on high\ndimensional input spaces. However, their generalization performance is not yet\ncompletely clarified from the theoretical point of view because they are\nnonidentifiable and singular learning machines. Moreover, a ReLU function is\nnot differentiable, to which algebraic or analytic methods in singular learning\ntheory cannot be applied. In this paper, we study a deep ReLU neural network in\noverparametrized cases and prove that the Bayesian free energy, which is equal\nto the minus log marginal likelihoodor the Bayesian stochastic complexity, is\nbounded even if the number of layers are larger than necessary to estimate an\nunknown data-generating function. Since the Bayesian generalization error is\nequal to the increase of the free energy as a function of a sample size, our\nresult also shows that the Bayesian generalization error does not increase even\nif a deep ReLU neural network is designed to be sufficiently large or in an\nopeverparametrized state.\n","authors":["Shuya Nagayasu","Sumio Watanabe"],"pdf_url":"https://arxiv.org/pdf/2303.15739v1.pdf","comment":"20pages, 2figure"},{"id":"http://arxiv.org/abs/2209.14547v2","updated":"2023-03-28T05:09:56Z","published":"2022-09-29T04:36:16Z","title":"A Secure Federated Learning Framework for Residential Short Term Load\n  Forecasting","summary":"  Smart meter measurements, though critical for accurate demand forecasting,\nface several drawbacks including consumers' privacy, data breach issues, to\nname a few. Recent literature has explored Federated Learning (FL) as a\npromising privacy-preserving machine learning alternative which enables\ncollaborative learning of a model without exposing private raw data for short\nterm load forecasting. Despite its virtue, standard FL is still vulnerable to\nan intractable cyber threat known as Byzantine attack carried out by faulty\nand/or malicious clients. Therefore, to improve the robustness of federated\nshort-term load forecasting against Byzantine threats, we develop a\nstate-of-the-art differentially private secured FL-based framework that ensures\nthe privacy of the individual smart meter's data while protect the security of\nFL models and architecture. Our proposed framework leverages the idea of\ngradient quantization through the Sign Stochastic Gradient Descent (SignSGD)\nalgorithm, where the clients only transmit the `sign' of the gradient to the\ncontrol centre after local model training. As we highlight through our\nexperiments involving benchmark neural networks with a set of Byzantine attack\nmodels, our proposed approach mitigates such threats quite effectively and thus\noutperforms conventional Fed-SGD models.\n","authors":["Muhammad Akbar Husnoo","Adnan Anwar","Nasser Hosseinzadeh","Shama Naz Islam","Abdun Naser Mahmood","Robin Doss"],"pdf_url":"https://arxiv.org/pdf/2209.14547v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.15725v1","updated":"2023-03-28T04:26:51Z","published":"2023-03-28T04:26:51Z","title":"Solving Regularized Exp, Cosh and Sinh Regression Problems","summary":"  In modern machine learning, attention computation is a fundamental task for\ntraining large language models such as Transformer, GPT-4 and ChatGPT. In this\nwork, we study exponential regression problem which is inspired by the\nsoftmax/exp unit in the attention mechanism in large language models. The\nstandard exponential regression is non-convex. We study the regularization\nversion of exponential regression problem which is a convex problem. We use\napproximate newton method to solve in input sparsity time.\n  Formally, in this problem, one is given matrix $A \\in \\mathbb{R}^{n \\times\nd}$, $b \\in \\mathbb{R}^n$, $w \\in \\mathbb{R}^n$ and any of functions $\\exp,\n\\cosh$ and $\\sinh$ denoted as $f$. The goal is to find the optimal $x$ that\nminimize $ 0.5 \\| f(Ax) - b \\|_2^2 + 0.5 \\| \\mathrm{diag}(w) A x \\|_2^2$. The\nstraightforward method is to use the naive Newton's method. Let\n$\\mathrm{nnz}(A)$ denote the number of non-zeros entries in matrix $A$. Let\n$\\omega$ denote the exponent of matrix multiplication. Currently, $\\omega\n\\approx 2.373$. Let $\\epsilon$ denote the accuracy error. In this paper, we\nmake use of the input sparsity and purpose an algorithm that use $\\log ( \\|x_0\n- x^*\\|_2 / \\epsilon)$ iterations and $\\widetilde{O}(\\mathrm{nnz}(A) +\nd^{\\omega} )$ per iteration time to solve the problem.\n","authors":["Zhihang Li","Zhao Song","Tianyi Zhou"],"pdf_url":"https://arxiv.org/pdf/2303.15725v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.15714v1","updated":"2023-03-28T03:55:03Z","published":"2023-03-28T03:55:03Z","title":"Explicit Planning Helps Language Models in Logical Reasoning","summary":"  Language models have been shown to perform remarkably well on a wide range of\nnatural language processing tasks. In this paper, we propose a novel system\nthat uses language models to perform multi-step logical reasoning. Our system\nincorporates explicit planning into its inference procedure, thus able to make\nmore informed reasoning decisions at each step by looking ahead into their\nfuture effects. In our experiments, our full system significantly outperforms\nother competing systems. On a multiple-choice question answering task, our\nsystem performs competitively compared to GPT-3-davinci despite having only\naround 1.5B parameters. We conduct several ablation studies to demonstrate that\nexplicit planning plays a crucial role in the system's performance.\n","authors":["Hongyu Zhao","Kangrui Wang","Mo Yu","Hongyuan Mei"],"pdf_url":"https://arxiv.org/pdf/2303.15714v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2107.07999v8","updated":"2023-03-28T03:50:33Z","published":"2021-07-16T16:39:10Z","title":"From block-Toeplitz matrices to differential equations on graphs:\n  towards a general theory for scalable masked Transformers","summary":"  In this paper we provide, to the best of our knowledge, the first\ncomprehensive approach for incorporating various masking mechanisms into\nTransformers architectures in a scalable way. We show that recent results on\nlinear causal attention (Choromanski et al., 2021) and log-linear RPE-attention\n(Luo et al., 2021) are special cases of this general mechanism. However by\ncasting the problem as a topological (graph-based) modulation of unmasked\nattention, we obtain several results unknown before, including efficient\nd-dimensional RPE-masking and graph-kernel masking. We leverage many\nmathematical techniques ranging from spectral analysis through dynamic\nprogramming and random walks to new algorithms for solving Markov processes on\ngraphs. We provide a corresponding empirical evaluation.\n","authors":["Krzysztof Choromanski","Han Lin","Haoxian Chen","Tianyi Zhang","Arijit Sehanobish","Valerii Likhosherstov","Jack Parker-Holder","Tamas Sarlos","Adrian Weller","Thomas Weingarten"],"pdf_url":"https://arxiv.org/pdf/2107.07999v8.pdf","comment":"20 pages, 12 figures"},{"id":"http://arxiv.org/abs/2303.15702v1","updated":"2023-03-28T03:11:21Z","published":"2023-03-28T03:11:21Z","title":"Distributed Graph Embedding with Information-Oriented Random Walks","summary":"  Graph embedding maps graph nodes to low-dimensional vectors, and is widely\nadopted in machine learning tasks. The increasing availability of billion-edge\ngraphs underscores the importance of learning efficient and effective\nembeddings on large graphs, such as link prediction on Twitter with over one\nbillion edges. Most existing graph embedding methods fall short of reaching\nhigh data scalability. In this paper, we present a general-purpose,\ndistributed, information-centric random walk-based graph embedding framework,\nDistGER, which can scale to embed billion-edge graphs. DistGER incrementally\ncomputes information-centric random walks. It further leverages a\nmulti-proximity-aware, streaming, parallel graph partitioning strategy,\nsimultaneously achieving high local partition quality and excellent workload\nbalancing across machines. DistGER also improves the distributed Skip-Gram\nlearning model to generate node embeddings by optimizing the access locality,\nCPU throughput, and synchronization efficiency. Experiments on real-world\ngraphs demonstrate that compared to state-of-the-art distributed graph\nembedding frameworks, including KnightKing, DistDGL, and Pytorch-BigGraph,\nDistGER exhibits 2.33x-129x acceleration, 45% reduction in cross-machines\ncommunication, and > 10% effectiveness improvement in downstream tasks.\n","authors":["Peng Fang","Arijit Khan","Siqiang Luo","Fang Wang","Dan Feng","Zhenli Li","Wei Yin","Yuchao Cao"],"pdf_url":"https://arxiv.org/pdf/2303.15702v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.11522v2","updated":"2023-03-28T03:11:16Z","published":"2023-03-21T00:53:37Z","title":"Online Learning for Equilibrium Pricing in Markets under Incomplete\n  Information","summary":"  The study of market equilibria is central to economic theory, particularly in\nefficiently allocating scarce resources. However, the computation of\nequilibrium prices at which the supply of goods matches their demand typically\nrelies on having access to complete information on private attributes of\nagents, e.g., suppliers' cost functions, which are often unavailable in\npractice. Motivated by this practical consideration, we consider the problem of\nsetting equilibrium prices in the incomplete information setting wherein a\nmarket operator seeks to satisfy the customer demand for a commodity by\npurchasing the required amount from competing suppliers with privately known\ncost functions unknown to the market operator. In this incomplete information\nsetting, we consider the online learning problem of learning equilibrium prices\nover time while jointly optimizing three performance metrics -- unmet demand,\ncost regret, and payment regret -- pertinent in the context of equilibrium\npricing over a horizon of $T$ periods. We first consider the setting when\nsuppliers' cost functions are fixed and develop algorithms that achieve a\nregret of $O(\\log \\log T)$ when the customer demand is constant over time, or\n$O(\\sqrt{T} \\log \\log T)$ when the demand is variable over time. Next, we\nconsider the setting when the suppliers' cost functions can vary over time and\nillustrate that no online algorithm can achieve sublinear regret on all three\nmetrics when the market operator has no information about how the cost\nfunctions change over time. Thus, we consider an augmented setting wherein the\noperator has access to hints/contexts that, without revealing the complete\nspecification of the cost functions, reflect the variation in the cost\nfunctions over time and propose an algorithm with sublinear regret in this\naugmented setting.\n","authors":["Devansh Jalota","Haoyuan Sun","Navid Azizan"],"pdf_url":"https://arxiv.org/pdf/2303.11522v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2210.01765v4","updated":"2023-03-28T03:01:29Z","published":"2022-10-04T17:30:31Z","title":"One Transformer Can Understand Both 2D & 3D Molecular Data","summary":"  Unlike vision and language data which usually has a unique format, molecules\ncan naturally be characterized using different chemical formulations. One can\nview a molecule as a 2D graph or define it as a collection of atoms located in\na 3D space. For molecular representation learning, most previous works designed\nneural networks only for a particular data format, making the learned models\nlikely to fail for other data formats. We believe a general-purpose neural\nnetwork model for chemistry should be able to handle molecular tasks across\ndata modalities. To achieve this goal, in this work, we develop a novel\nTransformer-based Molecular model called Transformer-M, which can take\nmolecular data of 2D or 3D formats as input and generate meaningful semantic\nrepresentations. Using the standard Transformer as the backbone architecture,\nTransformer-M develops two separated channels to encode 2D and 3D structural\ninformation and incorporate them with the atom features in the network modules.\nWhen the input data is in a particular format, the corresponding channel will\nbe activated, and the other will be disabled. By training on 2D and 3D\nmolecular data with properly designed supervised signals, Transformer-M\nautomatically learns to leverage knowledge from different data modalities and\ncorrectly capture the representations. We conducted extensive experiments for\nTransformer-M. All empirical results show that Transformer-M can simultaneously\nachieve strong performance on 2D and 3D tasks, suggesting its broad\napplicability. The code and models will be made publicly available at\nhttps://github.com/lsj2408/Transformer-M.\n","authors":["Shengjie Luo","Tianlang Chen","Yixian Xu","Shuxin Zheng","Tie-Yan Liu","Liwei Wang","Di He"],"pdf_url":"https://arxiv.org/pdf/2210.01765v4.pdf","comment":"20 pages; ICLR 2023, Camera Ready Version; Code:\n  https://github.com/lsj2408/Transformer-M"},{"id":"http://arxiv.org/abs/2303.15693v1","updated":"2023-03-28T02:46:10Z","published":"2023-03-28T02:46:10Z","title":"Large-scale pretraining on pathological images for fine-tuning of small\n  pathological benchmarks","summary":"  Pretraining a deep learning model on large image datasets is a standard step\nbefore fine-tuning the model on small targeted datasets. The large dataset is\nusually general images (e.g. imagenet2012) while the small dataset can be\nspecialized datasets that have different distributions from the large dataset.\nHowever, this 'large-to-small' strategy is not well-validated when the large\ndataset is specialized and has a similar distribution to small datasets. We\nnewly compiled three hematoxylin and eosin-stained image datasets, one large\n(PTCGA200) and two magnification-adjusted small datasets (PCam200 and\nsegPANDA200). Major deep learning models were trained with supervised and\nself-supervised learning methods and fine-tuned on the small datasets for tumor\nclassification and tissue segmentation benchmarks. ResNet50 pretrained with\nMoCov2, SimCLR, and BYOL on PTCGA200 was better than imagenet2012 pretraining\nwhen fine-tuned on PTCGA200 (accuracy of 83.94%, 86.41%, 84.91%, and 82.72%,\nrespectively). ResNet50 pre-trained on PTCGA200 with MoCov2 exceeded the\nCOCOtrain2017-pretrained baseline and was the best in ResNet50 for the tissue\nsegmentation benchmark (mIoU of 63.53% and 63.22%). We found re-training\nimagenet-pretrained models (ResNet50, BiT-M-R50x1, and ViT-S/16) on PTCGA200\nimproved downstream benchmarks.\n","authors":["Masataka Kawai","Noriaki Ota","Shinsuke Yamaoka"],"pdf_url":"https://arxiv.org/pdf/2303.15693v1.pdf","comment":"20 pages, 6 figures"},{"id":"http://arxiv.org/abs/2303.15689v1","updated":"2023-03-28T02:31:57Z","published":"2023-03-28T02:31:57Z","title":"Deep Incomplete Multi-view Clustering with Cross-view Partial Sample and\n  Prototype Alignment","summary":"  The success of existing multi-view clustering relies on the assumption of\nsample integrity across multiple views. However, in real-world scenarios,\nsamples of multi-view are partially available due to data corruption or sensor\nfailure, which leads to incomplete multi-view clustering study (IMVC). Although\nseveral attempts have been proposed to address IMVC, they suffer from the\nfollowing drawbacks: i) Existing methods mainly adopt cross-view contrastive\nlearning forcing the representations of each sample across views to be exactly\nthe same, which might ignore view discrepancy and flexibility in\nrepresentations; ii) Due to the absence of non-observed samples across multiple\nviews, the obtained prototypes of clusters might be unaligned and biased,\nleading to incorrect fusion. To address the above issues, we propose a\nCross-view Partial Sample and Prototype Alignment Network (CPSPAN) for Deep\nIncomplete Multi-view Clustering. Firstly, unlike existing contrastive-based\nmethods, we adopt pair-observed data alignment as 'proxy supervised signals' to\nguide instance-to-instance correspondence construction among views. Then,\nregarding of the shifted prototypes in IMVC, we further propose a prototype\nalignment module to achieve incomplete distribution calibration across views.\nExtensive experimental results showcase the effectiveness of our proposed\nmodules, attaining noteworthy performance improvements when compared to\nexisting IMVC competitors on benchmark datasets.\n","authors":["Jiaqi Jin","Siwei Wang","Zhibin Dong","Xinwang Liu","En Zhu"],"pdf_url":"https://arxiv.org/pdf/2303.15689v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.15682v1","updated":"2023-03-28T02:10:37Z","published":"2023-03-28T02:10:37Z","title":"Pre-training Transformers for Knowledge Graph Completion","summary":"  Learning transferable representation of knowledge graphs (KGs) is challenging\ndue to the heterogeneous, multi-relational nature of graph structures. Inspired\nby Transformer-based pretrained language models' success on learning\ntransferable representation for texts, we introduce a novel inductive KG\nrepresentation model (iHT) for KG completion by large-scale pre-training. iHT\nconsists of a entity encoder (e.g., BERT) and a neighbor-aware relational\nscoring function both parameterized by Transformers. We first pre-train iHT on\na large KG dataset, Wikidata5M. Our approach achieves new state-of-the-art\nresults on matched evaluations, with a relative improvement of more than 25% in\nmean reciprocal rank over previous SOTA models. When further fine-tuned on\nsmaller KGs with either entity and relational shifts, pre-trained iHT\nrepresentations are shown to be transferable, significantly improving the\nperformance on FB15K-237 and WN18RR.\n","authors":["Sanxing Chen","Hao Cheng","Xiaodong Liu","Jian Jiao","Yangfeng Ji","Jianfeng Gao"],"pdf_url":"https://arxiv.org/pdf/2303.15682v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.15681v1","updated":"2023-03-28T02:04:43Z","published":"2023-03-28T02:04:43Z","title":"GNN-based physics solver for time-independent PDEs","summary":"  Physics-based deep learning frameworks have shown to be effective in\naccurately modeling the dynamics of complex physical systems with\ngeneralization capability across problem inputs. However, time-independent\nproblems pose the challenge of requiring long-range exchange of information\nacross the computational domain for obtaining accurate predictions. In the\ncontext of graph neural networks (GNNs), this calls for deeper networks, which,\nin turn, may compromise or slow down the training process. In this work, we\npresent two GNN architectures to overcome this challenge - the Edge Augmented\nGNN and the Multi-GNN. We show that both these networks perform significantly\nbetter (by a factor of 1.5 to 2) than baseline methods when applied to\ntime-independent solid mechanics problems. Furthermore, the proposed\narchitectures generalize well to unseen domains, boundary conditions, and\nmaterials. Here, the treatment of variable domains is facilitated by a novel\ncoordinate transformation that enables rotation and translation invariance. By\nbroadening the range of problems that neural operators based on graph neural\nnetworks can tackle, this paper provides the groundwork for their application\nto complex scientific and industrial settings.\n","authors":["Rini Jasmine Gladstone","Helia Rahmani","Vishvas Suryakumar","Hadi Meidani","Marta D'Elia","Ahmad Zareei"],"pdf_url":"https://arxiv.org/pdf/2303.15681v1.pdf","comment":"12 pages, 2 figures"},{"id":"http://arxiv.org/abs/2210.00272v2","updated":"2023-03-28T02:00:42Z","published":"2022-10-01T13:14:03Z","title":"FINDE: Neural Differential Equations for Finding and Preserving\n  Invariant Quantities","summary":"  Many real-world dynamical systems are associated with first integrals (a.k.a.\ninvariant quantities), which are quantities that remain unchanged over time.\nThe discovery and understanding of first integrals are fundamental and\nimportant topics both in the natural sciences and in industrial applications.\nFirst integrals arise from the conservation laws of system energy, momentum,\nand mass, and from constraints on states; these are typically related to\nspecific geometric structures of the governing equations. Existing neural\nnetworks designed to ensure such first integrals have shown excellent accuracy\nin modeling from data. However, these models incorporate the underlying\nstructures, and in most situations where neural networks learn unknown systems,\nthese structures are also unknown. This limitation needs to be overcome for\nscientific discovery and modeling of unknown systems. To this end, we propose\nfirst integral-preserving neural differential equation (FINDE). By leveraging\nthe projection method and the discrete gradient method, FINDE finds and\npreserves first integrals from data, even in the absence of prior knowledge\nabout underlying structures. Experimental results demonstrate that FINDE can\npredict future states of target systems much longer and find various quantities\nconsistent with well-known first integrals in a unified manner.\n","authors":["Takashi Matsubara","Takaharu Yaguchi"],"pdf_url":"https://arxiv.org/pdf/2210.00272v2.pdf","comment":"25 pages"},{"id":"http://arxiv.org/abs/2303.15678v1","updated":"2023-03-28T01:58:45Z","published":"2023-03-28T01:58:45Z","title":"DisWOT: Student Architecture Search for Distillation WithOut Training","summary":"  Knowledge distillation (KD) is an effective training strategy to improve the\nlightweight student models under the guidance of cumbersome teachers. However,\nthe large architecture difference across the teacher-student pairs limits the\ndistillation gains. In contrast to previous adaptive distillation methods to\nreduce the teacher-student gap, we explore a novel training-free framework to\nsearch for the best student architectures for a given teacher. Our work first\nempirically show that the optimal model under vanilla training cannot be the\nwinner in distillation. Secondly, we find that the similarity of feature\nsemantics and sample relations between random-initialized teacher-student\nnetworks have good correlations with final distillation performances. Thus, we\nefficiently measure similarity matrixs conditioned on the semantic activation\nmaps to select the optimal student via an evolutionary algorithm without any\ntraining. In this way, our student architecture search for Distillation WithOut\nTraining (DisWOT) significantly improves the performance of the model in the\ndistillation stage with at least 180$\\times$ training acceleration.\nAdditionally, we extend similarity metrics in DisWOT as new distillers and\nKD-based zero-proxies. Our experiments on CIFAR, ImageNet and NAS-Bench-201\ndemonstrate that our technique achieves state-of-the-art results on different\nsearch spaces. Our project and code are available at\nhttps://lilujunai.github.io/DisWOT-CVPR2023/.\n","authors":["Peijie Dong","Lujun Li","Zimian Wei"],"pdf_url":"https://arxiv.org/pdf/2303.15678v1.pdf","comment":"Accepted by CVPR2023"},{"id":"http://arxiv.org/abs/2211.14573v2","updated":"2023-03-28T01:53:53Z","published":"2022-11-26T14:00:18Z","title":"Deep Curvilinear Editing: Commutative and Nonlinear Image Manipulation\n  for Pretrained Deep Generative Model","summary":"  Semantic editing of images is the fundamental goal of computer vision.\nAlthough deep learning methods, such as generative adversarial networks (GANs),\nare capable of producing high-quality images, they often do not have an\ninherent way of editing generated images semantically. Recent studies have\ninvestigated a way of manipulating the latent variable to determine the images\nto be generated. However, methods that assume linear semantic arithmetic have\ncertain limitations in terms of the quality of image editing, whereas methods\nthat discover nonlinear semantic pathways provide non-commutative editing,\nwhich is inconsistent when applied in different orders. This study proposes a\nnovel method called deep curvilinear editing (DeCurvEd) to determine semantic\ncommuting vector fields on the latent space. We theoretically demonstrate that\nowing to commutativity, the editing of multiple attributes depends only on the\nquantities and not on the order. Furthermore, we experimentally demonstrate\nthat compared to previous methods, the nonlinear and commutative nature of\nDeCurvEd facilitates the disentanglement of image attributes and provides\nhigher-quality editing.\n","authors":["Takehiro Aoshima","Takashi Matsubara"],"pdf_url":"https://arxiv.org/pdf/2211.14573v2.pdf","comment":"15 pages"},{"id":"http://arxiv.org/abs/2303.15672v1","updated":"2023-03-28T01:30:40Z","published":"2023-03-28T01:30:40Z","title":"Numerical Methods for Convex Multistage Stochastic Optimization","summary":"  Optimization problems involving sequential decisions in a stochastic\nenvironment were studied in Stochastic Programming (SP), Stochastic Optimal\nControl (SOC) and Markov Decision Processes (MDP). In this paper we mainly\nconcentrate on SP and SOC modelling approaches. In these frameworks there are\nnatural situations when the considered problems are convex. Classical approach\nto sequential optimization is based on dynamic programming. It has the problem\nof the so-called ``Curse of Dimensionality\", in that its computational\ncomplexity increases exponentially with increase of dimension of state\nvariables. Recent progress in solving convex multistage stochastic problems is\nbased on cutting planes approximations of the cost-to-go (value) functions of\ndynamic programming equations. Cutting planes type algorithms in dynamical\nsettings is one of the main topics of this paper. We also discuss Stochastic\nApproximation type methods applied to multistage stochastic optimization\nproblems. From the computational complexity point of view, these two types of\nmethods seem to be complimentary to each other. Cutting plane type methods can\nhandle multistage problems with a large number of stages, but a relatively\nsmaller number of state (decision) variables. On the other hand, stochastic\napproximation type methods can only deal with a small number of stages, but a\nlarge number of decision variables.\n","authors":["Guanghui Lan","Alexander Shapiro"],"pdf_url":"https://arxiv.org/pdf/2303.15672v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.14568v2","updated":"2023-03-28T01:27:51Z","published":"2023-03-25T21:31:41Z","title":"Measuring Classification Decision Certainty and Doubt","summary":"  Quantitative characterizations and estimations of uncertainty are of\nfundamental importance in optimization and decision-making processes. Herein,\nwe propose intuitive scores, which we call certainty and doubt, that can be\nused in both a Bayesian and frequentist framework to assess and compare the\nquality and uncertainty of predictions in (multi-)classification decision\nmachine learning problems.\n","authors":["Alexander M. Berenbeim","Iain J. Cruickshank","Susmit Jha","Robert H. Thomson","Nathaniel D. Bastian"],"pdf_url":"https://arxiv.org/pdf/2303.14568v2.pdf","comment":"4 pages"},{"id":"http://arxiv.org/abs/2303.15669v1","updated":"2023-03-28T01:26:00Z","published":"2023-03-28T01:26:00Z","title":"Unsupervised Pre-Training For Data-Efficient Text-to-Speech On Low\n  Resource Languages","summary":"  Neural text-to-speech (TTS) models can synthesize natural human speech when\ntrained on large amounts of transcribed speech. However, collecting such\nlarge-scale transcribed data is expensive. This paper proposes an unsupervised\npre-training method for a sequence-to-sequence TTS model by leveraging large\nuntranscribed speech data. With our pre-training, we can remarkably reduce the\namount of paired transcribed data required to train the model for the target\ndownstream TTS task. The main idea is to pre-train the model to reconstruct\nde-warped mel-spectrograms from warped ones, which may allow the model to learn\nproper temporal assignment relation between input and output sequences. In\naddition, we propose a data augmentation method that further improves the data\nefficiency in fine-tuning. We empirically demonstrate the effectiveness of our\nproposed method in low-resource language scenarios, achieving outstanding\nperformance compared to competing methods. The code and audio samples are\navailable at: https://github.com/cnaigithub/SpeechDewarping\n","authors":["Seongyeon Park","Myungseo Song","Bohyung Kim","Tae-Hyun Oh"],"pdf_url":"https://arxiv.org/pdf/2303.15669v1.pdf","comment":"ICASSP 2023"},{"id":"http://arxiv.org/abs/2210.10999v2","updated":"2023-03-28T01:22:54Z","published":"2022-10-20T03:59:11Z","title":"Task Phasing: Automated Curriculum Learning from Demonstrations","summary":"  Applying reinforcement learning (RL) to sparse reward domains is notoriously\nchallenging due to insufficient guiding signals. Common RL techniques for\naddressing such domains include (1) learning from demonstrations and (2)\ncurriculum learning. While these two approaches have been studied in detail,\nthey have rarely been considered together. This paper aims to do so by\nintroducing a principled task phasing approach that uses demonstrations to\nautomatically generate a curriculum sequence. Using inverse RL from\n(suboptimal) demonstrations we define a simple initial task. Our task phasing\napproach then provides a framework to gradually increase the complexity of the\ntask all the way to the target task, while retuning the RL agent in each\nphasing iteration. Two approaches for phasing are considered: (1) gradually\nincreasing the proportion of time steps an RL agent is in control, and (2)\nphasing out a guiding informative reward function. We present conditions that\nguarantee the convergence of these approaches to an optimal policy.\nExperimental results on 3 sparse reward domains demonstrate that our task\nphasing approaches outperform state-of-the-art approaches with respect to\nasymptotic performance.\n","authors":["Vaibhav Bajaj","Guni Sharon","Peter Stone"],"pdf_url":"https://arxiv.org/pdf/2210.10999v2.pdf","comment":"7 pages main paper, 7 figures, 4 pages appendix. Submitted to AAAI\n  2023 Conference"},{"id":"http://arxiv.org/abs/2204.08504v4","updated":"2023-03-28T01:16:23Z","published":"2022-04-05T17:34:47Z","title":"CGC: Contrastive Graph Clustering for Community Detection and Tracking","summary":"  Given entities and their interactions in the web data, which may have\noccurred at different time, how can we find communities of entities and track\ntheir evolution? In this paper, we approach this important task from graph\nclustering perspective. Recently, state-of-the-art clustering performance in\nvarious domains has been achieved by deep clustering methods. Especially, deep\ngraph clustering (DGC) methods have successfully extended deep clustering to\ngraph-structured data by learning node representations and cluster assignments\nin a joint optimization framework. Despite some differences in modeling choices\n(e.g., encoder architectures), existing DGC methods are mainly based on\nautoencoders and use the same clustering objective with relatively minor\nadaptations. Also, while many real-world graphs are dynamic, previous DGC\nmethods considered only static graphs. In this work, we develop CGC, a novel\nend-to-end framework for graph clustering, which fundamentally differs from\nexisting methods. CGC learns node embeddings and cluster assignments in a\ncontrastive graph learning framework, where positive and negative samples are\ncarefully selected in a multi-level scheme such that they reflect hierarchical\ncommunity structures and network homophily. Also, we extend CGC for\ntime-evolving data, where temporal graph clustering is performed in an\nincremental learning fashion, with the ability to detect change points.\nExtensive evaluation on real-world graphs demonstrates that the proposed CGC\nconsistently outperforms existing methods.\n","authors":["Namyong Park","Ryan Rossi","Eunyee Koh","Iftikhar Ahamath Burhanuddin","Sungchul Kim","Fan Du","Nesreen Ahmed","Christos Faloutsos"],"pdf_url":"https://arxiv.org/pdf/2204.08504v4.pdf","comment":"TheWebConf 2022 Research Track"},{"id":"http://arxiv.org/abs/2203.02194v4","updated":"2023-03-28T01:12:24Z","published":"2022-03-04T09:04:55Z","title":"Rethinking Reconstruction Autoencoder-Based Out-of-Distribution\n  Detection","summary":"  In some scenarios, classifier requires detecting out-of-distribution samples\nfar from its training data. With desirable characteristics, reconstruction\nautoencoder-based methods deal with this problem by using input reconstruction\nerror as a metric of novelty vs. normality. We formulate the essence of such\napproach as a quadruplet domain translation with an intrinsic bias to only\nquery for a proxy of conditional data uncertainty. Accordingly, an improvement\ndirection is formalized as maximumly compressing the autoencoder's latent space\nwhile ensuring its reconstructive power for acting as a described domain\ntranslator. From it, strategies are introduced including semantic\nreconstruction, data certainty decomposition and normalized L2 distance to\nsubstantially improve original methods, which together establish\nstate-of-the-art performance on various benchmarks, e.g., the FPR@95%TPR of\nCIFAR-100 vs. TinyImagenet-crop on Wide-ResNet is 0.2%. Importantly, our method\nworks without any additional data, hard-to-implement structure, time-consuming\npipeline, and even harming the classification accuracy of known classes.\n","authors":["Yibo Zhou"],"pdf_url":"https://arxiv.org/pdf/2203.02194v4.pdf","comment":"Accepted('Poster' presentation) as main conference paper of CVPR2022"},{"id":"http://arxiv.org/abs/2303.15663v1","updated":"2023-03-28T01:09:15Z","published":"2023-03-28T01:09:15Z","title":"Predicting Thermoelectric Power Factor of Bismuth Telluride During Laser\n  Powder Bed Fusion Additive Manufacturing","summary":"  An additive manufacturing (AM) process, like laser powder bed fusion, allows\nfor the fabrication of objects by spreading and melting powder in layers until\na freeform part shape is created. In order to improve the properties of the\nmaterial involved in the AM process, it is important to predict the material\ncharacterization property as a function of the processing conditions. In\nthermoelectric materials, the power factor is a measure of how efficiently the\nmaterial can convert heat to electricity. While earlier works have predicted\nthe material characterization properties of different thermoelectric materials\nusing various techniques, implementation of machine learning models to predict\nthe power factor of bismuth telluride (Bi2Te3) during the AM process has not\nbeen explored. This is important as Bi2Te3 is a standard material for low\ntemperature applications. Thus, we used data about manufacturing processing\nparameters involved and in-situ sensor monitoring data collected during AM of\nBi2Te3, to train different machine learning models in order to predict its\nthermoelectric power factor. We implemented supervised machine learning\ntechniques using 80% training and 20% test data and further used the\npermutation feature importance method to identify important processing\nparameters and in-situ sensor features which were best at predicting power\nfactor of the material. Ensemble-based methods like random forest, AdaBoost\nclassifier, and bagging classifier performed the best in predicting power\nfactor with the highest accuracy of 90% achieved by the bagging classifier\nmodel. Additionally, we found the top 15 processing parameters and in-situ\nsensor features to characterize the material manufacturing property like power\nfactor. These features could further be optimized to maximize power factor of\nthe thermoelectric material and improve the quality of the products built using\nthis material.\n","authors":["Ankita Agarwal","Tanvi Banerjee","Joy Gockel","Saniya LeBlanc","Joe Walker","John Middendorf"],"pdf_url":"https://arxiv.org/pdf/2303.15663v1.pdf","comment":"8 pages, 2 figures, 2 tables, accepted at Data Science for Smart\n  Manufacturing and Healthcare workshop (DS2-MH) at SIAM International\n  Conference on Data Mining (SDM23) conference"},{"id":"http://arxiv.org/abs/2303.15659v1","updated":"2023-03-28T01:02:17Z","published":"2023-03-28T01:02:17Z","title":"Boundary-to-Solution Mapping for Groundwater Flows in a Toth Basin","summary":"  In this paper, the authors propose a new approach to solving the groundwater\nflow equation in the Toth basin of arbitrary top and bottom topographies using\ndeep learning. Instead of using traditional numerical solvers, they use a\nDeepONet to produce the boundary-to-solution mapping. This mapping takes the\ngeometry of the physical domain along with the boundary conditions as inputs to\noutput the steady state solution of the groundwater flow equation. To implement\nthe DeepONet, the authors approximate the top and bottom boundaries using\ntruncated Fourier series or piecewise linear representations. They present two\ndifferent implementations of the DeepONet: one where the Toth basin is embedded\nin a rectangular computational domain, and another where the Toth basin with\narbitrary top and bottom boundaries is mapped into a rectangular computational\ndomain via a nonlinear transformation. They implement the DeepONet with respect\nto the Dirichlet and Robin boundary condition at the top and the Neumann\nboundary condition at the impervious bottom boundary, respectively. Using this\ndeep-learning enabled tool, the authors investigate the impact of surface\ntopography on the flow pattern by both the top surface and the bottom\nimpervious boundary with arbitrary geometries. They discover that the average\nslope of the top surface promotes long-distance transport, while the local\ncurvature controls localized circulations. Additionally, they find that the\nslope of the bottom impervious boundary can seriously impact the long-distance\ntransport of groundwater flows. Overall, this paper presents a new and\ninnovative approach to solving the groundwater flow equation using deep\nlearning, which allows for the investigation of the impact of surface\ntopography on groundwater flow patterns.\n","authors":["Jingwei Sun","Jun Li","Yonghong Hao","Cuiting Qi","Chunmei Ma","Huazhi Sun","Negash Begashaw","Gurcan Comet","Yi Sun","Qi Wang"],"pdf_url":"https://arxiv.org/pdf/2303.15659v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.15656v1","updated":"2023-03-28T00:44:06Z","published":"2023-03-28T00:44:06Z","title":"Predicting Adverse Neonatal Outcomes for Preterm Neonates with\n  Multi-Task Learning","summary":"  Diagnosis of adverse neonatal outcomes is crucial for preterm survival since\nit enables doctors to provide timely treatment. Machine learning (ML)\nalgorithms have been demonstrated to be effective in predicting adverse\nneonatal outcomes. However, most previous ML-based methods have only focused on\npredicting a single outcome, ignoring the potential correlations between\ndifferent outcomes, and potentially leading to suboptimal results and\noverfitting issues. In this work, we first analyze the correlations between\nthree adverse neonatal outcomes and then formulate the diagnosis of multiple\nneonatal outcomes as a multi-task learning (MTL) problem. We then propose an\nMTL framework to jointly predict multiple adverse neonatal outcomes. In\nparticular, the MTL framework contains shared hidden layers and multiple\ntask-specific branches. Extensive experiments have been conducted using\nElectronic Health Records (EHRs) from 121 preterm neonates. Empirical results\ndemonstrate the effectiveness of the MTL framework. Furthermore, the feature\nimportance is analyzed for each neonatal outcome, providing insights into model\ninterpretability.\n","authors":["Jingyang Lin","Junyu Chen","Hanjia Lyu","Igor Khodak","Divya Chhabra","Colby L Day Richardson","Irina Prelipcean","Andrew M Dylag","Jiebo Luo"],"pdf_url":"https://arxiv.org/pdf/2303.15656v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.15652v1","updated":"2023-03-28T00:23:23Z","published":"2023-03-28T00:23:23Z","title":"Structured Dynamic Pricing: Optimal Regret in a Global Shrinkage Model","summary":"  We consider dynamic pricing strategies in a streamed longitudinal data set-up\nwhere the objective is to maximize, over time, the cumulative profit across a\nlarge number of customer segments. We consider a dynamic probit model with the\nconsumers' preferences as well as price sensitivity varying over time. Building\non the well-known finding that consumers sharing similar characteristics act in\nsimilar ways, we consider a global shrinkage structure, which assumes that the\nconsumers' preferences across the different segments can be well approximated\nby a spatial autoregressive (SAR) model. In such a streamed longitudinal\nset-up, we measure the performance of a dynamic pricing policy via regret,\nwhich is the expected revenue loss compared to a clairvoyant that knows the\nsequence of model parameters in advance. We propose a pricing policy based on\npenalized stochastic gradient descent (PSGD) and explicitly characterize its\nregret as functions of time, the temporal variability in the model parameters\nas well as the strength of the auto-correlation network structure spanning the\nvaried customer segments. Our regret analysis results not only demonstrate\nasymptotic optimality of the proposed policy but also show that for policy\nplanning it is essential to incorporate available structural information as\npolicies based on unshrunken models are highly sub-optimal in the\naforementioned set-up.\n","authors":["Rashmi Ranjan Bhuyan","Adel Javanmard","Sungchul Kim","Gourab Mukherjee","Ryan A. Rossi","Tong Yu","Handong Zhao"],"pdf_url":"https://arxiv.org/pdf/2303.15652v1.pdf","comment":"34 pages, 5 figures"},{"id":"http://arxiv.org/abs/2210.00674v2","updated":"2023-03-28T00:19:48Z","published":"2022-10-03T01:31:34Z","title":"Multi-view information fusion using multi-view variational autoencoders\n  to predict proximal femoral strength","summary":"  The aim of this paper is to design a deep learning-based model to predict\nproximal femoral strength using multi-view information fusion. Method: We\ndeveloped new models using multi-view variational autoencoder (MVAE) for\nfeature representation learning and a product of expert (PoE) model for\nmulti-view information fusion. We applied the proposed models to an in-house\nLouisiana Osteoporosis Study (LOS) cohort with 931 male subjects, including 345\nAfrican Americans and 586 Caucasians. With an analytical solution of the\nproduct of Gaussian distribution, we adopted variational inference to train the\ndesigned MVAE-PoE model to perform common latent feature extraction. We\nperformed genome-wide association studies (GWAS) to select 256 genetic variants\nwith the lowest p-values for each proximal femoral strength and integrated\nwhole genome sequence (WGS) features and DXA-derived imaging features to\npredict proximal femoral strength. Results: The best prediction model for fall\nfracture load was acquired by integrating WGS features and DXA-derived imaging\nfeatures. The designed models achieved the mean absolute percentage error of\n18.04%, 6.84% and 7.95% for predicting proximal femoral fracture loads using\nlinear models of fall loading, nonlinear models of fall loading, and nonlinear\nmodels of stance loading, respectively. Compared to existing multi-view\ninformation fusion methods, the proposed MVAE-PoE achieved the best\nperformance. Conclusion: The proposed models are capable of predicting proximal\nfemoral strength using WGS features and DXA-derived imaging features. Though\nthis tool is not a substitute for FEA using QCT images, it would make improved\nassessment of hip fracture risk more widely available while avoiding the\nincreased radiation dosage and clinical costs from QCT.\n","authors":["Chen Zhao","Joyce H Keyak","Xuewei Cao","Qiuying Sha","Li Wu","Zhe Luo","Lanjuan Zhao","Qing Tian","Chuan Qiu","Ray Su","Hui Shen","Hong-Wen Deng","Weihua Zhou"],"pdf_url":"https://arxiv.org/pdf/2210.00674v2.pdf","comment":"16 pages, 3 figures"},{"id":"http://arxiv.org/abs/2003.13898v3","updated":"2023-03-28T00:15:58Z","published":"2020-03-31T01:23:21Z","title":"Edge Guided GANs with Contrastive Learning for Semantic Image Synthesis","summary":"  We propose a novel ECGAN for the challenging semantic image synthesis task.\nAlthough considerable improvement has been achieved, the quality of synthesized\nimages is far from satisfactory due to three largely unresolved challenges. 1)\nThe semantic labels do not provide detailed structural information, making it\ndifficult to synthesize local details and structures. 2) The widely adopted CNN\noperations such as convolution, down-sampling, and normalization usually cause\nspatial resolution loss and thus cannot fully preserve the original semantic\ninformation, leading to semantically inconsistent results. 3) Existing semantic\nimage synthesis methods focus on modeling local semantic information from a\nsingle input semantic layout. However, they ignore global semantic information\nof multiple input semantic layouts, i.e., semantic cross-relations between\npixels across different input layouts. To tackle 1), we propose to use edge as\nan intermediate representation which is further adopted to guide image\ngeneration via a proposed attention guided edge transfer module. Edge\ninformation is produced by a convolutional generator and introduces detailed\nstructure information. To tackle 2), we design an effective module to\nselectively highlight class-dependent feature maps according to the original\nsemantic layout to preserve the semantic information. To tackle 3), inspired by\ncurrent methods in contrastive learning, we propose a novel contrastive\nlearning method, which aims to enforce pixel embeddings belonging to the same\nsemantic class to generate more similar image content than those from different\nclasses. Doing so can capture more semantic relations by explicitly exploring\nthe structures of labeled pixels from multiple input semantic layouts.\nExperiments on three challenging datasets show that our ECGAN achieves\nsignificantly better results than state-of-the-art methods.\n","authors":["Hao Tang","Xiaojuan Qi","Guolei Sun","Dan Xu","Nicu Sebe","Radu Timofte","Luc Van Gool"],"pdf_url":"https://arxiv.org/pdf/2003.13898v3.pdf","comment":null}],"Multimedia":[{"id":"http://arxiv.org/abs/2303.16199v1","updated":"2023-03-28T17:59:12Z","published":"2023-03-28T17:59:12Z","title":"LLaMA-Adapter: Efficient Fine-tuning of Language Models with Zero-init\n  Attention","summary":"  We present LLaMA-Adapter, a lightweight adaption method to efficiently\nfine-tune LLaMA into an instruction-following model. Using 52K self-instruct\ndemonstrations, LLaMA-Adapter only introduces 1.2M learnable parameters upon\nthe frozen LLaMA 7B model, and costs less than one hour for fine-tuning on 8\nA100 GPUs. Specifically, we adopt a set of learnable adaption prompts, and\nprepend them to the input text tokens at higher transformer layers. Then, a\nzero-init attention mechanism with zero gating is proposed, which adaptively\ninjects the new instructional cues into LLaMA, while effectively preserves its\npre-trained knowledge. With efficient training, LLaMA-Adapter generates\nhigh-quality responses, comparable to Alpaca with fully fine-tuned 7B\nparameters. Furthermore, our approach can be simply extended to multi-modal\ninput, e.g., images, for image-conditioned LLaMA, which achieves superior\nreasoning capacity on ScienceQA. We release our code at\nhttps://github.com/ZrrSkywalker/LLaMA-Adapter.\n","authors":["Renrui Zhang","Jiaming Han","Aojun Zhou","Xiangfei Hu","Shilin Yan","Pan Lu","Hongsheng Li","Peng Gao","Yu Qiao"],"pdf_url":"https://arxiv.org/pdf/2303.16199v1.pdf","comment":"Work in Progress. Code is available at\n  https://github.com/ZrrSkywalker/LLaMA-Adapter"},{"id":"http://arxiv.org/abs/2303.16163v1","updated":"2023-03-28T17:27:27Z","published":"2023-03-28T17:27:27Z","title":"Comparison of HDR quality metrics in Per-Clip Lagrangian multiplier\n  optimisation with AV1","summary":"  The complexity of modern codecs along with the increased need of delivering\nhigh-quality videos at low bitrates has reinforced the idea of a per-clip\ntailoring of parameters for optimised rate-distortion performance. While the\nobjective quality metrics used for Standard Dynamic Range (SDR) videos have\nbeen well studied, the transitioning of consumer displays to support High\nDynamic Range (HDR) videos, poses a new challenge to rate-distortion\noptimisation. In this paper, we review the popular HDR metrics DeltaE100\n(DE100), PSNRL100, wPSNR, and HDR-VQM. We measure the impact of employing these\nmetrics in per-clip direct search optimisation of the rate-distortion Lagrange\nmultiplier in AV1. We report, on 35 HDR videos, average Bjontegaard Delta Rate\n(BD-Rate) gains of 4.675%, 2.226%, and 7.253% in terms of DE100, PSNRL100, and\nHDR-VQM. We also show that the inclusion of chroma in the quality metrics has a\nsignificant impact on optimisation, which can only be partially addressed by\nthe use of chroma offsets.\n","authors":[" Vibhoothi","François Pitié","Angeliki Katsenou","Yeping Su","Balu Adsumilli","Anil Kokaram"],"pdf_url":"https://arxiv.org/pdf/2303.16163v1.pdf","comment":"Accepted version for ICME 2023 Special Session, \"Optimised Media\n  Delivery\""},{"id":"http://arxiv.org/abs/2301.03829v2","updated":"2023-03-28T15:54:39Z","published":"2023-01-10T07:51:36Z","title":"From Plate to Prevention: A Dietary Nutrient-aided Platform for Health\n  Promotion in Singapore","summary":"  Singapore has been striving to improve the provision of healthcare services\nto her people. In this course, the government has taken note of the deficiency\nin regulating and supervising people's nutrient intake, which is identified as\na contributing factor to the development of chronic diseases. Consequently,\nthis issue has garnered significant attention. In this paper, we share our\nexperience in addressing this issue and attaining medical-grade nutrient intake\ninformation to benefit Singaporeans in different aspects. To this end, we\ndevelop the FoodSG platform to incubate diverse healthcare-oriented\napplications as a service in Singapore, taking into account their shared\nrequirements. We further identify the profound meaning of localized food\ndatasets and systematically clean and curate a localized Singaporean food\ndataset FoodSG-233. To overcome the hurdle in recognition performance brought\nby Singaporean multifarious food dishes, we propose to integrate supervised\ncontrastive learning into our food recognition model FoodSG-SCL for the\nintrinsic capability to mine hard positive/negative samples and therefore boost\nthe accuracy. Through a comprehensive evaluation, we present performance\nresults of the proposed model and insights on food-related healthcare\napplications. The FoodSG-233 dataset has been released in\nhttps://foodlg.comp.nus.edu.sg/.\n","authors":["Kaiping Zheng","Thao Nguyen","Jesslyn Hwei Sing Chong","Charlene Enhui Goh","Melanie Herschel","Hee Hoon Lee","Changshuo Liu","Beng Chin Ooi","Wei Wang","James Yip"],"pdf_url":"https://arxiv.org/pdf/2301.03829v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2211.14053v2","updated":"2023-03-28T08:48:52Z","published":"2022-11-25T12:17:30Z","title":"Re^2TAL: Rewiring Pretrained Video Backbones for Reversible Temporal\n  Action Localization","summary":"  Temporal action localization (TAL) requires long-form reasoning to predict\nactions of various durations and complex content. Given limited GPU memory,\ntraining TAL end to end (i.e., from videos to predictions) on long videos is a\nsignificant challenge. Most methods can only train on pre-extracted features\nwithout optimizing them for the localization problem, consequently limiting\nlocalization performance. In this work, to extend the potential in TAL\nnetworks, we propose a novel end-to-end method Re2TAL, which rewires pretrained\nvideo backbones for reversible TAL. Re2TAL builds a backbone with reversible\nmodules, where the input can be recovered from the output such that the bulky\nintermediate activations can be cleared from memory during training. Instead of\ndesigning one single type of reversible module, we propose a network rewiring\nmechanism, to transform any module with a residual connection to a reversible\nmodule without changing any parameters. This provides two benefits: (1) a large\nvariety of reversible networks are easily obtained from existing and even\nfuture model designs, and (2) the reversible models require much less training\neffort as they reuse the pre-trained parameters of their original\nnon-reversible versions. Re2TAL, only using the RGB modality, reaches 37.01%\naverage mAP on ActivityNet-v1.3, a new state-of-the-art record, and mAP 64.9%\nat tIoU=0.5 on THUMOS-14, outperforming all other RGB-only methods.\n","authors":["Chen Zhao","Shuming Liu","Karttikeya Mangalam","Bernard Ghanem"],"pdf_url":"https://arxiv.org/pdf/2211.14053v2.pdf","comment":null}]}}